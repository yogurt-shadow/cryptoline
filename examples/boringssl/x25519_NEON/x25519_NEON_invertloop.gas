._invertloop () at /home/armbian/Sources/boringssl/crypto/curve25519/asm/x25519-asm-arm.S:1583
	add	r2, r3, #144	; 0x90
	mov	r4, #0
	mov	r5, #2
	cmp	r1, #1
	moveq	r5, #1
	addeq	r2, r3, #336	; 0x150
	addeq	r4, r3, #48	; 0x30
	cmp	r1, #2
	moveq	r5, #1
	addeq	r2, r3, #48	; 0x30
	cmp	r1, #3
	moveq	r5, #5
	addeq	r4, r3, #336	; 0x150
	cmp	r1, #4
	moveq	r5, #10
	cmp	r1, #5
	moveq	r5, #20
	cmp	r1, #6
	moveq	r5, #10
	addeq	r2, r3, #336	; 0x150
	addeq	r4, r3, #336	; 0x150
	cmp	r1, #7
	moveq	r5, #50	; 0x32
	cmp	r1, #8
	moveq	r5, #100	; 0x64
	cmp	r1, #9
	moveq	r5, #50	; 0x32
	addeq	r2, r3, #336	; 0x150
	cmp	r1, #10
	moveq	r5, #5
	addeq	r2, r3, #48	; 0x30
	cmp	r1, #11
	moveq	r5, #0
	addeq	r2, r3, #96	; 0x60
	add	r6, r3, #144	; 0x90
	add	r7, r3, #288	; 0x120
	vld1.8	{d0-d1}, [r6 :128]!                      #! EA = L0xbeffe930, Value = 0xfe2e1700
	vld1.8	{d2-d3}, [r6 :128]!                      #! EA = L0xbeffe940, Value = 0xfec2a896
	vld1.8	{d4}, [r6 :64]                           #! EA = L0xbeffe950, Value = 0x00712e1f
	vst1.8	{d0-d1}, [r7 :128]!                      #! EA = L0xbeffe9c0, Value = 0xfe7e6b77
	vst1.8	{d2-d3}, [r7 :128]!                      #! EA = L0xbeffe9d0, Value = 0x025bb5eb
	vst1.8	{d4}, [r7 :64]                           #! EA = L0xbeffe9e0, Value = 0xfdd44e67
	cmp	r5, #0
._squaringloop () at /home/armbian/Sources/boringssl/crypto/curve25519/asm/x25519-asm-arm.S:1628
	#beq	0x11f2c <._skipsquaringloop>
	add	r6, r3, #288	; 0x120
	add	r7, r3, #288	; 0x120
	add	r8, r3, #288	; 0x120
	vmov.i32	q0, #19	; 0x00000013
	vmov.i32	q1, #0	; 0x00000000
	vmov.i32	q2, #1	; 0x00000001
	vzip.32	q1, q2
	vld1.8	{d4-d5}, [r7 :128]!                      #! EA = L0xbeffe9c0, Value = 0xfe2e1700
	vld1.8	{d6-d7}, [r7 :128]!                      #! EA = L0xbeffe9d0, Value = 0xfec2a896
	vld1.8	{d9}, [r7 :64]                           #! EA = L0xbeffe9e0, Value = 0x00712e1f
	vld1.8	{d10-d11}, [r6 :128]!                    #! EA = L0xbeffe9c0, Value = 0xfe2e1700
	add	r7, sp, #416	; 0x1a0
	vld1.8	{d12-d13}, [r6 :128]!                    #! EA = L0xbeffe9d0, Value = 0xfec2a896
	vmul.i32	q7, q2, q0
	vld1.8	{d8}, [r6 :64]                           #! EA = L0xbeffe9e0, Value = 0x00712e1f
	vext.8	d17, d11, d10, #4
	vmul.i32	q9, q3, q0
	vext.8	d16, d10, d8, #4
	vshl.u32	q10, q5, q1
	vext.8	d22, d14, d4, #4
	vext.8	d24, d18, d6, #4
	vshl.u32	q13, q6, q1
	vshl.u32	d28, d8, d2
	vrev64.32	d22, d22
	vmul.i32	d1, d9, d1
	vrev64.32	d24, d24
	vext.8	d29, d8, d13, #4
	vext.8	d0, d1, d9, #4
	vrev64.32	d0, d0
	vext.8	d2, d9, d1, #4
	vext.8	d23, d15, d5, #4
	vmull.s32	q4, d20, d4
	vrev64.32	d23, d23
	vmlal.s32	q4, d21, d1
	vrev64.32	d2, d2
	vmlal.s32	q4, d26, d19
	vext.8	d3, d5, d15, #4
	vmlal.s32	q4, d27, d18
	vrev64.32	d3, d3
	vmlal.s32	q4, d28, d15
	vext.8	d14, d12, d11, #4
	vmull.s32	q5, d16, d23
	vext.8	d15, d13, d12, #4
	vmlal.s32	q5, d17, d4
	vst1.8	{d8}, [r7 :64]!                          #! EA = L0xbeffea20, Value = 0x00000000
	vmlal.s32	q5, d14, d1
	vext.8	d12, d9, d8, #0
	vmlal.s32	q5, d15, d19
	vmov.i64	d13, #0x0000000000000000
	vmlal.s32	q5, d29, d18
	vext.8	d25, d19, d7, #4
	vmlal.s32	q6, d20, d5
	vrev64.32	d25, d25
	vmlal.s32	q6, d21, d4
	vst1.8	{d11}, [r7 :64]!                         #! EA = L0xbeffea28, Value = 0x00000000
	vmlal.s32	q6, d26, d1
	vext.8	d9, d10, d10, #0
	vmlal.s32	q6, d27, d19
	vmov.i64	d8, #0x0000000000000000
	vmlal.s32	q6, d28, d18
	vmlal.s32	q4, d16, d24
	vmlal.s32	q4, d17, d5
	vmlal.s32	q4, d14, d4
	vst1.8	{d12}, [r7 :64]!                         #! EA = L0xbeffea30, Value = 0x00000000
	vmlal.s32	q4, d15, d1
	vext.8	d10, d13, d12, #0
	vmlal.s32	q4, d29, d19
	vmov.i64	d11, #0x0000000000000000
	vmlal.s32	q5, d20, d6
	vmlal.s32	q5, d21, d5
	vmlal.s32	q5, d26, d4
	vext.8	d13, d8, d8, #0
	vmlal.s32	q5, d27, d1
	vmov.i64	d12, #0x0000000000000000
	vmlal.s32	q5, d28, d19
	vst1.8	{d9}, [r7 :64]!                          #! EA = L0xbeffea38, Value = 0x00000000
	vmlal.s32	q6, d16, d25
	vmlal.s32	q6, d17, d6
	vst1.8	{d10}, [r7 :64]                          #! EA = L0xbeffea40, Value = 0x00000000
	vmlal.s32	q6, d14, d5
	vext.8	d8, d11, d10, #0
	vmlal.s32	q6, d15, d4
	vmov.i64	d9, #0x0000000000000000
	vmlal.s32	q6, d29, d1
	vmlal.s32	q4, d20, d7
	vmlal.s32	q4, d21, d6
	vmlal.s32	q4, d26, d5
	vext.8	d11, d12, d12, #0
	vmlal.s32	q4, d27, d4
	vmov.i64	d10, #0x0000000000000000
	vmlal.s32	q4, d28, d1
	vmlal.s32	q5, d16, d0
	sub	r6, r7, #32
	vmlal.s32	q5, d17, d7
	vmlal.s32	q5, d14, d6
	vext.8	d30, d9, d8, #0
	vmlal.s32	q5, d15, d5
	vld1.8	{d31}, [r6 :64]!                         #! EA = L0xbeffea20, Value = 0xf5fad148
	vmlal.s32	q5, d29, d4
	vmlal.s32	q15, d20, d0
	vext.8	d0, d6, d18, #4
	vmlal.s32	q15, d21, d25
	vrev64.32	d0, d0
	vmlal.s32	q15, d26, d24
	vext.8	d1, d7, d19, #4
	vext.8	d7, d10, d10, #0
	vmlal.s32	q15, d27, d23
	vrev64.32	d1, d1
	vld1.8	{d6}, [r6 :64]                           #! EA = L0xbeffea28, Value = 0xa83e1641
	vmlal.s32	q15, d28, d22
	vmlal.s32	q3, d16, d4
	add	r6, r6, #24
	vmlal.s32	q3, d17, d2
	vext.8	d4, d31, d30, #0
	vorr	d17, d11, d11
	vmlal.s32	q3, d14, d1
	vext.8	d11, d13, d13, #0
	vext.8	d13, d30, d30, #0
	vmlal.s32	q3, d15, d0
	vext.8	d1, d8, d8, #0
	vmlal.s32	q3, d29, d3
	vld1.8	{d5}, [r6 :64]                           #! EA = L0xbeffea40, Value = 0x65f80f1e
	sub	r6, r6, #16
	vext.8	d10, d6, d6, #0
	vmov.i8	q1, #255	; 0xff
	vshl.s64	q4, q1, #25
	add	r7, sp, #512	; 0x200
	vld1.8	{d14-d15}, [r7 :128]                     #! EA = L0xbeffea80, Value = 0x02000000
	vadd.i64	q9, q2, q7
	vshl.s64	q1, q1, #26
	vshr.s64	q10, q9, #26
	vld1.8	{d0}, [r6 :64]!                          #! EA = L0xbeffea30, Value = 0xb7ac7208
	vadd.i64	q5, q5, q10
	vand	q9, q9, q1
	vld1.8	{d16}, [r6 :64]!                         #! EA = L0xbeffea38, Value = 0x3636eea2
	add	r6, sp, #528	; 0x210
	vld1.8	{d20-d21}, [r6 :128]                     #! EA = L0xbeffea90, Value = 0x01000000
	vadd.i64	q11, q5, q10
	vsub.i64	q2, q2, q9
	vshr.s64	q9, q11, #25
	vext.8	d12, d5, d4, #0
	vand	q11, q11, q4
	vadd.i64	q0, q0, q9
	vorr	d19, d7, d7
	vadd.i64	q3, q0, q7
	vsub.i64	q5, q5, q11
	vshr.s64	q11, q3, #26
	vext.8	d18, d11, d10, #0
	vand	q3, q3, q1
	vadd.i64	q8, q8, q11
	vadd.i64	q11, q8, q10
	vsub.i64	q0, q0, q3
	vshr.s64	q3, q11, #25
	vand	q11, q11, q4
	vadd.i64	q3, q6, q3
	vadd.i64	q6, q3, q7
	vsub.i64	q8, q8, q11
	vshr.s64	q11, q6, #26
	vand	q6, q6, q1
	vadd.i64	q9, q9, q11
	vadd.i64	d25, d19, d21
	vsub.i64	q3, q3, q6
	vshr.s64	d23, d25, #25
	vand	q4, q12, q4
	vadd.i64	d21, d23, d23
	vshl.s64	d25, d23, #4
	vadd.i64	d21, d21, d23
	vadd.i64	d25, d25, d21
	vadd.i64	d4, d4, d25
	vzip.32	q0, q8
	vadd.i64	d12, d4, d14
	add	r6, r8, #8
	vst1.8	{d0}, [r6 :64]                           #! EA = L0xbeffe9c8, Value = 0x00613a50
	vsub.i64	d19, d19, d9
	add	r6, r6, #16
	vst1.8	{d16}, [r6 :64]                          #! EA = L0xbeffe9d8, Value = 0x0149f5aa
	vshr.s64	d22, d12, #26
	vand	q0, q6, q1
	vadd.i64	d10, d10, d22
	vzip.32	q3, q9
	vsub.i64	d4, d4, d0
	sub	r6, r6, #8
	vst1.8	{d6}, [r6 :64]                           #! EA = L0xbeffe9d0, Value = 0xfec2a896
	add	r6, r6, #16
	vst1.8	{d18}, [r6 :64]                          #! EA = L0xbeffe9e0, Value = 0x00712e1f
	vzip.32	q2, q5
	sub	r6, r6, #32
	vst1.8	{d4}, [r6 :64]                           #! EA = L0xbeffe9c0, Value = 0xfe2e1700
	subs	r5, r5, #1
	bhi	0x11c34 <._squaringloop>
	add	r6, r3, #288	; 0x120
	add	r7, r3, #288	; 0x120
	add	r8, r3, #288	; 0x120
	vmov.i32	q0, #19	; 0x00000013
	vmov.i32	q1, #0	; 0x00000000
	vmov.i32	q2, #1	; 0x00000001
	vzip.32	q1, q2
	vld1.8	{d4-d5}, [r7 :128]!                      #! EA = L0xbeffe9c0, Value = 0xfebf0ffb
	vld1.8	{d6-d7}, [r7 :128]!                      #! EA = L0xbeffe9d0, Value = 0x01177ab8
	vld1.8	{d9}, [r7 :64]                           #! EA = L0xbeffe9e0, Value = 0xfed1a11a
	vld1.8	{d10-d11}, [r6 :128]!                    #! EA = L0xbeffe9c0, Value = 0xfebf0ffb
	add	r7, sp, #416	; 0x1a0
	vld1.8	{d12-d13}, [r6 :128]!                    #! EA = L0xbeffe9d0, Value = 0x01177ab8
	vmul.i32	q7, q2, q0
	vld1.8	{d8}, [r6 :64]                           #! EA = L0xbeffe9e0, Value = 0xfed1a11a
	vext.8	d17, d11, d10, #4
	vmul.i32	q9, q3, q0
	vext.8	d16, d10, d8, #4
	vshl.u32	q10, q5, q1
	vext.8	d22, d14, d4, #4
	vext.8	d24, d18, d6, #4
	vshl.u32	q13, q6, q1
	vshl.u32	d28, d8, d2
	vrev64.32	d22, d22
	vmul.i32	d1, d9, d1
	vrev64.32	d24, d24
	vext.8	d29, d8, d13, #4
	vext.8	d0, d1, d9, #4
	vrev64.32	d0, d0
	vext.8	d2, d9, d1, #4
	vext.8	d23, d15, d5, #4
	vmull.s32	q4, d20, d4
	vrev64.32	d23, d23
	vmlal.s32	q4, d21, d1
	vrev64.32	d2, d2
	vmlal.s32	q4, d26, d19
	vext.8	d3, d5, d15, #4
	vmlal.s32	q4, d27, d18
	vrev64.32	d3, d3
	vmlal.s32	q4, d28, d15
	vext.8	d14, d12, d11, #4
	vmull.s32	q5, d16, d23
	vext.8	d15, d13, d12, #4
	vmlal.s32	q5, d17, d4
	vst1.8	{d8}, [r7 :64]!                          #! EA = L0xbeffea20, Value = 0xf5fad148
	vmlal.s32	q5, d14, d1
	vext.8	d12, d9, d8, #0
	vmlal.s32	q5, d15, d19
	vmov.i64	d13, #0x0000000000000000
	vmlal.s32	q5, d29, d18
	vext.8	d25, d19, d7, #4
	vmlal.s32	q6, d20, d5
	vrev64.32	d25, d25
	vmlal.s32	q6, d21, d4
	vst1.8	{d11}, [r7 :64]!                         #! EA = L0xbeffea28, Value = 0xa83e1641
	vmlal.s32	q6, d26, d1
	vext.8	d9, d10, d10, #0
	vmlal.s32	q6, d27, d19
	vmov.i64	d8, #0x0000000000000000
	vmlal.s32	q6, d28, d18
	vmlal.s32	q4, d16, d24
	vmlal.s32	q4, d17, d5
	vmlal.s32	q4, d14, d4
	vst1.8	{d12}, [r7 :64]!                         #! EA = L0xbeffea30, Value = 0xb7ac7208
	vmlal.s32	q4, d15, d1
	vext.8	d10, d13, d12, #0
	vmlal.s32	q4, d29, d19
	vmov.i64	d11, #0x0000000000000000
	vmlal.s32	q5, d20, d6
	vmlal.s32	q5, d21, d5
	vmlal.s32	q5, d26, d4
	vext.8	d13, d8, d8, #0
	vmlal.s32	q5, d27, d1
	vmov.i64	d12, #0x0000000000000000
	vmlal.s32	q5, d28, d19
	vst1.8	{d9}, [r7 :64]!                          #! EA = L0xbeffea38, Value = 0x3636eea2
	vmlal.s32	q6, d16, d25
	vmlal.s32	q6, d17, d6
	vst1.8	{d10}, [r7 :64]                          #! EA = L0xbeffea40, Value = 0x65f80f1e
	vmlal.s32	q6, d14, d5
	vext.8	d8, d11, d10, #0
	vmlal.s32	q6, d15, d4
	vmov.i64	d9, #0x0000000000000000
	vmlal.s32	q6, d29, d1
	vmlal.s32	q4, d20, d7
	vmlal.s32	q4, d21, d6
	vmlal.s32	q4, d26, d5
	vext.8	d11, d12, d12, #0
	vmlal.s32	q4, d27, d4
	vmov.i64	d10, #0x0000000000000000
	vmlal.s32	q4, d28, d1
	vmlal.s32	q5, d16, d0
	sub	r6, r7, #32
	vmlal.s32	q5, d17, d7
	vmlal.s32	q5, d14, d6
	vext.8	d30, d9, d8, #0
	vmlal.s32	q5, d15, d5
	vld1.8	{d31}, [r6 :64]!                         #! EA = L0xbeffea20, Value = 0xd194e651
	vmlal.s32	q5, d29, d4
	vmlal.s32	q15, d20, d0
	vext.8	d0, d6, d18, #4
	vmlal.s32	q15, d21, d25
	vrev64.32	d0, d0
	vmlal.s32	q15, d26, d24
	vext.8	d1, d7, d19, #4
	vext.8	d7, d10, d10, #0
	vmlal.s32	q15, d27, d23
	vrev64.32	d1, d1
	vld1.8	{d6}, [r6 :64]                           #! EA = L0xbeffea28, Value = 0x1c0f96db
	vmlal.s32	q15, d28, d22
	vmlal.s32	q3, d16, d4
	add	r6, r6, #24
	vmlal.s32	q3, d17, d2
	vext.8	d4, d31, d30, #0
	vorr	d17, d11, d11
	vmlal.s32	q3, d14, d1
	vext.8	d11, d13, d13, #0
	vext.8	d13, d30, d30, #0
	vmlal.s32	q3, d15, d0
	vext.8	d1, d8, d8, #0
	vmlal.s32	q3, d29, d3
	vld1.8	{d5}, [r6 :64]                           #! EA = L0xbeffea40, Value = 0xf6580d90
	sub	r6, r6, #16
	vext.8	d10, d6, d6, #0
	vmov.i8	q1, #255	; 0xff
	vshl.s64	q4, q1, #25
	add	r7, sp, #512	; 0x200
	vld1.8	{d14-d15}, [r7 :128]                     #! EA = L0xbeffea80, Value = 0x02000000
	vadd.i64	q9, q2, q7
	vshl.s64	q1, q1, #26
	vshr.s64	q10, q9, #26
	vld1.8	{d0}, [r6 :64]!                          #! EA = L0xbeffea30, Value = 0xaa1ed329
	vadd.i64	q5, q5, q10
	vand	q9, q9, q1
	vld1.8	{d16}, [r6 :64]!                         #! EA = L0xbeffea38, Value = 0xf77044d8
	add	r6, sp, #528	; 0x210
	vld1.8	{d20-d21}, [r6 :128]                     #! EA = L0xbeffea90, Value = 0x01000000
	vadd.i64	q11, q5, q10
	vsub.i64	q2, q2, q9
	vshr.s64	q9, q11, #25
	vext.8	d12, d5, d4, #0
	vand	q11, q11, q4
	vadd.i64	q0, q0, q9
	vorr	d19, d7, d7
	vadd.i64	q3, q0, q7
	vsub.i64	q5, q5, q11
	vshr.s64	q11, q3, #26
	vext.8	d18, d11, d10, #0
	vand	q3, q3, q1
	vadd.i64	q8, q8, q11
	vadd.i64	q11, q8, q10
	vsub.i64	q0, q0, q3
	vshr.s64	q3, q11, #25
	vand	q11, q11, q4
	vadd.i64	q3, q6, q3
	vadd.i64	q6, q3, q7
	vsub.i64	q8, q8, q11
	vshr.s64	q11, q6, #26
	vand	q6, q6, q1
	vadd.i64	q9, q9, q11
	vadd.i64	d25, d19, d21
	vsub.i64	q3, q3, q6
	vshr.s64	d23, d25, #25
	vand	q4, q12, q4
	vadd.i64	d21, d23, d23
	vshl.s64	d25, d23, #4
	vadd.i64	d21, d21, d23
	vadd.i64	d25, d25, d21
	vadd.i64	d4, d4, d25
	vzip.32	q0, q8
	vadd.i64	d12, d4, d14
	add	r6, r8, #8
	vst1.8	{d0}, [r6 :64]                           #! EA = L0xbeffe9c8, Value = 0xfff3d9ae
	vsub.i64	d19, d19, d9
	add	r6, r6, #16
	vst1.8	{d16}, [r6 :64]                          #! EA = L0xbeffe9d8, Value = 0xfe95255f
	vshr.s64	d22, d12, #26
	vand	q0, q6, q1
	vadd.i64	d10, d10, d22
	vzip.32	q3, q9
	vsub.i64	d4, d4, d0
	sub	r6, r6, #8
	vst1.8	{d6}, [r6 :64]                           #! EA = L0xbeffe9d0, Value = 0x01177ab8
	add	r6, r6, #16
	vst1.8	{d18}, [r6 :64]                          #! EA = L0xbeffe9e0, Value = 0xfed1a11a
	vzip.32	q2, q5
	sub	r6, r6, #32
	vst1.8	{d4}, [r6 :64]                           #! EA = L0xbeffe9c0, Value = 0xfebf0ffb
	subs	r5, r5, #1
	bhi	0x11c34 <._squaringloop>
._skipsquaringloop () at /home/armbian/Sources/boringssl/crypto/curve25519/asm/x25519-asm-arm.S:1819
	mov	r2, r2
	add	r5, r3, #288	; 0x120
	add	r6, r3, #144	; 0x90
	vmov.i32	q0, #19	; 0x00000013
	vmov.i32	q1, #0	; 0x00000000
	vmov.i32	q2, #1	; 0x00000001
	vzip.32	q1, q2
	vld1.8	{d4-d5}, [r5 :128]!                      #! EA = L0xbeffe9c0, Value = 0xfe1cfece
	vld1.8	{d6-d7}, [r5 :128]!                      #! EA = L0xbeffe9d0, Value = 0xfee51589
	vld1.8	{d9}, [r5 :64]                           #! EA = L0xbeffe9e0, Value = 0x000379bc
	vld1.8	{d10-d11}, [r2 :128]!                    #! EA = L0xbeffe930, Value = 0xfe2e1700
	add	r5, sp, #416	; 0x1a0
	vld1.8	{d12-d13}, [r2 :128]!                    #! EA = L0xbeffe940, Value = 0xfec2a896
	vmul.i32	q7, q2, q0
	vld1.8	{d8}, [r2 :64]                           #! EA = L0xbeffe950, Value = 0x00712e1f
	vext.8	d17, d11, d10, #4
	vmul.i32	q9, q3, q0
	vext.8	d16, d10, d8, #4
	vshl.u32	q10, q5, q1
	vext.8	d22, d14, d4, #4
	vext.8	d24, d18, d6, #4
	vshl.u32	q13, q6, q1
	vshl.u32	d28, d8, d2
	vrev64.32	d22, d22
	vmul.i32	d1, d9, d1
	vrev64.32	d24, d24
	vext.8	d29, d8, d13, #4
	vext.8	d0, d1, d9, #4
	vrev64.32	d0, d0
	vext.8	d2, d9, d1, #4
	vext.8	d23, d15, d5, #4
	vmull.s32	q4, d20, d4
	vrev64.32	d23, d23
	vmlal.s32	q4, d21, d1
	vrev64.32	d2, d2
	vmlal.s32	q4, d26, d19
	vext.8	d3, d5, d15, #4
	vmlal.s32	q4, d27, d18
	vrev64.32	d3, d3
	vmlal.s32	q4, d28, d15
	vext.8	d14, d12, d11, #4
	vmull.s32	q5, d16, d23
	vext.8	d15, d13, d12, #4
	vmlal.s32	q5, d17, d4
	vst1.8	{d8}, [r5 :64]!                          #! EA = L0xbeffea20, Value = 0xd194e651
	vmlal.s32	q5, d14, d1
	vext.8	d12, d9, d8, #0
	vmlal.s32	q5, d15, d19
	vmov.i64	d13, #0x0000000000000000
	vmlal.s32	q5, d29, d18
	vext.8	d25, d19, d7, #4
	vmlal.s32	q6, d20, d5
	vrev64.32	d25, d25
	vmlal.s32	q6, d21, d4
	vst1.8	{d11}, [r5 :64]!                         #! EA = L0xbeffea28, Value = 0x1c0f96db
	vmlal.s32	q6, d26, d1
	vext.8	d9, d10, d10, #0
	vmlal.s32	q6, d27, d19
	vmov.i64	d8, #0x0000000000000000
	vmlal.s32	q6, d28, d18
	vmlal.s32	q4, d16, d24
	vmlal.s32	q4, d17, d5
	vmlal.s32	q4, d14, d4
	vst1.8	{d12}, [r5 :64]!                         #! EA = L0xbeffea30, Value = 0xaa1ed329
	vmlal.s32	q4, d15, d1
	vext.8	d10, d13, d12, #0
	vmlal.s32	q4, d29, d19
	vmov.i64	d11, #0x0000000000000000
	vmlal.s32	q5, d20, d6
	vmlal.s32	q5, d21, d5
	vmlal.s32	q5, d26, d4
	vext.8	d13, d8, d8, #0
	vmlal.s32	q5, d27, d1
	vmov.i64	d12, #0x0000000000000000
	vmlal.s32	q5, d28, d19
	vst1.8	{d9}, [r5 :64]!                          #! EA = L0xbeffea38, Value = 0xf77044d8
	vmlal.s32	q6, d16, d25
	vmlal.s32	q6, d17, d6
	vst1.8	{d10}, [r5 :64]                          #! EA = L0xbeffea40, Value = 0xf6580d90
	vmlal.s32	q6, d14, d5
	vext.8	d8, d11, d10, #0
	vmlal.s32	q6, d15, d4
	vmov.i64	d9, #0x0000000000000000
	vmlal.s32	q6, d29, d1
	vmlal.s32	q4, d20, d7
	vmlal.s32	q4, d21, d6
	vmlal.s32	q4, d26, d5
	vext.8	d11, d12, d12, #0
	vmlal.s32	q4, d27, d4
	vmov.i64	d10, #0x0000000000000000
	vmlal.s32	q4, d28, d1
	vmlal.s32	q5, d16, d0
	sub	r2, r5, #32
	vmlal.s32	q5, d17, d7
	vmlal.s32	q5, d14, d6
	vext.8	d30, d9, d8, #0
	vmlal.s32	q5, d15, d5
	vld1.8	{d31}, [r2 :64]!                         #! EA = L0xbeffea20, Value = 0xd77bf9ec
	vmlal.s32	q5, d29, d4
	vmlal.s32	q15, d20, d0
	vext.8	d0, d6, d18, #4
	vmlal.s32	q15, d21, d25
	vrev64.32	d0, d0
	vmlal.s32	q15, d26, d24
	vext.8	d1, d7, d19, #4
	vext.8	d7, d10, d10, #0
	vmlal.s32	q15, d27, d23
	vrev64.32	d1, d1
	vld1.8	{d6}, [r2 :64]                           #! EA = L0xbeffea28, Value = 0x4ad42ee3
	vmlal.s32	q15, d28, d22
	vmlal.s32	q3, d16, d4
	add	r2, r2, #24
	vmlal.s32	q3, d17, d2
	vext.8	d4, d31, d30, #0
	vorr	d17, d11, d11
	vmlal.s32	q3, d14, d1
	vext.8	d11, d13, d13, #0
	vext.8	d13, d30, d30, #0
	vmlal.s32	q3, d15, d0
	vext.8	d1, d8, d8, #0
	vmlal.s32	q3, d29, d3
	vld1.8	{d5}, [r2 :64]                           #! EA = L0xbeffea40, Value = 0x33b0e469
	sub	r2, r2, #16
	vext.8	d10, d6, d6, #0
	vmov.i8	q1, #255	; 0xff
	vshl.s64	q4, q1, #25
	add	r5, sp, #512	; 0x200
	vld1.8	{d14-d15}, [r5 :128]                     #! EA = L0xbeffea80, Value = 0x02000000
	vadd.i64	q9, q2, q7
	vshl.s64	q1, q1, #26
	vshr.s64	q10, q9, #26
	vld1.8	{d0}, [r2 :64]!                          #! EA = L0xbeffea30, Value = 0xc8fe7289
	vadd.i64	q5, q5, q10
	vand	q9, q9, q1
	vld1.8	{d16}, [r2 :64]!                         #! EA = L0xbeffea38, Value = 0xe05fb711
	add	r2, sp, #528	; 0x210
	vld1.8	{d20-d21}, [r2 :128]                     #! EA = L0xbeffea90, Value = 0x01000000
	vadd.i64	q11, q5, q10
	vsub.i64	q2, q2, q9
	vshr.s64	q9, q11, #25
	vext.8	d12, d5, d4, #0
	vand	q11, q11, q4
	vadd.i64	q0, q0, q9
	vorr	d19, d7, d7
	vadd.i64	q3, q0, q7
	vsub.i64	q5, q5, q11
	vshr.s64	q11, q3, #26
	vext.8	d18, d11, d10, #0
	vand	q3, q3, q1
	vadd.i64	q8, q8, q11
	vadd.i64	q11, q8, q10
	vsub.i64	q0, q0, q3
	vshr.s64	q3, q11, #25
	vand	q11, q11, q4
	vadd.i64	q3, q6, q3
	vadd.i64	q6, q3, q7
	vsub.i64	q8, q8, q11
	vshr.s64	q11, q6, #26
	vand	q6, q6, q1
	vadd.i64	q9, q9, q11
	vadd.i64	d25, d19, d21
	vsub.i64	q3, q3, q6
	vshr.s64	d23, d25, #25
	vand	q4, q12, q4
	vadd.i64	d21, d23, d23
	vshl.s64	d25, d23, #4
	vadd.i64	d21, d21, d23
	vadd.i64	d25, d25, d21
	vadd.i64	d4, d4, d25
	vzip.32	q0, q8
	vadd.i64	d12, d4, d14
	add	r2, r6, #8
	vst1.8	{d0}, [r2 :64]                           #! EA = L0xbeffe938, Value = 0x00613a50
	vsub.i64	d19, d19, d9
	add	r2, r2, #16
	vst1.8	{d16}, [r2 :64]                          #! EA = L0xbeffe948, Value = 0x0149f5aa
	vshr.s64	d22, d12, #26
	vand	q0, q6, q1
	vadd.i64	d10, d10, d22
	vzip.32	q3, q9
	vsub.i64	d4, d4, d0
	sub	r2, r2, #8
	vst1.8	{d6}, [r2 :64]                           #! EA = L0xbeffe940, Value = 0xfec2a896
	add	r2, r2, #16
	vst1.8	{d18}, [r2 :64]                          #! EA = L0xbeffe950, Value = 0x00712e1f
	vzip.32	q2, q5
	sub	r2, r2, #32
	vst1.8	{d4}, [r2 :64]                           #! EA = L0xbeffe930, Value = 0xfe2e1700
	cmp	r4, #0
._skippostcopy () at /home/armbian/Sources/boringssl/crypto/curve25519/asm/x25519-asm-arm.S:2018
	#beq	0x12244 <._skippostcopy>
	cmp	r1, #1
._skipfinalcopy () at /home/armbian/Sources/boringssl/crypto/curve25519/asm/x25519-asm-arm.S:2029
	#bne	0x1226c <._skipfinalcopy>
	add	r1, r1, #1
	cmp	r1, #12
	bcc	0x11b84 <._invertloop>
