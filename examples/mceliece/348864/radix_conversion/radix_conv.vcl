(* quine: cv -v -disable_safety -slicing -jobs 24 mcbits-radix-conv/radix_conv.vcl
Condition generated with print_cond.py (output at cond-full.txt).

Parsing CryptoLine file:                [OK]            0.629637 seconds
Checking well-formedness:               [OK]            0.253314 seconds
Transforming to SSA form:               [OK]            0.316921 seconds
Normalizing specification:              [OK]            0.338994 seconds
Rewriting assignments:                  [OK]            0.604782 seconds
Verifying range assertions:             [OK]            263.851408 seconds
Verifying range specification:          [OK]            0.030541 seconds
Rewriting value-preserved casting:      [OK]            0.042472 seconds
Verifying algebraic assertions:         [OK]            0.339974 seconds
Verifying algebraic specification:      [OK]            440.263712 seconds
Verification result:                    [OK]            706.760238 seconds

p.s. Safety conditions (x 23993) are verified separately with -isafety in ~3 hr.
*)

proc main (
  uint12 x,
  uint12 z,
  uint13 modulus,
  bit in0_0, bit in0_1, bit in0_2, bit in0_3, bit in0_4, bit in0_5, bit in0_6, bit in0_7, bit in0_8, bit in0_9, bit in0_10, bit in0_11, bit in0_12, bit in0_13, bit in0_14, bit in0_15, bit in0_16, bit in0_17, bit in0_18, bit in0_19, bit in0_20, bit in0_21, bit in0_22, bit in0_23, bit in0_24, bit in0_25, bit in0_26, bit in0_27, bit in0_28, bit in0_29, bit in0_30, bit in0_31, bit in0_32, bit in0_33, bit in0_34, bit in0_35, bit in0_36, bit in0_37, bit in0_38, bit in0_39, bit in0_40, bit in0_41, bit in0_42, bit in0_43, bit in0_44, bit in0_45, bit in0_46, bit in0_47, bit in0_48, bit in0_49, bit in0_50, bit in0_51, bit in0_52, bit in0_53, bit in0_54, bit in0_55, bit in0_56, bit in0_57, bit in0_58, bit in0_59, bit in0_60, bit in0_61, bit in0_62, bit in0_63,
  bit in1_0, bit in1_1, bit in1_2, bit in1_3, bit in1_4, bit in1_5, bit in1_6, bit in1_7, bit in1_8, bit in1_9, bit in1_10, bit in1_11, bit in1_12, bit in1_13, bit in1_14, bit in1_15, bit in1_16, bit in1_17, bit in1_18, bit in1_19, bit in1_20, bit in1_21, bit in1_22, bit in1_23, bit in1_24, bit in1_25, bit in1_26, bit in1_27, bit in1_28, bit in1_29, bit in1_30, bit in1_31, bit in1_32, bit in1_33, bit in1_34, bit in1_35, bit in1_36, bit in1_37, bit in1_38, bit in1_39, bit in1_40, bit in1_41, bit in1_42, bit in1_43, bit in1_44, bit in1_45, bit in1_46, bit in1_47, bit in1_48, bit in1_49, bit in1_50, bit in1_51, bit in1_52, bit in1_53, bit in1_54, bit in1_55, bit in1_56, bit in1_57, bit in1_58, bit in1_59, bit in1_60, bit in1_61, bit in1_62, bit in1_63,
  bit in2_0, bit in2_1, bit in2_2, bit in2_3, bit in2_4, bit in2_5, bit in2_6, bit in2_7, bit in2_8, bit in2_9, bit in2_10, bit in2_11, bit in2_12, bit in2_13, bit in2_14, bit in2_15, bit in2_16, bit in2_17, bit in2_18, bit in2_19, bit in2_20, bit in2_21, bit in2_22, bit in2_23, bit in2_24, bit in2_25, bit in2_26, bit in2_27, bit in2_28, bit in2_29, bit in2_30, bit in2_31, bit in2_32, bit in2_33, bit in2_34, bit in2_35, bit in2_36, bit in2_37, bit in2_38, bit in2_39, bit in2_40, bit in2_41, bit in2_42, bit in2_43, bit in2_44, bit in2_45, bit in2_46, bit in2_47, bit in2_48, bit in2_49, bit in2_50, bit in2_51, bit in2_52, bit in2_53, bit in2_54, bit in2_55, bit in2_56, bit in2_57, bit in2_58, bit in2_59, bit in2_60, bit in2_61, bit in2_62, bit in2_63,
  bit in3_0, bit in3_1, bit in3_2, bit in3_3, bit in3_4, bit in3_5, bit in3_6, bit in3_7, bit in3_8, bit in3_9, bit in3_10, bit in3_11, bit in3_12, bit in3_13, bit in3_14, bit in3_15, bit in3_16, bit in3_17, bit in3_18, bit in3_19, bit in3_20, bit in3_21, bit in3_22, bit in3_23, bit in3_24, bit in3_25, bit in3_26, bit in3_27, bit in3_28, bit in3_29, bit in3_30, bit in3_31, bit in3_32, bit in3_33, bit in3_34, bit in3_35, bit in3_36, bit in3_37, bit in3_38, bit in3_39, bit in3_40, bit in3_41, bit in3_42, bit in3_43, bit in3_44, bit in3_45, bit in3_46, bit in3_47, bit in3_48, bit in3_49, bit in3_50, bit in3_51, bit in3_52, bit in3_53, bit in3_54, bit in3_55, bit in3_56, bit in3_57, bit in3_58, bit in3_59, bit in3_60, bit in3_61, bit in3_62, bit in3_63,
  bit in4_0, bit in4_1, bit in4_2, bit in4_3, bit in4_4, bit in4_5, bit in4_6, bit in4_7, bit in4_8, bit in4_9, bit in4_10, bit in4_11, bit in4_12, bit in4_13, bit in4_14, bit in4_15, bit in4_16, bit in4_17, bit in4_18, bit in4_19, bit in4_20, bit in4_21, bit in4_22, bit in4_23, bit in4_24, bit in4_25, bit in4_26, bit in4_27, bit in4_28, bit in4_29, bit in4_30, bit in4_31, bit in4_32, bit in4_33, bit in4_34, bit in4_35, bit in4_36, bit in4_37, bit in4_38, bit in4_39, bit in4_40, bit in4_41, bit in4_42, bit in4_43, bit in4_44, bit in4_45, bit in4_46, bit in4_47, bit in4_48, bit in4_49, bit in4_50, bit in4_51, bit in4_52, bit in4_53, bit in4_54, bit in4_55, bit in4_56, bit in4_57, bit in4_58, bit in4_59, bit in4_60, bit in4_61, bit in4_62, bit in4_63,
  bit in5_0, bit in5_1, bit in5_2, bit in5_3, bit in5_4, bit in5_5, bit in5_6, bit in5_7, bit in5_8, bit in5_9, bit in5_10, bit in5_11, bit in5_12, bit in5_13, bit in5_14, bit in5_15, bit in5_16, bit in5_17, bit in5_18, bit in5_19, bit in5_20, bit in5_21, bit in5_22, bit in5_23, bit in5_24, bit in5_25, bit in5_26, bit in5_27, bit in5_28, bit in5_29, bit in5_30, bit in5_31, bit in5_32, bit in5_33, bit in5_34, bit in5_35, bit in5_36, bit in5_37, bit in5_38, bit in5_39, bit in5_40, bit in5_41, bit in5_42, bit in5_43, bit in5_44, bit in5_45, bit in5_46, bit in5_47, bit in5_48, bit in5_49, bit in5_50, bit in5_51, bit in5_52, bit in5_53, bit in5_54, bit in5_55, bit in5_56, bit in5_57, bit in5_58, bit in5_59, bit in5_60, bit in5_61, bit in5_62, bit in5_63,
  bit in6_0, bit in6_1, bit in6_2, bit in6_3, bit in6_4, bit in6_5, bit in6_6, bit in6_7, bit in6_8, bit in6_9, bit in6_10, bit in6_11, bit in6_12, bit in6_13, bit in6_14, bit in6_15, bit in6_16, bit in6_17, bit in6_18, bit in6_19, bit in6_20, bit in6_21, bit in6_22, bit in6_23, bit in6_24, bit in6_25, bit in6_26, bit in6_27, bit in6_28, bit in6_29, bit in6_30, bit in6_31, bit in6_32, bit in6_33, bit in6_34, bit in6_35, bit in6_36, bit in6_37, bit in6_38, bit in6_39, bit in6_40, bit in6_41, bit in6_42, bit in6_43, bit in6_44, bit in6_45, bit in6_46, bit in6_47, bit in6_48, bit in6_49, bit in6_50, bit in6_51, bit in6_52, bit in6_53, bit in6_54, bit in6_55, bit in6_56, bit in6_57, bit in6_58, bit in6_59, bit in6_60, bit in6_61, bit in6_62, bit in6_63,
  bit in7_0, bit in7_1, bit in7_2, bit in7_3, bit in7_4, bit in7_5, bit in7_6, bit in7_7, bit in7_8, bit in7_9, bit in7_10, bit in7_11, bit in7_12, bit in7_13, bit in7_14, bit in7_15, bit in7_16, bit in7_17, bit in7_18, bit in7_19, bit in7_20, bit in7_21, bit in7_22, bit in7_23, bit in7_24, bit in7_25, bit in7_26, bit in7_27, bit in7_28, bit in7_29, bit in7_30, bit in7_31, bit in7_32, bit in7_33, bit in7_34, bit in7_35, bit in7_36, bit in7_37, bit in7_38, bit in7_39, bit in7_40, bit in7_41, bit in7_42, bit in7_43, bit in7_44, bit in7_45, bit in7_46, bit in7_47, bit in7_48, bit in7_49, bit in7_50, bit in7_51, bit in7_52, bit in7_53, bit in7_54, bit in7_55, bit in7_56, bit in7_57, bit in7_58, bit in7_59, bit in7_60, bit in7_61, bit in7_62, bit in7_63,
  bit in8_0, bit in8_1, bit in8_2, bit in8_3, bit in8_4, bit in8_5, bit in8_6, bit in8_7, bit in8_8, bit in8_9, bit in8_10, bit in8_11, bit in8_12, bit in8_13, bit in8_14, bit in8_15, bit in8_16, bit in8_17, bit in8_18, bit in8_19, bit in8_20, bit in8_21, bit in8_22, bit in8_23, bit in8_24, bit in8_25, bit in8_26, bit in8_27, bit in8_28, bit in8_29, bit in8_30, bit in8_31, bit in8_32, bit in8_33, bit in8_34, bit in8_35, bit in8_36, bit in8_37, bit in8_38, bit in8_39, bit in8_40, bit in8_41, bit in8_42, bit in8_43, bit in8_44, bit in8_45, bit in8_46, bit in8_47, bit in8_48, bit in8_49, bit in8_50, bit in8_51, bit in8_52, bit in8_53, bit in8_54, bit in8_55, bit in8_56, bit in8_57, bit in8_58, bit in8_59, bit in8_60, bit in8_61, bit in8_62, bit in8_63,
  bit in9_0, bit in9_1, bit in9_2, bit in9_3, bit in9_4, bit in9_5, bit in9_6, bit in9_7, bit in9_8, bit in9_9, bit in9_10, bit in9_11, bit in9_12, bit in9_13, bit in9_14, bit in9_15, bit in9_16, bit in9_17, bit in9_18, bit in9_19, bit in9_20, bit in9_21, bit in9_22, bit in9_23, bit in9_24, bit in9_25, bit in9_26, bit in9_27, bit in9_28, bit in9_29, bit in9_30, bit in9_31, bit in9_32, bit in9_33, bit in9_34, bit in9_35, bit in9_36, bit in9_37, bit in9_38, bit in9_39, bit in9_40, bit in9_41, bit in9_42, bit in9_43, bit in9_44, bit in9_45, bit in9_46, bit in9_47, bit in9_48, bit in9_49, bit in9_50, bit in9_51, bit in9_52, bit in9_53, bit in9_54, bit in9_55, bit in9_56, bit in9_57, bit in9_58, bit in9_59, bit in9_60, bit in9_61, bit in9_62, bit in9_63,
  bit in10_0, bit in10_1, bit in10_2, bit in10_3, bit in10_4, bit in10_5, bit in10_6, bit in10_7, bit in10_8, bit in10_9, bit in10_10, bit in10_11, bit in10_12, bit in10_13, bit in10_14, bit in10_15, bit in10_16, bit in10_17, bit in10_18, bit in10_19, bit in10_20, bit in10_21, bit in10_22, bit in10_23, bit in10_24, bit in10_25, bit in10_26, bit in10_27, bit in10_28, bit in10_29, bit in10_30, bit in10_31, bit in10_32, bit in10_33, bit in10_34, bit in10_35, bit in10_36, bit in10_37, bit in10_38, bit in10_39, bit in10_40, bit in10_41, bit in10_42, bit in10_43, bit in10_44, bit in10_45, bit in10_46, bit in10_47, bit in10_48, bit in10_49, bit in10_50, bit in10_51, bit in10_52, bit in10_53, bit in10_54, bit in10_55, bit in10_56, bit in10_57, bit in10_58, bit in10_59, bit in10_60, bit in10_61, bit in10_62, bit in10_63,
  bit in11_0, bit in11_1, bit in11_2, bit in11_3, bit in11_4, bit in11_5, bit in11_6, bit in11_7, bit in11_8, bit in11_9, bit in11_10, bit in11_11, bit in11_12, bit in11_13, bit in11_14, bit in11_15, bit in11_16, bit in11_17, bit in11_18, bit in11_19, bit in11_20, bit in11_21, bit in11_22, bit in11_23, bit in11_24, bit in11_25, bit in11_26, bit in11_27, bit in11_28, bit in11_29, bit in11_30, bit in11_31, bit in11_32, bit in11_33, bit in11_34, bit in11_35, bit in11_36, bit in11_37, bit in11_38, bit in11_39, bit in11_40, bit in11_41, bit in11_42, bit in11_43, bit in11_44, bit in11_45, bit in11_46, bit in11_47, bit in11_48, bit in11_49, bit in11_50, bit in11_51, bit in11_52, bit in11_53, bit in11_54, bit in11_55, bit in11_56, bit in11_57, bit in11_58, bit in11_59, bit in11_60, bit in11_61, bit in11_62, bit in11_63
) =
{
  modulus = z ** 12 + z ** 3 + 1
  &&
  true
}
(* setup initial values of registers *)
mov rsp 0x7fffffffd830@uint64;
mov rdi 0x7fffffffdaa0@uint64;

(* pack input coefficients *)
mov %L0x7fffffffdaa0 [in0_0, in0_1, in0_2, in0_3, in0_4, in0_5, in0_6, in0_7, in0_8, in0_9, in0_10, in0_11, in0_12, in0_13, in0_14, in0_15, in0_16, in0_17, in0_18, in0_19, in0_20, in0_21, in0_22, in0_23, in0_24, in0_25, in0_26, in0_27, in0_28, in0_29, in0_30, in0_31, in0_32, in0_33, in0_34, in0_35, in0_36, in0_37, in0_38, in0_39, in0_40, in0_41, in0_42, in0_43, in0_44, in0_45, in0_46, in0_47, in0_48, in0_49, in0_50, in0_51, in0_52, in0_53, in0_54, in0_55, in0_56, in0_57, in0_58, in0_59, in0_60, in0_61, in0_62, in0_63];
mov %L0x7fffffffdaa8 [in1_0, in1_1, in1_2, in1_3, in1_4, in1_5, in1_6, in1_7, in1_8, in1_9, in1_10, in1_11, in1_12, in1_13, in1_14, in1_15, in1_16, in1_17, in1_18, in1_19, in1_20, in1_21, in1_22, in1_23, in1_24, in1_25, in1_26, in1_27, in1_28, in1_29, in1_30, in1_31, in1_32, in1_33, in1_34, in1_35, in1_36, in1_37, in1_38, in1_39, in1_40, in1_41, in1_42, in1_43, in1_44, in1_45, in1_46, in1_47, in1_48, in1_49, in1_50, in1_51, in1_52, in1_53, in1_54, in1_55, in1_56, in1_57, in1_58, in1_59, in1_60, in1_61, in1_62, in1_63];
mov %L0x7fffffffdab0 [in2_0, in2_1, in2_2, in2_3, in2_4, in2_5, in2_6, in2_7, in2_8, in2_9, in2_10, in2_11, in2_12, in2_13, in2_14, in2_15, in2_16, in2_17, in2_18, in2_19, in2_20, in2_21, in2_22, in2_23, in2_24, in2_25, in2_26, in2_27, in2_28, in2_29, in2_30, in2_31, in2_32, in2_33, in2_34, in2_35, in2_36, in2_37, in2_38, in2_39, in2_40, in2_41, in2_42, in2_43, in2_44, in2_45, in2_46, in2_47, in2_48, in2_49, in2_50, in2_51, in2_52, in2_53, in2_54, in2_55, in2_56, in2_57, in2_58, in2_59, in2_60, in2_61, in2_62, in2_63];
mov %L0x7fffffffdab8 [in3_0, in3_1, in3_2, in3_3, in3_4, in3_5, in3_6, in3_7, in3_8, in3_9, in3_10, in3_11, in3_12, in3_13, in3_14, in3_15, in3_16, in3_17, in3_18, in3_19, in3_20, in3_21, in3_22, in3_23, in3_24, in3_25, in3_26, in3_27, in3_28, in3_29, in3_30, in3_31, in3_32, in3_33, in3_34, in3_35, in3_36, in3_37, in3_38, in3_39, in3_40, in3_41, in3_42, in3_43, in3_44, in3_45, in3_46, in3_47, in3_48, in3_49, in3_50, in3_51, in3_52, in3_53, in3_54, in3_55, in3_56, in3_57, in3_58, in3_59, in3_60, in3_61, in3_62, in3_63];
mov %L0x7fffffffdac0 [in4_0, in4_1, in4_2, in4_3, in4_4, in4_5, in4_6, in4_7, in4_8, in4_9, in4_10, in4_11, in4_12, in4_13, in4_14, in4_15, in4_16, in4_17, in4_18, in4_19, in4_20, in4_21, in4_22, in4_23, in4_24, in4_25, in4_26, in4_27, in4_28, in4_29, in4_30, in4_31, in4_32, in4_33, in4_34, in4_35, in4_36, in4_37, in4_38, in4_39, in4_40, in4_41, in4_42, in4_43, in4_44, in4_45, in4_46, in4_47, in4_48, in4_49, in4_50, in4_51, in4_52, in4_53, in4_54, in4_55, in4_56, in4_57, in4_58, in4_59, in4_60, in4_61, in4_62, in4_63];
mov %L0x7fffffffdac8 [in5_0, in5_1, in5_2, in5_3, in5_4, in5_5, in5_6, in5_7, in5_8, in5_9, in5_10, in5_11, in5_12, in5_13, in5_14, in5_15, in5_16, in5_17, in5_18, in5_19, in5_20, in5_21, in5_22, in5_23, in5_24, in5_25, in5_26, in5_27, in5_28, in5_29, in5_30, in5_31, in5_32, in5_33, in5_34, in5_35, in5_36, in5_37, in5_38, in5_39, in5_40, in5_41, in5_42, in5_43, in5_44, in5_45, in5_46, in5_47, in5_48, in5_49, in5_50, in5_51, in5_52, in5_53, in5_54, in5_55, in5_56, in5_57, in5_58, in5_59, in5_60, in5_61, in5_62, in5_63];
mov %L0x7fffffffdad0 [in6_0, in6_1, in6_2, in6_3, in6_4, in6_5, in6_6, in6_7, in6_8, in6_9, in6_10, in6_11, in6_12, in6_13, in6_14, in6_15, in6_16, in6_17, in6_18, in6_19, in6_20, in6_21, in6_22, in6_23, in6_24, in6_25, in6_26, in6_27, in6_28, in6_29, in6_30, in6_31, in6_32, in6_33, in6_34, in6_35, in6_36, in6_37, in6_38, in6_39, in6_40, in6_41, in6_42, in6_43, in6_44, in6_45, in6_46, in6_47, in6_48, in6_49, in6_50, in6_51, in6_52, in6_53, in6_54, in6_55, in6_56, in6_57, in6_58, in6_59, in6_60, in6_61, in6_62, in6_63];
mov %L0x7fffffffdad8 [in7_0, in7_1, in7_2, in7_3, in7_4, in7_5, in7_6, in7_7, in7_8, in7_9, in7_10, in7_11, in7_12, in7_13, in7_14, in7_15, in7_16, in7_17, in7_18, in7_19, in7_20, in7_21, in7_22, in7_23, in7_24, in7_25, in7_26, in7_27, in7_28, in7_29, in7_30, in7_31, in7_32, in7_33, in7_34, in7_35, in7_36, in7_37, in7_38, in7_39, in7_40, in7_41, in7_42, in7_43, in7_44, in7_45, in7_46, in7_47, in7_48, in7_49, in7_50, in7_51, in7_52, in7_53, in7_54, in7_55, in7_56, in7_57, in7_58, in7_59, in7_60, in7_61, in7_62, in7_63];
mov %L0x7fffffffdae0 [in8_0, in8_1, in8_2, in8_3, in8_4, in8_5, in8_6, in8_7, in8_8, in8_9, in8_10, in8_11, in8_12, in8_13, in8_14, in8_15, in8_16, in8_17, in8_18, in8_19, in8_20, in8_21, in8_22, in8_23, in8_24, in8_25, in8_26, in8_27, in8_28, in8_29, in8_30, in8_31, in8_32, in8_33, in8_34, in8_35, in8_36, in8_37, in8_38, in8_39, in8_40, in8_41, in8_42, in8_43, in8_44, in8_45, in8_46, in8_47, in8_48, in8_49, in8_50, in8_51, in8_52, in8_53, in8_54, in8_55, in8_56, in8_57, in8_58, in8_59, in8_60, in8_61, in8_62, in8_63];
mov %L0x7fffffffdae8 [in9_0, in9_1, in9_2, in9_3, in9_4, in9_5, in9_6, in9_7, in9_8, in9_9, in9_10, in9_11, in9_12, in9_13, in9_14, in9_15, in9_16, in9_17, in9_18, in9_19, in9_20, in9_21, in9_22, in9_23, in9_24, in9_25, in9_26, in9_27, in9_28, in9_29, in9_30, in9_31, in9_32, in9_33, in9_34, in9_35, in9_36, in9_37, in9_38, in9_39, in9_40, in9_41, in9_42, in9_43, in9_44, in9_45, in9_46, in9_47, in9_48, in9_49, in9_50, in9_51, in9_52, in9_53, in9_54, in9_55, in9_56, in9_57, in9_58, in9_59, in9_60, in9_61, in9_62, in9_63];
mov %L0x7fffffffdaf0 [in10_0, in10_1, in10_2, in10_3, in10_4, in10_5, in10_6, in10_7, in10_8, in10_9, in10_10, in10_11, in10_12, in10_13, in10_14, in10_15, in10_16, in10_17, in10_18, in10_19, in10_20, in10_21, in10_22, in10_23, in10_24, in10_25, in10_26, in10_27, in10_28, in10_29, in10_30, in10_31, in10_32, in10_33, in10_34, in10_35, in10_36, in10_37, in10_38, in10_39, in10_40, in10_41, in10_42, in10_43, in10_44, in10_45, in10_46, in10_47, in10_48, in10_49, in10_50, in10_51, in10_52, in10_53, in10_54, in10_55, in10_56, in10_57, in10_58, in10_59, in10_60, in10_61, in10_62, in10_63];
mov %L0x7fffffffdaf8 [in11_0, in11_1, in11_2, in11_3, in11_4, in11_5, in11_6, in11_7, in11_8, in11_9, in11_10, in11_11, in11_12, in11_13, in11_14, in11_15, in11_16, in11_17, in11_18, in11_19, in11_20, in11_21, in11_22, in11_23, in11_24, in11_25, in11_26, in11_27, in11_28, in11_29, in11_30, in11_31, in11_32, in11_33, in11_34, in11_35, in11_36, in11_37, in11_38, in11_39, in11_40, in11_41, in11_42, in11_43, in11_44, in11_45, in11_46, in11_47, in11_48, in11_49, in11_50, in11_51, in11_52, in11_53, in11_54, in11_55, in11_56, in11_57, in11_58, in11_59, in11_60, in11_61, in11_62, in11_63];

nondet rb0_0@bit; nondet rb0_1@bit; nondet rb0_2@bit; nondet rb0_3@bit; nondet rb0_4@bit; nondet rb0_5@bit; nondet rb0_6@bit; nondet rb0_7@bit; nondet rb0_8@bit; nondet rb0_9@bit; nondet rb0_10@bit; nondet rb0_11@bit; nondet rb0_12@bit; nondet rb0_13@bit; nondet rb0_14@bit; nondet rb0_15@bit; nondet rb0_16@bit; nondet rb0_17@bit; nondet rb0_18@bit; nondet rb0_19@bit; nondet rb0_20@bit; nondet rb0_21@bit; nondet rb0_22@bit; nondet rb0_23@bit; nondet rb0_24@bit; nondet rb0_25@bit; nondet rb0_26@bit; nondet rb0_27@bit; nondet rb0_28@bit; nondet rb0_29@bit; nondet rb0_30@bit; nondet rb0_31@bit; nondet rb0_32@bit; nondet rb0_33@bit; nondet rb0_34@bit; nondet rb0_35@bit; nondet rb0_36@bit; nondet rb0_37@bit; nondet rb0_38@bit; nondet rb0_39@bit; nondet rb0_40@bit; nondet rb0_41@bit; nondet rb0_42@bit; nondet rb0_43@bit; nondet rb0_44@bit; nondet rb0_45@bit; nondet rb0_46@bit; nondet rb0_47@bit; nondet rb0_48@bit; nondet rb0_49@bit; nondet rb0_50@bit; nondet rb0_51@bit; nondet rb0_52@bit; nondet rb0_53@bit; nondet rb0_54@bit; nondet rb0_55@bit; nondet rb0_56@bit; nondet rb0_57@bit; nondet rb0_58@bit; nondet rb0_59@bit; nondet rb0_60@bit; nondet rb0_61@bit; nondet rb0_62@bit; nondet rb0_63@bit;
nondet rb1_0@bit; nondet rb1_1@bit; nondet rb1_2@bit; nondet rb1_3@bit; nondet rb1_4@bit; nondet rb1_5@bit; nondet rb1_6@bit; nondet rb1_7@bit; nondet rb1_8@bit; nondet rb1_9@bit; nondet rb1_10@bit; nondet rb1_11@bit; nondet rb1_12@bit; nondet rb1_13@bit; nondet rb1_14@bit; nondet rb1_15@bit; nondet rb1_16@bit; nondet rb1_17@bit; nondet rb1_18@bit; nondet rb1_19@bit; nondet rb1_20@bit; nondet rb1_21@bit; nondet rb1_22@bit; nondet rb1_23@bit; nondet rb1_24@bit; nondet rb1_25@bit; nondet rb1_26@bit; nondet rb1_27@bit; nondet rb1_28@bit; nondet rb1_29@bit; nondet rb1_30@bit; nondet rb1_31@bit; nondet rb1_32@bit; nondet rb1_33@bit; nondet rb1_34@bit; nondet rb1_35@bit; nondet rb1_36@bit; nondet rb1_37@bit; nondet rb1_38@bit; nondet rb1_39@bit; nondet rb1_40@bit; nondet rb1_41@bit; nondet rb1_42@bit; nondet rb1_43@bit; nondet rb1_44@bit; nondet rb1_45@bit; nondet rb1_46@bit; nondet rb1_47@bit; nondet rb1_48@bit; nondet rb1_49@bit; nondet rb1_50@bit; nondet rb1_51@bit; nondet rb1_52@bit; nondet rb1_53@bit; nondet rb1_54@bit; nondet rb1_55@bit; nondet rb1_56@bit; nondet rb1_57@bit; nondet rb1_58@bit; nondet rb1_59@bit; nondet rb1_60@bit; nondet rb1_61@bit; nondet rb1_62@bit; nondet rb1_63@bit;
nondet rb2_0@bit; nondet rb2_1@bit; nondet rb2_2@bit; nondet rb2_3@bit; nondet rb2_4@bit; nondet rb2_5@bit; nondet rb2_6@bit; nondet rb2_7@bit; nondet rb2_8@bit; nondet rb2_9@bit; nondet rb2_10@bit; nondet rb2_11@bit; nondet rb2_12@bit; nondet rb2_13@bit; nondet rb2_14@bit; nondet rb2_15@bit; nondet rb2_16@bit; nondet rb2_17@bit; nondet rb2_18@bit; nondet rb2_19@bit; nondet rb2_20@bit; nondet rb2_21@bit; nondet rb2_22@bit; nondet rb2_23@bit; nondet rb2_24@bit; nondet rb2_25@bit; nondet rb2_26@bit; nondet rb2_27@bit; nondet rb2_28@bit; nondet rb2_29@bit; nondet rb2_30@bit; nondet rb2_31@bit; nondet rb2_32@bit; nondet rb2_33@bit; nondet rb2_34@bit; nondet rb2_35@bit; nondet rb2_36@bit; nondet rb2_37@bit; nondet rb2_38@bit; nondet rb2_39@bit; nondet rb2_40@bit; nondet rb2_41@bit; nondet rb2_42@bit; nondet rb2_43@bit; nondet rb2_44@bit; nondet rb2_45@bit; nondet rb2_46@bit; nondet rb2_47@bit; nondet rb2_48@bit; nondet rb2_49@bit; nondet rb2_50@bit; nondet rb2_51@bit; nondet rb2_52@bit; nondet rb2_53@bit; nondet rb2_54@bit; nondet rb2_55@bit; nondet rb2_56@bit; nondet rb2_57@bit; nondet rb2_58@bit; nondet rb2_59@bit; nondet rb2_60@bit; nondet rb2_61@bit; nondet rb2_62@bit; nondet rb2_63@bit;
nondet rb3_0@bit; nondet rb3_1@bit; nondet rb3_2@bit; nondet rb3_3@bit; nondet rb3_4@bit; nondet rb3_5@bit; nondet rb3_6@bit; nondet rb3_7@bit; nondet rb3_8@bit; nondet rb3_9@bit; nondet rb3_10@bit; nondet rb3_11@bit; nondet rb3_12@bit; nondet rb3_13@bit; nondet rb3_14@bit; nondet rb3_15@bit; nondet rb3_16@bit; nondet rb3_17@bit; nondet rb3_18@bit; nondet rb3_19@bit; nondet rb3_20@bit; nondet rb3_21@bit; nondet rb3_22@bit; nondet rb3_23@bit; nondet rb3_24@bit; nondet rb3_25@bit; nondet rb3_26@bit; nondet rb3_27@bit; nondet rb3_28@bit; nondet rb3_29@bit; nondet rb3_30@bit; nondet rb3_31@bit; nondet rb3_32@bit; nondet rb3_33@bit; nondet rb3_34@bit; nondet rb3_35@bit; nondet rb3_36@bit; nondet rb3_37@bit; nondet rb3_38@bit; nondet rb3_39@bit; nondet rb3_40@bit; nondet rb3_41@bit; nondet rb3_42@bit; nondet rb3_43@bit; nondet rb3_44@bit; nondet rb3_45@bit; nondet rb3_46@bit; nondet rb3_47@bit; nondet rb3_48@bit; nondet rb3_49@bit; nondet rb3_50@bit; nondet rb3_51@bit; nondet rb3_52@bit; nondet rb3_53@bit; nondet rb3_54@bit; nondet rb3_55@bit; nondet rb3_56@bit; nondet rb3_57@bit; nondet rb3_58@bit; nondet rb3_59@bit; nondet rb3_60@bit; nondet rb3_61@bit; nondet rb3_62@bit; nondet rb3_63@bit;
nondet rb4_0@bit; nondet rb4_1@bit; nondet rb4_2@bit; nondet rb4_3@bit; nondet rb4_4@bit; nondet rb4_5@bit; nondet rb4_6@bit; nondet rb4_7@bit; nondet rb4_8@bit; nondet rb4_9@bit; nondet rb4_10@bit; nondet rb4_11@bit; nondet rb4_12@bit; nondet rb4_13@bit; nondet rb4_14@bit; nondet rb4_15@bit; nondet rb4_16@bit; nondet rb4_17@bit; nondet rb4_18@bit; nondet rb4_19@bit; nondet rb4_20@bit; nondet rb4_21@bit; nondet rb4_22@bit; nondet rb4_23@bit; nondet rb4_24@bit; nondet rb4_25@bit; nondet rb4_26@bit; nondet rb4_27@bit; nondet rb4_28@bit; nondet rb4_29@bit; nondet rb4_30@bit; nondet rb4_31@bit; nondet rb4_32@bit; nondet rb4_33@bit; nondet rb4_34@bit; nondet rb4_35@bit; nondet rb4_36@bit; nondet rb4_37@bit; nondet rb4_38@bit; nondet rb4_39@bit; nondet rb4_40@bit; nondet rb4_41@bit; nondet rb4_42@bit; nondet rb4_43@bit; nondet rb4_44@bit; nondet rb4_45@bit; nondet rb4_46@bit; nondet rb4_47@bit; nondet rb4_48@bit; nondet rb4_49@bit; nondet rb4_50@bit; nondet rb4_51@bit; nondet rb4_52@bit; nondet rb4_53@bit; nondet rb4_54@bit; nondet rb4_55@bit; nondet rb4_56@bit; nondet rb4_57@bit; nondet rb4_58@bit; nondet rb4_59@bit; nondet rb4_60@bit; nondet rb4_61@bit; nondet rb4_62@bit; nondet rb4_63@bit;
nondet rb5_0@bit; nondet rb5_1@bit; nondet rb5_2@bit; nondet rb5_3@bit; nondet rb5_4@bit; nondet rb5_5@bit; nondet rb5_6@bit; nondet rb5_7@bit; nondet rb5_8@bit; nondet rb5_9@bit; nondet rb5_10@bit; nondet rb5_11@bit; nondet rb5_12@bit; nondet rb5_13@bit; nondet rb5_14@bit; nondet rb5_15@bit; nondet rb5_16@bit; nondet rb5_17@bit; nondet rb5_18@bit; nondet rb5_19@bit; nondet rb5_20@bit; nondet rb5_21@bit; nondet rb5_22@bit; nondet rb5_23@bit; nondet rb5_24@bit; nondet rb5_25@bit; nondet rb5_26@bit; nondet rb5_27@bit; nondet rb5_28@bit; nondet rb5_29@bit; nondet rb5_30@bit; nondet rb5_31@bit; nondet rb5_32@bit; nondet rb5_33@bit; nondet rb5_34@bit; nondet rb5_35@bit; nondet rb5_36@bit; nondet rb5_37@bit; nondet rb5_38@bit; nondet rb5_39@bit; nondet rb5_40@bit; nondet rb5_41@bit; nondet rb5_42@bit; nondet rb5_43@bit; nondet rb5_44@bit; nondet rb5_45@bit; nondet rb5_46@bit; nondet rb5_47@bit; nondet rb5_48@bit; nondet rb5_49@bit; nondet rb5_50@bit; nondet rb5_51@bit; nondet rb5_52@bit; nondet rb5_53@bit; nondet rb5_54@bit; nondet rb5_55@bit; nondet rb5_56@bit; nondet rb5_57@bit; nondet rb5_58@bit; nondet rb5_59@bit; nondet rb5_60@bit; nondet rb5_61@bit; nondet rb5_62@bit; nondet rb5_63@bit;
nondet rb6_0@bit; nondet rb6_1@bit; nondet rb6_2@bit; nondet rb6_3@bit; nondet rb6_4@bit; nondet rb6_5@bit; nondet rb6_6@bit; nondet rb6_7@bit; nondet rb6_8@bit; nondet rb6_9@bit; nondet rb6_10@bit; nondet rb6_11@bit; nondet rb6_12@bit; nondet rb6_13@bit; nondet rb6_14@bit; nondet rb6_15@bit; nondet rb6_16@bit; nondet rb6_17@bit; nondet rb6_18@bit; nondet rb6_19@bit; nondet rb6_20@bit; nondet rb6_21@bit; nondet rb6_22@bit; nondet rb6_23@bit; nondet rb6_24@bit; nondet rb6_25@bit; nondet rb6_26@bit; nondet rb6_27@bit; nondet rb6_28@bit; nondet rb6_29@bit; nondet rb6_30@bit; nondet rb6_31@bit; nondet rb6_32@bit; nondet rb6_33@bit; nondet rb6_34@bit; nondet rb6_35@bit; nondet rb6_36@bit; nondet rb6_37@bit; nondet rb6_38@bit; nondet rb6_39@bit; nondet rb6_40@bit; nondet rb6_41@bit; nondet rb6_42@bit; nondet rb6_43@bit; nondet rb6_44@bit; nondet rb6_45@bit; nondet rb6_46@bit; nondet rb6_47@bit; nondet rb6_48@bit; nondet rb6_49@bit; nondet rb6_50@bit; nondet rb6_51@bit; nondet rb6_52@bit; nondet rb6_53@bit; nondet rb6_54@bit; nondet rb6_55@bit; nondet rb6_56@bit; nondet rb6_57@bit; nondet rb6_58@bit; nondet rb6_59@bit; nondet rb6_60@bit; nondet rb6_61@bit; nondet rb6_62@bit; nondet rb6_63@bit;
nondet rb7_0@bit; nondet rb7_1@bit; nondet rb7_2@bit; nondet rb7_3@bit; nondet rb7_4@bit; nondet rb7_5@bit; nondet rb7_6@bit; nondet rb7_7@bit; nondet rb7_8@bit; nondet rb7_9@bit; nondet rb7_10@bit; nondet rb7_11@bit; nondet rb7_12@bit; nondet rb7_13@bit; nondet rb7_14@bit; nondet rb7_15@bit; nondet rb7_16@bit; nondet rb7_17@bit; nondet rb7_18@bit; nondet rb7_19@bit; nondet rb7_20@bit; nondet rb7_21@bit; nondet rb7_22@bit; nondet rb7_23@bit; nondet rb7_24@bit; nondet rb7_25@bit; nondet rb7_26@bit; nondet rb7_27@bit; nondet rb7_28@bit; nondet rb7_29@bit; nondet rb7_30@bit; nondet rb7_31@bit; nondet rb7_32@bit; nondet rb7_33@bit; nondet rb7_34@bit; nondet rb7_35@bit; nondet rb7_36@bit; nondet rb7_37@bit; nondet rb7_38@bit; nondet rb7_39@bit; nondet rb7_40@bit; nondet rb7_41@bit; nondet rb7_42@bit; nondet rb7_43@bit; nondet rb7_44@bit; nondet rb7_45@bit; nondet rb7_46@bit; nondet rb7_47@bit; nondet rb7_48@bit; nondet rb7_49@bit; nondet rb7_50@bit; nondet rb7_51@bit; nondet rb7_52@bit; nondet rb7_53@bit; nondet rb7_54@bit; nondet rb7_55@bit; nondet rb7_56@bit; nondet rb7_57@bit; nondet rb7_58@bit; nondet rb7_59@bit; nondet rb7_60@bit; nondet rb7_61@bit; nondet rb7_62@bit; nondet rb7_63@bit;
nondet rb8_0@bit; nondet rb8_1@bit; nondet rb8_2@bit; nondet rb8_3@bit; nondet rb8_4@bit; nondet rb8_5@bit; nondet rb8_6@bit; nondet rb8_7@bit; nondet rb8_8@bit; nondet rb8_9@bit; nondet rb8_10@bit; nondet rb8_11@bit; nondet rb8_12@bit; nondet rb8_13@bit; nondet rb8_14@bit; nondet rb8_15@bit; nondet rb8_16@bit; nondet rb8_17@bit; nondet rb8_18@bit; nondet rb8_19@bit; nondet rb8_20@bit; nondet rb8_21@bit; nondet rb8_22@bit; nondet rb8_23@bit; nondet rb8_24@bit; nondet rb8_25@bit; nondet rb8_26@bit; nondet rb8_27@bit; nondet rb8_28@bit; nondet rb8_29@bit; nondet rb8_30@bit; nondet rb8_31@bit; nondet rb8_32@bit; nondet rb8_33@bit; nondet rb8_34@bit; nondet rb8_35@bit; nondet rb8_36@bit; nondet rb8_37@bit; nondet rb8_38@bit; nondet rb8_39@bit; nondet rb8_40@bit; nondet rb8_41@bit; nondet rb8_42@bit; nondet rb8_43@bit; nondet rb8_44@bit; nondet rb8_45@bit; nondet rb8_46@bit; nondet rb8_47@bit; nondet rb8_48@bit; nondet rb8_49@bit; nondet rb8_50@bit; nondet rb8_51@bit; nondet rb8_52@bit; nondet rb8_53@bit; nondet rb8_54@bit; nondet rb8_55@bit; nondet rb8_56@bit; nondet rb8_57@bit; nondet rb8_58@bit; nondet rb8_59@bit; nondet rb8_60@bit; nondet rb8_61@bit; nondet rb8_62@bit; nondet rb8_63@bit;
nondet rb9_0@bit; nondet rb9_1@bit; nondet rb9_2@bit; nondet rb9_3@bit; nondet rb9_4@bit; nondet rb9_5@bit; nondet rb9_6@bit; nondet rb9_7@bit; nondet rb9_8@bit; nondet rb9_9@bit; nondet rb9_10@bit; nondet rb9_11@bit; nondet rb9_12@bit; nondet rb9_13@bit; nondet rb9_14@bit; nondet rb9_15@bit; nondet rb9_16@bit; nondet rb9_17@bit; nondet rb9_18@bit; nondet rb9_19@bit; nondet rb9_20@bit; nondet rb9_21@bit; nondet rb9_22@bit; nondet rb9_23@bit; nondet rb9_24@bit; nondet rb9_25@bit; nondet rb9_26@bit; nondet rb9_27@bit; nondet rb9_28@bit; nondet rb9_29@bit; nondet rb9_30@bit; nondet rb9_31@bit; nondet rb9_32@bit; nondet rb9_33@bit; nondet rb9_34@bit; nondet rb9_35@bit; nondet rb9_36@bit; nondet rb9_37@bit; nondet rb9_38@bit; nondet rb9_39@bit; nondet rb9_40@bit; nondet rb9_41@bit; nondet rb9_42@bit; nondet rb9_43@bit; nondet rb9_44@bit; nondet rb9_45@bit; nondet rb9_46@bit; nondet rb9_47@bit; nondet rb9_48@bit; nondet rb9_49@bit; nondet rb9_50@bit; nondet rb9_51@bit; nondet rb9_52@bit; nondet rb9_53@bit; nondet rb9_54@bit; nondet rb9_55@bit; nondet rb9_56@bit; nondet rb9_57@bit; nondet rb9_58@bit; nondet rb9_59@bit; nondet rb9_60@bit; nondet rb9_61@bit; nondet rb9_62@bit; nondet rb9_63@bit;
nondet rb10_0@bit; nondet rb10_1@bit; nondet rb10_2@bit; nondet rb10_3@bit; nondet rb10_4@bit; nondet rb10_5@bit; nondet rb10_6@bit; nondet rb10_7@bit; nondet rb10_8@bit; nondet rb10_9@bit; nondet rb10_10@bit; nondet rb10_11@bit; nondet rb10_12@bit; nondet rb10_13@bit; nondet rb10_14@bit; nondet rb10_15@bit; nondet rb10_16@bit; nondet rb10_17@bit; nondet rb10_18@bit; nondet rb10_19@bit; nondet rb10_20@bit; nondet rb10_21@bit; nondet rb10_22@bit; nondet rb10_23@bit; nondet rb10_24@bit; nondet rb10_25@bit; nondet rb10_26@bit; nondet rb10_27@bit; nondet rb10_28@bit; nondet rb10_29@bit; nondet rb10_30@bit; nondet rb10_31@bit; nondet rb10_32@bit; nondet rb10_33@bit; nondet rb10_34@bit; nondet rb10_35@bit; nondet rb10_36@bit; nondet rb10_37@bit; nondet rb10_38@bit; nondet rb10_39@bit; nondet rb10_40@bit; nondet rb10_41@bit; nondet rb10_42@bit; nondet rb10_43@bit; nondet rb10_44@bit; nondet rb10_45@bit; nondet rb10_46@bit; nondet rb10_47@bit; nondet rb10_48@bit; nondet rb10_49@bit; nondet rb10_50@bit; nondet rb10_51@bit; nondet rb10_52@bit; nondet rb10_53@bit; nondet rb10_54@bit; nondet rb10_55@bit; nondet rb10_56@bit; nondet rb10_57@bit; nondet rb10_58@bit; nondet rb10_59@bit; nondet rb10_60@bit; nondet rb10_61@bit; nondet rb10_62@bit; nondet rb10_63@bit;
nondet rb11_0@bit; nondet rb11_1@bit; nondet rb11_2@bit; nondet rb11_3@bit; nondet rb11_4@bit; nondet rb11_5@bit; nondet rb11_6@bit; nondet rb11_7@bit; nondet rb11_8@bit; nondet rb11_9@bit; nondet rb11_10@bit; nondet rb11_11@bit; nondet rb11_12@bit; nondet rb11_13@bit; nondet rb11_14@bit; nondet rb11_15@bit; nondet rb11_16@bit; nondet rb11_17@bit; nondet rb11_18@bit; nondet rb11_19@bit; nondet rb11_20@bit; nondet rb11_21@bit; nondet rb11_22@bit; nondet rb11_23@bit; nondet rb11_24@bit; nondet rb11_25@bit; nondet rb11_26@bit; nondet rb11_27@bit; nondet rb11_28@bit; nondet rb11_29@bit; nondet rb11_30@bit; nondet rb11_31@bit; nondet rb11_32@bit; nondet rb11_33@bit; nondet rb11_34@bit; nondet rb11_35@bit; nondet rb11_36@bit; nondet rb11_37@bit; nondet rb11_38@bit; nondet rb11_39@bit; nondet rb11_40@bit; nondet rb11_41@bit; nondet rb11_42@bit; nondet rb11_43@bit; nondet rb11_44@bit; nondet rb11_45@bit; nondet rb11_46@bit; nondet rb11_47@bit; nondet rb11_48@bit; nondet rb11_49@bit; nondet rb11_50@bit; nondet rb11_51@bit; nondet rb11_52@bit; nondet rb11_53@bit; nondet rb11_54@bit; nondet rb11_55@bit; nondet rb11_56@bit; nondet rb11_57@bit; nondet rb11_58@bit; nondet rb11_59@bit; nondet rb11_60@bit; nondet rb11_61@bit; nondet rb11_62@bit; nondet rb11_63@bit;

nondet x0@uint12;
mov x x0;

ghost input_poly@uint12: input_poly =
  (in0_0 * z ** 0 + in1_0 * z ** 1 + in2_0 * z ** 2 + in3_0 * z ** 3 + in4_0 * z ** 4 + in5_0 * z ** 5 + in6_0 * z ** 6 + in7_0 * z ** 7 + in8_0 * z ** 8 + in9_0 * z ** 9 + in10_0 * z ** 10 + in11_0 * z ** 11) * x ** 0 +
  (in0_1 * z ** 0 + in1_1 * z ** 1 + in2_1 * z ** 2 + in3_1 * z ** 3 + in4_1 * z ** 4 + in5_1 * z ** 5 + in6_1 * z ** 6 + in7_1 * z ** 7 + in8_1 * z ** 8 + in9_1 * z ** 9 + in10_1 * z ** 10 + in11_1 * z ** 11) * x ** 1 +
  (in0_2 * z ** 0 + in1_2 * z ** 1 + in2_2 * z ** 2 + in3_2 * z ** 3 + in4_2 * z ** 4 + in5_2 * z ** 5 + in6_2 * z ** 6 + in7_2 * z ** 7 + in8_2 * z ** 8 + in9_2 * z ** 9 + in10_2 * z ** 10 + in11_2 * z ** 11) * x ** 2 +
  (in0_3 * z ** 0 + in1_3 * z ** 1 + in2_3 * z ** 2 + in3_3 * z ** 3 + in4_3 * z ** 4 + in5_3 * z ** 5 + in6_3 * z ** 6 + in7_3 * z ** 7 + in8_3 * z ** 8 + in9_3 * z ** 9 + in10_3 * z ** 10 + in11_3 * z ** 11) * x ** 3 +
  (in0_4 * z ** 0 + in1_4 * z ** 1 + in2_4 * z ** 2 + in3_4 * z ** 3 + in4_4 * z ** 4 + in5_4 * z ** 5 + in6_4 * z ** 6 + in7_4 * z ** 7 + in8_4 * z ** 8 + in9_4 * z ** 9 + in10_4 * z ** 10 + in11_4 * z ** 11) * x ** 4 +
  (in0_5 * z ** 0 + in1_5 * z ** 1 + in2_5 * z ** 2 + in3_5 * z ** 3 + in4_5 * z ** 4 + in5_5 * z ** 5 + in6_5 * z ** 6 + in7_5 * z ** 7 + in8_5 * z ** 8 + in9_5 * z ** 9 + in10_5 * z ** 10 + in11_5 * z ** 11) * x ** 5 +
  (in0_6 * z ** 0 + in1_6 * z ** 1 + in2_6 * z ** 2 + in3_6 * z ** 3 + in4_6 * z ** 4 + in5_6 * z ** 5 + in6_6 * z ** 6 + in7_6 * z ** 7 + in8_6 * z ** 8 + in9_6 * z ** 9 + in10_6 * z ** 10 + in11_6 * z ** 11) * x ** 6 +
  (in0_7 * z ** 0 + in1_7 * z ** 1 + in2_7 * z ** 2 + in3_7 * z ** 3 + in4_7 * z ** 4 + in5_7 * z ** 5 + in6_7 * z ** 6 + in7_7 * z ** 7 + in8_7 * z ** 8 + in9_7 * z ** 9 + in10_7 * z ** 10 + in11_7 * z ** 11) * x ** 7 +
  (in0_8 * z ** 0 + in1_8 * z ** 1 + in2_8 * z ** 2 + in3_8 * z ** 3 + in4_8 * z ** 4 + in5_8 * z ** 5 + in6_8 * z ** 6 + in7_8 * z ** 7 + in8_8 * z ** 8 + in9_8 * z ** 9 + in10_8 * z ** 10 + in11_8 * z ** 11) * x ** 8 +
  (in0_9 * z ** 0 + in1_9 * z ** 1 + in2_9 * z ** 2 + in3_9 * z ** 3 + in4_9 * z ** 4 + in5_9 * z ** 5 + in6_9 * z ** 6 + in7_9 * z ** 7 + in8_9 * z ** 8 + in9_9 * z ** 9 + in10_9 * z ** 10 + in11_9 * z ** 11) * x ** 9 +
  (in0_10 * z ** 0 + in1_10 * z ** 1 + in2_10 * z ** 2 + in3_10 * z ** 3 + in4_10 * z ** 4 + in5_10 * z ** 5 + in6_10 * z ** 6 + in7_10 * z ** 7 + in8_10 * z ** 8 + in9_10 * z ** 9 + in10_10 * z ** 10 + in11_10 * z ** 11) * x ** 10 +
  (in0_11 * z ** 0 + in1_11 * z ** 1 + in2_11 * z ** 2 + in3_11 * z ** 3 + in4_11 * z ** 4 + in5_11 * z ** 5 + in6_11 * z ** 6 + in7_11 * z ** 7 + in8_11 * z ** 8 + in9_11 * z ** 9 + in10_11 * z ** 10 + in11_11 * z ** 11) * x ** 11 +
  (in0_12 * z ** 0 + in1_12 * z ** 1 + in2_12 * z ** 2 + in3_12 * z ** 3 + in4_12 * z ** 4 + in5_12 * z ** 5 + in6_12 * z ** 6 + in7_12 * z ** 7 + in8_12 * z ** 8 + in9_12 * z ** 9 + in10_12 * z ** 10 + in11_12 * z ** 11) * x ** 12 +
  (in0_13 * z ** 0 + in1_13 * z ** 1 + in2_13 * z ** 2 + in3_13 * z ** 3 + in4_13 * z ** 4 + in5_13 * z ** 5 + in6_13 * z ** 6 + in7_13 * z ** 7 + in8_13 * z ** 8 + in9_13 * z ** 9 + in10_13 * z ** 10 + in11_13 * z ** 11) * x ** 13 +
  (in0_14 * z ** 0 + in1_14 * z ** 1 + in2_14 * z ** 2 + in3_14 * z ** 3 + in4_14 * z ** 4 + in5_14 * z ** 5 + in6_14 * z ** 6 + in7_14 * z ** 7 + in8_14 * z ** 8 + in9_14 * z ** 9 + in10_14 * z ** 10 + in11_14 * z ** 11) * x ** 14 +
  (in0_15 * z ** 0 + in1_15 * z ** 1 + in2_15 * z ** 2 + in3_15 * z ** 3 + in4_15 * z ** 4 + in5_15 * z ** 5 + in6_15 * z ** 6 + in7_15 * z ** 7 + in8_15 * z ** 8 + in9_15 * z ** 9 + in10_15 * z ** 10 + in11_15 * z ** 11) * x ** 15 +
  (in0_16 * z ** 0 + in1_16 * z ** 1 + in2_16 * z ** 2 + in3_16 * z ** 3 + in4_16 * z ** 4 + in5_16 * z ** 5 + in6_16 * z ** 6 + in7_16 * z ** 7 + in8_16 * z ** 8 + in9_16 * z ** 9 + in10_16 * z ** 10 + in11_16 * z ** 11) * x ** 16 +
  (in0_17 * z ** 0 + in1_17 * z ** 1 + in2_17 * z ** 2 + in3_17 * z ** 3 + in4_17 * z ** 4 + in5_17 * z ** 5 + in6_17 * z ** 6 + in7_17 * z ** 7 + in8_17 * z ** 8 + in9_17 * z ** 9 + in10_17 * z ** 10 + in11_17 * z ** 11) * x ** 17 +
  (in0_18 * z ** 0 + in1_18 * z ** 1 + in2_18 * z ** 2 + in3_18 * z ** 3 + in4_18 * z ** 4 + in5_18 * z ** 5 + in6_18 * z ** 6 + in7_18 * z ** 7 + in8_18 * z ** 8 + in9_18 * z ** 9 + in10_18 * z ** 10 + in11_18 * z ** 11) * x ** 18 +
  (in0_19 * z ** 0 + in1_19 * z ** 1 + in2_19 * z ** 2 + in3_19 * z ** 3 + in4_19 * z ** 4 + in5_19 * z ** 5 + in6_19 * z ** 6 + in7_19 * z ** 7 + in8_19 * z ** 8 + in9_19 * z ** 9 + in10_19 * z ** 10 + in11_19 * z ** 11) * x ** 19 +
  (in0_20 * z ** 0 + in1_20 * z ** 1 + in2_20 * z ** 2 + in3_20 * z ** 3 + in4_20 * z ** 4 + in5_20 * z ** 5 + in6_20 * z ** 6 + in7_20 * z ** 7 + in8_20 * z ** 8 + in9_20 * z ** 9 + in10_20 * z ** 10 + in11_20 * z ** 11) * x ** 20 +
  (in0_21 * z ** 0 + in1_21 * z ** 1 + in2_21 * z ** 2 + in3_21 * z ** 3 + in4_21 * z ** 4 + in5_21 * z ** 5 + in6_21 * z ** 6 + in7_21 * z ** 7 + in8_21 * z ** 8 + in9_21 * z ** 9 + in10_21 * z ** 10 + in11_21 * z ** 11) * x ** 21 +
  (in0_22 * z ** 0 + in1_22 * z ** 1 + in2_22 * z ** 2 + in3_22 * z ** 3 + in4_22 * z ** 4 + in5_22 * z ** 5 + in6_22 * z ** 6 + in7_22 * z ** 7 + in8_22 * z ** 8 + in9_22 * z ** 9 + in10_22 * z ** 10 + in11_22 * z ** 11) * x ** 22 +
  (in0_23 * z ** 0 + in1_23 * z ** 1 + in2_23 * z ** 2 + in3_23 * z ** 3 + in4_23 * z ** 4 + in5_23 * z ** 5 + in6_23 * z ** 6 + in7_23 * z ** 7 + in8_23 * z ** 8 + in9_23 * z ** 9 + in10_23 * z ** 10 + in11_23 * z ** 11) * x ** 23 +
  (in0_24 * z ** 0 + in1_24 * z ** 1 + in2_24 * z ** 2 + in3_24 * z ** 3 + in4_24 * z ** 4 + in5_24 * z ** 5 + in6_24 * z ** 6 + in7_24 * z ** 7 + in8_24 * z ** 8 + in9_24 * z ** 9 + in10_24 * z ** 10 + in11_24 * z ** 11) * x ** 24 +
  (in0_25 * z ** 0 + in1_25 * z ** 1 + in2_25 * z ** 2 + in3_25 * z ** 3 + in4_25 * z ** 4 + in5_25 * z ** 5 + in6_25 * z ** 6 + in7_25 * z ** 7 + in8_25 * z ** 8 + in9_25 * z ** 9 + in10_25 * z ** 10 + in11_25 * z ** 11) * x ** 25 +
  (in0_26 * z ** 0 + in1_26 * z ** 1 + in2_26 * z ** 2 + in3_26 * z ** 3 + in4_26 * z ** 4 + in5_26 * z ** 5 + in6_26 * z ** 6 + in7_26 * z ** 7 + in8_26 * z ** 8 + in9_26 * z ** 9 + in10_26 * z ** 10 + in11_26 * z ** 11) * x ** 26 +
  (in0_27 * z ** 0 + in1_27 * z ** 1 + in2_27 * z ** 2 + in3_27 * z ** 3 + in4_27 * z ** 4 + in5_27 * z ** 5 + in6_27 * z ** 6 + in7_27 * z ** 7 + in8_27 * z ** 8 + in9_27 * z ** 9 + in10_27 * z ** 10 + in11_27 * z ** 11) * x ** 27 +
  (in0_28 * z ** 0 + in1_28 * z ** 1 + in2_28 * z ** 2 + in3_28 * z ** 3 + in4_28 * z ** 4 + in5_28 * z ** 5 + in6_28 * z ** 6 + in7_28 * z ** 7 + in8_28 * z ** 8 + in9_28 * z ** 9 + in10_28 * z ** 10 + in11_28 * z ** 11) * x ** 28 +
  (in0_29 * z ** 0 + in1_29 * z ** 1 + in2_29 * z ** 2 + in3_29 * z ** 3 + in4_29 * z ** 4 + in5_29 * z ** 5 + in6_29 * z ** 6 + in7_29 * z ** 7 + in8_29 * z ** 8 + in9_29 * z ** 9 + in10_29 * z ** 10 + in11_29 * z ** 11) * x ** 29 +
  (in0_30 * z ** 0 + in1_30 * z ** 1 + in2_30 * z ** 2 + in3_30 * z ** 3 + in4_30 * z ** 4 + in5_30 * z ** 5 + in6_30 * z ** 6 + in7_30 * z ** 7 + in8_30 * z ** 8 + in9_30 * z ** 9 + in10_30 * z ** 10 + in11_30 * z ** 11) * x ** 30 +
  (in0_31 * z ** 0 + in1_31 * z ** 1 + in2_31 * z ** 2 + in3_31 * z ** 3 + in4_31 * z ** 4 + in5_31 * z ** 5 + in6_31 * z ** 6 + in7_31 * z ** 7 + in8_31 * z ** 8 + in9_31 * z ** 9 + in10_31 * z ** 10 + in11_31 * z ** 11) * x ** 31 +
  (in0_32 * z ** 0 + in1_32 * z ** 1 + in2_32 * z ** 2 + in3_32 * z ** 3 + in4_32 * z ** 4 + in5_32 * z ** 5 + in6_32 * z ** 6 + in7_32 * z ** 7 + in8_32 * z ** 8 + in9_32 * z ** 9 + in10_32 * z ** 10 + in11_32 * z ** 11) * x ** 32 +
  (in0_33 * z ** 0 + in1_33 * z ** 1 + in2_33 * z ** 2 + in3_33 * z ** 3 + in4_33 * z ** 4 + in5_33 * z ** 5 + in6_33 * z ** 6 + in7_33 * z ** 7 + in8_33 * z ** 8 + in9_33 * z ** 9 + in10_33 * z ** 10 + in11_33 * z ** 11) * x ** 33 +
  (in0_34 * z ** 0 + in1_34 * z ** 1 + in2_34 * z ** 2 + in3_34 * z ** 3 + in4_34 * z ** 4 + in5_34 * z ** 5 + in6_34 * z ** 6 + in7_34 * z ** 7 + in8_34 * z ** 8 + in9_34 * z ** 9 + in10_34 * z ** 10 + in11_34 * z ** 11) * x ** 34 +
  (in0_35 * z ** 0 + in1_35 * z ** 1 + in2_35 * z ** 2 + in3_35 * z ** 3 + in4_35 * z ** 4 + in5_35 * z ** 5 + in6_35 * z ** 6 + in7_35 * z ** 7 + in8_35 * z ** 8 + in9_35 * z ** 9 + in10_35 * z ** 10 + in11_35 * z ** 11) * x ** 35 +
  (in0_36 * z ** 0 + in1_36 * z ** 1 + in2_36 * z ** 2 + in3_36 * z ** 3 + in4_36 * z ** 4 + in5_36 * z ** 5 + in6_36 * z ** 6 + in7_36 * z ** 7 + in8_36 * z ** 8 + in9_36 * z ** 9 + in10_36 * z ** 10 + in11_36 * z ** 11) * x ** 36 +
  (in0_37 * z ** 0 + in1_37 * z ** 1 + in2_37 * z ** 2 + in3_37 * z ** 3 + in4_37 * z ** 4 + in5_37 * z ** 5 + in6_37 * z ** 6 + in7_37 * z ** 7 + in8_37 * z ** 8 + in9_37 * z ** 9 + in10_37 * z ** 10 + in11_37 * z ** 11) * x ** 37 +
  (in0_38 * z ** 0 + in1_38 * z ** 1 + in2_38 * z ** 2 + in3_38 * z ** 3 + in4_38 * z ** 4 + in5_38 * z ** 5 + in6_38 * z ** 6 + in7_38 * z ** 7 + in8_38 * z ** 8 + in9_38 * z ** 9 + in10_38 * z ** 10 + in11_38 * z ** 11) * x ** 38 +
  (in0_39 * z ** 0 + in1_39 * z ** 1 + in2_39 * z ** 2 + in3_39 * z ** 3 + in4_39 * z ** 4 + in5_39 * z ** 5 + in6_39 * z ** 6 + in7_39 * z ** 7 + in8_39 * z ** 8 + in9_39 * z ** 9 + in10_39 * z ** 10 + in11_39 * z ** 11) * x ** 39 +
  (in0_40 * z ** 0 + in1_40 * z ** 1 + in2_40 * z ** 2 + in3_40 * z ** 3 + in4_40 * z ** 4 + in5_40 * z ** 5 + in6_40 * z ** 6 + in7_40 * z ** 7 + in8_40 * z ** 8 + in9_40 * z ** 9 + in10_40 * z ** 10 + in11_40 * z ** 11) * x ** 40 +
  (in0_41 * z ** 0 + in1_41 * z ** 1 + in2_41 * z ** 2 + in3_41 * z ** 3 + in4_41 * z ** 4 + in5_41 * z ** 5 + in6_41 * z ** 6 + in7_41 * z ** 7 + in8_41 * z ** 8 + in9_41 * z ** 9 + in10_41 * z ** 10 + in11_41 * z ** 11) * x ** 41 +
  (in0_42 * z ** 0 + in1_42 * z ** 1 + in2_42 * z ** 2 + in3_42 * z ** 3 + in4_42 * z ** 4 + in5_42 * z ** 5 + in6_42 * z ** 6 + in7_42 * z ** 7 + in8_42 * z ** 8 + in9_42 * z ** 9 + in10_42 * z ** 10 + in11_42 * z ** 11) * x ** 42 +
  (in0_43 * z ** 0 + in1_43 * z ** 1 + in2_43 * z ** 2 + in3_43 * z ** 3 + in4_43 * z ** 4 + in5_43 * z ** 5 + in6_43 * z ** 6 + in7_43 * z ** 7 + in8_43 * z ** 8 + in9_43 * z ** 9 + in10_43 * z ** 10 + in11_43 * z ** 11) * x ** 43 +
  (in0_44 * z ** 0 + in1_44 * z ** 1 + in2_44 * z ** 2 + in3_44 * z ** 3 + in4_44 * z ** 4 + in5_44 * z ** 5 + in6_44 * z ** 6 + in7_44 * z ** 7 + in8_44 * z ** 8 + in9_44 * z ** 9 + in10_44 * z ** 10 + in11_44 * z ** 11) * x ** 44 +
  (in0_45 * z ** 0 + in1_45 * z ** 1 + in2_45 * z ** 2 + in3_45 * z ** 3 + in4_45 * z ** 4 + in5_45 * z ** 5 + in6_45 * z ** 6 + in7_45 * z ** 7 + in8_45 * z ** 8 + in9_45 * z ** 9 + in10_45 * z ** 10 + in11_45 * z ** 11) * x ** 45 +
  (in0_46 * z ** 0 + in1_46 * z ** 1 + in2_46 * z ** 2 + in3_46 * z ** 3 + in4_46 * z ** 4 + in5_46 * z ** 5 + in6_46 * z ** 6 + in7_46 * z ** 7 + in8_46 * z ** 8 + in9_46 * z ** 9 + in10_46 * z ** 10 + in11_46 * z ** 11) * x ** 46 +
  (in0_47 * z ** 0 + in1_47 * z ** 1 + in2_47 * z ** 2 + in3_47 * z ** 3 + in4_47 * z ** 4 + in5_47 * z ** 5 + in6_47 * z ** 6 + in7_47 * z ** 7 + in8_47 * z ** 8 + in9_47 * z ** 9 + in10_47 * z ** 10 + in11_47 * z ** 11) * x ** 47 +
  (in0_48 * z ** 0 + in1_48 * z ** 1 + in2_48 * z ** 2 + in3_48 * z ** 3 + in4_48 * z ** 4 + in5_48 * z ** 5 + in6_48 * z ** 6 + in7_48 * z ** 7 + in8_48 * z ** 8 + in9_48 * z ** 9 + in10_48 * z ** 10 + in11_48 * z ** 11) * x ** 48 +
  (in0_49 * z ** 0 + in1_49 * z ** 1 + in2_49 * z ** 2 + in3_49 * z ** 3 + in4_49 * z ** 4 + in5_49 * z ** 5 + in6_49 * z ** 6 + in7_49 * z ** 7 + in8_49 * z ** 8 + in9_49 * z ** 9 + in10_49 * z ** 10 + in11_49 * z ** 11) * x ** 49 +
  (in0_50 * z ** 0 + in1_50 * z ** 1 + in2_50 * z ** 2 + in3_50 * z ** 3 + in4_50 * z ** 4 + in5_50 * z ** 5 + in6_50 * z ** 6 + in7_50 * z ** 7 + in8_50 * z ** 8 + in9_50 * z ** 9 + in10_50 * z ** 10 + in11_50 * z ** 11) * x ** 50 +
  (in0_51 * z ** 0 + in1_51 * z ** 1 + in2_51 * z ** 2 + in3_51 * z ** 3 + in4_51 * z ** 4 + in5_51 * z ** 5 + in6_51 * z ** 6 + in7_51 * z ** 7 + in8_51 * z ** 8 + in9_51 * z ** 9 + in10_51 * z ** 10 + in11_51 * z ** 11) * x ** 51 +
  (in0_52 * z ** 0 + in1_52 * z ** 1 + in2_52 * z ** 2 + in3_52 * z ** 3 + in4_52 * z ** 4 + in5_52 * z ** 5 + in6_52 * z ** 6 + in7_52 * z ** 7 + in8_52 * z ** 8 + in9_52 * z ** 9 + in10_52 * z ** 10 + in11_52 * z ** 11) * x ** 52 +
  (in0_53 * z ** 0 + in1_53 * z ** 1 + in2_53 * z ** 2 + in3_53 * z ** 3 + in4_53 * z ** 4 + in5_53 * z ** 5 + in6_53 * z ** 6 + in7_53 * z ** 7 + in8_53 * z ** 8 + in9_53 * z ** 9 + in10_53 * z ** 10 + in11_53 * z ** 11) * x ** 53 +
  (in0_54 * z ** 0 + in1_54 * z ** 1 + in2_54 * z ** 2 + in3_54 * z ** 3 + in4_54 * z ** 4 + in5_54 * z ** 5 + in6_54 * z ** 6 + in7_54 * z ** 7 + in8_54 * z ** 8 + in9_54 * z ** 9 + in10_54 * z ** 10 + in11_54 * z ** 11) * x ** 54 +
  (in0_55 * z ** 0 + in1_55 * z ** 1 + in2_55 * z ** 2 + in3_55 * z ** 3 + in4_55 * z ** 4 + in5_55 * z ** 5 + in6_55 * z ** 6 + in7_55 * z ** 7 + in8_55 * z ** 8 + in9_55 * z ** 9 + in10_55 * z ** 10 + in11_55 * z ** 11) * x ** 55 +
  (in0_56 * z ** 0 + in1_56 * z ** 1 + in2_56 * z ** 2 + in3_56 * z ** 3 + in4_56 * z ** 4 + in5_56 * z ** 5 + in6_56 * z ** 6 + in7_56 * z ** 7 + in8_56 * z ** 8 + in9_56 * z ** 9 + in10_56 * z ** 10 + in11_56 * z ** 11) * x ** 56 +
  (in0_57 * z ** 0 + in1_57 * z ** 1 + in2_57 * z ** 2 + in3_57 * z ** 3 + in4_57 * z ** 4 + in5_57 * z ** 5 + in6_57 * z ** 6 + in7_57 * z ** 7 + in8_57 * z ** 8 + in9_57 * z ** 9 + in10_57 * z ** 10 + in11_57 * z ** 11) * x ** 57 +
  (in0_58 * z ** 0 + in1_58 * z ** 1 + in2_58 * z ** 2 + in3_58 * z ** 3 + in4_58 * z ** 4 + in5_58 * z ** 5 + in6_58 * z ** 6 + in7_58 * z ** 7 + in8_58 * z ** 8 + in9_58 * z ** 9 + in10_58 * z ** 10 + in11_58 * z ** 11) * x ** 58 +
  (in0_59 * z ** 0 + in1_59 * z ** 1 + in2_59 * z ** 2 + in3_59 * z ** 3 + in4_59 * z ** 4 + in5_59 * z ** 5 + in6_59 * z ** 6 + in7_59 * z ** 7 + in8_59 * z ** 8 + in9_59 * z ** 9 + in10_59 * z ** 10 + in11_59 * z ** 11) * x ** 59 +
  (in0_60 * z ** 0 + in1_60 * z ** 1 + in2_60 * z ** 2 + in3_60 * z ** 3 + in4_60 * z ** 4 + in5_60 * z ** 5 + in6_60 * z ** 6 + in7_60 * z ** 7 + in8_60 * z ** 8 + in9_60 * z ** 9 + in10_60 * z ** 10 + in11_60 * z ** 11) * x ** 60 +
  (in0_61 * z ** 0 + in1_61 * z ** 1 + in2_61 * z ** 2 + in3_61 * z ** 3 + in4_61 * z ** 4 + in5_61 * z ** 5 + in6_61 * z ** 6 + in7_61 * z ** 7 + in8_61 * z ** 8 + in9_61 * z ** 9 + in10_61 * z ** 10 + in11_61 * z ** 11) * x ** 61 +
  (in0_62 * z ** 0 + in1_62 * z ** 1 + in2_62 * z ** 2 + in3_62 * z ** 3 + in4_62 * z ** 4 + in5_62 * z ** 5 + in6_62 * z ** 6 + in7_62 * z ** 7 + in8_62 * z ** 8 + in9_62 * z ** 9 + in10_62 * z ** 10 + in11_62 * z ** 11) * x ** 62 +
  (in0_63 * z ** 0 + in1_63 * z ** 1 + in2_63 * z ** 2 + in3_63 * z ** 3 + in4_63 * z ** 4 + in5_63 * z ** 5 + in6_63 * z ** 6 + in7_63 * z ** 7 + in8_63 * z ** 8 + in9_63 * z ** 9 + in10_63 * z ** 10 + in11_63 * z ** 11) * x ** 63
&& true;

(* mask #0: 0x8888888888888888 *)
mov %rax [0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit];
(* mask #1: 0x0000ffff00000000 *)
mov %r12 [0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* mask #2: 0x8888888888888888 *)
mov %L0x7fffffffd830 [0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit];
(* mask #3: 0x4444444444444444 *)
mov %L0x7fffffffd838 [0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit, 0@bit, 0@bit, 1@bit, 0@bit];
(* mask #4: 0xc0c0c0c0c0c0c0c0 *)
mov %L0x7fffffffd840 [0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit];
(* mask #5: 0x3030303030303030 *)
mov %L0x7fffffffd848 [0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 0@bit, 0@bit];
(* mask #6: 0xf000f000f000f000 *)
mov %L0x7fffffffd850 [0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit];
(* mask #7: 0x0f000f000f000f00 *)
mov %L0x7fffffffd858 [0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* mask #8: 0xff000000ff000000 *)
mov %L0x7fffffffd860 [0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit];
(* mask #9: 0x00ff000000ff0000 *)
mov %L0x7fffffffd868 [0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* mask #10: 0xffff000000000000 *)
mov %L0x7fffffffd870 [0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit, 1@bit];

(* #! -> SP = 0x7fffffffda98 *)
#! 0x7fffffffda98 = 0x7fffffffda98;
(* mov    %rdi,%r15                                #! PC = 0x555555555229 *)
mov r15 rdi;
(* mov    $0x3c,%ecx                               #! PC = 0x55555555522c *)
vpc ecx@uint32 0x3c@uint8; mov cl 0x3c@uint8;
(* xor    %ebp,%ebp                                #! PC = 0x555555555250 *)
mov ebp 0@sint32;
(* mov    %rax,(%rsp)                              #! EA = L0x7fffffffd830; PC = 0x55555555525a *)
mov %L0x7fffffffd830 %rax;
(* mov    %rsp,%r14                                #! PC = 0x555555555263 *)
mov r14 rsp;
(* mov    %r15,%r9                                 #! PC = 0x5555555552e4 *)
mov r9 r15;
(* mov    $0x1,%r8d                                #! PC = 0x5555555552e7 *)
mov r8d 0x1@uint32;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaa0; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaa0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaa0; PC = 0x555555555330 *)
mov %L0x7fffffffdaa0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaa8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaa8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaa8; PC = 0x555555555330 *)
mov %L0x7fffffffdaa8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdab0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdab0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdab0; PC = 0x555555555330 *)
mov %L0x7fffffffdab0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdab8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdab8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdab8; PC = 0x555555555330 *)
mov %L0x7fffffffdab8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdac0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdac0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdac0; PC = 0x555555555330 *)
mov %L0x7fffffffdac0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdac8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdac8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdac8; PC = 0x555555555330 *)
mov %L0x7fffffffdac8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdad0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdad0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdad0; PC = 0x555555555330 *)
mov %L0x7fffffffdad0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdad8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdad8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdad8; PC = 0x555555555330 *)
mov %L0x7fffffffdad8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdae0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdae0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdae0; PC = 0x555555555330 *)
mov %L0x7fffffffdae0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdae8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdae8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdae8; PC = 0x555555555330 *)
mov %L0x7fffffffdae8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaf0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaf0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaf0; PC = 0x555555555330 *)
mov %L0x7fffffffdaf0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaf8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaf8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd838; Value = 0x4444444444444444; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd838;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd830; Value = 0x8888888888888888; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd830;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 0@8;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 1 *)
assert true && cl = 1@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 1 *)
assert true && cl = 1@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaf8; PC = 0x555555555330 *)
mov %L0x7fffffffdaf8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* add    $0x1,%rbp                                #! PC = 0x555555555347 *)
add ebp ebp 0x1@sint32;

mov [out0_0, out0_1, out0_2, out0_3, out0_4, out0_5, out0_6, out0_7, out0_8, out0_9, out0_10, out0_11, out0_12, out0_13, out0_14, out0_15, out0_16, out0_17, out0_18, out0_19, out0_20, out0_21, out0_22, out0_23, out0_24, out0_25, out0_26, out0_27, out0_28, out0_29, out0_30, out0_31, out0_32, out0_33, out0_34, out0_35, out0_36, out0_37, out0_38, out0_39, out0_40, out0_41, out0_42, out0_43, out0_44, out0_45, out0_46, out0_47, out0_48, out0_49, out0_50, out0_51, out0_52, out0_53, out0_54, out0_55, out0_56, out0_57, out0_58, out0_59, out0_60, out0_61, out0_62, out0_63] %L0x7fffffffdaa0;
mov [out1_0, out1_1, out1_2, out1_3, out1_4, out1_5, out1_6, out1_7, out1_8, out1_9, out1_10, out1_11, out1_12, out1_13, out1_14, out1_15, out1_16, out1_17, out1_18, out1_19, out1_20, out1_21, out1_22, out1_23, out1_24, out1_25, out1_26, out1_27, out1_28, out1_29, out1_30, out1_31, out1_32, out1_33, out1_34, out1_35, out1_36, out1_37, out1_38, out1_39, out1_40, out1_41, out1_42, out1_43, out1_44, out1_45, out1_46, out1_47, out1_48, out1_49, out1_50, out1_51, out1_52, out1_53, out1_54, out1_55, out1_56, out1_57, out1_58, out1_59, out1_60, out1_61, out1_62, out1_63] %L0x7fffffffdaa8;
mov [out2_0, out2_1, out2_2, out2_3, out2_4, out2_5, out2_6, out2_7, out2_8, out2_9, out2_10, out2_11, out2_12, out2_13, out2_14, out2_15, out2_16, out2_17, out2_18, out2_19, out2_20, out2_21, out2_22, out2_23, out2_24, out2_25, out2_26, out2_27, out2_28, out2_29, out2_30, out2_31, out2_32, out2_33, out2_34, out2_35, out2_36, out2_37, out2_38, out2_39, out2_40, out2_41, out2_42, out2_43, out2_44, out2_45, out2_46, out2_47, out2_48, out2_49, out2_50, out2_51, out2_52, out2_53, out2_54, out2_55, out2_56, out2_57, out2_58, out2_59, out2_60, out2_61, out2_62, out2_63] %L0x7fffffffdab0;
mov [out3_0, out3_1, out3_2, out3_3, out3_4, out3_5, out3_6, out3_7, out3_8, out3_9, out3_10, out3_11, out3_12, out3_13, out3_14, out3_15, out3_16, out3_17, out3_18, out3_19, out3_20, out3_21, out3_22, out3_23, out3_24, out3_25, out3_26, out3_27, out3_28, out3_29, out3_30, out3_31, out3_32, out3_33, out3_34, out3_35, out3_36, out3_37, out3_38, out3_39, out3_40, out3_41, out3_42, out3_43, out3_44, out3_45, out3_46, out3_47, out3_48, out3_49, out3_50, out3_51, out3_52, out3_53, out3_54, out3_55, out3_56, out3_57, out3_58, out3_59, out3_60, out3_61, out3_62, out3_63] %L0x7fffffffdab8;
mov [out4_0, out4_1, out4_2, out4_3, out4_4, out4_5, out4_6, out4_7, out4_8, out4_9, out4_10, out4_11, out4_12, out4_13, out4_14, out4_15, out4_16, out4_17, out4_18, out4_19, out4_20, out4_21, out4_22, out4_23, out4_24, out4_25, out4_26, out4_27, out4_28, out4_29, out4_30, out4_31, out4_32, out4_33, out4_34, out4_35, out4_36, out4_37, out4_38, out4_39, out4_40, out4_41, out4_42, out4_43, out4_44, out4_45, out4_46, out4_47, out4_48, out4_49, out4_50, out4_51, out4_52, out4_53, out4_54, out4_55, out4_56, out4_57, out4_58, out4_59, out4_60, out4_61, out4_62, out4_63] %L0x7fffffffdac0;
mov [out5_0, out5_1, out5_2, out5_3, out5_4, out5_5, out5_6, out5_7, out5_8, out5_9, out5_10, out5_11, out5_12, out5_13, out5_14, out5_15, out5_16, out5_17, out5_18, out5_19, out5_20, out5_21, out5_22, out5_23, out5_24, out5_25, out5_26, out5_27, out5_28, out5_29, out5_30, out5_31, out5_32, out5_33, out5_34, out5_35, out5_36, out5_37, out5_38, out5_39, out5_40, out5_41, out5_42, out5_43, out5_44, out5_45, out5_46, out5_47, out5_48, out5_49, out5_50, out5_51, out5_52, out5_53, out5_54, out5_55, out5_56, out5_57, out5_58, out5_59, out5_60, out5_61, out5_62, out5_63] %L0x7fffffffdac8;
mov [out6_0, out6_1, out6_2, out6_3, out6_4, out6_5, out6_6, out6_7, out6_8, out6_9, out6_10, out6_11, out6_12, out6_13, out6_14, out6_15, out6_16, out6_17, out6_18, out6_19, out6_20, out6_21, out6_22, out6_23, out6_24, out6_25, out6_26, out6_27, out6_28, out6_29, out6_30, out6_31, out6_32, out6_33, out6_34, out6_35, out6_36, out6_37, out6_38, out6_39, out6_40, out6_41, out6_42, out6_43, out6_44, out6_45, out6_46, out6_47, out6_48, out6_49, out6_50, out6_51, out6_52, out6_53, out6_54, out6_55, out6_56, out6_57, out6_58, out6_59, out6_60, out6_61, out6_62, out6_63] %L0x7fffffffdad0;
mov [out7_0, out7_1, out7_2, out7_3, out7_4, out7_5, out7_6, out7_7, out7_8, out7_9, out7_10, out7_11, out7_12, out7_13, out7_14, out7_15, out7_16, out7_17, out7_18, out7_19, out7_20, out7_21, out7_22, out7_23, out7_24, out7_25, out7_26, out7_27, out7_28, out7_29, out7_30, out7_31, out7_32, out7_33, out7_34, out7_35, out7_36, out7_37, out7_38, out7_39, out7_40, out7_41, out7_42, out7_43, out7_44, out7_45, out7_46, out7_47, out7_48, out7_49, out7_50, out7_51, out7_52, out7_53, out7_54, out7_55, out7_56, out7_57, out7_58, out7_59, out7_60, out7_61, out7_62, out7_63] %L0x7fffffffdad8;
mov [out8_0, out8_1, out8_2, out8_3, out8_4, out8_5, out8_6, out8_7, out8_8, out8_9, out8_10, out8_11, out8_12, out8_13, out8_14, out8_15, out8_16, out8_17, out8_18, out8_19, out8_20, out8_21, out8_22, out8_23, out8_24, out8_25, out8_26, out8_27, out8_28, out8_29, out8_30, out8_31, out8_32, out8_33, out8_34, out8_35, out8_36, out8_37, out8_38, out8_39, out8_40, out8_41, out8_42, out8_43, out8_44, out8_45, out8_46, out8_47, out8_48, out8_49, out8_50, out8_51, out8_52, out8_53, out8_54, out8_55, out8_56, out8_57, out8_58, out8_59, out8_60, out8_61, out8_62, out8_63] %L0x7fffffffdae0;
mov [out9_0, out9_1, out9_2, out9_3, out9_4, out9_5, out9_6, out9_7, out9_8, out9_9, out9_10, out9_11, out9_12, out9_13, out9_14, out9_15, out9_16, out9_17, out9_18, out9_19, out9_20, out9_21, out9_22, out9_23, out9_24, out9_25, out9_26, out9_27, out9_28, out9_29, out9_30, out9_31, out9_32, out9_33, out9_34, out9_35, out9_36, out9_37, out9_38, out9_39, out9_40, out9_41, out9_42, out9_43, out9_44, out9_45, out9_46, out9_47, out9_48, out9_49, out9_50, out9_51, out9_52, out9_53, out9_54, out9_55, out9_56, out9_57, out9_58, out9_59, out9_60, out9_61, out9_62, out9_63] %L0x7fffffffdae8;
mov [out10_0, out10_1, out10_2, out10_3, out10_4, out10_5, out10_6, out10_7, out10_8, out10_9, out10_10, out10_11, out10_12, out10_13, out10_14, out10_15, out10_16, out10_17, out10_18, out10_19, out10_20, out10_21, out10_22, out10_23, out10_24, out10_25, out10_26, out10_27, out10_28, out10_29, out10_30, out10_31, out10_32, out10_33, out10_34, out10_35, out10_36, out10_37, out10_38, out10_39, out10_40, out10_41, out10_42, out10_43, out10_44, out10_45, out10_46, out10_47, out10_48, out10_49, out10_50, out10_51, out10_52, out10_53, out10_54, out10_55, out10_56, out10_57, out10_58, out10_59, out10_60, out10_61, out10_62, out10_63] %L0x7fffffffdaf0;
mov [out11_0, out11_1, out11_2, out11_3, out11_4, out11_5, out11_6, out11_7, out11_8, out11_9, out11_10, out11_11, out11_12, out11_13, out11_14, out11_15, out11_16, out11_17, out11_18, out11_19, out11_20, out11_21, out11_22, out11_23, out11_24, out11_25, out11_26, out11_27, out11_28, out11_29, out11_30, out11_31, out11_32, out11_33, out11_34, out11_35, out11_36, out11_37, out11_38, out11_39, out11_40, out11_41, out11_42, out11_43, out11_44, out11_45, out11_46, out11_47, out11_48, out11_49, out11_50, out11_51, out11_52, out11_53, out11_54, out11_55, out11_56, out11_57, out11_58, out11_59, out11_60, out11_61, out11_62, out11_63] %L0x7fffffffdaf8;

ghost cvrted0_0@uint12: cvrted0_0 = out0_0 * z**0 + out1_0 * z**1 + out2_0 * z**2 + out3_0 * z**3 + out4_0 * z**4 + out5_0 * z**5 + out6_0 * z**6 + out7_0 * z**7 + out8_0 * z**8 + out9_0 * z**9 + out10_0 * z**10 + out11_0 * z**11 && true;
ghost cvrted0_1@uint12: cvrted0_1 = out0_1 * z**0 + out1_1 * z**1 + out2_1 * z**2 + out3_1 * z**3 + out4_1 * z**4 + out5_1 * z**5 + out6_1 * z**6 + out7_1 * z**7 + out8_1 * z**8 + out9_1 * z**9 + out10_1 * z**10 + out11_1 * z**11 && true;
ghost cvrted0_2@uint12: cvrted0_2 = out0_2 * z**0 + out1_2 * z**1 + out2_2 * z**2 + out3_2 * z**3 + out4_2 * z**4 + out5_2 * z**5 + out6_2 * z**6 + out7_2 * z**7 + out8_2 * z**8 + out9_2 * z**9 + out10_2 * z**10 + out11_2 * z**11 && true;
ghost cvrted0_3@uint12: cvrted0_3 = out0_3 * z**0 + out1_3 * z**1 + out2_3 * z**2 + out3_3 * z**3 + out4_3 * z**4 + out5_3 * z**5 + out6_3 * z**6 + out7_3 * z**7 + out8_3 * z**8 + out9_3 * z**9 + out10_3 * z**10 + out11_3 * z**11 && true;
ghost cvrted0_4@uint12: cvrted0_4 = out0_4 * z**0 + out1_4 * z**1 + out2_4 * z**2 + out3_4 * z**3 + out4_4 * z**4 + out5_4 * z**5 + out6_4 * z**6 + out7_4 * z**7 + out8_4 * z**8 + out9_4 * z**9 + out10_4 * z**10 + out11_4 * z**11 && true;
ghost cvrted0_5@uint12: cvrted0_5 = out0_5 * z**0 + out1_5 * z**1 + out2_5 * z**2 + out3_5 * z**3 + out4_5 * z**4 + out5_5 * z**5 + out6_5 * z**6 + out7_5 * z**7 + out8_5 * z**8 + out9_5 * z**9 + out10_5 * z**10 + out11_5 * z**11 && true;
ghost cvrted0_6@uint12: cvrted0_6 = out0_6 * z**0 + out1_6 * z**1 + out2_6 * z**2 + out3_6 * z**3 + out4_6 * z**4 + out5_6 * z**5 + out6_6 * z**6 + out7_6 * z**7 + out8_6 * z**8 + out9_6 * z**9 + out10_6 * z**10 + out11_6 * z**11 && true;
ghost cvrted0_7@uint12: cvrted0_7 = out0_7 * z**0 + out1_7 * z**1 + out2_7 * z**2 + out3_7 * z**3 + out4_7 * z**4 + out5_7 * z**5 + out6_7 * z**6 + out7_7 * z**7 + out8_7 * z**8 + out9_7 * z**9 + out10_7 * z**10 + out11_7 * z**11 && true;
ghost cvrted0_8@uint12: cvrted0_8 = out0_8 * z**0 + out1_8 * z**1 + out2_8 * z**2 + out3_8 * z**3 + out4_8 * z**4 + out5_8 * z**5 + out6_8 * z**6 + out7_8 * z**7 + out8_8 * z**8 + out9_8 * z**9 + out10_8 * z**10 + out11_8 * z**11 && true;
ghost cvrted0_9@uint12: cvrted0_9 = out0_9 * z**0 + out1_9 * z**1 + out2_9 * z**2 + out3_9 * z**3 + out4_9 * z**4 + out5_9 * z**5 + out6_9 * z**6 + out7_9 * z**7 + out8_9 * z**8 + out9_9 * z**9 + out10_9 * z**10 + out11_9 * z**11 && true;
ghost cvrted0_10@uint12: cvrted0_10 = out0_10 * z**0 + out1_10 * z**1 + out2_10 * z**2 + out3_10 * z**3 + out4_10 * z**4 + out5_10 * z**5 + out6_10 * z**6 + out7_10 * z**7 + out8_10 * z**8 + out9_10 * z**9 + out10_10 * z**10 + out11_10 * z**11 && true;
ghost cvrted0_11@uint12: cvrted0_11 = out0_11 * z**0 + out1_11 * z**1 + out2_11 * z**2 + out3_11 * z**3 + out4_11 * z**4 + out5_11 * z**5 + out6_11 * z**6 + out7_11 * z**7 + out8_11 * z**8 + out9_11 * z**9 + out10_11 * z**10 + out11_11 * z**11 && true;
ghost cvrted0_12@uint12: cvrted0_12 = out0_12 * z**0 + out1_12 * z**1 + out2_12 * z**2 + out3_12 * z**3 + out4_12 * z**4 + out5_12 * z**5 + out6_12 * z**6 + out7_12 * z**7 + out8_12 * z**8 + out9_12 * z**9 + out10_12 * z**10 + out11_12 * z**11 && true;
ghost cvrted0_13@uint12: cvrted0_13 = out0_13 * z**0 + out1_13 * z**1 + out2_13 * z**2 + out3_13 * z**3 + out4_13 * z**4 + out5_13 * z**5 + out6_13 * z**6 + out7_13 * z**7 + out8_13 * z**8 + out9_13 * z**9 + out10_13 * z**10 + out11_13 * z**11 && true;
ghost cvrted0_14@uint12: cvrted0_14 = out0_14 * z**0 + out1_14 * z**1 + out2_14 * z**2 + out3_14 * z**3 + out4_14 * z**4 + out5_14 * z**5 + out6_14 * z**6 + out7_14 * z**7 + out8_14 * z**8 + out9_14 * z**9 + out10_14 * z**10 + out11_14 * z**11 && true;
ghost cvrted0_15@uint12: cvrted0_15 = out0_15 * z**0 + out1_15 * z**1 + out2_15 * z**2 + out3_15 * z**3 + out4_15 * z**4 + out5_15 * z**5 + out6_15 * z**6 + out7_15 * z**7 + out8_15 * z**8 + out9_15 * z**9 + out10_15 * z**10 + out11_15 * z**11 && true;
ghost cvrted0_16@uint12: cvrted0_16 = out0_16 * z**0 + out1_16 * z**1 + out2_16 * z**2 + out3_16 * z**3 + out4_16 * z**4 + out5_16 * z**5 + out6_16 * z**6 + out7_16 * z**7 + out8_16 * z**8 + out9_16 * z**9 + out10_16 * z**10 + out11_16 * z**11 && true;
ghost cvrted0_17@uint12: cvrted0_17 = out0_17 * z**0 + out1_17 * z**1 + out2_17 * z**2 + out3_17 * z**3 + out4_17 * z**4 + out5_17 * z**5 + out6_17 * z**6 + out7_17 * z**7 + out8_17 * z**8 + out9_17 * z**9 + out10_17 * z**10 + out11_17 * z**11 && true;
ghost cvrted0_18@uint12: cvrted0_18 = out0_18 * z**0 + out1_18 * z**1 + out2_18 * z**2 + out3_18 * z**3 + out4_18 * z**4 + out5_18 * z**5 + out6_18 * z**6 + out7_18 * z**7 + out8_18 * z**8 + out9_18 * z**9 + out10_18 * z**10 + out11_18 * z**11 && true;
ghost cvrted0_19@uint12: cvrted0_19 = out0_19 * z**0 + out1_19 * z**1 + out2_19 * z**2 + out3_19 * z**3 + out4_19 * z**4 + out5_19 * z**5 + out6_19 * z**6 + out7_19 * z**7 + out8_19 * z**8 + out9_19 * z**9 + out10_19 * z**10 + out11_19 * z**11 && true;
ghost cvrted0_20@uint12: cvrted0_20 = out0_20 * z**0 + out1_20 * z**1 + out2_20 * z**2 + out3_20 * z**3 + out4_20 * z**4 + out5_20 * z**5 + out6_20 * z**6 + out7_20 * z**7 + out8_20 * z**8 + out9_20 * z**9 + out10_20 * z**10 + out11_20 * z**11 && true;
ghost cvrted0_21@uint12: cvrted0_21 = out0_21 * z**0 + out1_21 * z**1 + out2_21 * z**2 + out3_21 * z**3 + out4_21 * z**4 + out5_21 * z**5 + out6_21 * z**6 + out7_21 * z**7 + out8_21 * z**8 + out9_21 * z**9 + out10_21 * z**10 + out11_21 * z**11 && true;
ghost cvrted0_22@uint12: cvrted0_22 = out0_22 * z**0 + out1_22 * z**1 + out2_22 * z**2 + out3_22 * z**3 + out4_22 * z**4 + out5_22 * z**5 + out6_22 * z**6 + out7_22 * z**7 + out8_22 * z**8 + out9_22 * z**9 + out10_22 * z**10 + out11_22 * z**11 && true;
ghost cvrted0_23@uint12: cvrted0_23 = out0_23 * z**0 + out1_23 * z**1 + out2_23 * z**2 + out3_23 * z**3 + out4_23 * z**4 + out5_23 * z**5 + out6_23 * z**6 + out7_23 * z**7 + out8_23 * z**8 + out9_23 * z**9 + out10_23 * z**10 + out11_23 * z**11 && true;
ghost cvrted0_24@uint12: cvrted0_24 = out0_24 * z**0 + out1_24 * z**1 + out2_24 * z**2 + out3_24 * z**3 + out4_24 * z**4 + out5_24 * z**5 + out6_24 * z**6 + out7_24 * z**7 + out8_24 * z**8 + out9_24 * z**9 + out10_24 * z**10 + out11_24 * z**11 && true;
ghost cvrted0_25@uint12: cvrted0_25 = out0_25 * z**0 + out1_25 * z**1 + out2_25 * z**2 + out3_25 * z**3 + out4_25 * z**4 + out5_25 * z**5 + out6_25 * z**6 + out7_25 * z**7 + out8_25 * z**8 + out9_25 * z**9 + out10_25 * z**10 + out11_25 * z**11 && true;
ghost cvrted0_26@uint12: cvrted0_26 = out0_26 * z**0 + out1_26 * z**1 + out2_26 * z**2 + out3_26 * z**3 + out4_26 * z**4 + out5_26 * z**5 + out6_26 * z**6 + out7_26 * z**7 + out8_26 * z**8 + out9_26 * z**9 + out10_26 * z**10 + out11_26 * z**11 && true;
ghost cvrted0_27@uint12: cvrted0_27 = out0_27 * z**0 + out1_27 * z**1 + out2_27 * z**2 + out3_27 * z**3 + out4_27 * z**4 + out5_27 * z**5 + out6_27 * z**6 + out7_27 * z**7 + out8_27 * z**8 + out9_27 * z**9 + out10_27 * z**10 + out11_27 * z**11 && true;
ghost cvrted0_28@uint12: cvrted0_28 = out0_28 * z**0 + out1_28 * z**1 + out2_28 * z**2 + out3_28 * z**3 + out4_28 * z**4 + out5_28 * z**5 + out6_28 * z**6 + out7_28 * z**7 + out8_28 * z**8 + out9_28 * z**9 + out10_28 * z**10 + out11_28 * z**11 && true;
ghost cvrted0_29@uint12: cvrted0_29 = out0_29 * z**0 + out1_29 * z**1 + out2_29 * z**2 + out3_29 * z**3 + out4_29 * z**4 + out5_29 * z**5 + out6_29 * z**6 + out7_29 * z**7 + out8_29 * z**8 + out9_29 * z**9 + out10_29 * z**10 + out11_29 * z**11 && true;
ghost cvrted0_30@uint12: cvrted0_30 = out0_30 * z**0 + out1_30 * z**1 + out2_30 * z**2 + out3_30 * z**3 + out4_30 * z**4 + out5_30 * z**5 + out6_30 * z**6 + out7_30 * z**7 + out8_30 * z**8 + out9_30 * z**9 + out10_30 * z**10 + out11_30 * z**11 && true;
ghost cvrted0_31@uint12: cvrted0_31 = out0_31 * z**0 + out1_31 * z**1 + out2_31 * z**2 + out3_31 * z**3 + out4_31 * z**4 + out5_31 * z**5 + out6_31 * z**6 + out7_31 * z**7 + out8_31 * z**8 + out9_31 * z**9 + out10_31 * z**10 + out11_31 * z**11 && true;
ghost cvrted0_32@uint12: cvrted0_32 = out0_32 * z**0 + out1_32 * z**1 + out2_32 * z**2 + out3_32 * z**3 + out4_32 * z**4 + out5_32 * z**5 + out6_32 * z**6 + out7_32 * z**7 + out8_32 * z**8 + out9_32 * z**9 + out10_32 * z**10 + out11_32 * z**11 && true;
ghost cvrted0_33@uint12: cvrted0_33 = out0_33 * z**0 + out1_33 * z**1 + out2_33 * z**2 + out3_33 * z**3 + out4_33 * z**4 + out5_33 * z**5 + out6_33 * z**6 + out7_33 * z**7 + out8_33 * z**8 + out9_33 * z**9 + out10_33 * z**10 + out11_33 * z**11 && true;
ghost cvrted0_34@uint12: cvrted0_34 = out0_34 * z**0 + out1_34 * z**1 + out2_34 * z**2 + out3_34 * z**3 + out4_34 * z**4 + out5_34 * z**5 + out6_34 * z**6 + out7_34 * z**7 + out8_34 * z**8 + out9_34 * z**9 + out10_34 * z**10 + out11_34 * z**11 && true;
ghost cvrted0_35@uint12: cvrted0_35 = out0_35 * z**0 + out1_35 * z**1 + out2_35 * z**2 + out3_35 * z**3 + out4_35 * z**4 + out5_35 * z**5 + out6_35 * z**6 + out7_35 * z**7 + out8_35 * z**8 + out9_35 * z**9 + out10_35 * z**10 + out11_35 * z**11 && true;
ghost cvrted0_36@uint12: cvrted0_36 = out0_36 * z**0 + out1_36 * z**1 + out2_36 * z**2 + out3_36 * z**3 + out4_36 * z**4 + out5_36 * z**5 + out6_36 * z**6 + out7_36 * z**7 + out8_36 * z**8 + out9_36 * z**9 + out10_36 * z**10 + out11_36 * z**11 && true;
ghost cvrted0_37@uint12: cvrted0_37 = out0_37 * z**0 + out1_37 * z**1 + out2_37 * z**2 + out3_37 * z**3 + out4_37 * z**4 + out5_37 * z**5 + out6_37 * z**6 + out7_37 * z**7 + out8_37 * z**8 + out9_37 * z**9 + out10_37 * z**10 + out11_37 * z**11 && true;
ghost cvrted0_38@uint12: cvrted0_38 = out0_38 * z**0 + out1_38 * z**1 + out2_38 * z**2 + out3_38 * z**3 + out4_38 * z**4 + out5_38 * z**5 + out6_38 * z**6 + out7_38 * z**7 + out8_38 * z**8 + out9_38 * z**9 + out10_38 * z**10 + out11_38 * z**11 && true;
ghost cvrted0_39@uint12: cvrted0_39 = out0_39 * z**0 + out1_39 * z**1 + out2_39 * z**2 + out3_39 * z**3 + out4_39 * z**4 + out5_39 * z**5 + out6_39 * z**6 + out7_39 * z**7 + out8_39 * z**8 + out9_39 * z**9 + out10_39 * z**10 + out11_39 * z**11 && true;
ghost cvrted0_40@uint12: cvrted0_40 = out0_40 * z**0 + out1_40 * z**1 + out2_40 * z**2 + out3_40 * z**3 + out4_40 * z**4 + out5_40 * z**5 + out6_40 * z**6 + out7_40 * z**7 + out8_40 * z**8 + out9_40 * z**9 + out10_40 * z**10 + out11_40 * z**11 && true;
ghost cvrted0_41@uint12: cvrted0_41 = out0_41 * z**0 + out1_41 * z**1 + out2_41 * z**2 + out3_41 * z**3 + out4_41 * z**4 + out5_41 * z**5 + out6_41 * z**6 + out7_41 * z**7 + out8_41 * z**8 + out9_41 * z**9 + out10_41 * z**10 + out11_41 * z**11 && true;
ghost cvrted0_42@uint12: cvrted0_42 = out0_42 * z**0 + out1_42 * z**1 + out2_42 * z**2 + out3_42 * z**3 + out4_42 * z**4 + out5_42 * z**5 + out6_42 * z**6 + out7_42 * z**7 + out8_42 * z**8 + out9_42 * z**9 + out10_42 * z**10 + out11_42 * z**11 && true;
ghost cvrted0_43@uint12: cvrted0_43 = out0_43 * z**0 + out1_43 * z**1 + out2_43 * z**2 + out3_43 * z**3 + out4_43 * z**4 + out5_43 * z**5 + out6_43 * z**6 + out7_43 * z**7 + out8_43 * z**8 + out9_43 * z**9 + out10_43 * z**10 + out11_43 * z**11 && true;
ghost cvrted0_44@uint12: cvrted0_44 = out0_44 * z**0 + out1_44 * z**1 + out2_44 * z**2 + out3_44 * z**3 + out4_44 * z**4 + out5_44 * z**5 + out6_44 * z**6 + out7_44 * z**7 + out8_44 * z**8 + out9_44 * z**9 + out10_44 * z**10 + out11_44 * z**11 && true;
ghost cvrted0_45@uint12: cvrted0_45 = out0_45 * z**0 + out1_45 * z**1 + out2_45 * z**2 + out3_45 * z**3 + out4_45 * z**4 + out5_45 * z**5 + out6_45 * z**6 + out7_45 * z**7 + out8_45 * z**8 + out9_45 * z**9 + out10_45 * z**10 + out11_45 * z**11 && true;
ghost cvrted0_46@uint12: cvrted0_46 = out0_46 * z**0 + out1_46 * z**1 + out2_46 * z**2 + out3_46 * z**3 + out4_46 * z**4 + out5_46 * z**5 + out6_46 * z**6 + out7_46 * z**7 + out8_46 * z**8 + out9_46 * z**9 + out10_46 * z**10 + out11_46 * z**11 && true;
ghost cvrted0_47@uint12: cvrted0_47 = out0_47 * z**0 + out1_47 * z**1 + out2_47 * z**2 + out3_47 * z**3 + out4_47 * z**4 + out5_47 * z**5 + out6_47 * z**6 + out7_47 * z**7 + out8_47 * z**8 + out9_47 * z**9 + out10_47 * z**10 + out11_47 * z**11 && true;
ghost cvrted0_48@uint12: cvrted0_48 = out0_48 * z**0 + out1_48 * z**1 + out2_48 * z**2 + out3_48 * z**3 + out4_48 * z**4 + out5_48 * z**5 + out6_48 * z**6 + out7_48 * z**7 + out8_48 * z**8 + out9_48 * z**9 + out10_48 * z**10 + out11_48 * z**11 && true;
ghost cvrted0_49@uint12: cvrted0_49 = out0_49 * z**0 + out1_49 * z**1 + out2_49 * z**2 + out3_49 * z**3 + out4_49 * z**4 + out5_49 * z**5 + out6_49 * z**6 + out7_49 * z**7 + out8_49 * z**8 + out9_49 * z**9 + out10_49 * z**10 + out11_49 * z**11 && true;
ghost cvrted0_50@uint12: cvrted0_50 = out0_50 * z**0 + out1_50 * z**1 + out2_50 * z**2 + out3_50 * z**3 + out4_50 * z**4 + out5_50 * z**5 + out6_50 * z**6 + out7_50 * z**7 + out8_50 * z**8 + out9_50 * z**9 + out10_50 * z**10 + out11_50 * z**11 && true;
ghost cvrted0_51@uint12: cvrted0_51 = out0_51 * z**0 + out1_51 * z**1 + out2_51 * z**2 + out3_51 * z**3 + out4_51 * z**4 + out5_51 * z**5 + out6_51 * z**6 + out7_51 * z**7 + out8_51 * z**8 + out9_51 * z**9 + out10_51 * z**10 + out11_51 * z**11 && true;
ghost cvrted0_52@uint12: cvrted0_52 = out0_52 * z**0 + out1_52 * z**1 + out2_52 * z**2 + out3_52 * z**3 + out4_52 * z**4 + out5_52 * z**5 + out6_52 * z**6 + out7_52 * z**7 + out8_52 * z**8 + out9_52 * z**9 + out10_52 * z**10 + out11_52 * z**11 && true;
ghost cvrted0_53@uint12: cvrted0_53 = out0_53 * z**0 + out1_53 * z**1 + out2_53 * z**2 + out3_53 * z**3 + out4_53 * z**4 + out5_53 * z**5 + out6_53 * z**6 + out7_53 * z**7 + out8_53 * z**8 + out9_53 * z**9 + out10_53 * z**10 + out11_53 * z**11 && true;
ghost cvrted0_54@uint12: cvrted0_54 = out0_54 * z**0 + out1_54 * z**1 + out2_54 * z**2 + out3_54 * z**3 + out4_54 * z**4 + out5_54 * z**5 + out6_54 * z**6 + out7_54 * z**7 + out8_54 * z**8 + out9_54 * z**9 + out10_54 * z**10 + out11_54 * z**11 && true;
ghost cvrted0_55@uint12: cvrted0_55 = out0_55 * z**0 + out1_55 * z**1 + out2_55 * z**2 + out3_55 * z**3 + out4_55 * z**4 + out5_55 * z**5 + out6_55 * z**6 + out7_55 * z**7 + out8_55 * z**8 + out9_55 * z**9 + out10_55 * z**10 + out11_55 * z**11 && true;
ghost cvrted0_56@uint12: cvrted0_56 = out0_56 * z**0 + out1_56 * z**1 + out2_56 * z**2 + out3_56 * z**3 + out4_56 * z**4 + out5_56 * z**5 + out6_56 * z**6 + out7_56 * z**7 + out8_56 * z**8 + out9_56 * z**9 + out10_56 * z**10 + out11_56 * z**11 && true;
ghost cvrted0_57@uint12: cvrted0_57 = out0_57 * z**0 + out1_57 * z**1 + out2_57 * z**2 + out3_57 * z**3 + out4_57 * z**4 + out5_57 * z**5 + out6_57 * z**6 + out7_57 * z**7 + out8_57 * z**8 + out9_57 * z**9 + out10_57 * z**10 + out11_57 * z**11 && true;
ghost cvrted0_58@uint12: cvrted0_58 = out0_58 * z**0 + out1_58 * z**1 + out2_58 * z**2 + out3_58 * z**3 + out4_58 * z**4 + out5_58 * z**5 + out6_58 * z**6 + out7_58 * z**7 + out8_58 * z**8 + out9_58 * z**9 + out10_58 * z**10 + out11_58 * z**11 && true;
ghost cvrted0_59@uint12: cvrted0_59 = out0_59 * z**0 + out1_59 * z**1 + out2_59 * z**2 + out3_59 * z**3 + out4_59 * z**4 + out5_59 * z**5 + out6_59 * z**6 + out7_59 * z**7 + out8_59 * z**8 + out9_59 * z**9 + out10_59 * z**10 + out11_59 * z**11 && true;
ghost cvrted0_60@uint12: cvrted0_60 = out0_60 * z**0 + out1_60 * z**1 + out2_60 * z**2 + out3_60 * z**3 + out4_60 * z**4 + out5_60 * z**5 + out6_60 * z**6 + out7_60 * z**7 + out8_60 * z**8 + out9_60 * z**9 + out10_60 * z**10 + out11_60 * z**11 && true;
ghost cvrted0_61@uint12: cvrted0_61 = out0_61 * z**0 + out1_61 * z**1 + out2_61 * z**2 + out3_61 * z**3 + out4_61 * z**4 + out5_61 * z**5 + out6_61 * z**6 + out7_61 * z**7 + out8_61 * z**8 + out9_61 * z**9 + out10_61 * z**10 + out11_61 * z**11 && true;
ghost cvrted0_62@uint12: cvrted0_62 = out0_62 * z**0 + out1_62 * z**1 + out2_62 * z**2 + out3_62 * z**3 + out4_62 * z**4 + out5_62 * z**5 + out6_62 * z**6 + out7_62 * z**7 + out8_62 * z**8 + out9_62 * z**9 + out10_62 * z**10 + out11_62 * z**11 && true;
ghost cvrted0_63@uint12: cvrted0_63 = out0_63 * z**0 + out1_63 * z**1 + out2_63 * z**2 + out3_63 * z**3 + out4_63 * z**4 + out5_63 * z**5 + out6_63 * z**6 + out7_63 * z**7 + out8_63 * z**8 + out9_63 * z**9 + out10_63 * z**10 + out11_63 * z**11 && true;

ecut eqmod input_poly (
  (cvrted0_0 + x * cvrted0_1) * (x ** 2 + x) ** 0 +
  (cvrted0_2 + x * cvrted0_3) * (x ** 2 + x) ** 1 +
  (cvrted0_4 + x * cvrted0_5) * (x ** 2 + x) ** 2 +
  (cvrted0_6 + x * cvrted0_7) * (x ** 2 + x) ** 3 +
  (cvrted0_8 + x * cvrted0_9) * (x ** 2 + x) ** 4 +
  (cvrted0_10 + x * cvrted0_11) * (x ** 2 + x) ** 5 +
  (cvrted0_12 + x * cvrted0_13) * (x ** 2 + x) ** 6 +
  (cvrted0_14 + x * cvrted0_15) * (x ** 2 + x) ** 7 +
  (cvrted0_16 + x * cvrted0_17) * (x ** 2 + x) ** 8 +
  (cvrted0_18 + x * cvrted0_19) * (x ** 2 + x) ** 9 +
  (cvrted0_20 + x * cvrted0_21) * (x ** 2 + x) ** 10 +
  (cvrted0_22 + x * cvrted0_23) * (x ** 2 + x) ** 11 +
  (cvrted0_24 + x * cvrted0_25) * (x ** 2 + x) ** 12 +
  (cvrted0_26 + x * cvrted0_27) * (x ** 2 + x) ** 13 +
  (cvrted0_28 + x * cvrted0_29) * (x ** 2 + x) ** 14 +
  (cvrted0_30 + x * cvrted0_31) * (x ** 2 + x) ** 15 +
  (cvrted0_32 + x * cvrted0_33) * (x ** 2 + x) ** 16 +
  (cvrted0_34 + x * cvrted0_35) * (x ** 2 + x) ** 17 +
  (cvrted0_36 + x * cvrted0_37) * (x ** 2 + x) ** 18 +
  (cvrted0_38 + x * cvrted0_39) * (x ** 2 + x) ** 19 +
  (cvrted0_40 + x * cvrted0_41) * (x ** 2 + x) ** 20 +
  (cvrted0_42 + x * cvrted0_43) * (x ** 2 + x) ** 21 +
  (cvrted0_44 + x * cvrted0_45) * (x ** 2 + x) ** 22 +
  (cvrted0_46 + x * cvrted0_47) * (x ** 2 + x) ** 23 +
  (cvrted0_48 + x * cvrted0_49) * (x ** 2 + x) ** 24 +
  (cvrted0_50 + x * cvrted0_51) * (x ** 2 + x) ** 25 +
  (cvrted0_52 + x * cvrted0_53) * (x ** 2 + x) ** 26 +
  (cvrted0_54 + x * cvrted0_55) * (x ** 2 + x) ** 27 +
  (cvrted0_56 + x * cvrted0_57) * (x ** 2 + x) ** 28 +
  (cvrted0_58 + x * cvrted0_59) * (x ** 2 + x) ** 29 +
  (cvrted0_60 + x * cvrted0_61) * (x ** 2 + x) ** 30 +
  (cvrted0_62 + x * cvrted0_63) * (x ** 2 + x) ** 31
) 2;

(* #call   0x555555555380 <_PQCLEAN_MCELIECE348864_AVX_vec_mul_asm>#! PC = 0x555555555352 *)
#call   0x555555555380 <_PQCLEAN_MCELIECE348864_AVX_vec_mul_asm>#! 0x555555555352 = 0x555555555352;
(* #! -> SP = 0x7fffffffd828 *)
#! 0x7fffffffd828 = 0x7fffffffd828;
(* #! <- SP = 0x7fffffffd828 *)
#! 0x7fffffffd828 = 0x7fffffffd828;
(* #ret                                            #! PC = 0x555555555380 *)
#ret                                            #! 0x555555555380 = 0x555555555380;

nondet res0_0@bit; nondet res0_1@bit; nondet res0_2@bit; nondet res0_3@bit;
nondet res0_4@bit; nondet res0_5@bit; nondet res0_6@bit; nondet res0_7@bit;
nondet res0_8@bit; nondet res0_9@bit; nondet res0_10@bit; nondet res0_11@bit;
nondet res0_12@bit; nondet res0_13@bit; nondet res0_14@bit; nondet res0_15@bit;
nondet res0_16@bit; nondet res0_17@bit; nondet res0_18@bit; nondet res0_19@bit;
nondet res0_20@bit; nondet res0_21@bit; nondet res0_22@bit; nondet res0_23@bit;
nondet res0_24@bit; nondet res0_25@bit; nondet res0_26@bit; nondet res0_27@bit;
nondet res0_28@bit; nondet res0_29@bit; nondet res0_30@bit; nondet res0_31@bit;
nondet res0_32@bit; nondet res0_33@bit; nondet res0_34@bit; nondet res0_35@bit;
nondet res0_36@bit; nondet res0_37@bit; nondet res0_38@bit; nondet res0_39@bit;
nondet res0_40@bit; nondet res0_41@bit; nondet res0_42@bit; nondet res0_43@bit;
nondet res0_44@bit; nondet res0_45@bit; nondet res0_46@bit; nondet res0_47@bit;
nondet res0_48@bit; nondet res0_49@bit; nondet res0_50@bit; nondet res0_51@bit;
nondet res0_52@bit; nondet res0_53@bit; nondet res0_54@bit; nondet res0_55@bit;
nondet res0_56@bit; nondet res0_57@bit; nondet res0_58@bit; nondet res0_59@bit;
nondet res0_60@bit; nondet res0_61@bit; nondet res0_62@bit; nondet res0_63@bit;

assume and [
  eqmod res0_0 (cvrted0_0 * ((* 0  0 *) 1)) [2, modulus],
  eqmod res0_1 (cvrted0_1 * ((* 0  1 *) 1)) [2, modulus],
  eqmod res0_2 (cvrted0_2 * ((* 0  2 *) z**2 + z)) [2, modulus],
  eqmod res0_3 (cvrted0_3 * ((* 0  3 *) z**2 + z)) [2, modulus],
  eqmod res0_4 (cvrted0_4 * ((* 0  4 *) z**4 + z**2)) [2, modulus],
  eqmod res0_5 (cvrted0_5 * ((* 0  5 *) z**4 + z**2)) [2, modulus],
  eqmod res0_6 (cvrted0_6 * ((* 0  6 *) z**6 + z**5 + z**4 + z**3)) [2, modulus],
  eqmod res0_7 (cvrted0_7 * ((* 0  7 *) z**6 + z**5 + z**4 + z**3)) [2, modulus],
  eqmod res0_8 (cvrted0_8 * ((* 0  8 *) z**8 + z**4)) [2, modulus],
  eqmod res0_9 (cvrted0_9 * ((* 0  9 *) z**8 + z**4)) [2, modulus],
  eqmod res0_10 (cvrted0_10 * ((* 0 10 *) z**10 + z**9 + z**6 + z**5)) [2, modulus],
  eqmod res0_11 (cvrted0_11 * ((* 0 11 *) z**10 + z**9 + z**6 + z**5)) [2, modulus],
  eqmod res0_12 (cvrted0_12 * ((* 0 12 *) z**10 + z**8 + z**6 + z**3 + 1)) [2, modulus],
  eqmod res0_13 (cvrted0_13 * ((* 0 13 *) z**10 + z**8 + z**6 + z**3 + 1)) [2, modulus],
  eqmod res0_14 (cvrted0_14 * ((* 0 14 *) z**11 + z**10 + z**9 + z**8 + z**7 + z**5 + z**4 + z**3 + z**2 + z + 1)) [2, modulus],
  eqmod res0_15 (cvrted0_15 * ((* 0 15 *) z**11 + z**10 + z**9 + z**8 + z**7 + z**5 + z**4 + z**3 + z**2 + z + 1)) [2, modulus],
  eqmod res0_16 (cvrted0_16 * ((* 0 16 *) z**8 + z**7 + z**4)) [2, modulus],
  eqmod res0_17 (cvrted0_17 * ((* 0 17 *) z**8 + z**7 + z**4)) [2, modulus],
  eqmod res0_18 (cvrted0_18 * ((* 0 18 *) z**10 + z**8 + z**6 + z**5)) [2, modulus],
  eqmod res0_19 (cvrted0_19 * ((* 0 19 *) z**10 + z**8 + z**6 + z**5)) [2, modulus],
  eqmod res0_20 (cvrted0_20 * ((* 0 20 *) z**11 + z**10 + z**9 + z**8 + z**6 + z**3 + 1)) [2, modulus],
  eqmod res0_21 (cvrted0_21 * ((* 0 21 *) z**11 + z**10 + z**9 + z**8 + z**6 + z**3 + 1)) [2, modulus],
  eqmod res0_22 (cvrted0_22 * ((* 0 22 *) z**9 + z**8 + z**7 + z**5 + z**2)) [2, modulus],
  eqmod res0_23 (cvrted0_23 * ((* 0 23 *) z**9 + z**8 + z**7 + z**5 + z**2)) [2, modulus],
  eqmod res0_24 (cvrted0_24 * ((* 0 24 *) z**11 + z**8 + z**7 + z**6 + z**4 + z**3)) [2, modulus],
  eqmod res0_25 (cvrted0_25 * ((* 0 25 *) z**11 + z**8 + z**7 + z**6 + z**4 + z**3)) [2, modulus],
  eqmod res0_26 (cvrted0_26 * ((* 0 26 *) z**10 + z**7 + z**6 + z**3 + z + 1)) [2, modulus],
  eqmod res0_27 (cvrted0_27 * ((* 0 27 *) z**10 + z**7 + z**6 + z**3 + z + 1)) [2, modulus],
  eqmod res0_28 (cvrted0_28 * ((* 0 28 *) z**11 + z**9 + z**7 + z**5 + z**4 + z + 1)) [2, modulus],
  eqmod res0_29 (cvrted0_29 * ((* 0 29 *) z**11 + z**9 + z**7 + z**5 + z**4 + z + 1)) [2, modulus],
  eqmod res0_30 (cvrted0_30 * ((* 0 30 *) z**11 + z**10 + z**9 + z**8 + z**7 + z**5 + z**4 + 1)) [2, modulus],
  eqmod res0_31 (cvrted0_31 * ((* 0 31 *) z**11 + z**10 + z**9 + z**8 + z**7 + z**5 + z**4 + 1)) [2, modulus],
  eqmod res0_32 (cvrted0_32 * ((* 0 32 *) z**8 + z**7 + z**5 + z**4 + z**2)) [2, modulus],
  eqmod res0_33 (cvrted0_33 * ((* 0 33 *) z**8 + z**7 + z**5 + z**4 + z**2)) [2, modulus],
  eqmod res0_34 (cvrted0_34 * ((* 0 34 *) z**10 + z**8 + z**7 + z**5 + z**4 + z**3)) [2, modulus],
  eqmod res0_35 (cvrted0_35 * ((* 0 35 *) z**10 + z**8 + z**7 + z**5 + z**4 + z**3)) [2, modulus],
  eqmod res0_36 (cvrted0_36 * ((* 0 36 *) z**11 + z**10 + z**8 + z**7 + z**4 + z**3 + 1)) [2, modulus],
  eqmod res0_37 (cvrted0_37 * ((* 0 37 *) z**11 + z**10 + z**8 + z**7 + z**4 + z**3 + 1)) [2, modulus],
  eqmod res0_38 (cvrted0_38 * ((* 0 38 *) z**11 + z**10 + z**8 + z**6 + z**2)) [2, modulus],
  eqmod res0_39 (cvrted0_39 * ((* 0 39 *) z**11 + z**10 + z**8 + z**6 + z**2)) [2, modulus],
  eqmod res0_40 (cvrted0_40 * ((* 0 40 *) z**11 + z**10 + z**9 + z**8 + z**7 + z**3 + z)) [2, modulus],
  eqmod res0_41 (cvrted0_41 * ((* 0 41 *) z**11 + z**10 + z**9 + z**8 + z**7 + z**3 + z)) [2, modulus],
  eqmod res0_42 (cvrted0_42 * ((* 0 42 *) z**8 + z**5 + z**3 + z**2 + z)) [2, modulus],
  eqmod res0_43 (cvrted0_43 * ((* 0 43 *) z**8 + z**5 + z**3 + z**2 + z)) [2, modulus],
  eqmod res0_44 (cvrted0_44 * ((* 0 44 *) z**10 + z**9 + z**7 + z**6 + z**5 + z**2)) [2, modulus],
  eqmod res0_45 (cvrted0_45 * ((* 0 45 *) z**10 + z**9 + z**7 + z**6 + z**5 + z**2)) [2, modulus],
  eqmod res0_46 (cvrted0_46 * ((* 0 46 *) z**10 + z**9 + z**6 + z**4 + 1)) [2, modulus],
  eqmod res0_47 (cvrted0_47 * ((* 0 47 *) z**10 + z**9 + z**6 + z**4 + 1)) [2, modulus],
  eqmod res0_48 (cvrted0_48 * ((* 0 48 *) z**10 + z**8 + z**7 + z**6 + z**5 + z**3 + z**2 + z + 1)) [2, modulus],
  eqmod res0_49 (cvrted0_49 * ((* 0 49 *) z**10 + z**8 + z**7 + z**6 + z**5 + z**3 + z**2 + z + 1)) [2, modulus],
  eqmod res0_50 (cvrted0_50 * ((* 0 50 *) z**11 + z**10 + z**6 + z**5 + z**3 + z + 1)) [2, modulus],
  eqmod res0_51 (cvrted0_51 * ((* 0 51 *) z**11 + z**10 + z**6 + z**5 + z**3 + z + 1)) [2, modulus],
  eqmod res0_52 (cvrted0_52 * ((* 0 52 *) z**11 + z**8 + z**6 + z**5 + z**3)) [2, modulus],
  eqmod res0_53 (cvrted0_53 * ((* 0 53 *) z**11 + z**8 + z**6 + z**5 + z**3)) [2, modulus],
  eqmod res0_54 (cvrted0_54 * ((* 0 54 *) z**10 + z**9 + z**8 + z**6 + z**5 + z**3 + z + 1)) [2, modulus],
  eqmod res0_55 (cvrted0_55 * ((* 0 55 *) z**10 + z**9 + z**8 + z**6 + z**5 + z**3 + z + 1)) [2, modulus],
  eqmod res0_56 (cvrted0_56 * ((* 0 56 *) z**9 + z**8 + z**6 + z**5 + z**4 + z + 1)) [2, modulus],
  eqmod res0_57 (cvrted0_57 * ((* 0 57 *) z**9 + z**8 + z**6 + z**5 + z**4 + z + 1)) [2, modulus],
  eqmod res0_58 (cvrted0_58 * ((* 0 58 *) z**11 + z**9 + z**8 + z**5 + z**3 + z)) [2, modulus],
  eqmod res0_59 (cvrted0_59 * ((* 0 59 *) z**11 + z**9 + z**8 + z**5 + z**3 + z)) [2, modulus],
  eqmod res0_60 (cvrted0_60 * ((* 0 60 *) z**11 + z**9 + z**7 + z**6 + z**5 + z**2 + z + 1)) [2, modulus],
  eqmod res0_61 (cvrted0_61 * ((* 0 61 *) z**11 + z**9 + z**7 + z**6 + z**5 + z**2 + z + 1)) [2, modulus],
  eqmod res0_62 (cvrted0_62 * ((* 0 62 *) z**11 + z**10 + z**9 + z**6 + z**3 + 1)) [2, modulus],
  eqmod res0_63 (cvrted0_63 * ((* 0 63 *) z**11 + z**10 + z**9 + z**6 + z**3 + 1)) [2, modulus]
] && true;

nondet x1@uint12;

ecut and [
  eqmod (
    (cvrted0_0 + x * cvrted0_1) * ((z ** 2 + z) * x1) ** 0 +
    (cvrted0_2 + x * cvrted0_3) * ((z ** 2 + z) * x1) ** 1 +
    (cvrted0_4 + x * cvrted0_5) * ((z ** 2 + z) * x1) ** 2 +
    (cvrted0_6 + x * cvrted0_7) * ((z ** 2 + z) * x1) ** 3 +
    (cvrted0_8 + x * cvrted0_9) * ((z ** 2 + z) * x1) ** 4 +
    (cvrted0_10 + x * cvrted0_11) * ((z ** 2 + z) * x1) ** 5 +
    (cvrted0_12 + x * cvrted0_13) * ((z ** 2 + z) * x1) ** 6 +
    (cvrted0_14 + x * cvrted0_15) * ((z ** 2 + z) * x1) ** 7 +
    (cvrted0_16 + x * cvrted0_17) * ((z ** 2 + z) * x1) ** 8 +
    (cvrted0_18 + x * cvrted0_19) * ((z ** 2 + z) * x1) ** 9 +
    (cvrted0_20 + x * cvrted0_21) * ((z ** 2 + z) * x1) ** 10 +
    (cvrted0_22 + x * cvrted0_23) * ((z ** 2 + z) * x1) ** 11 +
    (cvrted0_24 + x * cvrted0_25) * ((z ** 2 + z) * x1) ** 12 +
    (cvrted0_26 + x * cvrted0_27) * ((z ** 2 + z) * x1) ** 13 +
    (cvrted0_28 + x * cvrted0_29) * ((z ** 2 + z) * x1) ** 14 +
    (cvrted0_30 + x * cvrted0_31) * ((z ** 2 + z) * x1) ** 15 +
    (cvrted0_32 + x * cvrted0_33) * ((z ** 2 + z) * x1) ** 16 +
    (cvrted0_34 + x * cvrted0_35) * ((z ** 2 + z) * x1) ** 17 +
    (cvrted0_36 + x * cvrted0_37) * ((z ** 2 + z) * x1) ** 18 +
    (cvrted0_38 + x * cvrted0_39) * ((z ** 2 + z) * x1) ** 19 +
    (cvrted0_40 + x * cvrted0_41) * ((z ** 2 + z) * x1) ** 20 +
    (cvrted0_42 + x * cvrted0_43) * ((z ** 2 + z) * x1) ** 21 +
    (cvrted0_44 + x * cvrted0_45) * ((z ** 2 + z) * x1) ** 22 +
    (cvrted0_46 + x * cvrted0_47) * ((z ** 2 + z) * x1) ** 23 +
    (cvrted0_48 + x * cvrted0_49) * ((z ** 2 + z) * x1) ** 24 +
    (cvrted0_50 + x * cvrted0_51) * ((z ** 2 + z) * x1) ** 25 +
    (cvrted0_52 + x * cvrted0_53) * ((z ** 2 + z) * x1) ** 26 +
    (cvrted0_54 + x * cvrted0_55) * ((z ** 2 + z) * x1) ** 27 +
    (cvrted0_56 + x * cvrted0_57) * ((z ** 2 + z) * x1) ** 28 +
    (cvrted0_58 + x * cvrted0_59) * ((z ** 2 + z) * x1) ** 29 +
    (cvrted0_60 + x * cvrted0_61) * ((z ** 2 + z) * x1) ** 30 +
    (cvrted0_62 + x * cvrted0_63) * ((z ** 2 + z) * x1) ** 31
  ) (
    (res0_0 + x * res0_1) * x1 ** 0 +
    (res0_2 + x * res0_3) * x1 ** 1 +
    (res0_4 + x * res0_5) * x1 ** 2 +
    (res0_6 + x * res0_7) * x1 ** 3 +
    (res0_8 + x * res0_9) * x1 ** 4 +
    (res0_10 + x * res0_11) * x1 ** 5 +
    (res0_12 + x * res0_13) * x1 ** 6 +
    (res0_14 + x * res0_15) * x1 ** 7 +
    (res0_16 + x * res0_17) * x1 ** 8 +
    (res0_18 + x * res0_19) * x1 ** 9 +
    (res0_20 + x * res0_21) * x1 ** 10 +
    (res0_22 + x * res0_23) * x1 ** 11 +
    (res0_24 + x * res0_25) * x1 ** 12 +
    (res0_26 + x * res0_27) * x1 ** 13 +
    (res0_28 + x * res0_29) * x1 ** 14 +
    (res0_30 + x * res0_31) * x1 ** 15 +
    (res0_32 + x * res0_33) * x1 ** 16 +
    (res0_34 + x * res0_35) * x1 ** 17 +
    (res0_36 + x * res0_37) * x1 ** 18 +
    (res0_38 + x * res0_39) * x1 ** 19 +
    (res0_40 + x * res0_41) * x1 ** 20 +
    (res0_42 + x * res0_43) * x1 ** 21 +
    (res0_44 + x * res0_45) * x1 ** 22 +
    (res0_46 + x * res0_47) * x1 ** 23 +
    (res0_48 + x * res0_49) * x1 ** 24 +
    (res0_50 + x * res0_51) * x1 ** 25 +
    (res0_52 + x * res0_53) * x1 ** 26 +
    (res0_54 + x * res0_55) * x1 ** 27 +
    (res0_56 + x * res0_57) * x1 ** 28 +
    (res0_58 + x * res0_59) * x1 ** 29 +
    (res0_60 + x * res0_61) * x1 ** 30 +
    (res0_62 + x * res0_63) * x1 ** 31
  ) [2, modulus]
] prove with [precondition];

mov x x1;

nondet inp1_0@uint12; assume inp1_0 =
  res0_0 * x**0 + res0_2 * x**1 + res0_4 * x**2 + res0_6 * x**3 +
  res0_8 * x**4 + res0_10 * x**5 + res0_12 * x**6 + res0_14 * x**7 +
  res0_16 * x**8 + res0_18 * x**9 + res0_20 * x**10 + res0_22 * x**11 +
  res0_24 * x**12 + res0_26 * x**13 + res0_28 * x**14 + res0_30 * x**15 +
  res0_32 * x**16 + res0_34 * x**17 + res0_36 * x**18 + res0_38 * x**19 +
  res0_40 * x**20 + res0_42 * x**21 + res0_44 * x**22 + res0_46 * x**23 +
  res0_48 * x**24 + res0_50 * x**25 + res0_52 * x**26 + res0_54 * x**27 +
  res0_56 * x**28 + res0_58 * x**29 + res0_60 * x**30 + res0_62 * x**31
&& true;
nondet inp1_1@uint12; assume inp1_1 =
  res0_1 * x**0 + res0_3 * x**1 + res0_5 * x**2 + res0_7 * x**3 +
  res0_9 * x**4 + res0_11 * x**5 + res0_13 * x**6 + res0_15 * x**7 +
  res0_17 * x**8 + res0_19 * x**9 + res0_21 * x**10 + res0_23 * x**11 +
  res0_25 * x**12 + res0_27 * x**13 + res0_29 * x**14 + res0_31 * x**15 +
  res0_33 * x**16 + res0_35 * x**17 + res0_37 * x**18 + res0_39 * x**19 +
  res0_41 * x**20 + res0_43 * x**21 + res0_45 * x**22 + res0_47 * x**23 +
  res0_49 * x**24 + res0_51 * x**25 + res0_53 * x**26 + res0_55 * x**27 +
  res0_57 * x**28 + res0_59 * x**29 + res0_61 * x**30 + res0_63 * x**31
&& true;

nondet rb0_0@bit; nondet rb0_1@bit; nondet rb0_2@bit; nondet rb0_3@bit; nondet rb0_4@bit; nondet rb0_5@bit; nondet rb0_6@bit; nondet rb0_7@bit; nondet rb0_8@bit; nondet rb0_9@bit; nondet rb0_10@bit; nondet rb0_11@bit; nondet rb0_12@bit; nondet rb0_13@bit; nondet rb0_14@bit; nondet rb0_15@bit; nondet rb0_16@bit; nondet rb0_17@bit; nondet rb0_18@bit; nondet rb0_19@bit; nondet rb0_20@bit; nondet rb0_21@bit; nondet rb0_22@bit; nondet rb0_23@bit; nondet rb0_24@bit; nondet rb0_25@bit; nondet rb0_26@bit; nondet rb0_27@bit; nondet rb0_28@bit; nondet rb0_29@bit; nondet rb0_30@bit; nondet rb0_31@bit; nondet rb0_32@bit; nondet rb0_33@bit; nondet rb0_34@bit; nondet rb0_35@bit; nondet rb0_36@bit; nondet rb0_37@bit; nondet rb0_38@bit; nondet rb0_39@bit; nondet rb0_40@bit; nondet rb0_41@bit; nondet rb0_42@bit; nondet rb0_43@bit; nondet rb0_44@bit; nondet rb0_45@bit; nondet rb0_46@bit; nondet rb0_47@bit; nondet rb0_48@bit; nondet rb0_49@bit; nondet rb0_50@bit; nondet rb0_51@bit; nondet rb0_52@bit; nondet rb0_53@bit; nondet rb0_54@bit; nondet rb0_55@bit; nondet rb0_56@bit; nondet rb0_57@bit; nondet rb0_58@bit; nondet rb0_59@bit; nondet rb0_60@bit; nondet rb0_61@bit; nondet rb0_62@bit; nondet rb0_63@bit;
nondet rb1_0@bit; nondet rb1_1@bit; nondet rb1_2@bit; nondet rb1_3@bit; nondet rb1_4@bit; nondet rb1_5@bit; nondet rb1_6@bit; nondet rb1_7@bit; nondet rb1_8@bit; nondet rb1_9@bit; nondet rb1_10@bit; nondet rb1_11@bit; nondet rb1_12@bit; nondet rb1_13@bit; nondet rb1_14@bit; nondet rb1_15@bit; nondet rb1_16@bit; nondet rb1_17@bit; nondet rb1_18@bit; nondet rb1_19@bit; nondet rb1_20@bit; nondet rb1_21@bit; nondet rb1_22@bit; nondet rb1_23@bit; nondet rb1_24@bit; nondet rb1_25@bit; nondet rb1_26@bit; nondet rb1_27@bit; nondet rb1_28@bit; nondet rb1_29@bit; nondet rb1_30@bit; nondet rb1_31@bit; nondet rb1_32@bit; nondet rb1_33@bit; nondet rb1_34@bit; nondet rb1_35@bit; nondet rb1_36@bit; nondet rb1_37@bit; nondet rb1_38@bit; nondet rb1_39@bit; nondet rb1_40@bit; nondet rb1_41@bit; nondet rb1_42@bit; nondet rb1_43@bit; nondet rb1_44@bit; nondet rb1_45@bit; nondet rb1_46@bit; nondet rb1_47@bit; nondet rb1_48@bit; nondet rb1_49@bit; nondet rb1_50@bit; nondet rb1_51@bit; nondet rb1_52@bit; nondet rb1_53@bit; nondet rb1_54@bit; nondet rb1_55@bit; nondet rb1_56@bit; nondet rb1_57@bit; nondet rb1_58@bit; nondet rb1_59@bit; nondet rb1_60@bit; nondet rb1_61@bit; nondet rb1_62@bit; nondet rb1_63@bit;
nondet rb2_0@bit; nondet rb2_1@bit; nondet rb2_2@bit; nondet rb2_3@bit; nondet rb2_4@bit; nondet rb2_5@bit; nondet rb2_6@bit; nondet rb2_7@bit; nondet rb2_8@bit; nondet rb2_9@bit; nondet rb2_10@bit; nondet rb2_11@bit; nondet rb2_12@bit; nondet rb2_13@bit; nondet rb2_14@bit; nondet rb2_15@bit; nondet rb2_16@bit; nondet rb2_17@bit; nondet rb2_18@bit; nondet rb2_19@bit; nondet rb2_20@bit; nondet rb2_21@bit; nondet rb2_22@bit; nondet rb2_23@bit; nondet rb2_24@bit; nondet rb2_25@bit; nondet rb2_26@bit; nondet rb2_27@bit; nondet rb2_28@bit; nondet rb2_29@bit; nondet rb2_30@bit; nondet rb2_31@bit; nondet rb2_32@bit; nondet rb2_33@bit; nondet rb2_34@bit; nondet rb2_35@bit; nondet rb2_36@bit; nondet rb2_37@bit; nondet rb2_38@bit; nondet rb2_39@bit; nondet rb2_40@bit; nondet rb2_41@bit; nondet rb2_42@bit; nondet rb2_43@bit; nondet rb2_44@bit; nondet rb2_45@bit; nondet rb2_46@bit; nondet rb2_47@bit; nondet rb2_48@bit; nondet rb2_49@bit; nondet rb2_50@bit; nondet rb2_51@bit; nondet rb2_52@bit; nondet rb2_53@bit; nondet rb2_54@bit; nondet rb2_55@bit; nondet rb2_56@bit; nondet rb2_57@bit; nondet rb2_58@bit; nondet rb2_59@bit; nondet rb2_60@bit; nondet rb2_61@bit; nondet rb2_62@bit; nondet rb2_63@bit;
nondet rb3_0@bit; nondet rb3_1@bit; nondet rb3_2@bit; nondet rb3_3@bit; nondet rb3_4@bit; nondet rb3_5@bit; nondet rb3_6@bit; nondet rb3_7@bit; nondet rb3_8@bit; nondet rb3_9@bit; nondet rb3_10@bit; nondet rb3_11@bit; nondet rb3_12@bit; nondet rb3_13@bit; nondet rb3_14@bit; nondet rb3_15@bit; nondet rb3_16@bit; nondet rb3_17@bit; nondet rb3_18@bit; nondet rb3_19@bit; nondet rb3_20@bit; nondet rb3_21@bit; nondet rb3_22@bit; nondet rb3_23@bit; nondet rb3_24@bit; nondet rb3_25@bit; nondet rb3_26@bit; nondet rb3_27@bit; nondet rb3_28@bit; nondet rb3_29@bit; nondet rb3_30@bit; nondet rb3_31@bit; nondet rb3_32@bit; nondet rb3_33@bit; nondet rb3_34@bit; nondet rb3_35@bit; nondet rb3_36@bit; nondet rb3_37@bit; nondet rb3_38@bit; nondet rb3_39@bit; nondet rb3_40@bit; nondet rb3_41@bit; nondet rb3_42@bit; nondet rb3_43@bit; nondet rb3_44@bit; nondet rb3_45@bit; nondet rb3_46@bit; nondet rb3_47@bit; nondet rb3_48@bit; nondet rb3_49@bit; nondet rb3_50@bit; nondet rb3_51@bit; nondet rb3_52@bit; nondet rb3_53@bit; nondet rb3_54@bit; nondet rb3_55@bit; nondet rb3_56@bit; nondet rb3_57@bit; nondet rb3_58@bit; nondet rb3_59@bit; nondet rb3_60@bit; nondet rb3_61@bit; nondet rb3_62@bit; nondet rb3_63@bit;
nondet rb4_0@bit; nondet rb4_1@bit; nondet rb4_2@bit; nondet rb4_3@bit; nondet rb4_4@bit; nondet rb4_5@bit; nondet rb4_6@bit; nondet rb4_7@bit; nondet rb4_8@bit; nondet rb4_9@bit; nondet rb4_10@bit; nondet rb4_11@bit; nondet rb4_12@bit; nondet rb4_13@bit; nondet rb4_14@bit; nondet rb4_15@bit; nondet rb4_16@bit; nondet rb4_17@bit; nondet rb4_18@bit; nondet rb4_19@bit; nondet rb4_20@bit; nondet rb4_21@bit; nondet rb4_22@bit; nondet rb4_23@bit; nondet rb4_24@bit; nondet rb4_25@bit; nondet rb4_26@bit; nondet rb4_27@bit; nondet rb4_28@bit; nondet rb4_29@bit; nondet rb4_30@bit; nondet rb4_31@bit; nondet rb4_32@bit; nondet rb4_33@bit; nondet rb4_34@bit; nondet rb4_35@bit; nondet rb4_36@bit; nondet rb4_37@bit; nondet rb4_38@bit; nondet rb4_39@bit; nondet rb4_40@bit; nondet rb4_41@bit; nondet rb4_42@bit; nondet rb4_43@bit; nondet rb4_44@bit; nondet rb4_45@bit; nondet rb4_46@bit; nondet rb4_47@bit; nondet rb4_48@bit; nondet rb4_49@bit; nondet rb4_50@bit; nondet rb4_51@bit; nondet rb4_52@bit; nondet rb4_53@bit; nondet rb4_54@bit; nondet rb4_55@bit; nondet rb4_56@bit; nondet rb4_57@bit; nondet rb4_58@bit; nondet rb4_59@bit; nondet rb4_60@bit; nondet rb4_61@bit; nondet rb4_62@bit; nondet rb4_63@bit;
nondet rb5_0@bit; nondet rb5_1@bit; nondet rb5_2@bit; nondet rb5_3@bit; nondet rb5_4@bit; nondet rb5_5@bit; nondet rb5_6@bit; nondet rb5_7@bit; nondet rb5_8@bit; nondet rb5_9@bit; nondet rb5_10@bit; nondet rb5_11@bit; nondet rb5_12@bit; nondet rb5_13@bit; nondet rb5_14@bit; nondet rb5_15@bit; nondet rb5_16@bit; nondet rb5_17@bit; nondet rb5_18@bit; nondet rb5_19@bit; nondet rb5_20@bit; nondet rb5_21@bit; nondet rb5_22@bit; nondet rb5_23@bit; nondet rb5_24@bit; nondet rb5_25@bit; nondet rb5_26@bit; nondet rb5_27@bit; nondet rb5_28@bit; nondet rb5_29@bit; nondet rb5_30@bit; nondet rb5_31@bit; nondet rb5_32@bit; nondet rb5_33@bit; nondet rb5_34@bit; nondet rb5_35@bit; nondet rb5_36@bit; nondet rb5_37@bit; nondet rb5_38@bit; nondet rb5_39@bit; nondet rb5_40@bit; nondet rb5_41@bit; nondet rb5_42@bit; nondet rb5_43@bit; nondet rb5_44@bit; nondet rb5_45@bit; nondet rb5_46@bit; nondet rb5_47@bit; nondet rb5_48@bit; nondet rb5_49@bit; nondet rb5_50@bit; nondet rb5_51@bit; nondet rb5_52@bit; nondet rb5_53@bit; nondet rb5_54@bit; nondet rb5_55@bit; nondet rb5_56@bit; nondet rb5_57@bit; nondet rb5_58@bit; nondet rb5_59@bit; nondet rb5_60@bit; nondet rb5_61@bit; nondet rb5_62@bit; nondet rb5_63@bit;
nondet rb6_0@bit; nondet rb6_1@bit; nondet rb6_2@bit; nondet rb6_3@bit; nondet rb6_4@bit; nondet rb6_5@bit; nondet rb6_6@bit; nondet rb6_7@bit; nondet rb6_8@bit; nondet rb6_9@bit; nondet rb6_10@bit; nondet rb6_11@bit; nondet rb6_12@bit; nondet rb6_13@bit; nondet rb6_14@bit; nondet rb6_15@bit; nondet rb6_16@bit; nondet rb6_17@bit; nondet rb6_18@bit; nondet rb6_19@bit; nondet rb6_20@bit; nondet rb6_21@bit; nondet rb6_22@bit; nondet rb6_23@bit; nondet rb6_24@bit; nondet rb6_25@bit; nondet rb6_26@bit; nondet rb6_27@bit; nondet rb6_28@bit; nondet rb6_29@bit; nondet rb6_30@bit; nondet rb6_31@bit; nondet rb6_32@bit; nondet rb6_33@bit; nondet rb6_34@bit; nondet rb6_35@bit; nondet rb6_36@bit; nondet rb6_37@bit; nondet rb6_38@bit; nondet rb6_39@bit; nondet rb6_40@bit; nondet rb6_41@bit; nondet rb6_42@bit; nondet rb6_43@bit; nondet rb6_44@bit; nondet rb6_45@bit; nondet rb6_46@bit; nondet rb6_47@bit; nondet rb6_48@bit; nondet rb6_49@bit; nondet rb6_50@bit; nondet rb6_51@bit; nondet rb6_52@bit; nondet rb6_53@bit; nondet rb6_54@bit; nondet rb6_55@bit; nondet rb6_56@bit; nondet rb6_57@bit; nondet rb6_58@bit; nondet rb6_59@bit; nondet rb6_60@bit; nondet rb6_61@bit; nondet rb6_62@bit; nondet rb6_63@bit;
nondet rb7_0@bit; nondet rb7_1@bit; nondet rb7_2@bit; nondet rb7_3@bit; nondet rb7_4@bit; nondet rb7_5@bit; nondet rb7_6@bit; nondet rb7_7@bit; nondet rb7_8@bit; nondet rb7_9@bit; nondet rb7_10@bit; nondet rb7_11@bit; nondet rb7_12@bit; nondet rb7_13@bit; nondet rb7_14@bit; nondet rb7_15@bit; nondet rb7_16@bit; nondet rb7_17@bit; nondet rb7_18@bit; nondet rb7_19@bit; nondet rb7_20@bit; nondet rb7_21@bit; nondet rb7_22@bit; nondet rb7_23@bit; nondet rb7_24@bit; nondet rb7_25@bit; nondet rb7_26@bit; nondet rb7_27@bit; nondet rb7_28@bit; nondet rb7_29@bit; nondet rb7_30@bit; nondet rb7_31@bit; nondet rb7_32@bit; nondet rb7_33@bit; nondet rb7_34@bit; nondet rb7_35@bit; nondet rb7_36@bit; nondet rb7_37@bit; nondet rb7_38@bit; nondet rb7_39@bit; nondet rb7_40@bit; nondet rb7_41@bit; nondet rb7_42@bit; nondet rb7_43@bit; nondet rb7_44@bit; nondet rb7_45@bit; nondet rb7_46@bit; nondet rb7_47@bit; nondet rb7_48@bit; nondet rb7_49@bit; nondet rb7_50@bit; nondet rb7_51@bit; nondet rb7_52@bit; nondet rb7_53@bit; nondet rb7_54@bit; nondet rb7_55@bit; nondet rb7_56@bit; nondet rb7_57@bit; nondet rb7_58@bit; nondet rb7_59@bit; nondet rb7_60@bit; nondet rb7_61@bit; nondet rb7_62@bit; nondet rb7_63@bit;
nondet rb8_0@bit; nondet rb8_1@bit; nondet rb8_2@bit; nondet rb8_3@bit; nondet rb8_4@bit; nondet rb8_5@bit; nondet rb8_6@bit; nondet rb8_7@bit; nondet rb8_8@bit; nondet rb8_9@bit; nondet rb8_10@bit; nondet rb8_11@bit; nondet rb8_12@bit; nondet rb8_13@bit; nondet rb8_14@bit; nondet rb8_15@bit; nondet rb8_16@bit; nondet rb8_17@bit; nondet rb8_18@bit; nondet rb8_19@bit; nondet rb8_20@bit; nondet rb8_21@bit; nondet rb8_22@bit; nondet rb8_23@bit; nondet rb8_24@bit; nondet rb8_25@bit; nondet rb8_26@bit; nondet rb8_27@bit; nondet rb8_28@bit; nondet rb8_29@bit; nondet rb8_30@bit; nondet rb8_31@bit; nondet rb8_32@bit; nondet rb8_33@bit; nondet rb8_34@bit; nondet rb8_35@bit; nondet rb8_36@bit; nondet rb8_37@bit; nondet rb8_38@bit; nondet rb8_39@bit; nondet rb8_40@bit; nondet rb8_41@bit; nondet rb8_42@bit; nondet rb8_43@bit; nondet rb8_44@bit; nondet rb8_45@bit; nondet rb8_46@bit; nondet rb8_47@bit; nondet rb8_48@bit; nondet rb8_49@bit; nondet rb8_50@bit; nondet rb8_51@bit; nondet rb8_52@bit; nondet rb8_53@bit; nondet rb8_54@bit; nondet rb8_55@bit; nondet rb8_56@bit; nondet rb8_57@bit; nondet rb8_58@bit; nondet rb8_59@bit; nondet rb8_60@bit; nondet rb8_61@bit; nondet rb8_62@bit; nondet rb8_63@bit;
nondet rb9_0@bit; nondet rb9_1@bit; nondet rb9_2@bit; nondet rb9_3@bit; nondet rb9_4@bit; nondet rb9_5@bit; nondet rb9_6@bit; nondet rb9_7@bit; nondet rb9_8@bit; nondet rb9_9@bit; nondet rb9_10@bit; nondet rb9_11@bit; nondet rb9_12@bit; nondet rb9_13@bit; nondet rb9_14@bit; nondet rb9_15@bit; nondet rb9_16@bit; nondet rb9_17@bit; nondet rb9_18@bit; nondet rb9_19@bit; nondet rb9_20@bit; nondet rb9_21@bit; nondet rb9_22@bit; nondet rb9_23@bit; nondet rb9_24@bit; nondet rb9_25@bit; nondet rb9_26@bit; nondet rb9_27@bit; nondet rb9_28@bit; nondet rb9_29@bit; nondet rb9_30@bit; nondet rb9_31@bit; nondet rb9_32@bit; nondet rb9_33@bit; nondet rb9_34@bit; nondet rb9_35@bit; nondet rb9_36@bit; nondet rb9_37@bit; nondet rb9_38@bit; nondet rb9_39@bit; nondet rb9_40@bit; nondet rb9_41@bit; nondet rb9_42@bit; nondet rb9_43@bit; nondet rb9_44@bit; nondet rb9_45@bit; nondet rb9_46@bit; nondet rb9_47@bit; nondet rb9_48@bit; nondet rb9_49@bit; nondet rb9_50@bit; nondet rb9_51@bit; nondet rb9_52@bit; nondet rb9_53@bit; nondet rb9_54@bit; nondet rb9_55@bit; nondet rb9_56@bit; nondet rb9_57@bit; nondet rb9_58@bit; nondet rb9_59@bit; nondet rb9_60@bit; nondet rb9_61@bit; nondet rb9_62@bit; nondet rb9_63@bit;
nondet rb10_0@bit; nondet rb10_1@bit; nondet rb10_2@bit; nondet rb10_3@bit; nondet rb10_4@bit; nondet rb10_5@bit; nondet rb10_6@bit; nondet rb10_7@bit; nondet rb10_8@bit; nondet rb10_9@bit; nondet rb10_10@bit; nondet rb10_11@bit; nondet rb10_12@bit; nondet rb10_13@bit; nondet rb10_14@bit; nondet rb10_15@bit; nondet rb10_16@bit; nondet rb10_17@bit; nondet rb10_18@bit; nondet rb10_19@bit; nondet rb10_20@bit; nondet rb10_21@bit; nondet rb10_22@bit; nondet rb10_23@bit; nondet rb10_24@bit; nondet rb10_25@bit; nondet rb10_26@bit; nondet rb10_27@bit; nondet rb10_28@bit; nondet rb10_29@bit; nondet rb10_30@bit; nondet rb10_31@bit; nondet rb10_32@bit; nondet rb10_33@bit; nondet rb10_34@bit; nondet rb10_35@bit; nondet rb10_36@bit; nondet rb10_37@bit; nondet rb10_38@bit; nondet rb10_39@bit; nondet rb10_40@bit; nondet rb10_41@bit; nondet rb10_42@bit; nondet rb10_43@bit; nondet rb10_44@bit; nondet rb10_45@bit; nondet rb10_46@bit; nondet rb10_47@bit; nondet rb10_48@bit; nondet rb10_49@bit; nondet rb10_50@bit; nondet rb10_51@bit; nondet rb10_52@bit; nondet rb10_53@bit; nondet rb10_54@bit; nondet rb10_55@bit; nondet rb10_56@bit; nondet rb10_57@bit; nondet rb10_58@bit; nondet rb10_59@bit; nondet rb10_60@bit; nondet rb10_61@bit; nondet rb10_62@bit; nondet rb10_63@bit;
nondet rb11_0@bit; nondet rb11_1@bit; nondet rb11_2@bit; nondet rb11_3@bit; nondet rb11_4@bit; nondet rb11_5@bit; nondet rb11_6@bit; nondet rb11_7@bit; nondet rb11_8@bit; nondet rb11_9@bit; nondet rb11_10@bit; nondet rb11_11@bit; nondet rb11_12@bit; nondet rb11_13@bit; nondet rb11_14@bit; nondet rb11_15@bit; nondet rb11_16@bit; nondet rb11_17@bit; nondet rb11_18@bit; nondet rb11_19@bit; nondet rb11_20@bit; nondet rb11_21@bit; nondet rb11_22@bit; nondet rb11_23@bit; nondet rb11_24@bit; nondet rb11_25@bit; nondet rb11_26@bit; nondet rb11_27@bit; nondet rb11_28@bit; nondet rb11_29@bit; nondet rb11_30@bit; nondet rb11_31@bit; nondet rb11_32@bit; nondet rb11_33@bit; nondet rb11_34@bit; nondet rb11_35@bit; nondet rb11_36@bit; nondet rb11_37@bit; nondet rb11_38@bit; nondet rb11_39@bit; nondet rb11_40@bit; nondet rb11_41@bit; nondet rb11_42@bit; nondet rb11_43@bit; nondet rb11_44@bit; nondet rb11_45@bit; nondet rb11_46@bit; nondet rb11_47@bit; nondet rb11_48@bit; nondet rb11_49@bit; nondet rb11_50@bit; nondet rb11_51@bit; nondet rb11_52@bit; nondet rb11_53@bit; nondet rb11_54@bit; nondet rb11_55@bit; nondet rb11_56@bit; nondet rb11_57@bit; nondet rb11_58@bit; nondet rb11_59@bit; nondet rb11_60@bit; nondet rb11_61@bit; nondet rb11_62@bit; nondet rb11_63@bit;
assume and [
  res0_0 = rb0_0 * z**0 + rb1_0 * z**1 + rb2_0 * z**2 + rb3_0 * z**3 + rb4_0 * z**4 + rb5_0 * z**5 + rb6_0 * z**6 + rb7_0 * z**7 + rb8_0 * z**8 + rb9_0 * z**9 + rb10_0 * z**10 + rb11_0 * z**11,
  res0_1 = rb0_1 * z**0 + rb1_1 * z**1 + rb2_1 * z**2 + rb3_1 * z**3 + rb4_1 * z**4 + rb5_1 * z**5 + rb6_1 * z**6 + rb7_1 * z**7 + rb8_1 * z**8 + rb9_1 * z**9 + rb10_1 * z**10 + rb11_1 * z**11,
  res0_2 = rb0_2 * z**0 + rb1_2 * z**1 + rb2_2 * z**2 + rb3_2 * z**3 + rb4_2 * z**4 + rb5_2 * z**5 + rb6_2 * z**6 + rb7_2 * z**7 + rb8_2 * z**8 + rb9_2 * z**9 + rb10_2 * z**10 + rb11_2 * z**11,
  res0_3 = rb0_3 * z**0 + rb1_3 * z**1 + rb2_3 * z**2 + rb3_3 * z**3 + rb4_3 * z**4 + rb5_3 * z**5 + rb6_3 * z**6 + rb7_3 * z**7 + rb8_3 * z**8 + rb9_3 * z**9 + rb10_3 * z**10 + rb11_3 * z**11,
  res0_4 = rb0_4 * z**0 + rb1_4 * z**1 + rb2_4 * z**2 + rb3_4 * z**3 + rb4_4 * z**4 + rb5_4 * z**5 + rb6_4 * z**6 + rb7_4 * z**7 + rb8_4 * z**8 + rb9_4 * z**9 + rb10_4 * z**10 + rb11_4 * z**11,
  res0_5 = rb0_5 * z**0 + rb1_5 * z**1 + rb2_5 * z**2 + rb3_5 * z**3 + rb4_5 * z**4 + rb5_5 * z**5 + rb6_5 * z**6 + rb7_5 * z**7 + rb8_5 * z**8 + rb9_5 * z**9 + rb10_5 * z**10 + rb11_5 * z**11,
  res0_6 = rb0_6 * z**0 + rb1_6 * z**1 + rb2_6 * z**2 + rb3_6 * z**3 + rb4_6 * z**4 + rb5_6 * z**5 + rb6_6 * z**6 + rb7_6 * z**7 + rb8_6 * z**8 + rb9_6 * z**9 + rb10_6 * z**10 + rb11_6 * z**11,
  res0_7 = rb0_7 * z**0 + rb1_7 * z**1 + rb2_7 * z**2 + rb3_7 * z**3 + rb4_7 * z**4 + rb5_7 * z**5 + rb6_7 * z**6 + rb7_7 * z**7 + rb8_7 * z**8 + rb9_7 * z**9 + rb10_7 * z**10 + rb11_7 * z**11,
  res0_8 = rb0_8 * z**0 + rb1_8 * z**1 + rb2_8 * z**2 + rb3_8 * z**3 + rb4_8 * z**4 + rb5_8 * z**5 + rb6_8 * z**6 + rb7_8 * z**7 + rb8_8 * z**8 + rb9_8 * z**9 + rb10_8 * z**10 + rb11_8 * z**11,
  res0_9 = rb0_9 * z**0 + rb1_9 * z**1 + rb2_9 * z**2 + rb3_9 * z**3 + rb4_9 * z**4 + rb5_9 * z**5 + rb6_9 * z**6 + rb7_9 * z**7 + rb8_9 * z**8 + rb9_9 * z**9 + rb10_9 * z**10 + rb11_9 * z**11,
  res0_10 = rb0_10 * z**0 + rb1_10 * z**1 + rb2_10 * z**2 + rb3_10 * z**3 + rb4_10 * z**4 + rb5_10 * z**5 + rb6_10 * z**6 + rb7_10 * z**7 + rb8_10 * z**8 + rb9_10 * z**9 + rb10_10 * z**10 + rb11_10 * z**11,
  res0_11 = rb0_11 * z**0 + rb1_11 * z**1 + rb2_11 * z**2 + rb3_11 * z**3 + rb4_11 * z**4 + rb5_11 * z**5 + rb6_11 * z**6 + rb7_11 * z**7 + rb8_11 * z**8 + rb9_11 * z**9 + rb10_11 * z**10 + rb11_11 * z**11,
  res0_12 = rb0_12 * z**0 + rb1_12 * z**1 + rb2_12 * z**2 + rb3_12 * z**3 + rb4_12 * z**4 + rb5_12 * z**5 + rb6_12 * z**6 + rb7_12 * z**7 + rb8_12 * z**8 + rb9_12 * z**9 + rb10_12 * z**10 + rb11_12 * z**11,
  res0_13 = rb0_13 * z**0 + rb1_13 * z**1 + rb2_13 * z**2 + rb3_13 * z**3 + rb4_13 * z**4 + rb5_13 * z**5 + rb6_13 * z**6 + rb7_13 * z**7 + rb8_13 * z**8 + rb9_13 * z**9 + rb10_13 * z**10 + rb11_13 * z**11,
  res0_14 = rb0_14 * z**0 + rb1_14 * z**1 + rb2_14 * z**2 + rb3_14 * z**3 + rb4_14 * z**4 + rb5_14 * z**5 + rb6_14 * z**6 + rb7_14 * z**7 + rb8_14 * z**8 + rb9_14 * z**9 + rb10_14 * z**10 + rb11_14 * z**11,
  res0_15 = rb0_15 * z**0 + rb1_15 * z**1 + rb2_15 * z**2 + rb3_15 * z**3 + rb4_15 * z**4 + rb5_15 * z**5 + rb6_15 * z**6 + rb7_15 * z**7 + rb8_15 * z**8 + rb9_15 * z**9 + rb10_15 * z**10 + rb11_15 * z**11,
  res0_16 = rb0_16 * z**0 + rb1_16 * z**1 + rb2_16 * z**2 + rb3_16 * z**3 + rb4_16 * z**4 + rb5_16 * z**5 + rb6_16 * z**6 + rb7_16 * z**7 + rb8_16 * z**8 + rb9_16 * z**9 + rb10_16 * z**10 + rb11_16 * z**11,
  res0_17 = rb0_17 * z**0 + rb1_17 * z**1 + rb2_17 * z**2 + rb3_17 * z**3 + rb4_17 * z**4 + rb5_17 * z**5 + rb6_17 * z**6 + rb7_17 * z**7 + rb8_17 * z**8 + rb9_17 * z**9 + rb10_17 * z**10 + rb11_17 * z**11,
  res0_18 = rb0_18 * z**0 + rb1_18 * z**1 + rb2_18 * z**2 + rb3_18 * z**3 + rb4_18 * z**4 + rb5_18 * z**5 + rb6_18 * z**6 + rb7_18 * z**7 + rb8_18 * z**8 + rb9_18 * z**9 + rb10_18 * z**10 + rb11_18 * z**11,
  res0_19 = rb0_19 * z**0 + rb1_19 * z**1 + rb2_19 * z**2 + rb3_19 * z**3 + rb4_19 * z**4 + rb5_19 * z**5 + rb6_19 * z**6 + rb7_19 * z**7 + rb8_19 * z**8 + rb9_19 * z**9 + rb10_19 * z**10 + rb11_19 * z**11,
  res0_20 = rb0_20 * z**0 + rb1_20 * z**1 + rb2_20 * z**2 + rb3_20 * z**3 + rb4_20 * z**4 + rb5_20 * z**5 + rb6_20 * z**6 + rb7_20 * z**7 + rb8_20 * z**8 + rb9_20 * z**9 + rb10_20 * z**10 + rb11_20 * z**11,
  res0_21 = rb0_21 * z**0 + rb1_21 * z**1 + rb2_21 * z**2 + rb3_21 * z**3 + rb4_21 * z**4 + rb5_21 * z**5 + rb6_21 * z**6 + rb7_21 * z**7 + rb8_21 * z**8 + rb9_21 * z**9 + rb10_21 * z**10 + rb11_21 * z**11,
  res0_22 = rb0_22 * z**0 + rb1_22 * z**1 + rb2_22 * z**2 + rb3_22 * z**3 + rb4_22 * z**4 + rb5_22 * z**5 + rb6_22 * z**6 + rb7_22 * z**7 + rb8_22 * z**8 + rb9_22 * z**9 + rb10_22 * z**10 + rb11_22 * z**11,
  res0_23 = rb0_23 * z**0 + rb1_23 * z**1 + rb2_23 * z**2 + rb3_23 * z**3 + rb4_23 * z**4 + rb5_23 * z**5 + rb6_23 * z**6 + rb7_23 * z**7 + rb8_23 * z**8 + rb9_23 * z**9 + rb10_23 * z**10 + rb11_23 * z**11,
  res0_24 = rb0_24 * z**0 + rb1_24 * z**1 + rb2_24 * z**2 + rb3_24 * z**3 + rb4_24 * z**4 + rb5_24 * z**5 + rb6_24 * z**6 + rb7_24 * z**7 + rb8_24 * z**8 + rb9_24 * z**9 + rb10_24 * z**10 + rb11_24 * z**11,
  res0_25 = rb0_25 * z**0 + rb1_25 * z**1 + rb2_25 * z**2 + rb3_25 * z**3 + rb4_25 * z**4 + rb5_25 * z**5 + rb6_25 * z**6 + rb7_25 * z**7 + rb8_25 * z**8 + rb9_25 * z**9 + rb10_25 * z**10 + rb11_25 * z**11,
  res0_26 = rb0_26 * z**0 + rb1_26 * z**1 + rb2_26 * z**2 + rb3_26 * z**3 + rb4_26 * z**4 + rb5_26 * z**5 + rb6_26 * z**6 + rb7_26 * z**7 + rb8_26 * z**8 + rb9_26 * z**9 + rb10_26 * z**10 + rb11_26 * z**11,
  res0_27 = rb0_27 * z**0 + rb1_27 * z**1 + rb2_27 * z**2 + rb3_27 * z**3 + rb4_27 * z**4 + rb5_27 * z**5 + rb6_27 * z**6 + rb7_27 * z**7 + rb8_27 * z**8 + rb9_27 * z**9 + rb10_27 * z**10 + rb11_27 * z**11,
  res0_28 = rb0_28 * z**0 + rb1_28 * z**1 + rb2_28 * z**2 + rb3_28 * z**3 + rb4_28 * z**4 + rb5_28 * z**5 + rb6_28 * z**6 + rb7_28 * z**7 + rb8_28 * z**8 + rb9_28 * z**9 + rb10_28 * z**10 + rb11_28 * z**11,
  res0_29 = rb0_29 * z**0 + rb1_29 * z**1 + rb2_29 * z**2 + rb3_29 * z**3 + rb4_29 * z**4 + rb5_29 * z**5 + rb6_29 * z**6 + rb7_29 * z**7 + rb8_29 * z**8 + rb9_29 * z**9 + rb10_29 * z**10 + rb11_29 * z**11,
  res0_30 = rb0_30 * z**0 + rb1_30 * z**1 + rb2_30 * z**2 + rb3_30 * z**3 + rb4_30 * z**4 + rb5_30 * z**5 + rb6_30 * z**6 + rb7_30 * z**7 + rb8_30 * z**8 + rb9_30 * z**9 + rb10_30 * z**10 + rb11_30 * z**11,
  res0_31 = rb0_31 * z**0 + rb1_31 * z**1 + rb2_31 * z**2 + rb3_31 * z**3 + rb4_31 * z**4 + rb5_31 * z**5 + rb6_31 * z**6 + rb7_31 * z**7 + rb8_31 * z**8 + rb9_31 * z**9 + rb10_31 * z**10 + rb11_31 * z**11,
  res0_32 = rb0_32 * z**0 + rb1_32 * z**1 + rb2_32 * z**2 + rb3_32 * z**3 + rb4_32 * z**4 + rb5_32 * z**5 + rb6_32 * z**6 + rb7_32 * z**7 + rb8_32 * z**8 + rb9_32 * z**9 + rb10_32 * z**10 + rb11_32 * z**11,
  res0_33 = rb0_33 * z**0 + rb1_33 * z**1 + rb2_33 * z**2 + rb3_33 * z**3 + rb4_33 * z**4 + rb5_33 * z**5 + rb6_33 * z**6 + rb7_33 * z**7 + rb8_33 * z**8 + rb9_33 * z**9 + rb10_33 * z**10 + rb11_33 * z**11,
  res0_34 = rb0_34 * z**0 + rb1_34 * z**1 + rb2_34 * z**2 + rb3_34 * z**3 + rb4_34 * z**4 + rb5_34 * z**5 + rb6_34 * z**6 + rb7_34 * z**7 + rb8_34 * z**8 + rb9_34 * z**9 + rb10_34 * z**10 + rb11_34 * z**11,
  res0_35 = rb0_35 * z**0 + rb1_35 * z**1 + rb2_35 * z**2 + rb3_35 * z**3 + rb4_35 * z**4 + rb5_35 * z**5 + rb6_35 * z**6 + rb7_35 * z**7 + rb8_35 * z**8 + rb9_35 * z**9 + rb10_35 * z**10 + rb11_35 * z**11,
  res0_36 = rb0_36 * z**0 + rb1_36 * z**1 + rb2_36 * z**2 + rb3_36 * z**3 + rb4_36 * z**4 + rb5_36 * z**5 + rb6_36 * z**6 + rb7_36 * z**7 + rb8_36 * z**8 + rb9_36 * z**9 + rb10_36 * z**10 + rb11_36 * z**11,
  res0_37 = rb0_37 * z**0 + rb1_37 * z**1 + rb2_37 * z**2 + rb3_37 * z**3 + rb4_37 * z**4 + rb5_37 * z**5 + rb6_37 * z**6 + rb7_37 * z**7 + rb8_37 * z**8 + rb9_37 * z**9 + rb10_37 * z**10 + rb11_37 * z**11,
  res0_38 = rb0_38 * z**0 + rb1_38 * z**1 + rb2_38 * z**2 + rb3_38 * z**3 + rb4_38 * z**4 + rb5_38 * z**5 + rb6_38 * z**6 + rb7_38 * z**7 + rb8_38 * z**8 + rb9_38 * z**9 + rb10_38 * z**10 + rb11_38 * z**11,
  res0_39 = rb0_39 * z**0 + rb1_39 * z**1 + rb2_39 * z**2 + rb3_39 * z**3 + rb4_39 * z**4 + rb5_39 * z**5 + rb6_39 * z**6 + rb7_39 * z**7 + rb8_39 * z**8 + rb9_39 * z**9 + rb10_39 * z**10 + rb11_39 * z**11,
  res0_40 = rb0_40 * z**0 + rb1_40 * z**1 + rb2_40 * z**2 + rb3_40 * z**3 + rb4_40 * z**4 + rb5_40 * z**5 + rb6_40 * z**6 + rb7_40 * z**7 + rb8_40 * z**8 + rb9_40 * z**9 + rb10_40 * z**10 + rb11_40 * z**11,
  res0_41 = rb0_41 * z**0 + rb1_41 * z**1 + rb2_41 * z**2 + rb3_41 * z**3 + rb4_41 * z**4 + rb5_41 * z**5 + rb6_41 * z**6 + rb7_41 * z**7 + rb8_41 * z**8 + rb9_41 * z**9 + rb10_41 * z**10 + rb11_41 * z**11,
  res0_42 = rb0_42 * z**0 + rb1_42 * z**1 + rb2_42 * z**2 + rb3_42 * z**3 + rb4_42 * z**4 + rb5_42 * z**5 + rb6_42 * z**6 + rb7_42 * z**7 + rb8_42 * z**8 + rb9_42 * z**9 + rb10_42 * z**10 + rb11_42 * z**11,
  res0_43 = rb0_43 * z**0 + rb1_43 * z**1 + rb2_43 * z**2 + rb3_43 * z**3 + rb4_43 * z**4 + rb5_43 * z**5 + rb6_43 * z**6 + rb7_43 * z**7 + rb8_43 * z**8 + rb9_43 * z**9 + rb10_43 * z**10 + rb11_43 * z**11,
  res0_44 = rb0_44 * z**0 + rb1_44 * z**1 + rb2_44 * z**2 + rb3_44 * z**3 + rb4_44 * z**4 + rb5_44 * z**5 + rb6_44 * z**6 + rb7_44 * z**7 + rb8_44 * z**8 + rb9_44 * z**9 + rb10_44 * z**10 + rb11_44 * z**11,
  res0_45 = rb0_45 * z**0 + rb1_45 * z**1 + rb2_45 * z**2 + rb3_45 * z**3 + rb4_45 * z**4 + rb5_45 * z**5 + rb6_45 * z**6 + rb7_45 * z**7 + rb8_45 * z**8 + rb9_45 * z**9 + rb10_45 * z**10 + rb11_45 * z**11,
  res0_46 = rb0_46 * z**0 + rb1_46 * z**1 + rb2_46 * z**2 + rb3_46 * z**3 + rb4_46 * z**4 + rb5_46 * z**5 + rb6_46 * z**6 + rb7_46 * z**7 + rb8_46 * z**8 + rb9_46 * z**9 + rb10_46 * z**10 + rb11_46 * z**11,
  res0_47 = rb0_47 * z**0 + rb1_47 * z**1 + rb2_47 * z**2 + rb3_47 * z**3 + rb4_47 * z**4 + rb5_47 * z**5 + rb6_47 * z**6 + rb7_47 * z**7 + rb8_47 * z**8 + rb9_47 * z**9 + rb10_47 * z**10 + rb11_47 * z**11,
  res0_48 = rb0_48 * z**0 + rb1_48 * z**1 + rb2_48 * z**2 + rb3_48 * z**3 + rb4_48 * z**4 + rb5_48 * z**5 + rb6_48 * z**6 + rb7_48 * z**7 + rb8_48 * z**8 + rb9_48 * z**9 + rb10_48 * z**10 + rb11_48 * z**11,
  res0_49 = rb0_49 * z**0 + rb1_49 * z**1 + rb2_49 * z**2 + rb3_49 * z**3 + rb4_49 * z**4 + rb5_49 * z**5 + rb6_49 * z**6 + rb7_49 * z**7 + rb8_49 * z**8 + rb9_49 * z**9 + rb10_49 * z**10 + rb11_49 * z**11,
  res0_50 = rb0_50 * z**0 + rb1_50 * z**1 + rb2_50 * z**2 + rb3_50 * z**3 + rb4_50 * z**4 + rb5_50 * z**5 + rb6_50 * z**6 + rb7_50 * z**7 + rb8_50 * z**8 + rb9_50 * z**9 + rb10_50 * z**10 + rb11_50 * z**11,
  res0_51 = rb0_51 * z**0 + rb1_51 * z**1 + rb2_51 * z**2 + rb3_51 * z**3 + rb4_51 * z**4 + rb5_51 * z**5 + rb6_51 * z**6 + rb7_51 * z**7 + rb8_51 * z**8 + rb9_51 * z**9 + rb10_51 * z**10 + rb11_51 * z**11,
  res0_52 = rb0_52 * z**0 + rb1_52 * z**1 + rb2_52 * z**2 + rb3_52 * z**3 + rb4_52 * z**4 + rb5_52 * z**5 + rb6_52 * z**6 + rb7_52 * z**7 + rb8_52 * z**8 + rb9_52 * z**9 + rb10_52 * z**10 + rb11_52 * z**11,
  res0_53 = rb0_53 * z**0 + rb1_53 * z**1 + rb2_53 * z**2 + rb3_53 * z**3 + rb4_53 * z**4 + rb5_53 * z**5 + rb6_53 * z**6 + rb7_53 * z**7 + rb8_53 * z**8 + rb9_53 * z**9 + rb10_53 * z**10 + rb11_53 * z**11,
  res0_54 = rb0_54 * z**0 + rb1_54 * z**1 + rb2_54 * z**2 + rb3_54 * z**3 + rb4_54 * z**4 + rb5_54 * z**5 + rb6_54 * z**6 + rb7_54 * z**7 + rb8_54 * z**8 + rb9_54 * z**9 + rb10_54 * z**10 + rb11_54 * z**11,
  res0_55 = rb0_55 * z**0 + rb1_55 * z**1 + rb2_55 * z**2 + rb3_55 * z**3 + rb4_55 * z**4 + rb5_55 * z**5 + rb6_55 * z**6 + rb7_55 * z**7 + rb8_55 * z**8 + rb9_55 * z**9 + rb10_55 * z**10 + rb11_55 * z**11,
  res0_56 = rb0_56 * z**0 + rb1_56 * z**1 + rb2_56 * z**2 + rb3_56 * z**3 + rb4_56 * z**4 + rb5_56 * z**5 + rb6_56 * z**6 + rb7_56 * z**7 + rb8_56 * z**8 + rb9_56 * z**9 + rb10_56 * z**10 + rb11_56 * z**11,
  res0_57 = rb0_57 * z**0 + rb1_57 * z**1 + rb2_57 * z**2 + rb3_57 * z**3 + rb4_57 * z**4 + rb5_57 * z**5 + rb6_57 * z**6 + rb7_57 * z**7 + rb8_57 * z**8 + rb9_57 * z**9 + rb10_57 * z**10 + rb11_57 * z**11,
  res0_58 = rb0_58 * z**0 + rb1_58 * z**1 + rb2_58 * z**2 + rb3_58 * z**3 + rb4_58 * z**4 + rb5_58 * z**5 + rb6_58 * z**6 + rb7_58 * z**7 + rb8_58 * z**8 + rb9_58 * z**9 + rb10_58 * z**10 + rb11_58 * z**11,
  res0_59 = rb0_59 * z**0 + rb1_59 * z**1 + rb2_59 * z**2 + rb3_59 * z**3 + rb4_59 * z**4 + rb5_59 * z**5 + rb6_59 * z**6 + rb7_59 * z**7 + rb8_59 * z**8 + rb9_59 * z**9 + rb10_59 * z**10 + rb11_59 * z**11,
  res0_60 = rb0_60 * z**0 + rb1_60 * z**1 + rb2_60 * z**2 + rb3_60 * z**3 + rb4_60 * z**4 + rb5_60 * z**5 + rb6_60 * z**6 + rb7_60 * z**7 + rb8_60 * z**8 + rb9_60 * z**9 + rb10_60 * z**10 + rb11_60 * z**11,
  res0_61 = rb0_61 * z**0 + rb1_61 * z**1 + rb2_61 * z**2 + rb3_61 * z**3 + rb4_61 * z**4 + rb5_61 * z**5 + rb6_61 * z**6 + rb7_61 * z**7 + rb8_61 * z**8 + rb9_61 * z**9 + rb10_61 * z**10 + rb11_61 * z**11,
  res0_62 = rb0_62 * z**0 + rb1_62 * z**1 + rb2_62 * z**2 + rb3_62 * z**3 + rb4_62 * z**4 + rb5_62 * z**5 + rb6_62 * z**6 + rb7_62 * z**7 + rb8_62 * z**8 + rb9_62 * z**9 + rb10_62 * z**10 + rb11_62 * z**11,
  res0_63 = rb0_63 * z**0 + rb1_63 * z**1 + rb2_63 * z**2 + rb3_63 * z**3 + rb4_63 * z**4 + rb5_63 * z**5 + rb6_63 * z**6 + rb7_63 * z**7 + rb8_63 * z**8 + rb9_63 * z**9 + rb10_63 * z**10 + rb11_63 * z**11
] && true;

mov %L0x7fffffffdaa0 [rb0_0, rb0_1, rb0_2, rb0_3, rb0_4, rb0_5, rb0_6, rb0_7, rb0_8, rb0_9, rb0_10, rb0_11, rb0_12, rb0_13, rb0_14, rb0_15, rb0_16, rb0_17, rb0_18, rb0_19, rb0_20, rb0_21, rb0_22, rb0_23, rb0_24, rb0_25, rb0_26, rb0_27, rb0_28, rb0_29, rb0_30, rb0_31, rb0_32, rb0_33, rb0_34, rb0_35, rb0_36, rb0_37, rb0_38, rb0_39, rb0_40, rb0_41, rb0_42, rb0_43, rb0_44, rb0_45, rb0_46, rb0_47, rb0_48, rb0_49, rb0_50, rb0_51, rb0_52, rb0_53, rb0_54, rb0_55, rb0_56, rb0_57, rb0_58, rb0_59, rb0_60, rb0_61, rb0_62, rb0_63];
mov %L0x7fffffffdaa8 [rb1_0, rb1_1, rb1_2, rb1_3, rb1_4, rb1_5, rb1_6, rb1_7, rb1_8, rb1_9, rb1_10, rb1_11, rb1_12, rb1_13, rb1_14, rb1_15, rb1_16, rb1_17, rb1_18, rb1_19, rb1_20, rb1_21, rb1_22, rb1_23, rb1_24, rb1_25, rb1_26, rb1_27, rb1_28, rb1_29, rb1_30, rb1_31, rb1_32, rb1_33, rb1_34, rb1_35, rb1_36, rb1_37, rb1_38, rb1_39, rb1_40, rb1_41, rb1_42, rb1_43, rb1_44, rb1_45, rb1_46, rb1_47, rb1_48, rb1_49, rb1_50, rb1_51, rb1_52, rb1_53, rb1_54, rb1_55, rb1_56, rb1_57, rb1_58, rb1_59, rb1_60, rb1_61, rb1_62, rb1_63];
mov %L0x7fffffffdab0 [rb2_0, rb2_1, rb2_2, rb2_3, rb2_4, rb2_5, rb2_6, rb2_7, rb2_8, rb2_9, rb2_10, rb2_11, rb2_12, rb2_13, rb2_14, rb2_15, rb2_16, rb2_17, rb2_18, rb2_19, rb2_20, rb2_21, rb2_22, rb2_23, rb2_24, rb2_25, rb2_26, rb2_27, rb2_28, rb2_29, rb2_30, rb2_31, rb2_32, rb2_33, rb2_34, rb2_35, rb2_36, rb2_37, rb2_38, rb2_39, rb2_40, rb2_41, rb2_42, rb2_43, rb2_44, rb2_45, rb2_46, rb2_47, rb2_48, rb2_49, rb2_50, rb2_51, rb2_52, rb2_53, rb2_54, rb2_55, rb2_56, rb2_57, rb2_58, rb2_59, rb2_60, rb2_61, rb2_62, rb2_63];
mov %L0x7fffffffdab8 [rb3_0, rb3_1, rb3_2, rb3_3, rb3_4, rb3_5, rb3_6, rb3_7, rb3_8, rb3_9, rb3_10, rb3_11, rb3_12, rb3_13, rb3_14, rb3_15, rb3_16, rb3_17, rb3_18, rb3_19, rb3_20, rb3_21, rb3_22, rb3_23, rb3_24, rb3_25, rb3_26, rb3_27, rb3_28, rb3_29, rb3_30, rb3_31, rb3_32, rb3_33, rb3_34, rb3_35, rb3_36, rb3_37, rb3_38, rb3_39, rb3_40, rb3_41, rb3_42, rb3_43, rb3_44, rb3_45, rb3_46, rb3_47, rb3_48, rb3_49, rb3_50, rb3_51, rb3_52, rb3_53, rb3_54, rb3_55, rb3_56, rb3_57, rb3_58, rb3_59, rb3_60, rb3_61, rb3_62, rb3_63];
mov %L0x7fffffffdac0 [rb4_0, rb4_1, rb4_2, rb4_3, rb4_4, rb4_5, rb4_6, rb4_7, rb4_8, rb4_9, rb4_10, rb4_11, rb4_12, rb4_13, rb4_14, rb4_15, rb4_16, rb4_17, rb4_18, rb4_19, rb4_20, rb4_21, rb4_22, rb4_23, rb4_24, rb4_25, rb4_26, rb4_27, rb4_28, rb4_29, rb4_30, rb4_31, rb4_32, rb4_33, rb4_34, rb4_35, rb4_36, rb4_37, rb4_38, rb4_39, rb4_40, rb4_41, rb4_42, rb4_43, rb4_44, rb4_45, rb4_46, rb4_47, rb4_48, rb4_49, rb4_50, rb4_51, rb4_52, rb4_53, rb4_54, rb4_55, rb4_56, rb4_57, rb4_58, rb4_59, rb4_60, rb4_61, rb4_62, rb4_63];
mov %L0x7fffffffdac8 [rb5_0, rb5_1, rb5_2, rb5_3, rb5_4, rb5_5, rb5_6, rb5_7, rb5_8, rb5_9, rb5_10, rb5_11, rb5_12, rb5_13, rb5_14, rb5_15, rb5_16, rb5_17, rb5_18, rb5_19, rb5_20, rb5_21, rb5_22, rb5_23, rb5_24, rb5_25, rb5_26, rb5_27, rb5_28, rb5_29, rb5_30, rb5_31, rb5_32, rb5_33, rb5_34, rb5_35, rb5_36, rb5_37, rb5_38, rb5_39, rb5_40, rb5_41, rb5_42, rb5_43, rb5_44, rb5_45, rb5_46, rb5_47, rb5_48, rb5_49, rb5_50, rb5_51, rb5_52, rb5_53, rb5_54, rb5_55, rb5_56, rb5_57, rb5_58, rb5_59, rb5_60, rb5_61, rb5_62, rb5_63];
mov %L0x7fffffffdad0 [rb6_0, rb6_1, rb6_2, rb6_3, rb6_4, rb6_5, rb6_6, rb6_7, rb6_8, rb6_9, rb6_10, rb6_11, rb6_12, rb6_13, rb6_14, rb6_15, rb6_16, rb6_17, rb6_18, rb6_19, rb6_20, rb6_21, rb6_22, rb6_23, rb6_24, rb6_25, rb6_26, rb6_27, rb6_28, rb6_29, rb6_30, rb6_31, rb6_32, rb6_33, rb6_34, rb6_35, rb6_36, rb6_37, rb6_38, rb6_39, rb6_40, rb6_41, rb6_42, rb6_43, rb6_44, rb6_45, rb6_46, rb6_47, rb6_48, rb6_49, rb6_50, rb6_51, rb6_52, rb6_53, rb6_54, rb6_55, rb6_56, rb6_57, rb6_58, rb6_59, rb6_60, rb6_61, rb6_62, rb6_63];
mov %L0x7fffffffdad8 [rb7_0, rb7_1, rb7_2, rb7_3, rb7_4, rb7_5, rb7_6, rb7_7, rb7_8, rb7_9, rb7_10, rb7_11, rb7_12, rb7_13, rb7_14, rb7_15, rb7_16, rb7_17, rb7_18, rb7_19, rb7_20, rb7_21, rb7_22, rb7_23, rb7_24, rb7_25, rb7_26, rb7_27, rb7_28, rb7_29, rb7_30, rb7_31, rb7_32, rb7_33, rb7_34, rb7_35, rb7_36, rb7_37, rb7_38, rb7_39, rb7_40, rb7_41, rb7_42, rb7_43, rb7_44, rb7_45, rb7_46, rb7_47, rb7_48, rb7_49, rb7_50, rb7_51, rb7_52, rb7_53, rb7_54, rb7_55, rb7_56, rb7_57, rb7_58, rb7_59, rb7_60, rb7_61, rb7_62, rb7_63];
mov %L0x7fffffffdae0 [rb8_0, rb8_1, rb8_2, rb8_3, rb8_4, rb8_5, rb8_6, rb8_7, rb8_8, rb8_9, rb8_10, rb8_11, rb8_12, rb8_13, rb8_14, rb8_15, rb8_16, rb8_17, rb8_18, rb8_19, rb8_20, rb8_21, rb8_22, rb8_23, rb8_24, rb8_25, rb8_26, rb8_27, rb8_28, rb8_29, rb8_30, rb8_31, rb8_32, rb8_33, rb8_34, rb8_35, rb8_36, rb8_37, rb8_38, rb8_39, rb8_40, rb8_41, rb8_42, rb8_43, rb8_44, rb8_45, rb8_46, rb8_47, rb8_48, rb8_49, rb8_50, rb8_51, rb8_52, rb8_53, rb8_54, rb8_55, rb8_56, rb8_57, rb8_58, rb8_59, rb8_60, rb8_61, rb8_62, rb8_63];
mov %L0x7fffffffdae8 [rb9_0, rb9_1, rb9_2, rb9_3, rb9_4, rb9_5, rb9_6, rb9_7, rb9_8, rb9_9, rb9_10, rb9_11, rb9_12, rb9_13, rb9_14, rb9_15, rb9_16, rb9_17, rb9_18, rb9_19, rb9_20, rb9_21, rb9_22, rb9_23, rb9_24, rb9_25, rb9_26, rb9_27, rb9_28, rb9_29, rb9_30, rb9_31, rb9_32, rb9_33, rb9_34, rb9_35, rb9_36, rb9_37, rb9_38, rb9_39, rb9_40, rb9_41, rb9_42, rb9_43, rb9_44, rb9_45, rb9_46, rb9_47, rb9_48, rb9_49, rb9_50, rb9_51, rb9_52, rb9_53, rb9_54, rb9_55, rb9_56, rb9_57, rb9_58, rb9_59, rb9_60, rb9_61, rb9_62, rb9_63];
mov %L0x7fffffffdaf0 [rb10_0, rb10_1, rb10_2, rb10_3, rb10_4, rb10_5, rb10_6, rb10_7, rb10_8, rb10_9, rb10_10, rb10_11, rb10_12, rb10_13, rb10_14, rb10_15, rb10_16, rb10_17, rb10_18, rb10_19, rb10_20, rb10_21, rb10_22, rb10_23, rb10_24, rb10_25, rb10_26, rb10_27, rb10_28, rb10_29, rb10_30, rb10_31, rb10_32, rb10_33, rb10_34, rb10_35, rb10_36, rb10_37, rb10_38, rb10_39, rb10_40, rb10_41, rb10_42, rb10_43, rb10_44, rb10_45, rb10_46, rb10_47, rb10_48, rb10_49, rb10_50, rb10_51, rb10_52, rb10_53, rb10_54, rb10_55, rb10_56, rb10_57, rb10_58, rb10_59, rb10_60, rb10_61, rb10_62, rb10_63];
mov %L0x7fffffffdaf8 [rb11_0, rb11_1, rb11_2, rb11_3, rb11_4, rb11_5, rb11_6, rb11_7, rb11_8, rb11_9, rb11_10, rb11_11, rb11_12, rb11_13, rb11_14, rb11_15, rb11_16, rb11_17, rb11_18, rb11_19, rb11_20, rb11_21, rb11_22, rb11_23, rb11_24, rb11_25, rb11_26, rb11_27, rb11_28, rb11_29, rb11_30, rb11_31, rb11_32, rb11_33, rb11_34, rb11_35, rb11_36, rb11_37, rb11_38, rb11_39, rb11_40, rb11_41, rb11_42, rb11_43, rb11_44, rb11_45, rb11_46, rb11_47, rb11_48, rb11_49, rb11_50, rb11_51, rb11_52, rb11_53, rb11_54, rb11_55, rb11_56, rb11_57, rb11_58, rb11_59, rb11_60, rb11_61, rb11_62, rb11_63];


(* #jne    0x5555555552e4 <radix_conversions+196>  #! PC = 0x55555555535b *)
#jne    0x5555555552e4 <radix_conversions+196>  #! 0x55555555535b = 0x55555555535b;
(* mov    %r15,%r9                                 #! PC = 0x5555555552e4 *)
mov r9 r15;
(* mov    $0x1,%r8d                                #! PC = 0x5555555552e7 *)
mov r8d 0x1@uint32;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaa0; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaa0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaa0; PC = 0x555555555330 *)
mov %L0x7fffffffdaa0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaa8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaa8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaa8; PC = 0x555555555330 *)
mov %L0x7fffffffdaa8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdab0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdab0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdab0; PC = 0x555555555330 *)
mov %L0x7fffffffdab0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdab8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdab8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdab8; PC = 0x555555555330 *)
mov %L0x7fffffffdab8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdac0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdac0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdac0; PC = 0x555555555330 *)
mov %L0x7fffffffdac0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdac8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdac8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdac8; PC = 0x555555555330 *)
mov %L0x7fffffffdac8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdad0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdad0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdad0; PC = 0x555555555330 *)
mov %L0x7fffffffdad0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdad8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdad8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdad8; PC = 0x555555555330 *)
mov %L0x7fffffffdad8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdae0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdae0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdae0; PC = 0x555555555330 *)
mov %L0x7fffffffdae0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdae8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdae8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdae8; PC = 0x555555555330 *)
mov %L0x7fffffffdae8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaf0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaf0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaf0; PC = 0x555555555330 *)
mov %L0x7fffffffdaf0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaf8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaf8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd848; Value = 0x3030303030303030; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd848;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd840; Value = 0xc0c0c0c0c0c0c0c0; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd840;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 1@8;
shl r11d r11d 1;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 2 *)
assert true && cl = 2@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 2 *)
assert true && cl = 2@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaf8; PC = 0x555555555330 *)
mov %L0x7fffffffdaf8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* add    $0x1,%rbp                                #! PC = 0x555555555347 *)
add ebp ebp 0x1@sint32;



mov [out0_0, out0_1, out0_2, out0_3, out0_4, out0_5, out0_6, out0_7, out0_8, out0_9, out0_10, out0_11, out0_12, out0_13, out0_14, out0_15, out0_16, out0_17, out0_18, out0_19, out0_20, out0_21, out0_22, out0_23, out0_24, out0_25, out0_26, out0_27, out0_28, out0_29, out0_30, out0_31, out0_32, out0_33, out0_34, out0_35, out0_36, out0_37, out0_38, out0_39, out0_40, out0_41, out0_42, out0_43, out0_44, out0_45, out0_46, out0_47, out0_48, out0_49, out0_50, out0_51, out0_52, out0_53, out0_54, out0_55, out0_56, out0_57, out0_58, out0_59, out0_60, out0_61, out0_62, out0_63] %L0x7fffffffdaa0;
mov [out1_0, out1_1, out1_2, out1_3, out1_4, out1_5, out1_6, out1_7, out1_8, out1_9, out1_10, out1_11, out1_12, out1_13, out1_14, out1_15, out1_16, out1_17, out1_18, out1_19, out1_20, out1_21, out1_22, out1_23, out1_24, out1_25, out1_26, out1_27, out1_28, out1_29, out1_30, out1_31, out1_32, out1_33, out1_34, out1_35, out1_36, out1_37, out1_38, out1_39, out1_40, out1_41, out1_42, out1_43, out1_44, out1_45, out1_46, out1_47, out1_48, out1_49, out1_50, out1_51, out1_52, out1_53, out1_54, out1_55, out1_56, out1_57, out1_58, out1_59, out1_60, out1_61, out1_62, out1_63] %L0x7fffffffdaa8;
mov [out2_0, out2_1, out2_2, out2_3, out2_4, out2_5, out2_6, out2_7, out2_8, out2_9, out2_10, out2_11, out2_12, out2_13, out2_14, out2_15, out2_16, out2_17, out2_18, out2_19, out2_20, out2_21, out2_22, out2_23, out2_24, out2_25, out2_26, out2_27, out2_28, out2_29, out2_30, out2_31, out2_32, out2_33, out2_34, out2_35, out2_36, out2_37, out2_38, out2_39, out2_40, out2_41, out2_42, out2_43, out2_44, out2_45, out2_46, out2_47, out2_48, out2_49, out2_50, out2_51, out2_52, out2_53, out2_54, out2_55, out2_56, out2_57, out2_58, out2_59, out2_60, out2_61, out2_62, out2_63] %L0x7fffffffdab0;
mov [out3_0, out3_1, out3_2, out3_3, out3_4, out3_5, out3_6, out3_7, out3_8, out3_9, out3_10, out3_11, out3_12, out3_13, out3_14, out3_15, out3_16, out3_17, out3_18, out3_19, out3_20, out3_21, out3_22, out3_23, out3_24, out3_25, out3_26, out3_27, out3_28, out3_29, out3_30, out3_31, out3_32, out3_33, out3_34, out3_35, out3_36, out3_37, out3_38, out3_39, out3_40, out3_41, out3_42, out3_43, out3_44, out3_45, out3_46, out3_47, out3_48, out3_49, out3_50, out3_51, out3_52, out3_53, out3_54, out3_55, out3_56, out3_57, out3_58, out3_59, out3_60, out3_61, out3_62, out3_63] %L0x7fffffffdab8;
mov [out4_0, out4_1, out4_2, out4_3, out4_4, out4_5, out4_6, out4_7, out4_8, out4_9, out4_10, out4_11, out4_12, out4_13, out4_14, out4_15, out4_16, out4_17, out4_18, out4_19, out4_20, out4_21, out4_22, out4_23, out4_24, out4_25, out4_26, out4_27, out4_28, out4_29, out4_30, out4_31, out4_32, out4_33, out4_34, out4_35, out4_36, out4_37, out4_38, out4_39, out4_40, out4_41, out4_42, out4_43, out4_44, out4_45, out4_46, out4_47, out4_48, out4_49, out4_50, out4_51, out4_52, out4_53, out4_54, out4_55, out4_56, out4_57, out4_58, out4_59, out4_60, out4_61, out4_62, out4_63] %L0x7fffffffdac0;
mov [out5_0, out5_1, out5_2, out5_3, out5_4, out5_5, out5_6, out5_7, out5_8, out5_9, out5_10, out5_11, out5_12, out5_13, out5_14, out5_15, out5_16, out5_17, out5_18, out5_19, out5_20, out5_21, out5_22, out5_23, out5_24, out5_25, out5_26, out5_27, out5_28, out5_29, out5_30, out5_31, out5_32, out5_33, out5_34, out5_35, out5_36, out5_37, out5_38, out5_39, out5_40, out5_41, out5_42, out5_43, out5_44, out5_45, out5_46, out5_47, out5_48, out5_49, out5_50, out5_51, out5_52, out5_53, out5_54, out5_55, out5_56, out5_57, out5_58, out5_59, out5_60, out5_61, out5_62, out5_63] %L0x7fffffffdac8;
mov [out6_0, out6_1, out6_2, out6_3, out6_4, out6_5, out6_6, out6_7, out6_8, out6_9, out6_10, out6_11, out6_12, out6_13, out6_14, out6_15, out6_16, out6_17, out6_18, out6_19, out6_20, out6_21, out6_22, out6_23, out6_24, out6_25, out6_26, out6_27, out6_28, out6_29, out6_30, out6_31, out6_32, out6_33, out6_34, out6_35, out6_36, out6_37, out6_38, out6_39, out6_40, out6_41, out6_42, out6_43, out6_44, out6_45, out6_46, out6_47, out6_48, out6_49, out6_50, out6_51, out6_52, out6_53, out6_54, out6_55, out6_56, out6_57, out6_58, out6_59, out6_60, out6_61, out6_62, out6_63] %L0x7fffffffdad0;
mov [out7_0, out7_1, out7_2, out7_3, out7_4, out7_5, out7_6, out7_7, out7_8, out7_9, out7_10, out7_11, out7_12, out7_13, out7_14, out7_15, out7_16, out7_17, out7_18, out7_19, out7_20, out7_21, out7_22, out7_23, out7_24, out7_25, out7_26, out7_27, out7_28, out7_29, out7_30, out7_31, out7_32, out7_33, out7_34, out7_35, out7_36, out7_37, out7_38, out7_39, out7_40, out7_41, out7_42, out7_43, out7_44, out7_45, out7_46, out7_47, out7_48, out7_49, out7_50, out7_51, out7_52, out7_53, out7_54, out7_55, out7_56, out7_57, out7_58, out7_59, out7_60, out7_61, out7_62, out7_63] %L0x7fffffffdad8;
mov [out8_0, out8_1, out8_2, out8_3, out8_4, out8_5, out8_6, out8_7, out8_8, out8_9, out8_10, out8_11, out8_12, out8_13, out8_14, out8_15, out8_16, out8_17, out8_18, out8_19, out8_20, out8_21, out8_22, out8_23, out8_24, out8_25, out8_26, out8_27, out8_28, out8_29, out8_30, out8_31, out8_32, out8_33, out8_34, out8_35, out8_36, out8_37, out8_38, out8_39, out8_40, out8_41, out8_42, out8_43, out8_44, out8_45, out8_46, out8_47, out8_48, out8_49, out8_50, out8_51, out8_52, out8_53, out8_54, out8_55, out8_56, out8_57, out8_58, out8_59, out8_60, out8_61, out8_62, out8_63] %L0x7fffffffdae0;
mov [out9_0, out9_1, out9_2, out9_3, out9_4, out9_5, out9_6, out9_7, out9_8, out9_9, out9_10, out9_11, out9_12, out9_13, out9_14, out9_15, out9_16, out9_17, out9_18, out9_19, out9_20, out9_21, out9_22, out9_23, out9_24, out9_25, out9_26, out9_27, out9_28, out9_29, out9_30, out9_31, out9_32, out9_33, out9_34, out9_35, out9_36, out9_37, out9_38, out9_39, out9_40, out9_41, out9_42, out9_43, out9_44, out9_45, out9_46, out9_47, out9_48, out9_49, out9_50, out9_51, out9_52, out9_53, out9_54, out9_55, out9_56, out9_57, out9_58, out9_59, out9_60, out9_61, out9_62, out9_63] %L0x7fffffffdae8;
mov [out10_0, out10_1, out10_2, out10_3, out10_4, out10_5, out10_6, out10_7, out10_8, out10_9, out10_10, out10_11, out10_12, out10_13, out10_14, out10_15, out10_16, out10_17, out10_18, out10_19, out10_20, out10_21, out10_22, out10_23, out10_24, out10_25, out10_26, out10_27, out10_28, out10_29, out10_30, out10_31, out10_32, out10_33, out10_34, out10_35, out10_36, out10_37, out10_38, out10_39, out10_40, out10_41, out10_42, out10_43, out10_44, out10_45, out10_46, out10_47, out10_48, out10_49, out10_50, out10_51, out10_52, out10_53, out10_54, out10_55, out10_56, out10_57, out10_58, out10_59, out10_60, out10_61, out10_62, out10_63] %L0x7fffffffdaf0;
mov [out11_0, out11_1, out11_2, out11_3, out11_4, out11_5, out11_6, out11_7, out11_8, out11_9, out11_10, out11_11, out11_12, out11_13, out11_14, out11_15, out11_16, out11_17, out11_18, out11_19, out11_20, out11_21, out11_22, out11_23, out11_24, out11_25, out11_26, out11_27, out11_28, out11_29, out11_30, out11_31, out11_32, out11_33, out11_34, out11_35, out11_36, out11_37, out11_38, out11_39, out11_40, out11_41, out11_42, out11_43, out11_44, out11_45, out11_46, out11_47, out11_48, out11_49, out11_50, out11_51, out11_52, out11_53, out11_54, out11_55, out11_56, out11_57, out11_58, out11_59, out11_60, out11_61, out11_62, out11_63] %L0x7fffffffdaf8;

ghost cvrted1_0@uint12: cvrted1_0 = out0_0 * z**0 + out1_0 * z**1 + out2_0 * z**2 + out3_0 * z**3 + out4_0 * z**4 + out5_0 * z**5 + out6_0 * z**6 + out7_0 * z**7 + out8_0 * z**8 + out9_0 * z**9 + out10_0 * z**10 + out11_0 * z**11 && true;
ghost cvrted1_1@uint12: cvrted1_1 = out0_1 * z**0 + out1_1 * z**1 + out2_1 * z**2 + out3_1 * z**3 + out4_1 * z**4 + out5_1 * z**5 + out6_1 * z**6 + out7_1 * z**7 + out8_1 * z**8 + out9_1 * z**9 + out10_1 * z**10 + out11_1 * z**11 && true;
ghost cvrted1_2@uint12: cvrted1_2 = out0_2 * z**0 + out1_2 * z**1 + out2_2 * z**2 + out3_2 * z**3 + out4_2 * z**4 + out5_2 * z**5 + out6_2 * z**6 + out7_2 * z**7 + out8_2 * z**8 + out9_2 * z**9 + out10_2 * z**10 + out11_2 * z**11 && true;
ghost cvrted1_3@uint12: cvrted1_3 = out0_3 * z**0 + out1_3 * z**1 + out2_3 * z**2 + out3_3 * z**3 + out4_3 * z**4 + out5_3 * z**5 + out6_3 * z**6 + out7_3 * z**7 + out8_3 * z**8 + out9_3 * z**9 + out10_3 * z**10 + out11_3 * z**11 && true;
ghost cvrted1_4@uint12: cvrted1_4 = out0_4 * z**0 + out1_4 * z**1 + out2_4 * z**2 + out3_4 * z**3 + out4_4 * z**4 + out5_4 * z**5 + out6_4 * z**6 + out7_4 * z**7 + out8_4 * z**8 + out9_4 * z**9 + out10_4 * z**10 + out11_4 * z**11 && true;
ghost cvrted1_5@uint12: cvrted1_5 = out0_5 * z**0 + out1_5 * z**1 + out2_5 * z**2 + out3_5 * z**3 + out4_5 * z**4 + out5_5 * z**5 + out6_5 * z**6 + out7_5 * z**7 + out8_5 * z**8 + out9_5 * z**9 + out10_5 * z**10 + out11_5 * z**11 && true;
ghost cvrted1_6@uint12: cvrted1_6 = out0_6 * z**0 + out1_6 * z**1 + out2_6 * z**2 + out3_6 * z**3 + out4_6 * z**4 + out5_6 * z**5 + out6_6 * z**6 + out7_6 * z**7 + out8_6 * z**8 + out9_6 * z**9 + out10_6 * z**10 + out11_6 * z**11 && true;
ghost cvrted1_7@uint12: cvrted1_7 = out0_7 * z**0 + out1_7 * z**1 + out2_7 * z**2 + out3_7 * z**3 + out4_7 * z**4 + out5_7 * z**5 + out6_7 * z**6 + out7_7 * z**7 + out8_7 * z**8 + out9_7 * z**9 + out10_7 * z**10 + out11_7 * z**11 && true;
ghost cvrted1_8@uint12: cvrted1_8 = out0_8 * z**0 + out1_8 * z**1 + out2_8 * z**2 + out3_8 * z**3 + out4_8 * z**4 + out5_8 * z**5 + out6_8 * z**6 + out7_8 * z**7 + out8_8 * z**8 + out9_8 * z**9 + out10_8 * z**10 + out11_8 * z**11 && true;
ghost cvrted1_9@uint12: cvrted1_9 = out0_9 * z**0 + out1_9 * z**1 + out2_9 * z**2 + out3_9 * z**3 + out4_9 * z**4 + out5_9 * z**5 + out6_9 * z**6 + out7_9 * z**7 + out8_9 * z**8 + out9_9 * z**9 + out10_9 * z**10 + out11_9 * z**11 && true;
ghost cvrted1_10@uint12: cvrted1_10 = out0_10 * z**0 + out1_10 * z**1 + out2_10 * z**2 + out3_10 * z**3 + out4_10 * z**4 + out5_10 * z**5 + out6_10 * z**6 + out7_10 * z**7 + out8_10 * z**8 + out9_10 * z**9 + out10_10 * z**10 + out11_10 * z**11 && true;
ghost cvrted1_11@uint12: cvrted1_11 = out0_11 * z**0 + out1_11 * z**1 + out2_11 * z**2 + out3_11 * z**3 + out4_11 * z**4 + out5_11 * z**5 + out6_11 * z**6 + out7_11 * z**7 + out8_11 * z**8 + out9_11 * z**9 + out10_11 * z**10 + out11_11 * z**11 && true;
ghost cvrted1_12@uint12: cvrted1_12 = out0_12 * z**0 + out1_12 * z**1 + out2_12 * z**2 + out3_12 * z**3 + out4_12 * z**4 + out5_12 * z**5 + out6_12 * z**6 + out7_12 * z**7 + out8_12 * z**8 + out9_12 * z**9 + out10_12 * z**10 + out11_12 * z**11 && true;
ghost cvrted1_13@uint12: cvrted1_13 = out0_13 * z**0 + out1_13 * z**1 + out2_13 * z**2 + out3_13 * z**3 + out4_13 * z**4 + out5_13 * z**5 + out6_13 * z**6 + out7_13 * z**7 + out8_13 * z**8 + out9_13 * z**9 + out10_13 * z**10 + out11_13 * z**11 && true;
ghost cvrted1_14@uint12: cvrted1_14 = out0_14 * z**0 + out1_14 * z**1 + out2_14 * z**2 + out3_14 * z**3 + out4_14 * z**4 + out5_14 * z**5 + out6_14 * z**6 + out7_14 * z**7 + out8_14 * z**8 + out9_14 * z**9 + out10_14 * z**10 + out11_14 * z**11 && true;
ghost cvrted1_15@uint12: cvrted1_15 = out0_15 * z**0 + out1_15 * z**1 + out2_15 * z**2 + out3_15 * z**3 + out4_15 * z**4 + out5_15 * z**5 + out6_15 * z**6 + out7_15 * z**7 + out8_15 * z**8 + out9_15 * z**9 + out10_15 * z**10 + out11_15 * z**11 && true;
ghost cvrted1_16@uint12: cvrted1_16 = out0_16 * z**0 + out1_16 * z**1 + out2_16 * z**2 + out3_16 * z**3 + out4_16 * z**4 + out5_16 * z**5 + out6_16 * z**6 + out7_16 * z**7 + out8_16 * z**8 + out9_16 * z**9 + out10_16 * z**10 + out11_16 * z**11 && true;
ghost cvrted1_17@uint12: cvrted1_17 = out0_17 * z**0 + out1_17 * z**1 + out2_17 * z**2 + out3_17 * z**3 + out4_17 * z**4 + out5_17 * z**5 + out6_17 * z**6 + out7_17 * z**7 + out8_17 * z**8 + out9_17 * z**9 + out10_17 * z**10 + out11_17 * z**11 && true;
ghost cvrted1_18@uint12: cvrted1_18 = out0_18 * z**0 + out1_18 * z**1 + out2_18 * z**2 + out3_18 * z**3 + out4_18 * z**4 + out5_18 * z**5 + out6_18 * z**6 + out7_18 * z**7 + out8_18 * z**8 + out9_18 * z**9 + out10_18 * z**10 + out11_18 * z**11 && true;
ghost cvrted1_19@uint12: cvrted1_19 = out0_19 * z**0 + out1_19 * z**1 + out2_19 * z**2 + out3_19 * z**3 + out4_19 * z**4 + out5_19 * z**5 + out6_19 * z**6 + out7_19 * z**7 + out8_19 * z**8 + out9_19 * z**9 + out10_19 * z**10 + out11_19 * z**11 && true;
ghost cvrted1_20@uint12: cvrted1_20 = out0_20 * z**0 + out1_20 * z**1 + out2_20 * z**2 + out3_20 * z**3 + out4_20 * z**4 + out5_20 * z**5 + out6_20 * z**6 + out7_20 * z**7 + out8_20 * z**8 + out9_20 * z**9 + out10_20 * z**10 + out11_20 * z**11 && true;
ghost cvrted1_21@uint12: cvrted1_21 = out0_21 * z**0 + out1_21 * z**1 + out2_21 * z**2 + out3_21 * z**3 + out4_21 * z**4 + out5_21 * z**5 + out6_21 * z**6 + out7_21 * z**7 + out8_21 * z**8 + out9_21 * z**9 + out10_21 * z**10 + out11_21 * z**11 && true;
ghost cvrted1_22@uint12: cvrted1_22 = out0_22 * z**0 + out1_22 * z**1 + out2_22 * z**2 + out3_22 * z**3 + out4_22 * z**4 + out5_22 * z**5 + out6_22 * z**6 + out7_22 * z**7 + out8_22 * z**8 + out9_22 * z**9 + out10_22 * z**10 + out11_22 * z**11 && true;
ghost cvrted1_23@uint12: cvrted1_23 = out0_23 * z**0 + out1_23 * z**1 + out2_23 * z**2 + out3_23 * z**3 + out4_23 * z**4 + out5_23 * z**5 + out6_23 * z**6 + out7_23 * z**7 + out8_23 * z**8 + out9_23 * z**9 + out10_23 * z**10 + out11_23 * z**11 && true;
ghost cvrted1_24@uint12: cvrted1_24 = out0_24 * z**0 + out1_24 * z**1 + out2_24 * z**2 + out3_24 * z**3 + out4_24 * z**4 + out5_24 * z**5 + out6_24 * z**6 + out7_24 * z**7 + out8_24 * z**8 + out9_24 * z**9 + out10_24 * z**10 + out11_24 * z**11 && true;
ghost cvrted1_25@uint12: cvrted1_25 = out0_25 * z**0 + out1_25 * z**1 + out2_25 * z**2 + out3_25 * z**3 + out4_25 * z**4 + out5_25 * z**5 + out6_25 * z**6 + out7_25 * z**7 + out8_25 * z**8 + out9_25 * z**9 + out10_25 * z**10 + out11_25 * z**11 && true;
ghost cvrted1_26@uint12: cvrted1_26 = out0_26 * z**0 + out1_26 * z**1 + out2_26 * z**2 + out3_26 * z**3 + out4_26 * z**4 + out5_26 * z**5 + out6_26 * z**6 + out7_26 * z**7 + out8_26 * z**8 + out9_26 * z**9 + out10_26 * z**10 + out11_26 * z**11 && true;
ghost cvrted1_27@uint12: cvrted1_27 = out0_27 * z**0 + out1_27 * z**1 + out2_27 * z**2 + out3_27 * z**3 + out4_27 * z**4 + out5_27 * z**5 + out6_27 * z**6 + out7_27 * z**7 + out8_27 * z**8 + out9_27 * z**9 + out10_27 * z**10 + out11_27 * z**11 && true;
ghost cvrted1_28@uint12: cvrted1_28 = out0_28 * z**0 + out1_28 * z**1 + out2_28 * z**2 + out3_28 * z**3 + out4_28 * z**4 + out5_28 * z**5 + out6_28 * z**6 + out7_28 * z**7 + out8_28 * z**8 + out9_28 * z**9 + out10_28 * z**10 + out11_28 * z**11 && true;
ghost cvrted1_29@uint12: cvrted1_29 = out0_29 * z**0 + out1_29 * z**1 + out2_29 * z**2 + out3_29 * z**3 + out4_29 * z**4 + out5_29 * z**5 + out6_29 * z**6 + out7_29 * z**7 + out8_29 * z**8 + out9_29 * z**9 + out10_29 * z**10 + out11_29 * z**11 && true;
ghost cvrted1_30@uint12: cvrted1_30 = out0_30 * z**0 + out1_30 * z**1 + out2_30 * z**2 + out3_30 * z**3 + out4_30 * z**4 + out5_30 * z**5 + out6_30 * z**6 + out7_30 * z**7 + out8_30 * z**8 + out9_30 * z**9 + out10_30 * z**10 + out11_30 * z**11 && true;
ghost cvrted1_31@uint12: cvrted1_31 = out0_31 * z**0 + out1_31 * z**1 + out2_31 * z**2 + out3_31 * z**3 + out4_31 * z**4 + out5_31 * z**5 + out6_31 * z**6 + out7_31 * z**7 + out8_31 * z**8 + out9_31 * z**9 + out10_31 * z**10 + out11_31 * z**11 && true;
ghost cvrted1_32@uint12: cvrted1_32 = out0_32 * z**0 + out1_32 * z**1 + out2_32 * z**2 + out3_32 * z**3 + out4_32 * z**4 + out5_32 * z**5 + out6_32 * z**6 + out7_32 * z**7 + out8_32 * z**8 + out9_32 * z**9 + out10_32 * z**10 + out11_32 * z**11 && true;
ghost cvrted1_33@uint12: cvrted1_33 = out0_33 * z**0 + out1_33 * z**1 + out2_33 * z**2 + out3_33 * z**3 + out4_33 * z**4 + out5_33 * z**5 + out6_33 * z**6 + out7_33 * z**7 + out8_33 * z**8 + out9_33 * z**9 + out10_33 * z**10 + out11_33 * z**11 && true;
ghost cvrted1_34@uint12: cvrted1_34 = out0_34 * z**0 + out1_34 * z**1 + out2_34 * z**2 + out3_34 * z**3 + out4_34 * z**4 + out5_34 * z**5 + out6_34 * z**6 + out7_34 * z**7 + out8_34 * z**8 + out9_34 * z**9 + out10_34 * z**10 + out11_34 * z**11 && true;
ghost cvrted1_35@uint12: cvrted1_35 = out0_35 * z**0 + out1_35 * z**1 + out2_35 * z**2 + out3_35 * z**3 + out4_35 * z**4 + out5_35 * z**5 + out6_35 * z**6 + out7_35 * z**7 + out8_35 * z**8 + out9_35 * z**9 + out10_35 * z**10 + out11_35 * z**11 && true;
ghost cvrted1_36@uint12: cvrted1_36 = out0_36 * z**0 + out1_36 * z**1 + out2_36 * z**2 + out3_36 * z**3 + out4_36 * z**4 + out5_36 * z**5 + out6_36 * z**6 + out7_36 * z**7 + out8_36 * z**8 + out9_36 * z**9 + out10_36 * z**10 + out11_36 * z**11 && true;
ghost cvrted1_37@uint12: cvrted1_37 = out0_37 * z**0 + out1_37 * z**1 + out2_37 * z**2 + out3_37 * z**3 + out4_37 * z**4 + out5_37 * z**5 + out6_37 * z**6 + out7_37 * z**7 + out8_37 * z**8 + out9_37 * z**9 + out10_37 * z**10 + out11_37 * z**11 && true;
ghost cvrted1_38@uint12: cvrted1_38 = out0_38 * z**0 + out1_38 * z**1 + out2_38 * z**2 + out3_38 * z**3 + out4_38 * z**4 + out5_38 * z**5 + out6_38 * z**6 + out7_38 * z**7 + out8_38 * z**8 + out9_38 * z**9 + out10_38 * z**10 + out11_38 * z**11 && true;
ghost cvrted1_39@uint12: cvrted1_39 = out0_39 * z**0 + out1_39 * z**1 + out2_39 * z**2 + out3_39 * z**3 + out4_39 * z**4 + out5_39 * z**5 + out6_39 * z**6 + out7_39 * z**7 + out8_39 * z**8 + out9_39 * z**9 + out10_39 * z**10 + out11_39 * z**11 && true;
ghost cvrted1_40@uint12: cvrted1_40 = out0_40 * z**0 + out1_40 * z**1 + out2_40 * z**2 + out3_40 * z**3 + out4_40 * z**4 + out5_40 * z**5 + out6_40 * z**6 + out7_40 * z**7 + out8_40 * z**8 + out9_40 * z**9 + out10_40 * z**10 + out11_40 * z**11 && true;
ghost cvrted1_41@uint12: cvrted1_41 = out0_41 * z**0 + out1_41 * z**1 + out2_41 * z**2 + out3_41 * z**3 + out4_41 * z**4 + out5_41 * z**5 + out6_41 * z**6 + out7_41 * z**7 + out8_41 * z**8 + out9_41 * z**9 + out10_41 * z**10 + out11_41 * z**11 && true;
ghost cvrted1_42@uint12: cvrted1_42 = out0_42 * z**0 + out1_42 * z**1 + out2_42 * z**2 + out3_42 * z**3 + out4_42 * z**4 + out5_42 * z**5 + out6_42 * z**6 + out7_42 * z**7 + out8_42 * z**8 + out9_42 * z**9 + out10_42 * z**10 + out11_42 * z**11 && true;
ghost cvrted1_43@uint12: cvrted1_43 = out0_43 * z**0 + out1_43 * z**1 + out2_43 * z**2 + out3_43 * z**3 + out4_43 * z**4 + out5_43 * z**5 + out6_43 * z**6 + out7_43 * z**7 + out8_43 * z**8 + out9_43 * z**9 + out10_43 * z**10 + out11_43 * z**11 && true;
ghost cvrted1_44@uint12: cvrted1_44 = out0_44 * z**0 + out1_44 * z**1 + out2_44 * z**2 + out3_44 * z**3 + out4_44 * z**4 + out5_44 * z**5 + out6_44 * z**6 + out7_44 * z**7 + out8_44 * z**8 + out9_44 * z**9 + out10_44 * z**10 + out11_44 * z**11 && true;
ghost cvrted1_45@uint12: cvrted1_45 = out0_45 * z**0 + out1_45 * z**1 + out2_45 * z**2 + out3_45 * z**3 + out4_45 * z**4 + out5_45 * z**5 + out6_45 * z**6 + out7_45 * z**7 + out8_45 * z**8 + out9_45 * z**9 + out10_45 * z**10 + out11_45 * z**11 && true;
ghost cvrted1_46@uint12: cvrted1_46 = out0_46 * z**0 + out1_46 * z**1 + out2_46 * z**2 + out3_46 * z**3 + out4_46 * z**4 + out5_46 * z**5 + out6_46 * z**6 + out7_46 * z**7 + out8_46 * z**8 + out9_46 * z**9 + out10_46 * z**10 + out11_46 * z**11 && true;
ghost cvrted1_47@uint12: cvrted1_47 = out0_47 * z**0 + out1_47 * z**1 + out2_47 * z**2 + out3_47 * z**3 + out4_47 * z**4 + out5_47 * z**5 + out6_47 * z**6 + out7_47 * z**7 + out8_47 * z**8 + out9_47 * z**9 + out10_47 * z**10 + out11_47 * z**11 && true;
ghost cvrted1_48@uint12: cvrted1_48 = out0_48 * z**0 + out1_48 * z**1 + out2_48 * z**2 + out3_48 * z**3 + out4_48 * z**4 + out5_48 * z**5 + out6_48 * z**6 + out7_48 * z**7 + out8_48 * z**8 + out9_48 * z**9 + out10_48 * z**10 + out11_48 * z**11 && true;
ghost cvrted1_49@uint12: cvrted1_49 = out0_49 * z**0 + out1_49 * z**1 + out2_49 * z**2 + out3_49 * z**3 + out4_49 * z**4 + out5_49 * z**5 + out6_49 * z**6 + out7_49 * z**7 + out8_49 * z**8 + out9_49 * z**9 + out10_49 * z**10 + out11_49 * z**11 && true;
ghost cvrted1_50@uint12: cvrted1_50 = out0_50 * z**0 + out1_50 * z**1 + out2_50 * z**2 + out3_50 * z**3 + out4_50 * z**4 + out5_50 * z**5 + out6_50 * z**6 + out7_50 * z**7 + out8_50 * z**8 + out9_50 * z**9 + out10_50 * z**10 + out11_50 * z**11 && true;
ghost cvrted1_51@uint12: cvrted1_51 = out0_51 * z**0 + out1_51 * z**1 + out2_51 * z**2 + out3_51 * z**3 + out4_51 * z**4 + out5_51 * z**5 + out6_51 * z**6 + out7_51 * z**7 + out8_51 * z**8 + out9_51 * z**9 + out10_51 * z**10 + out11_51 * z**11 && true;
ghost cvrted1_52@uint12: cvrted1_52 = out0_52 * z**0 + out1_52 * z**1 + out2_52 * z**2 + out3_52 * z**3 + out4_52 * z**4 + out5_52 * z**5 + out6_52 * z**6 + out7_52 * z**7 + out8_52 * z**8 + out9_52 * z**9 + out10_52 * z**10 + out11_52 * z**11 && true;
ghost cvrted1_53@uint12: cvrted1_53 = out0_53 * z**0 + out1_53 * z**1 + out2_53 * z**2 + out3_53 * z**3 + out4_53 * z**4 + out5_53 * z**5 + out6_53 * z**6 + out7_53 * z**7 + out8_53 * z**8 + out9_53 * z**9 + out10_53 * z**10 + out11_53 * z**11 && true;
ghost cvrted1_54@uint12: cvrted1_54 = out0_54 * z**0 + out1_54 * z**1 + out2_54 * z**2 + out3_54 * z**3 + out4_54 * z**4 + out5_54 * z**5 + out6_54 * z**6 + out7_54 * z**7 + out8_54 * z**8 + out9_54 * z**9 + out10_54 * z**10 + out11_54 * z**11 && true;
ghost cvrted1_55@uint12: cvrted1_55 = out0_55 * z**0 + out1_55 * z**1 + out2_55 * z**2 + out3_55 * z**3 + out4_55 * z**4 + out5_55 * z**5 + out6_55 * z**6 + out7_55 * z**7 + out8_55 * z**8 + out9_55 * z**9 + out10_55 * z**10 + out11_55 * z**11 && true;
ghost cvrted1_56@uint12: cvrted1_56 = out0_56 * z**0 + out1_56 * z**1 + out2_56 * z**2 + out3_56 * z**3 + out4_56 * z**4 + out5_56 * z**5 + out6_56 * z**6 + out7_56 * z**7 + out8_56 * z**8 + out9_56 * z**9 + out10_56 * z**10 + out11_56 * z**11 && true;
ghost cvrted1_57@uint12: cvrted1_57 = out0_57 * z**0 + out1_57 * z**1 + out2_57 * z**2 + out3_57 * z**3 + out4_57 * z**4 + out5_57 * z**5 + out6_57 * z**6 + out7_57 * z**7 + out8_57 * z**8 + out9_57 * z**9 + out10_57 * z**10 + out11_57 * z**11 && true;
ghost cvrted1_58@uint12: cvrted1_58 = out0_58 * z**0 + out1_58 * z**1 + out2_58 * z**2 + out3_58 * z**3 + out4_58 * z**4 + out5_58 * z**5 + out6_58 * z**6 + out7_58 * z**7 + out8_58 * z**8 + out9_58 * z**9 + out10_58 * z**10 + out11_58 * z**11 && true;
ghost cvrted1_59@uint12: cvrted1_59 = out0_59 * z**0 + out1_59 * z**1 + out2_59 * z**2 + out3_59 * z**3 + out4_59 * z**4 + out5_59 * z**5 + out6_59 * z**6 + out7_59 * z**7 + out8_59 * z**8 + out9_59 * z**9 + out10_59 * z**10 + out11_59 * z**11 && true;
ghost cvrted1_60@uint12: cvrted1_60 = out0_60 * z**0 + out1_60 * z**1 + out2_60 * z**2 + out3_60 * z**3 + out4_60 * z**4 + out5_60 * z**5 + out6_60 * z**6 + out7_60 * z**7 + out8_60 * z**8 + out9_60 * z**9 + out10_60 * z**10 + out11_60 * z**11 && true;
ghost cvrted1_61@uint12: cvrted1_61 = out0_61 * z**0 + out1_61 * z**1 + out2_61 * z**2 + out3_61 * z**3 + out4_61 * z**4 + out5_61 * z**5 + out6_61 * z**6 + out7_61 * z**7 + out8_61 * z**8 + out9_61 * z**9 + out10_61 * z**10 + out11_61 * z**11 && true;
ghost cvrted1_62@uint12: cvrted1_62 = out0_62 * z**0 + out1_62 * z**1 + out2_62 * z**2 + out3_62 * z**3 + out4_62 * z**4 + out5_62 * z**5 + out6_62 * z**6 + out7_62 * z**7 + out8_62 * z**8 + out9_62 * z**9 + out10_62 * z**10 + out11_62 * z**11 && true;
ghost cvrted1_63@uint12: cvrted1_63 = out0_63 * z**0 + out1_63 * z**1 + out2_63 * z**2 + out3_63 * z**3 + out4_63 * z**4 + out5_63 * z**5 + out6_63 * z**6 + out7_63 * z**7 + out8_63 * z**8 + out9_63 * z**9 + out10_63 * z**10 + out11_63 * z**11 && true;

ecut and [
  eqmod inp1_0 (
    (cvrted1_0 + x * cvrted1_2) * (x ** 2 + x) ** 0 +
    (cvrted1_4 + x * cvrted1_6) * (x ** 2 + x) ** 1 +
    (cvrted1_8 + x * cvrted1_10) * (x ** 2 + x) ** 2 +
    (cvrted1_12 + x * cvrted1_14) * (x ** 2 + x) ** 3 +
    (cvrted1_16 + x * cvrted1_18) * (x ** 2 + x) ** 4 +
    (cvrted1_20 + x * cvrted1_22) * (x ** 2 + x) ** 5 +
    (cvrted1_24 + x * cvrted1_26) * (x ** 2 + x) ** 6 +
    (cvrted1_28 + x * cvrted1_30) * (x ** 2 + x) ** 7 +
    (cvrted1_32 + x * cvrted1_34) * (x ** 2 + x) ** 8 +
    (cvrted1_36 + x * cvrted1_38) * (x ** 2 + x) ** 9 +
    (cvrted1_40 + x * cvrted1_42) * (x ** 2 + x) ** 10 +
    (cvrted1_44 + x * cvrted1_46) * (x ** 2 + x) ** 11 +
    (cvrted1_48 + x * cvrted1_50) * (x ** 2 + x) ** 12 +
    (cvrted1_52 + x * cvrted1_54) * (x ** 2 + x) ** 13 +
    (cvrted1_56 + x * cvrted1_58) * (x ** 2 + x) ** 14 +
    (cvrted1_60 + x * cvrted1_62) * (x ** 2 + x) ** 15
  ) 2,
  eqmod inp1_1 (
    (cvrted1_1 + x * cvrted1_3) * (x ** 2 + x) ** 0 +
    (cvrted1_5 + x * cvrted1_7) * (x ** 2 + x) ** 1 +
    (cvrted1_9 + x * cvrted1_11) * (x ** 2 + x) ** 2 +
    (cvrted1_13 + x * cvrted1_15) * (x ** 2 + x) ** 3 +
    (cvrted1_17 + x * cvrted1_19) * (x ** 2 + x) ** 4 +
    (cvrted1_21 + x * cvrted1_23) * (x ** 2 + x) ** 5 +
    (cvrted1_25 + x * cvrted1_27) * (x ** 2 + x) ** 6 +
    (cvrted1_29 + x * cvrted1_31) * (x ** 2 + x) ** 7 +
    (cvrted1_33 + x * cvrted1_35) * (x ** 2 + x) ** 8 +
    (cvrted1_37 + x * cvrted1_39) * (x ** 2 + x) ** 9 +
    (cvrted1_41 + x * cvrted1_43) * (x ** 2 + x) ** 10 +
    (cvrted1_45 + x * cvrted1_47) * (x ** 2 + x) ** 11 +
    (cvrted1_49 + x * cvrted1_51) * (x ** 2 + x) ** 12 +
    (cvrted1_53 + x * cvrted1_55) * (x ** 2 + x) ** 13 +
    (cvrted1_57 + x * cvrted1_59) * (x ** 2 + x) ** 14 +
    (cvrted1_61 + x * cvrted1_63) * (x ** 2 + x) ** 15
  ) 2
];

(* #call   0x555555555380 <_PQCLEAN_MCELIECE348864_AVX_vec_mul_asm>#! PC = 0x555555555352 *)
#call   0x555555555380 <_PQCLEAN_MCELIECE348864_AVX_vec_mul_asm>#! 0x555555555352 = 0x555555555352;
(* #! -> SP = 0x7fffffffd828 *)
#! 0x7fffffffd828 = 0x7fffffffd828;
(* #! <- SP = 0x7fffffffd828 *)
#! 0x7fffffffd828 = 0x7fffffffd828;
(* #ret                                            #! PC = 0x555555555380 *)
#ret                                            #! 0x555555555380 = 0x555555555380;

nondet res1_0@bit; nondet res1_1@bit; nondet res1_2@bit; nondet res1_3@bit;
nondet res1_4@bit; nondet res1_5@bit; nondet res1_6@bit; nondet res1_7@bit;
nondet res1_8@bit; nondet res1_9@bit; nondet res1_10@bit; nondet res1_11@bit;
nondet res1_12@bit; nondet res1_13@bit; nondet res1_14@bit; nondet res1_15@bit;
nondet res1_16@bit; nondet res1_17@bit; nondet res1_18@bit; nondet res1_19@bit;
nondet res1_20@bit; nondet res1_21@bit; nondet res1_22@bit; nondet res1_23@bit;
nondet res1_24@bit; nondet res1_25@bit; nondet res1_26@bit; nondet res1_27@bit;
nondet res1_28@bit; nondet res1_29@bit; nondet res1_30@bit; nondet res1_31@bit;
nondet res1_32@bit; nondet res1_33@bit; nondet res1_34@bit; nondet res1_35@bit;
nondet res1_36@bit; nondet res1_37@bit; nondet res1_38@bit; nondet res1_39@bit;
nondet res1_40@bit; nondet res1_41@bit; nondet res1_42@bit; nondet res1_43@bit;
nondet res1_44@bit; nondet res1_45@bit; nondet res1_46@bit; nondet res1_47@bit;
nondet res1_48@bit; nondet res1_49@bit; nondet res1_50@bit; nondet res1_51@bit;
nondet res1_52@bit; nondet res1_53@bit; nondet res1_54@bit; nondet res1_55@bit;
nondet res1_56@bit; nondet res1_57@bit; nondet res1_58@bit; nondet res1_59@bit;
nondet res1_60@bit; nondet res1_61@bit; nondet res1_62@bit; nondet res1_63@bit;

assume and [
  eqmod res1_0 (cvrted1_0 * ((* 1  0 *) 1)) [2, modulus],
  eqmod res1_1 (cvrted1_1 * ((* 1  1 *) 1)) [2, modulus],
  eqmod res1_2 (cvrted1_2 * ((* 1  2 *) 1)) [2, modulus],
  eqmod res1_3 (cvrted1_3 * ((* 1  3 *) 1)) [2, modulus],
  eqmod res1_4 (cvrted1_4 * ((* 1  4 *) z**4 + z)) [2, modulus],
  eqmod res1_5 (cvrted1_5 * ((* 1  5 *) z**4 + z)) [2, modulus],
  eqmod res1_6 (cvrted1_6 * ((* 1  6 *) z**4 + z)) [2, modulus],
  eqmod res1_7 (cvrted1_7 * ((* 1  7 *) z**4 + z)) [2, modulus],
  eqmod res1_8 (cvrted1_8 * ((* 1  8 *) z**8 + z**2)) [2, modulus],
  eqmod res1_9 (cvrted1_9 * ((* 1  9 *) z**8 + z**2)) [2, modulus],
  eqmod res1_10 (cvrted1_10 * ((* 1 10 *) z**8 + z**2)) [2, modulus],
  eqmod res1_11 (cvrted1_11 * ((* 1 11 *) z**8 + z**2)) [2, modulus],
  eqmod res1_12 (cvrted1_12 * ((* 1 12 *) z**9 + z**6 + 1)) [2, modulus],
  eqmod res1_13 (cvrted1_13 * ((* 1 13 *) z**9 + z**6 + 1)) [2, modulus],
  eqmod res1_14 (cvrted1_14 * ((* 1 14 *) z**9 + z**6 + 1)) [2, modulus],
  eqmod res1_15 (cvrted1_15 * ((* 1 15 *) z**9 + z**6 + 1)) [2, modulus],
  eqmod res1_16 (cvrted1_16 * ((* 1 16 *) z**7)) [2, modulus],
  eqmod res1_17 (cvrted1_17 * ((* 1 17 *) z**7)) [2, modulus],
  eqmod res1_18 (cvrted1_18 * ((* 1 18 *) z**7)) [2, modulus],
  eqmod res1_19 (cvrted1_19 * ((* 1 19 *) z**7)) [2, modulus],
  eqmod res1_20 (cvrted1_20 * ((* 1 20 *) z**11 + z**8)) [2, modulus],
  eqmod res1_21 (cvrted1_21 * ((* 1 21 *) z**11 + z**8)) [2, modulus],
  eqmod res1_22 (cvrted1_22 * ((* 1 22 *) z**11 + z**8)) [2, modulus],
  eqmod res1_23 (cvrted1_23 * ((* 1 23 *) z**11 + z**8)) [2, modulus],
  eqmod res1_24 (cvrted1_24 * ((* 1 24 *) z**9 + z**6 + z**3)) [2, modulus],
  eqmod res1_25 (cvrted1_25 * ((* 1 25 *) z**9 + z**6 + z**3)) [2, modulus],
  eqmod res1_26 (cvrted1_26 * ((* 1 26 *) z**9 + z**6 + z**3)) [2, modulus],
  eqmod res1_27 (cvrted1_27 * ((* 1 27 *) z**9 + z**6 + z**3)) [2, modulus],
  eqmod res1_28 (cvrted1_28 * ((* 1 28 *) z)) [2, modulus],
  eqmod res1_29 (cvrted1_29 * ((* 1 29 *) z)) [2, modulus],
  eqmod res1_30 (cvrted1_30 * ((* 1 30 *) z)) [2, modulus],
  eqmod res1_31 (cvrted1_31 * ((* 1 31 *) z)) [2, modulus],
  eqmod res1_32 (cvrted1_32 * ((* 1 32 *) z**5 + z**2)) [2, modulus],
  eqmod res1_33 (cvrted1_33 * ((* 1 33 *) z**5 + z**2)) [2, modulus],
  eqmod res1_34 (cvrted1_34 * ((* 1 34 *) z**5 + z**2)) [2, modulus],
  eqmod res1_35 (cvrted1_35 * ((* 1 35 *) z**5 + z**2)) [2, modulus],
  eqmod res1_36 (cvrted1_36 * ((* 1 36 *) z**9 + z**3)) [2, modulus],
  eqmod res1_37 (cvrted1_37 * ((* 1 37 *) z**9 + z**3)) [2, modulus],
  eqmod res1_38 (cvrted1_38 * ((* 1 38 *) z**9 + z**3)) [2, modulus],
  eqmod res1_39 (cvrted1_39 * ((* 1 39 *) z**9 + z**3)) [2, modulus],
  eqmod res1_40 (cvrted1_40 * ((* 1 40 *) z**10 + z**7 + z)) [2, modulus],
  eqmod res1_41 (cvrted1_41 * ((* 1 41 *) z**10 + z**7 + z)) [2, modulus],
  eqmod res1_42 (cvrted1_42 * ((* 1 42 *) z**10 + z**7 + z)) [2, modulus],
  eqmod res1_43 (cvrted1_43 * ((* 1 43 *) z**10 + z**7 + z)) [2, modulus],
  eqmod res1_44 (cvrted1_44 * ((* 1 44 *) z**8)) [2, modulus],
  eqmod res1_45 (cvrted1_45 * ((* 1 45 *) z**8)) [2, modulus],
  eqmod res1_46 (cvrted1_46 * ((* 1 46 *) z**8)) [2, modulus],
  eqmod res1_47 (cvrted1_47 * ((* 1 47 *) z**8)) [2, modulus],
  eqmod res1_48 (cvrted1_48 * ((* 1 48 *) z**9 + z**3 + 1)) [2, modulus],
  eqmod res1_49 (cvrted1_49 * ((* 1 49 *) z**9 + z**3 + 1)) [2, modulus],
  eqmod res1_50 (cvrted1_50 * ((* 1 50 *) z**9 + z**3 + 1)) [2, modulus],
  eqmod res1_51 (cvrted1_51 * ((* 1 51 *) z**9 + z**3 + 1)) [2, modulus],
  eqmod res1_52 (cvrted1_52 * ((* 1 52 *) z**10 + z**7 + z**4)) [2, modulus],
  eqmod res1_53 (cvrted1_53 * ((* 1 53 *) z**10 + z**7 + z**4)) [2, modulus],
  eqmod res1_54 (cvrted1_54 * ((* 1 54 *) z**10 + z**7 + z**4)) [2, modulus],
  eqmod res1_55 (cvrted1_55 * ((* 1 55 *) z**10 + z**7 + z**4)) [2, modulus],
  eqmod res1_56 (cvrted1_56 * ((* 1 56 *) z**2)) [2, modulus],
  eqmod res1_57 (cvrted1_57 * ((* 1 57 *) z**2)) [2, modulus],
  eqmod res1_58 (cvrted1_58 * ((* 1 58 *) z**2)) [2, modulus],
  eqmod res1_59 (cvrted1_59 * ((* 1 59 *) z**2)) [2, modulus],
  eqmod res1_60 (cvrted1_60 * ((* 1 60 *) z**6 + z**3)) [2, modulus],
  eqmod res1_61 (cvrted1_61 * ((* 1 61 *) z**6 + z**3)) [2, modulus],
  eqmod res1_62 (cvrted1_62 * ((* 1 62 *) z**6 + z**3)) [2, modulus],
  eqmod res1_63 (cvrted1_63 * ((* 1 63 *) z**6 + z**3)) [2, modulus]
] && true;

nondet x2@uint12;

ecut and [
  eqmod (
    (cvrted1_0 + x * cvrted1_2) * ((z ** 4 + z) * x2) ** 0 +
    (cvrted1_4 + x * cvrted1_6) * ((z ** 4 + z) * x2) ** 1 +
    (cvrted1_8 + x * cvrted1_10) * ((z ** 4 + z) * x2) ** 2 +
    (cvrted1_12 + x * cvrted1_14) * ((z ** 4 + z) * x2) ** 3 +
    (cvrted1_16 + x * cvrted1_18) * ((z ** 4 + z) * x2) ** 4 +
    (cvrted1_20 + x * cvrted1_22) * ((z ** 4 + z) * x2) ** 5 +
    (cvrted1_24 + x * cvrted1_26) * ((z ** 4 + z) * x2) ** 6 +
    (cvrted1_28 + x * cvrted1_30) * ((z ** 4 + z) * x2) ** 7 +
    (cvrted1_32 + x * cvrted1_34) * ((z ** 4 + z) * x2) ** 8 +
    (cvrted1_36 + x * cvrted1_38) * ((z ** 4 + z) * x2) ** 9 +
    (cvrted1_40 + x * cvrted1_42) * ((z ** 4 + z) * x2) ** 10 +
    (cvrted1_44 + x * cvrted1_46) * ((z ** 4 + z) * x2) ** 11 +
    (cvrted1_48 + x * cvrted1_50) * ((z ** 4 + z) * x2) ** 12 +
    (cvrted1_52 + x * cvrted1_54) * ((z ** 4 + z) * x2) ** 13 +
    (cvrted1_56 + x * cvrted1_58) * ((z ** 4 + z) * x2) ** 14 +
    (cvrted1_60 + x * cvrted1_62) * ((z ** 4 + z) * x2) ** 15
  ) (
    (res1_0 + x * res1_2) * x2 ** 0 +
    (res1_4 + x * res1_6) * x2 ** 1 +
    (res1_8 + x * res1_10) * x2 ** 2 +
    (res1_12 + x * res1_14) * x2 ** 3 +
    (res1_16 + x * res1_18) * x2 ** 4 +
    (res1_20 + x * res1_22) * x2 ** 5 +
    (res1_24 + x * res1_26) * x2 ** 6 +
    (res1_28 + x * res1_30) * x2 ** 7 +
    (res1_32 + x * res1_34) * x2 ** 8 +
    (res1_36 + x * res1_38) * x2 ** 9 +
    (res1_40 + x * res1_42) * x2 ** 10 +
    (res1_44 + x * res1_46) * x2 ** 11 +
    (res1_48 + x * res1_50) * x2 ** 12 +
    (res1_52 + x * res1_54) * x2 ** 13 +
    (res1_56 + x * res1_58) * x2 ** 14 +
    (res1_60 + x * res1_62) * x2 ** 15
  ) [2, modulus],
  eqmod (
    (cvrted1_1 + x * cvrted1_3) * ((z ** 4 + z) * x2) ** 0 +
    (cvrted1_5 + x * cvrted1_7) * ((z ** 4 + z) * x2) ** 1 +
    (cvrted1_9 + x * cvrted1_11) * ((z ** 4 + z) * x2) ** 2 +
    (cvrted1_13 + x * cvrted1_15) * ((z ** 4 + z) * x2) ** 3 +
    (cvrted1_17 + x * cvrted1_19) * ((z ** 4 + z) * x2) ** 4 +
    (cvrted1_21 + x * cvrted1_23) * ((z ** 4 + z) * x2) ** 5 +
    (cvrted1_25 + x * cvrted1_27) * ((z ** 4 + z) * x2) ** 6 +
    (cvrted1_29 + x * cvrted1_31) * ((z ** 4 + z) * x2) ** 7 +
    (cvrted1_33 + x * cvrted1_35) * ((z ** 4 + z) * x2) ** 8 +
    (cvrted1_37 + x * cvrted1_39) * ((z ** 4 + z) * x2) ** 9 +
    (cvrted1_41 + x * cvrted1_43) * ((z ** 4 + z) * x2) ** 10 +
    (cvrted1_45 + x * cvrted1_47) * ((z ** 4 + z) * x2) ** 11 +
    (cvrted1_49 + x * cvrted1_51) * ((z ** 4 + z) * x2) ** 12 +
    (cvrted1_53 + x * cvrted1_55) * ((z ** 4 + z) * x2) ** 13 +
    (cvrted1_57 + x * cvrted1_59) * ((z ** 4 + z) * x2) ** 14 +
    (cvrted1_61 + x * cvrted1_63) * ((z ** 4 + z) * x2) ** 15
  ) (
    (res1_1 + x * res1_3) * x2 ** 0 +
    (res1_5 + x * res1_7) * x2 ** 1 +
    (res1_9 + x * res1_11) * x2 ** 2 +
    (res1_13 + x * res1_15) * x2 ** 3 +
    (res1_17 + x * res1_19) * x2 ** 4 +
    (res1_21 + x * res1_23) * x2 ** 5 +
    (res1_25 + x * res1_27) * x2 ** 6 +
    (res1_29 + x * res1_31) * x2 ** 7 +
    (res1_33 + x * res1_35) * x2 ** 8 +
    (res1_37 + x * res1_39) * x2 ** 9 +
    (res1_41 + x * res1_43) * x2 ** 10 +
    (res1_45 + x * res1_47) * x2 ** 11 +
    (res1_49 + x * res1_51) * x2 ** 12 +
    (res1_53 + x * res1_55) * x2 ** 13 +
    (res1_57 + x * res1_59) * x2 ** 14 +
    (res1_61 + x * res1_63) * x2 ** 15
  ) [2, modulus]
] prove with [precondition];

mov x x2;

nondet inp2_0@uint12; assume inp2_0 =
  res1_0 * x**0 + res1_4 * x**1 + res1_8 * x**2 + res1_12 * x**3 +
  res1_16 * x**4 + res1_20 * x**5 + res1_24 * x**6 + res1_28 * x**7 +
  res1_32 * x**8 + res1_36 * x**9 + res1_40 * x**10 + res1_44 * x**11 +
  res1_48 * x**12 + res1_52 * x**13 + res1_56 * x**14 + res1_60 * x**15
&& true;
nondet inp2_1@uint12; assume inp2_1 =
  res1_1 * x**0 + res1_5 * x**1 + res1_9 * x**2 + res1_13 * x**3 +
  res1_17 * x**4 + res1_21 * x**5 + res1_25 * x**6 + res1_29 * x**7 +
  res1_33 * x**8 + res1_37 * x**9 + res1_41 * x**10 + res1_45 * x**11 +
  res1_49 * x**12 + res1_53 * x**13 + res1_57 * x**14 + res1_61 * x**15
&& true;
nondet inp2_2@uint12; assume inp2_2 =
  res1_2 * x**0 + res1_6 * x**1 + res1_10 * x**2 + res1_14 * x**3 +
  res1_18 * x**4 + res1_22 * x**5 + res1_26 * x**6 + res1_30 * x**7 +
  res1_34 * x**8 + res1_38 * x**9 + res1_42 * x**10 + res1_46 * x**11 +
  res1_50 * x**12 + res1_54 * x**13 + res1_58 * x**14 + res1_62 * x**15
&& true;
nondet inp2_3@uint12; assume inp2_3 =
  res1_3 * x**0 + res1_7 * x**1 + res1_11 * x**2 + res1_15 * x**3 +
  res1_19 * x**4 + res1_23 * x**5 + res1_27 * x**6 + res1_31 * x**7 +
  res1_35 * x**8 + res1_39 * x**9 + res1_43 * x**10 + res1_47 * x**11 +
  res1_51 * x**12 + res1_55 * x**13 + res1_59 * x**14 + res1_63 * x**15
&& true;

nondet rb0_0@bit; nondet rb0_1@bit; nondet rb0_2@bit; nondet rb0_3@bit; nondet rb0_4@bit; nondet rb0_5@bit; nondet rb0_6@bit; nondet rb0_7@bit; nondet rb0_8@bit; nondet rb0_9@bit; nondet rb0_10@bit; nondet rb0_11@bit; nondet rb0_12@bit; nondet rb0_13@bit; nondet rb0_14@bit; nondet rb0_15@bit; nondet rb0_16@bit; nondet rb0_17@bit; nondet rb0_18@bit; nondet rb0_19@bit; nondet rb0_20@bit; nondet rb0_21@bit; nondet rb0_22@bit; nondet rb0_23@bit; nondet rb0_24@bit; nondet rb0_25@bit; nondet rb0_26@bit; nondet rb0_27@bit; nondet rb0_28@bit; nondet rb0_29@bit; nondet rb0_30@bit; nondet rb0_31@bit; nondet rb0_32@bit; nondet rb0_33@bit; nondet rb0_34@bit; nondet rb0_35@bit; nondet rb0_36@bit; nondet rb0_37@bit; nondet rb0_38@bit; nondet rb0_39@bit; nondet rb0_40@bit; nondet rb0_41@bit; nondet rb0_42@bit; nondet rb0_43@bit; nondet rb0_44@bit; nondet rb0_45@bit; nondet rb0_46@bit; nondet rb0_47@bit; nondet rb0_48@bit; nondet rb0_49@bit; nondet rb0_50@bit; nondet rb0_51@bit; nondet rb0_52@bit; nondet rb0_53@bit; nondet rb0_54@bit; nondet rb0_55@bit; nondet rb0_56@bit; nondet rb0_57@bit; nondet rb0_58@bit; nondet rb0_59@bit; nondet rb0_60@bit; nondet rb0_61@bit; nondet rb0_62@bit; nondet rb0_63@bit;
nondet rb1_0@bit; nondet rb1_1@bit; nondet rb1_2@bit; nondet rb1_3@bit; nondet rb1_4@bit; nondet rb1_5@bit; nondet rb1_6@bit; nondet rb1_7@bit; nondet rb1_8@bit; nondet rb1_9@bit; nondet rb1_10@bit; nondet rb1_11@bit; nondet rb1_12@bit; nondet rb1_13@bit; nondet rb1_14@bit; nondet rb1_15@bit; nondet rb1_16@bit; nondet rb1_17@bit; nondet rb1_18@bit; nondet rb1_19@bit; nondet rb1_20@bit; nondet rb1_21@bit; nondet rb1_22@bit; nondet rb1_23@bit; nondet rb1_24@bit; nondet rb1_25@bit; nondet rb1_26@bit; nondet rb1_27@bit; nondet rb1_28@bit; nondet rb1_29@bit; nondet rb1_30@bit; nondet rb1_31@bit; nondet rb1_32@bit; nondet rb1_33@bit; nondet rb1_34@bit; nondet rb1_35@bit; nondet rb1_36@bit; nondet rb1_37@bit; nondet rb1_38@bit; nondet rb1_39@bit; nondet rb1_40@bit; nondet rb1_41@bit; nondet rb1_42@bit; nondet rb1_43@bit; nondet rb1_44@bit; nondet rb1_45@bit; nondet rb1_46@bit; nondet rb1_47@bit; nondet rb1_48@bit; nondet rb1_49@bit; nondet rb1_50@bit; nondet rb1_51@bit; nondet rb1_52@bit; nondet rb1_53@bit; nondet rb1_54@bit; nondet rb1_55@bit; nondet rb1_56@bit; nondet rb1_57@bit; nondet rb1_58@bit; nondet rb1_59@bit; nondet rb1_60@bit; nondet rb1_61@bit; nondet rb1_62@bit; nondet rb1_63@bit;
nondet rb2_0@bit; nondet rb2_1@bit; nondet rb2_2@bit; nondet rb2_3@bit; nondet rb2_4@bit; nondet rb2_5@bit; nondet rb2_6@bit; nondet rb2_7@bit; nondet rb2_8@bit; nondet rb2_9@bit; nondet rb2_10@bit; nondet rb2_11@bit; nondet rb2_12@bit; nondet rb2_13@bit; nondet rb2_14@bit; nondet rb2_15@bit; nondet rb2_16@bit; nondet rb2_17@bit; nondet rb2_18@bit; nondet rb2_19@bit; nondet rb2_20@bit; nondet rb2_21@bit; nondet rb2_22@bit; nondet rb2_23@bit; nondet rb2_24@bit; nondet rb2_25@bit; nondet rb2_26@bit; nondet rb2_27@bit; nondet rb2_28@bit; nondet rb2_29@bit; nondet rb2_30@bit; nondet rb2_31@bit; nondet rb2_32@bit; nondet rb2_33@bit; nondet rb2_34@bit; nondet rb2_35@bit; nondet rb2_36@bit; nondet rb2_37@bit; nondet rb2_38@bit; nondet rb2_39@bit; nondet rb2_40@bit; nondet rb2_41@bit; nondet rb2_42@bit; nondet rb2_43@bit; nondet rb2_44@bit; nondet rb2_45@bit; nondet rb2_46@bit; nondet rb2_47@bit; nondet rb2_48@bit; nondet rb2_49@bit; nondet rb2_50@bit; nondet rb2_51@bit; nondet rb2_52@bit; nondet rb2_53@bit; nondet rb2_54@bit; nondet rb2_55@bit; nondet rb2_56@bit; nondet rb2_57@bit; nondet rb2_58@bit; nondet rb2_59@bit; nondet rb2_60@bit; nondet rb2_61@bit; nondet rb2_62@bit; nondet rb2_63@bit;
nondet rb3_0@bit; nondet rb3_1@bit; nondet rb3_2@bit; nondet rb3_3@bit; nondet rb3_4@bit; nondet rb3_5@bit; nondet rb3_6@bit; nondet rb3_7@bit; nondet rb3_8@bit; nondet rb3_9@bit; nondet rb3_10@bit; nondet rb3_11@bit; nondet rb3_12@bit; nondet rb3_13@bit; nondet rb3_14@bit; nondet rb3_15@bit; nondet rb3_16@bit; nondet rb3_17@bit; nondet rb3_18@bit; nondet rb3_19@bit; nondet rb3_20@bit; nondet rb3_21@bit; nondet rb3_22@bit; nondet rb3_23@bit; nondet rb3_24@bit; nondet rb3_25@bit; nondet rb3_26@bit; nondet rb3_27@bit; nondet rb3_28@bit; nondet rb3_29@bit; nondet rb3_30@bit; nondet rb3_31@bit; nondet rb3_32@bit; nondet rb3_33@bit; nondet rb3_34@bit; nondet rb3_35@bit; nondet rb3_36@bit; nondet rb3_37@bit; nondet rb3_38@bit; nondet rb3_39@bit; nondet rb3_40@bit; nondet rb3_41@bit; nondet rb3_42@bit; nondet rb3_43@bit; nondet rb3_44@bit; nondet rb3_45@bit; nondet rb3_46@bit; nondet rb3_47@bit; nondet rb3_48@bit; nondet rb3_49@bit; nondet rb3_50@bit; nondet rb3_51@bit; nondet rb3_52@bit; nondet rb3_53@bit; nondet rb3_54@bit; nondet rb3_55@bit; nondet rb3_56@bit; nondet rb3_57@bit; nondet rb3_58@bit; nondet rb3_59@bit; nondet rb3_60@bit; nondet rb3_61@bit; nondet rb3_62@bit; nondet rb3_63@bit;
nondet rb4_0@bit; nondet rb4_1@bit; nondet rb4_2@bit; nondet rb4_3@bit; nondet rb4_4@bit; nondet rb4_5@bit; nondet rb4_6@bit; nondet rb4_7@bit; nondet rb4_8@bit; nondet rb4_9@bit; nondet rb4_10@bit; nondet rb4_11@bit; nondet rb4_12@bit; nondet rb4_13@bit; nondet rb4_14@bit; nondet rb4_15@bit; nondet rb4_16@bit; nondet rb4_17@bit; nondet rb4_18@bit; nondet rb4_19@bit; nondet rb4_20@bit; nondet rb4_21@bit; nondet rb4_22@bit; nondet rb4_23@bit; nondet rb4_24@bit; nondet rb4_25@bit; nondet rb4_26@bit; nondet rb4_27@bit; nondet rb4_28@bit; nondet rb4_29@bit; nondet rb4_30@bit; nondet rb4_31@bit; nondet rb4_32@bit; nondet rb4_33@bit; nondet rb4_34@bit; nondet rb4_35@bit; nondet rb4_36@bit; nondet rb4_37@bit; nondet rb4_38@bit; nondet rb4_39@bit; nondet rb4_40@bit; nondet rb4_41@bit; nondet rb4_42@bit; nondet rb4_43@bit; nondet rb4_44@bit; nondet rb4_45@bit; nondet rb4_46@bit; nondet rb4_47@bit; nondet rb4_48@bit; nondet rb4_49@bit; nondet rb4_50@bit; nondet rb4_51@bit; nondet rb4_52@bit; nondet rb4_53@bit; nondet rb4_54@bit; nondet rb4_55@bit; nondet rb4_56@bit; nondet rb4_57@bit; nondet rb4_58@bit; nondet rb4_59@bit; nondet rb4_60@bit; nondet rb4_61@bit; nondet rb4_62@bit; nondet rb4_63@bit;
nondet rb5_0@bit; nondet rb5_1@bit; nondet rb5_2@bit; nondet rb5_3@bit; nondet rb5_4@bit; nondet rb5_5@bit; nondet rb5_6@bit; nondet rb5_7@bit; nondet rb5_8@bit; nondet rb5_9@bit; nondet rb5_10@bit; nondet rb5_11@bit; nondet rb5_12@bit; nondet rb5_13@bit; nondet rb5_14@bit; nondet rb5_15@bit; nondet rb5_16@bit; nondet rb5_17@bit; nondet rb5_18@bit; nondet rb5_19@bit; nondet rb5_20@bit; nondet rb5_21@bit; nondet rb5_22@bit; nondet rb5_23@bit; nondet rb5_24@bit; nondet rb5_25@bit; nondet rb5_26@bit; nondet rb5_27@bit; nondet rb5_28@bit; nondet rb5_29@bit; nondet rb5_30@bit; nondet rb5_31@bit; nondet rb5_32@bit; nondet rb5_33@bit; nondet rb5_34@bit; nondet rb5_35@bit; nondet rb5_36@bit; nondet rb5_37@bit; nondet rb5_38@bit; nondet rb5_39@bit; nondet rb5_40@bit; nondet rb5_41@bit; nondet rb5_42@bit; nondet rb5_43@bit; nondet rb5_44@bit; nondet rb5_45@bit; nondet rb5_46@bit; nondet rb5_47@bit; nondet rb5_48@bit; nondet rb5_49@bit; nondet rb5_50@bit; nondet rb5_51@bit; nondet rb5_52@bit; nondet rb5_53@bit; nondet rb5_54@bit; nondet rb5_55@bit; nondet rb5_56@bit; nondet rb5_57@bit; nondet rb5_58@bit; nondet rb5_59@bit; nondet rb5_60@bit; nondet rb5_61@bit; nondet rb5_62@bit; nondet rb5_63@bit;
nondet rb6_0@bit; nondet rb6_1@bit; nondet rb6_2@bit; nondet rb6_3@bit; nondet rb6_4@bit; nondet rb6_5@bit; nondet rb6_6@bit; nondet rb6_7@bit; nondet rb6_8@bit; nondet rb6_9@bit; nondet rb6_10@bit; nondet rb6_11@bit; nondet rb6_12@bit; nondet rb6_13@bit; nondet rb6_14@bit; nondet rb6_15@bit; nondet rb6_16@bit; nondet rb6_17@bit; nondet rb6_18@bit; nondet rb6_19@bit; nondet rb6_20@bit; nondet rb6_21@bit; nondet rb6_22@bit; nondet rb6_23@bit; nondet rb6_24@bit; nondet rb6_25@bit; nondet rb6_26@bit; nondet rb6_27@bit; nondet rb6_28@bit; nondet rb6_29@bit; nondet rb6_30@bit; nondet rb6_31@bit; nondet rb6_32@bit; nondet rb6_33@bit; nondet rb6_34@bit; nondet rb6_35@bit; nondet rb6_36@bit; nondet rb6_37@bit; nondet rb6_38@bit; nondet rb6_39@bit; nondet rb6_40@bit; nondet rb6_41@bit; nondet rb6_42@bit; nondet rb6_43@bit; nondet rb6_44@bit; nondet rb6_45@bit; nondet rb6_46@bit; nondet rb6_47@bit; nondet rb6_48@bit; nondet rb6_49@bit; nondet rb6_50@bit; nondet rb6_51@bit; nondet rb6_52@bit; nondet rb6_53@bit; nondet rb6_54@bit; nondet rb6_55@bit; nondet rb6_56@bit; nondet rb6_57@bit; nondet rb6_58@bit; nondet rb6_59@bit; nondet rb6_60@bit; nondet rb6_61@bit; nondet rb6_62@bit; nondet rb6_63@bit;
nondet rb7_0@bit; nondet rb7_1@bit; nondet rb7_2@bit; nondet rb7_3@bit; nondet rb7_4@bit; nondet rb7_5@bit; nondet rb7_6@bit; nondet rb7_7@bit; nondet rb7_8@bit; nondet rb7_9@bit; nondet rb7_10@bit; nondet rb7_11@bit; nondet rb7_12@bit; nondet rb7_13@bit; nondet rb7_14@bit; nondet rb7_15@bit; nondet rb7_16@bit; nondet rb7_17@bit; nondet rb7_18@bit; nondet rb7_19@bit; nondet rb7_20@bit; nondet rb7_21@bit; nondet rb7_22@bit; nondet rb7_23@bit; nondet rb7_24@bit; nondet rb7_25@bit; nondet rb7_26@bit; nondet rb7_27@bit; nondet rb7_28@bit; nondet rb7_29@bit; nondet rb7_30@bit; nondet rb7_31@bit; nondet rb7_32@bit; nondet rb7_33@bit; nondet rb7_34@bit; nondet rb7_35@bit; nondet rb7_36@bit; nondet rb7_37@bit; nondet rb7_38@bit; nondet rb7_39@bit; nondet rb7_40@bit; nondet rb7_41@bit; nondet rb7_42@bit; nondet rb7_43@bit; nondet rb7_44@bit; nondet rb7_45@bit; nondet rb7_46@bit; nondet rb7_47@bit; nondet rb7_48@bit; nondet rb7_49@bit; nondet rb7_50@bit; nondet rb7_51@bit; nondet rb7_52@bit; nondet rb7_53@bit; nondet rb7_54@bit; nondet rb7_55@bit; nondet rb7_56@bit; nondet rb7_57@bit; nondet rb7_58@bit; nondet rb7_59@bit; nondet rb7_60@bit; nondet rb7_61@bit; nondet rb7_62@bit; nondet rb7_63@bit;
nondet rb8_0@bit; nondet rb8_1@bit; nondet rb8_2@bit; nondet rb8_3@bit; nondet rb8_4@bit; nondet rb8_5@bit; nondet rb8_6@bit; nondet rb8_7@bit; nondet rb8_8@bit; nondet rb8_9@bit; nondet rb8_10@bit; nondet rb8_11@bit; nondet rb8_12@bit; nondet rb8_13@bit; nondet rb8_14@bit; nondet rb8_15@bit; nondet rb8_16@bit; nondet rb8_17@bit; nondet rb8_18@bit; nondet rb8_19@bit; nondet rb8_20@bit; nondet rb8_21@bit; nondet rb8_22@bit; nondet rb8_23@bit; nondet rb8_24@bit; nondet rb8_25@bit; nondet rb8_26@bit; nondet rb8_27@bit; nondet rb8_28@bit; nondet rb8_29@bit; nondet rb8_30@bit; nondet rb8_31@bit; nondet rb8_32@bit; nondet rb8_33@bit; nondet rb8_34@bit; nondet rb8_35@bit; nondet rb8_36@bit; nondet rb8_37@bit; nondet rb8_38@bit; nondet rb8_39@bit; nondet rb8_40@bit; nondet rb8_41@bit; nondet rb8_42@bit; nondet rb8_43@bit; nondet rb8_44@bit; nondet rb8_45@bit; nondet rb8_46@bit; nondet rb8_47@bit; nondet rb8_48@bit; nondet rb8_49@bit; nondet rb8_50@bit; nondet rb8_51@bit; nondet rb8_52@bit; nondet rb8_53@bit; nondet rb8_54@bit; nondet rb8_55@bit; nondet rb8_56@bit; nondet rb8_57@bit; nondet rb8_58@bit; nondet rb8_59@bit; nondet rb8_60@bit; nondet rb8_61@bit; nondet rb8_62@bit; nondet rb8_63@bit;
nondet rb9_0@bit; nondet rb9_1@bit; nondet rb9_2@bit; nondet rb9_3@bit; nondet rb9_4@bit; nondet rb9_5@bit; nondet rb9_6@bit; nondet rb9_7@bit; nondet rb9_8@bit; nondet rb9_9@bit; nondet rb9_10@bit; nondet rb9_11@bit; nondet rb9_12@bit; nondet rb9_13@bit; nondet rb9_14@bit; nondet rb9_15@bit; nondet rb9_16@bit; nondet rb9_17@bit; nondet rb9_18@bit; nondet rb9_19@bit; nondet rb9_20@bit; nondet rb9_21@bit; nondet rb9_22@bit; nondet rb9_23@bit; nondet rb9_24@bit; nondet rb9_25@bit; nondet rb9_26@bit; nondet rb9_27@bit; nondet rb9_28@bit; nondet rb9_29@bit; nondet rb9_30@bit; nondet rb9_31@bit; nondet rb9_32@bit; nondet rb9_33@bit; nondet rb9_34@bit; nondet rb9_35@bit; nondet rb9_36@bit; nondet rb9_37@bit; nondet rb9_38@bit; nondet rb9_39@bit; nondet rb9_40@bit; nondet rb9_41@bit; nondet rb9_42@bit; nondet rb9_43@bit; nondet rb9_44@bit; nondet rb9_45@bit; nondet rb9_46@bit; nondet rb9_47@bit; nondet rb9_48@bit; nondet rb9_49@bit; nondet rb9_50@bit; nondet rb9_51@bit; nondet rb9_52@bit; nondet rb9_53@bit; nondet rb9_54@bit; nondet rb9_55@bit; nondet rb9_56@bit; nondet rb9_57@bit; nondet rb9_58@bit; nondet rb9_59@bit; nondet rb9_60@bit; nondet rb9_61@bit; nondet rb9_62@bit; nondet rb9_63@bit;
nondet rb10_0@bit; nondet rb10_1@bit; nondet rb10_2@bit; nondet rb10_3@bit; nondet rb10_4@bit; nondet rb10_5@bit; nondet rb10_6@bit; nondet rb10_7@bit; nondet rb10_8@bit; nondet rb10_9@bit; nondet rb10_10@bit; nondet rb10_11@bit; nondet rb10_12@bit; nondet rb10_13@bit; nondet rb10_14@bit; nondet rb10_15@bit; nondet rb10_16@bit; nondet rb10_17@bit; nondet rb10_18@bit; nondet rb10_19@bit; nondet rb10_20@bit; nondet rb10_21@bit; nondet rb10_22@bit; nondet rb10_23@bit; nondet rb10_24@bit; nondet rb10_25@bit; nondet rb10_26@bit; nondet rb10_27@bit; nondet rb10_28@bit; nondet rb10_29@bit; nondet rb10_30@bit; nondet rb10_31@bit; nondet rb10_32@bit; nondet rb10_33@bit; nondet rb10_34@bit; nondet rb10_35@bit; nondet rb10_36@bit; nondet rb10_37@bit; nondet rb10_38@bit; nondet rb10_39@bit; nondet rb10_40@bit; nondet rb10_41@bit; nondet rb10_42@bit; nondet rb10_43@bit; nondet rb10_44@bit; nondet rb10_45@bit; nondet rb10_46@bit; nondet rb10_47@bit; nondet rb10_48@bit; nondet rb10_49@bit; nondet rb10_50@bit; nondet rb10_51@bit; nondet rb10_52@bit; nondet rb10_53@bit; nondet rb10_54@bit; nondet rb10_55@bit; nondet rb10_56@bit; nondet rb10_57@bit; nondet rb10_58@bit; nondet rb10_59@bit; nondet rb10_60@bit; nondet rb10_61@bit; nondet rb10_62@bit; nondet rb10_63@bit;
nondet rb11_0@bit; nondet rb11_1@bit; nondet rb11_2@bit; nondet rb11_3@bit; nondet rb11_4@bit; nondet rb11_5@bit; nondet rb11_6@bit; nondet rb11_7@bit; nondet rb11_8@bit; nondet rb11_9@bit; nondet rb11_10@bit; nondet rb11_11@bit; nondet rb11_12@bit; nondet rb11_13@bit; nondet rb11_14@bit; nondet rb11_15@bit; nondet rb11_16@bit; nondet rb11_17@bit; nondet rb11_18@bit; nondet rb11_19@bit; nondet rb11_20@bit; nondet rb11_21@bit; nondet rb11_22@bit; nondet rb11_23@bit; nondet rb11_24@bit; nondet rb11_25@bit; nondet rb11_26@bit; nondet rb11_27@bit; nondet rb11_28@bit; nondet rb11_29@bit; nondet rb11_30@bit; nondet rb11_31@bit; nondet rb11_32@bit; nondet rb11_33@bit; nondet rb11_34@bit; nondet rb11_35@bit; nondet rb11_36@bit; nondet rb11_37@bit; nondet rb11_38@bit; nondet rb11_39@bit; nondet rb11_40@bit; nondet rb11_41@bit; nondet rb11_42@bit; nondet rb11_43@bit; nondet rb11_44@bit; nondet rb11_45@bit; nondet rb11_46@bit; nondet rb11_47@bit; nondet rb11_48@bit; nondet rb11_49@bit; nondet rb11_50@bit; nondet rb11_51@bit; nondet rb11_52@bit; nondet rb11_53@bit; nondet rb11_54@bit; nondet rb11_55@bit; nondet rb11_56@bit; nondet rb11_57@bit; nondet rb11_58@bit; nondet rb11_59@bit; nondet rb11_60@bit; nondet rb11_61@bit; nondet rb11_62@bit; nondet rb11_63@bit;
assume and [
  res1_0 = rb0_0 * z**0 + rb1_0 * z**1 + rb2_0 * z**2 + rb3_0 * z**3 + rb4_0 * z**4 + rb5_0 * z**5 + rb6_0 * z**6 + rb7_0 * z**7 + rb8_0 * z**8 + rb9_0 * z**9 + rb10_0 * z**10 + rb11_0 * z**11,
  res1_1 = rb0_1 * z**0 + rb1_1 * z**1 + rb2_1 * z**2 + rb3_1 * z**3 + rb4_1 * z**4 + rb5_1 * z**5 + rb6_1 * z**6 + rb7_1 * z**7 + rb8_1 * z**8 + rb9_1 * z**9 + rb10_1 * z**10 + rb11_1 * z**11,
  res1_2 = rb0_2 * z**0 + rb1_2 * z**1 + rb2_2 * z**2 + rb3_2 * z**3 + rb4_2 * z**4 + rb5_2 * z**5 + rb6_2 * z**6 + rb7_2 * z**7 + rb8_2 * z**8 + rb9_2 * z**9 + rb10_2 * z**10 + rb11_2 * z**11,
  res1_3 = rb0_3 * z**0 + rb1_3 * z**1 + rb2_3 * z**2 + rb3_3 * z**3 + rb4_3 * z**4 + rb5_3 * z**5 + rb6_3 * z**6 + rb7_3 * z**7 + rb8_3 * z**8 + rb9_3 * z**9 + rb10_3 * z**10 + rb11_3 * z**11,
  res1_4 = rb0_4 * z**0 + rb1_4 * z**1 + rb2_4 * z**2 + rb3_4 * z**3 + rb4_4 * z**4 + rb5_4 * z**5 + rb6_4 * z**6 + rb7_4 * z**7 + rb8_4 * z**8 + rb9_4 * z**9 + rb10_4 * z**10 + rb11_4 * z**11,
  res1_5 = rb0_5 * z**0 + rb1_5 * z**1 + rb2_5 * z**2 + rb3_5 * z**3 + rb4_5 * z**4 + rb5_5 * z**5 + rb6_5 * z**6 + rb7_5 * z**7 + rb8_5 * z**8 + rb9_5 * z**9 + rb10_5 * z**10 + rb11_5 * z**11,
  res1_6 = rb0_6 * z**0 + rb1_6 * z**1 + rb2_6 * z**2 + rb3_6 * z**3 + rb4_6 * z**4 + rb5_6 * z**5 + rb6_6 * z**6 + rb7_6 * z**7 + rb8_6 * z**8 + rb9_6 * z**9 + rb10_6 * z**10 + rb11_6 * z**11,
  res1_7 = rb0_7 * z**0 + rb1_7 * z**1 + rb2_7 * z**2 + rb3_7 * z**3 + rb4_7 * z**4 + rb5_7 * z**5 + rb6_7 * z**6 + rb7_7 * z**7 + rb8_7 * z**8 + rb9_7 * z**9 + rb10_7 * z**10 + rb11_7 * z**11,
  res1_8 = rb0_8 * z**0 + rb1_8 * z**1 + rb2_8 * z**2 + rb3_8 * z**3 + rb4_8 * z**4 + rb5_8 * z**5 + rb6_8 * z**6 + rb7_8 * z**7 + rb8_8 * z**8 + rb9_8 * z**9 + rb10_8 * z**10 + rb11_8 * z**11,
  res1_9 = rb0_9 * z**0 + rb1_9 * z**1 + rb2_9 * z**2 + rb3_9 * z**3 + rb4_9 * z**4 + rb5_9 * z**5 + rb6_9 * z**6 + rb7_9 * z**7 + rb8_9 * z**8 + rb9_9 * z**9 + rb10_9 * z**10 + rb11_9 * z**11,
  res1_10 = rb0_10 * z**0 + rb1_10 * z**1 + rb2_10 * z**2 + rb3_10 * z**3 + rb4_10 * z**4 + rb5_10 * z**5 + rb6_10 * z**6 + rb7_10 * z**7 + rb8_10 * z**8 + rb9_10 * z**9 + rb10_10 * z**10 + rb11_10 * z**11,
  res1_11 = rb0_11 * z**0 + rb1_11 * z**1 + rb2_11 * z**2 + rb3_11 * z**3 + rb4_11 * z**4 + rb5_11 * z**5 + rb6_11 * z**6 + rb7_11 * z**7 + rb8_11 * z**8 + rb9_11 * z**9 + rb10_11 * z**10 + rb11_11 * z**11,
  res1_12 = rb0_12 * z**0 + rb1_12 * z**1 + rb2_12 * z**2 + rb3_12 * z**3 + rb4_12 * z**4 + rb5_12 * z**5 + rb6_12 * z**6 + rb7_12 * z**7 + rb8_12 * z**8 + rb9_12 * z**9 + rb10_12 * z**10 + rb11_12 * z**11,
  res1_13 = rb0_13 * z**0 + rb1_13 * z**1 + rb2_13 * z**2 + rb3_13 * z**3 + rb4_13 * z**4 + rb5_13 * z**5 + rb6_13 * z**6 + rb7_13 * z**7 + rb8_13 * z**8 + rb9_13 * z**9 + rb10_13 * z**10 + rb11_13 * z**11,
  res1_14 = rb0_14 * z**0 + rb1_14 * z**1 + rb2_14 * z**2 + rb3_14 * z**3 + rb4_14 * z**4 + rb5_14 * z**5 + rb6_14 * z**6 + rb7_14 * z**7 + rb8_14 * z**8 + rb9_14 * z**9 + rb10_14 * z**10 + rb11_14 * z**11,
  res1_15 = rb0_15 * z**0 + rb1_15 * z**1 + rb2_15 * z**2 + rb3_15 * z**3 + rb4_15 * z**4 + rb5_15 * z**5 + rb6_15 * z**6 + rb7_15 * z**7 + rb8_15 * z**8 + rb9_15 * z**9 + rb10_15 * z**10 + rb11_15 * z**11,
  res1_16 = rb0_16 * z**0 + rb1_16 * z**1 + rb2_16 * z**2 + rb3_16 * z**3 + rb4_16 * z**4 + rb5_16 * z**5 + rb6_16 * z**6 + rb7_16 * z**7 + rb8_16 * z**8 + rb9_16 * z**9 + rb10_16 * z**10 + rb11_16 * z**11,
  res1_17 = rb0_17 * z**0 + rb1_17 * z**1 + rb2_17 * z**2 + rb3_17 * z**3 + rb4_17 * z**4 + rb5_17 * z**5 + rb6_17 * z**6 + rb7_17 * z**7 + rb8_17 * z**8 + rb9_17 * z**9 + rb10_17 * z**10 + rb11_17 * z**11,
  res1_18 = rb0_18 * z**0 + rb1_18 * z**1 + rb2_18 * z**2 + rb3_18 * z**3 + rb4_18 * z**4 + rb5_18 * z**5 + rb6_18 * z**6 + rb7_18 * z**7 + rb8_18 * z**8 + rb9_18 * z**9 + rb10_18 * z**10 + rb11_18 * z**11,
  res1_19 = rb0_19 * z**0 + rb1_19 * z**1 + rb2_19 * z**2 + rb3_19 * z**3 + rb4_19 * z**4 + rb5_19 * z**5 + rb6_19 * z**6 + rb7_19 * z**7 + rb8_19 * z**8 + rb9_19 * z**9 + rb10_19 * z**10 + rb11_19 * z**11,
  res1_20 = rb0_20 * z**0 + rb1_20 * z**1 + rb2_20 * z**2 + rb3_20 * z**3 + rb4_20 * z**4 + rb5_20 * z**5 + rb6_20 * z**6 + rb7_20 * z**7 + rb8_20 * z**8 + rb9_20 * z**9 + rb10_20 * z**10 + rb11_20 * z**11,
  res1_21 = rb0_21 * z**0 + rb1_21 * z**1 + rb2_21 * z**2 + rb3_21 * z**3 + rb4_21 * z**4 + rb5_21 * z**5 + rb6_21 * z**6 + rb7_21 * z**7 + rb8_21 * z**8 + rb9_21 * z**9 + rb10_21 * z**10 + rb11_21 * z**11,
  res1_22 = rb0_22 * z**0 + rb1_22 * z**1 + rb2_22 * z**2 + rb3_22 * z**3 + rb4_22 * z**4 + rb5_22 * z**5 + rb6_22 * z**6 + rb7_22 * z**7 + rb8_22 * z**8 + rb9_22 * z**9 + rb10_22 * z**10 + rb11_22 * z**11,
  res1_23 = rb0_23 * z**0 + rb1_23 * z**1 + rb2_23 * z**2 + rb3_23 * z**3 + rb4_23 * z**4 + rb5_23 * z**5 + rb6_23 * z**6 + rb7_23 * z**7 + rb8_23 * z**8 + rb9_23 * z**9 + rb10_23 * z**10 + rb11_23 * z**11,
  res1_24 = rb0_24 * z**0 + rb1_24 * z**1 + rb2_24 * z**2 + rb3_24 * z**3 + rb4_24 * z**4 + rb5_24 * z**5 + rb6_24 * z**6 + rb7_24 * z**7 + rb8_24 * z**8 + rb9_24 * z**9 + rb10_24 * z**10 + rb11_24 * z**11,
  res1_25 = rb0_25 * z**0 + rb1_25 * z**1 + rb2_25 * z**2 + rb3_25 * z**3 + rb4_25 * z**4 + rb5_25 * z**5 + rb6_25 * z**6 + rb7_25 * z**7 + rb8_25 * z**8 + rb9_25 * z**9 + rb10_25 * z**10 + rb11_25 * z**11,
  res1_26 = rb0_26 * z**0 + rb1_26 * z**1 + rb2_26 * z**2 + rb3_26 * z**3 + rb4_26 * z**4 + rb5_26 * z**5 + rb6_26 * z**6 + rb7_26 * z**7 + rb8_26 * z**8 + rb9_26 * z**9 + rb10_26 * z**10 + rb11_26 * z**11,
  res1_27 = rb0_27 * z**0 + rb1_27 * z**1 + rb2_27 * z**2 + rb3_27 * z**3 + rb4_27 * z**4 + rb5_27 * z**5 + rb6_27 * z**6 + rb7_27 * z**7 + rb8_27 * z**8 + rb9_27 * z**9 + rb10_27 * z**10 + rb11_27 * z**11,
  res1_28 = rb0_28 * z**0 + rb1_28 * z**1 + rb2_28 * z**2 + rb3_28 * z**3 + rb4_28 * z**4 + rb5_28 * z**5 + rb6_28 * z**6 + rb7_28 * z**7 + rb8_28 * z**8 + rb9_28 * z**9 + rb10_28 * z**10 + rb11_28 * z**11,
  res1_29 = rb0_29 * z**0 + rb1_29 * z**1 + rb2_29 * z**2 + rb3_29 * z**3 + rb4_29 * z**4 + rb5_29 * z**5 + rb6_29 * z**6 + rb7_29 * z**7 + rb8_29 * z**8 + rb9_29 * z**9 + rb10_29 * z**10 + rb11_29 * z**11,
  res1_30 = rb0_30 * z**0 + rb1_30 * z**1 + rb2_30 * z**2 + rb3_30 * z**3 + rb4_30 * z**4 + rb5_30 * z**5 + rb6_30 * z**6 + rb7_30 * z**7 + rb8_30 * z**8 + rb9_30 * z**9 + rb10_30 * z**10 + rb11_30 * z**11,
  res1_31 = rb0_31 * z**0 + rb1_31 * z**1 + rb2_31 * z**2 + rb3_31 * z**3 + rb4_31 * z**4 + rb5_31 * z**5 + rb6_31 * z**6 + rb7_31 * z**7 + rb8_31 * z**8 + rb9_31 * z**9 + rb10_31 * z**10 + rb11_31 * z**11,
  res1_32 = rb0_32 * z**0 + rb1_32 * z**1 + rb2_32 * z**2 + rb3_32 * z**3 + rb4_32 * z**4 + rb5_32 * z**5 + rb6_32 * z**6 + rb7_32 * z**7 + rb8_32 * z**8 + rb9_32 * z**9 + rb10_32 * z**10 + rb11_32 * z**11,
  res1_33 = rb0_33 * z**0 + rb1_33 * z**1 + rb2_33 * z**2 + rb3_33 * z**3 + rb4_33 * z**4 + rb5_33 * z**5 + rb6_33 * z**6 + rb7_33 * z**7 + rb8_33 * z**8 + rb9_33 * z**9 + rb10_33 * z**10 + rb11_33 * z**11,
  res1_34 = rb0_34 * z**0 + rb1_34 * z**1 + rb2_34 * z**2 + rb3_34 * z**3 + rb4_34 * z**4 + rb5_34 * z**5 + rb6_34 * z**6 + rb7_34 * z**7 + rb8_34 * z**8 + rb9_34 * z**9 + rb10_34 * z**10 + rb11_34 * z**11,
  res1_35 = rb0_35 * z**0 + rb1_35 * z**1 + rb2_35 * z**2 + rb3_35 * z**3 + rb4_35 * z**4 + rb5_35 * z**5 + rb6_35 * z**6 + rb7_35 * z**7 + rb8_35 * z**8 + rb9_35 * z**9 + rb10_35 * z**10 + rb11_35 * z**11,
  res1_36 = rb0_36 * z**0 + rb1_36 * z**1 + rb2_36 * z**2 + rb3_36 * z**3 + rb4_36 * z**4 + rb5_36 * z**5 + rb6_36 * z**6 + rb7_36 * z**7 + rb8_36 * z**8 + rb9_36 * z**9 + rb10_36 * z**10 + rb11_36 * z**11,
  res1_37 = rb0_37 * z**0 + rb1_37 * z**1 + rb2_37 * z**2 + rb3_37 * z**3 + rb4_37 * z**4 + rb5_37 * z**5 + rb6_37 * z**6 + rb7_37 * z**7 + rb8_37 * z**8 + rb9_37 * z**9 + rb10_37 * z**10 + rb11_37 * z**11,
  res1_38 = rb0_38 * z**0 + rb1_38 * z**1 + rb2_38 * z**2 + rb3_38 * z**3 + rb4_38 * z**4 + rb5_38 * z**5 + rb6_38 * z**6 + rb7_38 * z**7 + rb8_38 * z**8 + rb9_38 * z**9 + rb10_38 * z**10 + rb11_38 * z**11,
  res1_39 = rb0_39 * z**0 + rb1_39 * z**1 + rb2_39 * z**2 + rb3_39 * z**3 + rb4_39 * z**4 + rb5_39 * z**5 + rb6_39 * z**6 + rb7_39 * z**7 + rb8_39 * z**8 + rb9_39 * z**9 + rb10_39 * z**10 + rb11_39 * z**11,
  res1_40 = rb0_40 * z**0 + rb1_40 * z**1 + rb2_40 * z**2 + rb3_40 * z**3 + rb4_40 * z**4 + rb5_40 * z**5 + rb6_40 * z**6 + rb7_40 * z**7 + rb8_40 * z**8 + rb9_40 * z**9 + rb10_40 * z**10 + rb11_40 * z**11,
  res1_41 = rb0_41 * z**0 + rb1_41 * z**1 + rb2_41 * z**2 + rb3_41 * z**3 + rb4_41 * z**4 + rb5_41 * z**5 + rb6_41 * z**6 + rb7_41 * z**7 + rb8_41 * z**8 + rb9_41 * z**9 + rb10_41 * z**10 + rb11_41 * z**11,
  res1_42 = rb0_42 * z**0 + rb1_42 * z**1 + rb2_42 * z**2 + rb3_42 * z**3 + rb4_42 * z**4 + rb5_42 * z**5 + rb6_42 * z**6 + rb7_42 * z**7 + rb8_42 * z**8 + rb9_42 * z**9 + rb10_42 * z**10 + rb11_42 * z**11,
  res1_43 = rb0_43 * z**0 + rb1_43 * z**1 + rb2_43 * z**2 + rb3_43 * z**3 + rb4_43 * z**4 + rb5_43 * z**5 + rb6_43 * z**6 + rb7_43 * z**7 + rb8_43 * z**8 + rb9_43 * z**9 + rb10_43 * z**10 + rb11_43 * z**11,
  res1_44 = rb0_44 * z**0 + rb1_44 * z**1 + rb2_44 * z**2 + rb3_44 * z**3 + rb4_44 * z**4 + rb5_44 * z**5 + rb6_44 * z**6 + rb7_44 * z**7 + rb8_44 * z**8 + rb9_44 * z**9 + rb10_44 * z**10 + rb11_44 * z**11,
  res1_45 = rb0_45 * z**0 + rb1_45 * z**1 + rb2_45 * z**2 + rb3_45 * z**3 + rb4_45 * z**4 + rb5_45 * z**5 + rb6_45 * z**6 + rb7_45 * z**7 + rb8_45 * z**8 + rb9_45 * z**9 + rb10_45 * z**10 + rb11_45 * z**11,
  res1_46 = rb0_46 * z**0 + rb1_46 * z**1 + rb2_46 * z**2 + rb3_46 * z**3 + rb4_46 * z**4 + rb5_46 * z**5 + rb6_46 * z**6 + rb7_46 * z**7 + rb8_46 * z**8 + rb9_46 * z**9 + rb10_46 * z**10 + rb11_46 * z**11,
  res1_47 = rb0_47 * z**0 + rb1_47 * z**1 + rb2_47 * z**2 + rb3_47 * z**3 + rb4_47 * z**4 + rb5_47 * z**5 + rb6_47 * z**6 + rb7_47 * z**7 + rb8_47 * z**8 + rb9_47 * z**9 + rb10_47 * z**10 + rb11_47 * z**11,
  res1_48 = rb0_48 * z**0 + rb1_48 * z**1 + rb2_48 * z**2 + rb3_48 * z**3 + rb4_48 * z**4 + rb5_48 * z**5 + rb6_48 * z**6 + rb7_48 * z**7 + rb8_48 * z**8 + rb9_48 * z**9 + rb10_48 * z**10 + rb11_48 * z**11,
  res1_49 = rb0_49 * z**0 + rb1_49 * z**1 + rb2_49 * z**2 + rb3_49 * z**3 + rb4_49 * z**4 + rb5_49 * z**5 + rb6_49 * z**6 + rb7_49 * z**7 + rb8_49 * z**8 + rb9_49 * z**9 + rb10_49 * z**10 + rb11_49 * z**11,
  res1_50 = rb0_50 * z**0 + rb1_50 * z**1 + rb2_50 * z**2 + rb3_50 * z**3 + rb4_50 * z**4 + rb5_50 * z**5 + rb6_50 * z**6 + rb7_50 * z**7 + rb8_50 * z**8 + rb9_50 * z**9 + rb10_50 * z**10 + rb11_50 * z**11,
  res1_51 = rb0_51 * z**0 + rb1_51 * z**1 + rb2_51 * z**2 + rb3_51 * z**3 + rb4_51 * z**4 + rb5_51 * z**5 + rb6_51 * z**6 + rb7_51 * z**7 + rb8_51 * z**8 + rb9_51 * z**9 + rb10_51 * z**10 + rb11_51 * z**11,
  res1_52 = rb0_52 * z**0 + rb1_52 * z**1 + rb2_52 * z**2 + rb3_52 * z**3 + rb4_52 * z**4 + rb5_52 * z**5 + rb6_52 * z**6 + rb7_52 * z**7 + rb8_52 * z**8 + rb9_52 * z**9 + rb10_52 * z**10 + rb11_52 * z**11,
  res1_53 = rb0_53 * z**0 + rb1_53 * z**1 + rb2_53 * z**2 + rb3_53 * z**3 + rb4_53 * z**4 + rb5_53 * z**5 + rb6_53 * z**6 + rb7_53 * z**7 + rb8_53 * z**8 + rb9_53 * z**9 + rb10_53 * z**10 + rb11_53 * z**11,
  res1_54 = rb0_54 * z**0 + rb1_54 * z**1 + rb2_54 * z**2 + rb3_54 * z**3 + rb4_54 * z**4 + rb5_54 * z**5 + rb6_54 * z**6 + rb7_54 * z**7 + rb8_54 * z**8 + rb9_54 * z**9 + rb10_54 * z**10 + rb11_54 * z**11,
  res1_55 = rb0_55 * z**0 + rb1_55 * z**1 + rb2_55 * z**2 + rb3_55 * z**3 + rb4_55 * z**4 + rb5_55 * z**5 + rb6_55 * z**6 + rb7_55 * z**7 + rb8_55 * z**8 + rb9_55 * z**9 + rb10_55 * z**10 + rb11_55 * z**11,
  res1_56 = rb0_56 * z**0 + rb1_56 * z**1 + rb2_56 * z**2 + rb3_56 * z**3 + rb4_56 * z**4 + rb5_56 * z**5 + rb6_56 * z**6 + rb7_56 * z**7 + rb8_56 * z**8 + rb9_56 * z**9 + rb10_56 * z**10 + rb11_56 * z**11,
  res1_57 = rb0_57 * z**0 + rb1_57 * z**1 + rb2_57 * z**2 + rb3_57 * z**3 + rb4_57 * z**4 + rb5_57 * z**5 + rb6_57 * z**6 + rb7_57 * z**7 + rb8_57 * z**8 + rb9_57 * z**9 + rb10_57 * z**10 + rb11_57 * z**11,
  res1_58 = rb0_58 * z**0 + rb1_58 * z**1 + rb2_58 * z**2 + rb3_58 * z**3 + rb4_58 * z**4 + rb5_58 * z**5 + rb6_58 * z**6 + rb7_58 * z**7 + rb8_58 * z**8 + rb9_58 * z**9 + rb10_58 * z**10 + rb11_58 * z**11,
  res1_59 = rb0_59 * z**0 + rb1_59 * z**1 + rb2_59 * z**2 + rb3_59 * z**3 + rb4_59 * z**4 + rb5_59 * z**5 + rb6_59 * z**6 + rb7_59 * z**7 + rb8_59 * z**8 + rb9_59 * z**9 + rb10_59 * z**10 + rb11_59 * z**11,
  res1_60 = rb0_60 * z**0 + rb1_60 * z**1 + rb2_60 * z**2 + rb3_60 * z**3 + rb4_60 * z**4 + rb5_60 * z**5 + rb6_60 * z**6 + rb7_60 * z**7 + rb8_60 * z**8 + rb9_60 * z**9 + rb10_60 * z**10 + rb11_60 * z**11,
  res1_61 = rb0_61 * z**0 + rb1_61 * z**1 + rb2_61 * z**2 + rb3_61 * z**3 + rb4_61 * z**4 + rb5_61 * z**5 + rb6_61 * z**6 + rb7_61 * z**7 + rb8_61 * z**8 + rb9_61 * z**9 + rb10_61 * z**10 + rb11_61 * z**11,
  res1_62 = rb0_62 * z**0 + rb1_62 * z**1 + rb2_62 * z**2 + rb3_62 * z**3 + rb4_62 * z**4 + rb5_62 * z**5 + rb6_62 * z**6 + rb7_62 * z**7 + rb8_62 * z**8 + rb9_62 * z**9 + rb10_62 * z**10 + rb11_62 * z**11,
  res1_63 = rb0_63 * z**0 + rb1_63 * z**1 + rb2_63 * z**2 + rb3_63 * z**3 + rb4_63 * z**4 + rb5_63 * z**5 + rb6_63 * z**6 + rb7_63 * z**7 + rb8_63 * z**8 + rb9_63 * z**9 + rb10_63 * z**10 + rb11_63 * z**11
] && true;

mov %L0x7fffffffdaa0 [rb0_0, rb0_1, rb0_2, rb0_3, rb0_4, rb0_5, rb0_6, rb0_7, rb0_8, rb0_9, rb0_10, rb0_11, rb0_12, rb0_13, rb0_14, rb0_15, rb0_16, rb0_17, rb0_18, rb0_19, rb0_20, rb0_21, rb0_22, rb0_23, rb0_24, rb0_25, rb0_26, rb0_27, rb0_28, rb0_29, rb0_30, rb0_31, rb0_32, rb0_33, rb0_34, rb0_35, rb0_36, rb0_37, rb0_38, rb0_39, rb0_40, rb0_41, rb0_42, rb0_43, rb0_44, rb0_45, rb0_46, rb0_47, rb0_48, rb0_49, rb0_50, rb0_51, rb0_52, rb0_53, rb0_54, rb0_55, rb0_56, rb0_57, rb0_58, rb0_59, rb0_60, rb0_61, rb0_62, rb0_63];
mov %L0x7fffffffdaa8 [rb1_0, rb1_1, rb1_2, rb1_3, rb1_4, rb1_5, rb1_6, rb1_7, rb1_8, rb1_9, rb1_10, rb1_11, rb1_12, rb1_13, rb1_14, rb1_15, rb1_16, rb1_17, rb1_18, rb1_19, rb1_20, rb1_21, rb1_22, rb1_23, rb1_24, rb1_25, rb1_26, rb1_27, rb1_28, rb1_29, rb1_30, rb1_31, rb1_32, rb1_33, rb1_34, rb1_35, rb1_36, rb1_37, rb1_38, rb1_39, rb1_40, rb1_41, rb1_42, rb1_43, rb1_44, rb1_45, rb1_46, rb1_47, rb1_48, rb1_49, rb1_50, rb1_51, rb1_52, rb1_53, rb1_54, rb1_55, rb1_56, rb1_57, rb1_58, rb1_59, rb1_60, rb1_61, rb1_62, rb1_63];
mov %L0x7fffffffdab0 [rb2_0, rb2_1, rb2_2, rb2_3, rb2_4, rb2_5, rb2_6, rb2_7, rb2_8, rb2_9, rb2_10, rb2_11, rb2_12, rb2_13, rb2_14, rb2_15, rb2_16, rb2_17, rb2_18, rb2_19, rb2_20, rb2_21, rb2_22, rb2_23, rb2_24, rb2_25, rb2_26, rb2_27, rb2_28, rb2_29, rb2_30, rb2_31, rb2_32, rb2_33, rb2_34, rb2_35, rb2_36, rb2_37, rb2_38, rb2_39, rb2_40, rb2_41, rb2_42, rb2_43, rb2_44, rb2_45, rb2_46, rb2_47, rb2_48, rb2_49, rb2_50, rb2_51, rb2_52, rb2_53, rb2_54, rb2_55, rb2_56, rb2_57, rb2_58, rb2_59, rb2_60, rb2_61, rb2_62, rb2_63];
mov %L0x7fffffffdab8 [rb3_0, rb3_1, rb3_2, rb3_3, rb3_4, rb3_5, rb3_6, rb3_7, rb3_8, rb3_9, rb3_10, rb3_11, rb3_12, rb3_13, rb3_14, rb3_15, rb3_16, rb3_17, rb3_18, rb3_19, rb3_20, rb3_21, rb3_22, rb3_23, rb3_24, rb3_25, rb3_26, rb3_27, rb3_28, rb3_29, rb3_30, rb3_31, rb3_32, rb3_33, rb3_34, rb3_35, rb3_36, rb3_37, rb3_38, rb3_39, rb3_40, rb3_41, rb3_42, rb3_43, rb3_44, rb3_45, rb3_46, rb3_47, rb3_48, rb3_49, rb3_50, rb3_51, rb3_52, rb3_53, rb3_54, rb3_55, rb3_56, rb3_57, rb3_58, rb3_59, rb3_60, rb3_61, rb3_62, rb3_63];
mov %L0x7fffffffdac0 [rb4_0, rb4_1, rb4_2, rb4_3, rb4_4, rb4_5, rb4_6, rb4_7, rb4_8, rb4_9, rb4_10, rb4_11, rb4_12, rb4_13, rb4_14, rb4_15, rb4_16, rb4_17, rb4_18, rb4_19, rb4_20, rb4_21, rb4_22, rb4_23, rb4_24, rb4_25, rb4_26, rb4_27, rb4_28, rb4_29, rb4_30, rb4_31, rb4_32, rb4_33, rb4_34, rb4_35, rb4_36, rb4_37, rb4_38, rb4_39, rb4_40, rb4_41, rb4_42, rb4_43, rb4_44, rb4_45, rb4_46, rb4_47, rb4_48, rb4_49, rb4_50, rb4_51, rb4_52, rb4_53, rb4_54, rb4_55, rb4_56, rb4_57, rb4_58, rb4_59, rb4_60, rb4_61, rb4_62, rb4_63];
mov %L0x7fffffffdac8 [rb5_0, rb5_1, rb5_2, rb5_3, rb5_4, rb5_5, rb5_6, rb5_7, rb5_8, rb5_9, rb5_10, rb5_11, rb5_12, rb5_13, rb5_14, rb5_15, rb5_16, rb5_17, rb5_18, rb5_19, rb5_20, rb5_21, rb5_22, rb5_23, rb5_24, rb5_25, rb5_26, rb5_27, rb5_28, rb5_29, rb5_30, rb5_31, rb5_32, rb5_33, rb5_34, rb5_35, rb5_36, rb5_37, rb5_38, rb5_39, rb5_40, rb5_41, rb5_42, rb5_43, rb5_44, rb5_45, rb5_46, rb5_47, rb5_48, rb5_49, rb5_50, rb5_51, rb5_52, rb5_53, rb5_54, rb5_55, rb5_56, rb5_57, rb5_58, rb5_59, rb5_60, rb5_61, rb5_62, rb5_63];
mov %L0x7fffffffdad0 [rb6_0, rb6_1, rb6_2, rb6_3, rb6_4, rb6_5, rb6_6, rb6_7, rb6_8, rb6_9, rb6_10, rb6_11, rb6_12, rb6_13, rb6_14, rb6_15, rb6_16, rb6_17, rb6_18, rb6_19, rb6_20, rb6_21, rb6_22, rb6_23, rb6_24, rb6_25, rb6_26, rb6_27, rb6_28, rb6_29, rb6_30, rb6_31, rb6_32, rb6_33, rb6_34, rb6_35, rb6_36, rb6_37, rb6_38, rb6_39, rb6_40, rb6_41, rb6_42, rb6_43, rb6_44, rb6_45, rb6_46, rb6_47, rb6_48, rb6_49, rb6_50, rb6_51, rb6_52, rb6_53, rb6_54, rb6_55, rb6_56, rb6_57, rb6_58, rb6_59, rb6_60, rb6_61, rb6_62, rb6_63];
mov %L0x7fffffffdad8 [rb7_0, rb7_1, rb7_2, rb7_3, rb7_4, rb7_5, rb7_6, rb7_7, rb7_8, rb7_9, rb7_10, rb7_11, rb7_12, rb7_13, rb7_14, rb7_15, rb7_16, rb7_17, rb7_18, rb7_19, rb7_20, rb7_21, rb7_22, rb7_23, rb7_24, rb7_25, rb7_26, rb7_27, rb7_28, rb7_29, rb7_30, rb7_31, rb7_32, rb7_33, rb7_34, rb7_35, rb7_36, rb7_37, rb7_38, rb7_39, rb7_40, rb7_41, rb7_42, rb7_43, rb7_44, rb7_45, rb7_46, rb7_47, rb7_48, rb7_49, rb7_50, rb7_51, rb7_52, rb7_53, rb7_54, rb7_55, rb7_56, rb7_57, rb7_58, rb7_59, rb7_60, rb7_61, rb7_62, rb7_63];
mov %L0x7fffffffdae0 [rb8_0, rb8_1, rb8_2, rb8_3, rb8_4, rb8_5, rb8_6, rb8_7, rb8_8, rb8_9, rb8_10, rb8_11, rb8_12, rb8_13, rb8_14, rb8_15, rb8_16, rb8_17, rb8_18, rb8_19, rb8_20, rb8_21, rb8_22, rb8_23, rb8_24, rb8_25, rb8_26, rb8_27, rb8_28, rb8_29, rb8_30, rb8_31, rb8_32, rb8_33, rb8_34, rb8_35, rb8_36, rb8_37, rb8_38, rb8_39, rb8_40, rb8_41, rb8_42, rb8_43, rb8_44, rb8_45, rb8_46, rb8_47, rb8_48, rb8_49, rb8_50, rb8_51, rb8_52, rb8_53, rb8_54, rb8_55, rb8_56, rb8_57, rb8_58, rb8_59, rb8_60, rb8_61, rb8_62, rb8_63];
mov %L0x7fffffffdae8 [rb9_0, rb9_1, rb9_2, rb9_3, rb9_4, rb9_5, rb9_6, rb9_7, rb9_8, rb9_9, rb9_10, rb9_11, rb9_12, rb9_13, rb9_14, rb9_15, rb9_16, rb9_17, rb9_18, rb9_19, rb9_20, rb9_21, rb9_22, rb9_23, rb9_24, rb9_25, rb9_26, rb9_27, rb9_28, rb9_29, rb9_30, rb9_31, rb9_32, rb9_33, rb9_34, rb9_35, rb9_36, rb9_37, rb9_38, rb9_39, rb9_40, rb9_41, rb9_42, rb9_43, rb9_44, rb9_45, rb9_46, rb9_47, rb9_48, rb9_49, rb9_50, rb9_51, rb9_52, rb9_53, rb9_54, rb9_55, rb9_56, rb9_57, rb9_58, rb9_59, rb9_60, rb9_61, rb9_62, rb9_63];
mov %L0x7fffffffdaf0 [rb10_0, rb10_1, rb10_2, rb10_3, rb10_4, rb10_5, rb10_6, rb10_7, rb10_8, rb10_9, rb10_10, rb10_11, rb10_12, rb10_13, rb10_14, rb10_15, rb10_16, rb10_17, rb10_18, rb10_19, rb10_20, rb10_21, rb10_22, rb10_23, rb10_24, rb10_25, rb10_26, rb10_27, rb10_28, rb10_29, rb10_30, rb10_31, rb10_32, rb10_33, rb10_34, rb10_35, rb10_36, rb10_37, rb10_38, rb10_39, rb10_40, rb10_41, rb10_42, rb10_43, rb10_44, rb10_45, rb10_46, rb10_47, rb10_48, rb10_49, rb10_50, rb10_51, rb10_52, rb10_53, rb10_54, rb10_55, rb10_56, rb10_57, rb10_58, rb10_59, rb10_60, rb10_61, rb10_62, rb10_63];
mov %L0x7fffffffdaf8 [rb11_0, rb11_1, rb11_2, rb11_3, rb11_4, rb11_5, rb11_6, rb11_7, rb11_8, rb11_9, rb11_10, rb11_11, rb11_12, rb11_13, rb11_14, rb11_15, rb11_16, rb11_17, rb11_18, rb11_19, rb11_20, rb11_21, rb11_22, rb11_23, rb11_24, rb11_25, rb11_26, rb11_27, rb11_28, rb11_29, rb11_30, rb11_31, rb11_32, rb11_33, rb11_34, rb11_35, rb11_36, rb11_37, rb11_38, rb11_39, rb11_40, rb11_41, rb11_42, rb11_43, rb11_44, rb11_45, rb11_46, rb11_47, rb11_48, rb11_49, rb11_50, rb11_51, rb11_52, rb11_53, rb11_54, rb11_55, rb11_56, rb11_57, rb11_58, rb11_59, rb11_60, rb11_61, rb11_62, rb11_63];


(* #jne    0x5555555552e4 <radix_conversions+196>  #! PC = 0x55555555535b *)
#jne    0x5555555552e4 <radix_conversions+196>  #! 0x55555555535b = 0x55555555535b;
(* mov    %r15,%r9                                 #! PC = 0x5555555552e4 *)
mov r9 r15;
(* mov    $0x1,%r8d                                #! PC = 0x5555555552e7 *)
mov r8d 0x1@uint32;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaa0; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaa0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaa0; PC = 0x555555555330 *)
mov %L0x7fffffffdaa0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaa8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaa8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaa8; PC = 0x555555555330 *)
mov %L0x7fffffffdaa8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdab0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdab0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdab0; PC = 0x555555555330 *)
mov %L0x7fffffffdab0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdab8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdab8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdab8; PC = 0x555555555330 *)
mov %L0x7fffffffdab8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdac0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdac0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdac0; PC = 0x555555555330 *)
mov %L0x7fffffffdac0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdac8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdac8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdac8; PC = 0x555555555330 *)
mov %L0x7fffffffdac8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdad0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdad0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdad0; PC = 0x555555555330 *)
mov %L0x7fffffffdad0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdad8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdad8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdad8; PC = 0x555555555330 *)
mov %L0x7fffffffdad8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdae0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdae0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdae0; PC = 0x555555555330 *)
mov %L0x7fffffffdae0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdae8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdae8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdae8; PC = 0x555555555330 *)
mov %L0x7fffffffdae8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaf0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaf0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaf0; PC = 0x555555555330 *)
mov %L0x7fffffffdaf0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaf8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaf8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd858; Value = 0x0f000f000f000f00; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd858;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd850; Value = 0xf000f000f000f000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd850;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 2@8;
shl r11d r11d 2;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 4 *)
assert true && cl = 4@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 4 *)
assert true && cl = 4@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaf8; PC = 0x555555555330 *)
mov %L0x7fffffffdaf8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* add    $0x1,%rbp                                #! PC = 0x555555555347 *)
add ebp ebp 0x1@sint32;



mov [out0_0, out0_1, out0_2, out0_3, out0_4, out0_5, out0_6, out0_7, out0_8, out0_9, out0_10, out0_11, out0_12, out0_13, out0_14, out0_15, out0_16, out0_17, out0_18, out0_19, out0_20, out0_21, out0_22, out0_23, out0_24, out0_25, out0_26, out0_27, out0_28, out0_29, out0_30, out0_31, out0_32, out0_33, out0_34, out0_35, out0_36, out0_37, out0_38, out0_39, out0_40, out0_41, out0_42, out0_43, out0_44, out0_45, out0_46, out0_47, out0_48, out0_49, out0_50, out0_51, out0_52, out0_53, out0_54, out0_55, out0_56, out0_57, out0_58, out0_59, out0_60, out0_61, out0_62, out0_63] %L0x7fffffffdaa0;
mov [out1_0, out1_1, out1_2, out1_3, out1_4, out1_5, out1_6, out1_7, out1_8, out1_9, out1_10, out1_11, out1_12, out1_13, out1_14, out1_15, out1_16, out1_17, out1_18, out1_19, out1_20, out1_21, out1_22, out1_23, out1_24, out1_25, out1_26, out1_27, out1_28, out1_29, out1_30, out1_31, out1_32, out1_33, out1_34, out1_35, out1_36, out1_37, out1_38, out1_39, out1_40, out1_41, out1_42, out1_43, out1_44, out1_45, out1_46, out1_47, out1_48, out1_49, out1_50, out1_51, out1_52, out1_53, out1_54, out1_55, out1_56, out1_57, out1_58, out1_59, out1_60, out1_61, out1_62, out1_63] %L0x7fffffffdaa8;
mov [out2_0, out2_1, out2_2, out2_3, out2_4, out2_5, out2_6, out2_7, out2_8, out2_9, out2_10, out2_11, out2_12, out2_13, out2_14, out2_15, out2_16, out2_17, out2_18, out2_19, out2_20, out2_21, out2_22, out2_23, out2_24, out2_25, out2_26, out2_27, out2_28, out2_29, out2_30, out2_31, out2_32, out2_33, out2_34, out2_35, out2_36, out2_37, out2_38, out2_39, out2_40, out2_41, out2_42, out2_43, out2_44, out2_45, out2_46, out2_47, out2_48, out2_49, out2_50, out2_51, out2_52, out2_53, out2_54, out2_55, out2_56, out2_57, out2_58, out2_59, out2_60, out2_61, out2_62, out2_63] %L0x7fffffffdab0;
mov [out3_0, out3_1, out3_2, out3_3, out3_4, out3_5, out3_6, out3_7, out3_8, out3_9, out3_10, out3_11, out3_12, out3_13, out3_14, out3_15, out3_16, out3_17, out3_18, out3_19, out3_20, out3_21, out3_22, out3_23, out3_24, out3_25, out3_26, out3_27, out3_28, out3_29, out3_30, out3_31, out3_32, out3_33, out3_34, out3_35, out3_36, out3_37, out3_38, out3_39, out3_40, out3_41, out3_42, out3_43, out3_44, out3_45, out3_46, out3_47, out3_48, out3_49, out3_50, out3_51, out3_52, out3_53, out3_54, out3_55, out3_56, out3_57, out3_58, out3_59, out3_60, out3_61, out3_62, out3_63] %L0x7fffffffdab8;
mov [out4_0, out4_1, out4_2, out4_3, out4_4, out4_5, out4_6, out4_7, out4_8, out4_9, out4_10, out4_11, out4_12, out4_13, out4_14, out4_15, out4_16, out4_17, out4_18, out4_19, out4_20, out4_21, out4_22, out4_23, out4_24, out4_25, out4_26, out4_27, out4_28, out4_29, out4_30, out4_31, out4_32, out4_33, out4_34, out4_35, out4_36, out4_37, out4_38, out4_39, out4_40, out4_41, out4_42, out4_43, out4_44, out4_45, out4_46, out4_47, out4_48, out4_49, out4_50, out4_51, out4_52, out4_53, out4_54, out4_55, out4_56, out4_57, out4_58, out4_59, out4_60, out4_61, out4_62, out4_63] %L0x7fffffffdac0;
mov [out5_0, out5_1, out5_2, out5_3, out5_4, out5_5, out5_6, out5_7, out5_8, out5_9, out5_10, out5_11, out5_12, out5_13, out5_14, out5_15, out5_16, out5_17, out5_18, out5_19, out5_20, out5_21, out5_22, out5_23, out5_24, out5_25, out5_26, out5_27, out5_28, out5_29, out5_30, out5_31, out5_32, out5_33, out5_34, out5_35, out5_36, out5_37, out5_38, out5_39, out5_40, out5_41, out5_42, out5_43, out5_44, out5_45, out5_46, out5_47, out5_48, out5_49, out5_50, out5_51, out5_52, out5_53, out5_54, out5_55, out5_56, out5_57, out5_58, out5_59, out5_60, out5_61, out5_62, out5_63] %L0x7fffffffdac8;
mov [out6_0, out6_1, out6_2, out6_3, out6_4, out6_5, out6_6, out6_7, out6_8, out6_9, out6_10, out6_11, out6_12, out6_13, out6_14, out6_15, out6_16, out6_17, out6_18, out6_19, out6_20, out6_21, out6_22, out6_23, out6_24, out6_25, out6_26, out6_27, out6_28, out6_29, out6_30, out6_31, out6_32, out6_33, out6_34, out6_35, out6_36, out6_37, out6_38, out6_39, out6_40, out6_41, out6_42, out6_43, out6_44, out6_45, out6_46, out6_47, out6_48, out6_49, out6_50, out6_51, out6_52, out6_53, out6_54, out6_55, out6_56, out6_57, out6_58, out6_59, out6_60, out6_61, out6_62, out6_63] %L0x7fffffffdad0;
mov [out7_0, out7_1, out7_2, out7_3, out7_4, out7_5, out7_6, out7_7, out7_8, out7_9, out7_10, out7_11, out7_12, out7_13, out7_14, out7_15, out7_16, out7_17, out7_18, out7_19, out7_20, out7_21, out7_22, out7_23, out7_24, out7_25, out7_26, out7_27, out7_28, out7_29, out7_30, out7_31, out7_32, out7_33, out7_34, out7_35, out7_36, out7_37, out7_38, out7_39, out7_40, out7_41, out7_42, out7_43, out7_44, out7_45, out7_46, out7_47, out7_48, out7_49, out7_50, out7_51, out7_52, out7_53, out7_54, out7_55, out7_56, out7_57, out7_58, out7_59, out7_60, out7_61, out7_62, out7_63] %L0x7fffffffdad8;
mov [out8_0, out8_1, out8_2, out8_3, out8_4, out8_5, out8_6, out8_7, out8_8, out8_9, out8_10, out8_11, out8_12, out8_13, out8_14, out8_15, out8_16, out8_17, out8_18, out8_19, out8_20, out8_21, out8_22, out8_23, out8_24, out8_25, out8_26, out8_27, out8_28, out8_29, out8_30, out8_31, out8_32, out8_33, out8_34, out8_35, out8_36, out8_37, out8_38, out8_39, out8_40, out8_41, out8_42, out8_43, out8_44, out8_45, out8_46, out8_47, out8_48, out8_49, out8_50, out8_51, out8_52, out8_53, out8_54, out8_55, out8_56, out8_57, out8_58, out8_59, out8_60, out8_61, out8_62, out8_63] %L0x7fffffffdae0;
mov [out9_0, out9_1, out9_2, out9_3, out9_4, out9_5, out9_6, out9_7, out9_8, out9_9, out9_10, out9_11, out9_12, out9_13, out9_14, out9_15, out9_16, out9_17, out9_18, out9_19, out9_20, out9_21, out9_22, out9_23, out9_24, out9_25, out9_26, out9_27, out9_28, out9_29, out9_30, out9_31, out9_32, out9_33, out9_34, out9_35, out9_36, out9_37, out9_38, out9_39, out9_40, out9_41, out9_42, out9_43, out9_44, out9_45, out9_46, out9_47, out9_48, out9_49, out9_50, out9_51, out9_52, out9_53, out9_54, out9_55, out9_56, out9_57, out9_58, out9_59, out9_60, out9_61, out9_62, out9_63] %L0x7fffffffdae8;
mov [out10_0, out10_1, out10_2, out10_3, out10_4, out10_5, out10_6, out10_7, out10_8, out10_9, out10_10, out10_11, out10_12, out10_13, out10_14, out10_15, out10_16, out10_17, out10_18, out10_19, out10_20, out10_21, out10_22, out10_23, out10_24, out10_25, out10_26, out10_27, out10_28, out10_29, out10_30, out10_31, out10_32, out10_33, out10_34, out10_35, out10_36, out10_37, out10_38, out10_39, out10_40, out10_41, out10_42, out10_43, out10_44, out10_45, out10_46, out10_47, out10_48, out10_49, out10_50, out10_51, out10_52, out10_53, out10_54, out10_55, out10_56, out10_57, out10_58, out10_59, out10_60, out10_61, out10_62, out10_63] %L0x7fffffffdaf0;
mov [out11_0, out11_1, out11_2, out11_3, out11_4, out11_5, out11_6, out11_7, out11_8, out11_9, out11_10, out11_11, out11_12, out11_13, out11_14, out11_15, out11_16, out11_17, out11_18, out11_19, out11_20, out11_21, out11_22, out11_23, out11_24, out11_25, out11_26, out11_27, out11_28, out11_29, out11_30, out11_31, out11_32, out11_33, out11_34, out11_35, out11_36, out11_37, out11_38, out11_39, out11_40, out11_41, out11_42, out11_43, out11_44, out11_45, out11_46, out11_47, out11_48, out11_49, out11_50, out11_51, out11_52, out11_53, out11_54, out11_55, out11_56, out11_57, out11_58, out11_59, out11_60, out11_61, out11_62, out11_63] %L0x7fffffffdaf8;

ghost cvrted2_0@uint12: cvrted2_0 = out0_0 * z**0 + out1_0 * z**1 + out2_0 * z**2 + out3_0 * z**3 + out4_0 * z**4 + out5_0 * z**5 + out6_0 * z**6 + out7_0 * z**7 + out8_0 * z**8 + out9_0 * z**9 + out10_0 * z**10 + out11_0 * z**11 && true;
ghost cvrted2_1@uint12: cvrted2_1 = out0_1 * z**0 + out1_1 * z**1 + out2_1 * z**2 + out3_1 * z**3 + out4_1 * z**4 + out5_1 * z**5 + out6_1 * z**6 + out7_1 * z**7 + out8_1 * z**8 + out9_1 * z**9 + out10_1 * z**10 + out11_1 * z**11 && true;
ghost cvrted2_2@uint12: cvrted2_2 = out0_2 * z**0 + out1_2 * z**1 + out2_2 * z**2 + out3_2 * z**3 + out4_2 * z**4 + out5_2 * z**5 + out6_2 * z**6 + out7_2 * z**7 + out8_2 * z**8 + out9_2 * z**9 + out10_2 * z**10 + out11_2 * z**11 && true;
ghost cvrted2_3@uint12: cvrted2_3 = out0_3 * z**0 + out1_3 * z**1 + out2_3 * z**2 + out3_3 * z**3 + out4_3 * z**4 + out5_3 * z**5 + out6_3 * z**6 + out7_3 * z**7 + out8_3 * z**8 + out9_3 * z**9 + out10_3 * z**10 + out11_3 * z**11 && true;
ghost cvrted2_4@uint12: cvrted2_4 = out0_4 * z**0 + out1_4 * z**1 + out2_4 * z**2 + out3_4 * z**3 + out4_4 * z**4 + out5_4 * z**5 + out6_4 * z**6 + out7_4 * z**7 + out8_4 * z**8 + out9_4 * z**9 + out10_4 * z**10 + out11_4 * z**11 && true;
ghost cvrted2_5@uint12: cvrted2_5 = out0_5 * z**0 + out1_5 * z**1 + out2_5 * z**2 + out3_5 * z**3 + out4_5 * z**4 + out5_5 * z**5 + out6_5 * z**6 + out7_5 * z**7 + out8_5 * z**8 + out9_5 * z**9 + out10_5 * z**10 + out11_5 * z**11 && true;
ghost cvrted2_6@uint12: cvrted2_6 = out0_6 * z**0 + out1_6 * z**1 + out2_6 * z**2 + out3_6 * z**3 + out4_6 * z**4 + out5_6 * z**5 + out6_6 * z**6 + out7_6 * z**7 + out8_6 * z**8 + out9_6 * z**9 + out10_6 * z**10 + out11_6 * z**11 && true;
ghost cvrted2_7@uint12: cvrted2_7 = out0_7 * z**0 + out1_7 * z**1 + out2_7 * z**2 + out3_7 * z**3 + out4_7 * z**4 + out5_7 * z**5 + out6_7 * z**6 + out7_7 * z**7 + out8_7 * z**8 + out9_7 * z**9 + out10_7 * z**10 + out11_7 * z**11 && true;
ghost cvrted2_8@uint12: cvrted2_8 = out0_8 * z**0 + out1_8 * z**1 + out2_8 * z**2 + out3_8 * z**3 + out4_8 * z**4 + out5_8 * z**5 + out6_8 * z**6 + out7_8 * z**7 + out8_8 * z**8 + out9_8 * z**9 + out10_8 * z**10 + out11_8 * z**11 && true;
ghost cvrted2_9@uint12: cvrted2_9 = out0_9 * z**0 + out1_9 * z**1 + out2_9 * z**2 + out3_9 * z**3 + out4_9 * z**4 + out5_9 * z**5 + out6_9 * z**6 + out7_9 * z**7 + out8_9 * z**8 + out9_9 * z**9 + out10_9 * z**10 + out11_9 * z**11 && true;
ghost cvrted2_10@uint12: cvrted2_10 = out0_10 * z**0 + out1_10 * z**1 + out2_10 * z**2 + out3_10 * z**3 + out4_10 * z**4 + out5_10 * z**5 + out6_10 * z**6 + out7_10 * z**7 + out8_10 * z**8 + out9_10 * z**9 + out10_10 * z**10 + out11_10 * z**11 && true;
ghost cvrted2_11@uint12: cvrted2_11 = out0_11 * z**0 + out1_11 * z**1 + out2_11 * z**2 + out3_11 * z**3 + out4_11 * z**4 + out5_11 * z**5 + out6_11 * z**6 + out7_11 * z**7 + out8_11 * z**8 + out9_11 * z**9 + out10_11 * z**10 + out11_11 * z**11 && true;
ghost cvrted2_12@uint12: cvrted2_12 = out0_12 * z**0 + out1_12 * z**1 + out2_12 * z**2 + out3_12 * z**3 + out4_12 * z**4 + out5_12 * z**5 + out6_12 * z**6 + out7_12 * z**7 + out8_12 * z**8 + out9_12 * z**9 + out10_12 * z**10 + out11_12 * z**11 && true;
ghost cvrted2_13@uint12: cvrted2_13 = out0_13 * z**0 + out1_13 * z**1 + out2_13 * z**2 + out3_13 * z**3 + out4_13 * z**4 + out5_13 * z**5 + out6_13 * z**6 + out7_13 * z**7 + out8_13 * z**8 + out9_13 * z**9 + out10_13 * z**10 + out11_13 * z**11 && true;
ghost cvrted2_14@uint12: cvrted2_14 = out0_14 * z**0 + out1_14 * z**1 + out2_14 * z**2 + out3_14 * z**3 + out4_14 * z**4 + out5_14 * z**5 + out6_14 * z**6 + out7_14 * z**7 + out8_14 * z**8 + out9_14 * z**9 + out10_14 * z**10 + out11_14 * z**11 && true;
ghost cvrted2_15@uint12: cvrted2_15 = out0_15 * z**0 + out1_15 * z**1 + out2_15 * z**2 + out3_15 * z**3 + out4_15 * z**4 + out5_15 * z**5 + out6_15 * z**6 + out7_15 * z**7 + out8_15 * z**8 + out9_15 * z**9 + out10_15 * z**10 + out11_15 * z**11 && true;
ghost cvrted2_16@uint12: cvrted2_16 = out0_16 * z**0 + out1_16 * z**1 + out2_16 * z**2 + out3_16 * z**3 + out4_16 * z**4 + out5_16 * z**5 + out6_16 * z**6 + out7_16 * z**7 + out8_16 * z**8 + out9_16 * z**9 + out10_16 * z**10 + out11_16 * z**11 && true;
ghost cvrted2_17@uint12: cvrted2_17 = out0_17 * z**0 + out1_17 * z**1 + out2_17 * z**2 + out3_17 * z**3 + out4_17 * z**4 + out5_17 * z**5 + out6_17 * z**6 + out7_17 * z**7 + out8_17 * z**8 + out9_17 * z**9 + out10_17 * z**10 + out11_17 * z**11 && true;
ghost cvrted2_18@uint12: cvrted2_18 = out0_18 * z**0 + out1_18 * z**1 + out2_18 * z**2 + out3_18 * z**3 + out4_18 * z**4 + out5_18 * z**5 + out6_18 * z**6 + out7_18 * z**7 + out8_18 * z**8 + out9_18 * z**9 + out10_18 * z**10 + out11_18 * z**11 && true;
ghost cvrted2_19@uint12: cvrted2_19 = out0_19 * z**0 + out1_19 * z**1 + out2_19 * z**2 + out3_19 * z**3 + out4_19 * z**4 + out5_19 * z**5 + out6_19 * z**6 + out7_19 * z**7 + out8_19 * z**8 + out9_19 * z**9 + out10_19 * z**10 + out11_19 * z**11 && true;
ghost cvrted2_20@uint12: cvrted2_20 = out0_20 * z**0 + out1_20 * z**1 + out2_20 * z**2 + out3_20 * z**3 + out4_20 * z**4 + out5_20 * z**5 + out6_20 * z**6 + out7_20 * z**7 + out8_20 * z**8 + out9_20 * z**9 + out10_20 * z**10 + out11_20 * z**11 && true;
ghost cvrted2_21@uint12: cvrted2_21 = out0_21 * z**0 + out1_21 * z**1 + out2_21 * z**2 + out3_21 * z**3 + out4_21 * z**4 + out5_21 * z**5 + out6_21 * z**6 + out7_21 * z**7 + out8_21 * z**8 + out9_21 * z**9 + out10_21 * z**10 + out11_21 * z**11 && true;
ghost cvrted2_22@uint12: cvrted2_22 = out0_22 * z**0 + out1_22 * z**1 + out2_22 * z**2 + out3_22 * z**3 + out4_22 * z**4 + out5_22 * z**5 + out6_22 * z**6 + out7_22 * z**7 + out8_22 * z**8 + out9_22 * z**9 + out10_22 * z**10 + out11_22 * z**11 && true;
ghost cvrted2_23@uint12: cvrted2_23 = out0_23 * z**0 + out1_23 * z**1 + out2_23 * z**2 + out3_23 * z**3 + out4_23 * z**4 + out5_23 * z**5 + out6_23 * z**6 + out7_23 * z**7 + out8_23 * z**8 + out9_23 * z**9 + out10_23 * z**10 + out11_23 * z**11 && true;
ghost cvrted2_24@uint12: cvrted2_24 = out0_24 * z**0 + out1_24 * z**1 + out2_24 * z**2 + out3_24 * z**3 + out4_24 * z**4 + out5_24 * z**5 + out6_24 * z**6 + out7_24 * z**7 + out8_24 * z**8 + out9_24 * z**9 + out10_24 * z**10 + out11_24 * z**11 && true;
ghost cvrted2_25@uint12: cvrted2_25 = out0_25 * z**0 + out1_25 * z**1 + out2_25 * z**2 + out3_25 * z**3 + out4_25 * z**4 + out5_25 * z**5 + out6_25 * z**6 + out7_25 * z**7 + out8_25 * z**8 + out9_25 * z**9 + out10_25 * z**10 + out11_25 * z**11 && true;
ghost cvrted2_26@uint12: cvrted2_26 = out0_26 * z**0 + out1_26 * z**1 + out2_26 * z**2 + out3_26 * z**3 + out4_26 * z**4 + out5_26 * z**5 + out6_26 * z**6 + out7_26 * z**7 + out8_26 * z**8 + out9_26 * z**9 + out10_26 * z**10 + out11_26 * z**11 && true;
ghost cvrted2_27@uint12: cvrted2_27 = out0_27 * z**0 + out1_27 * z**1 + out2_27 * z**2 + out3_27 * z**3 + out4_27 * z**4 + out5_27 * z**5 + out6_27 * z**6 + out7_27 * z**7 + out8_27 * z**8 + out9_27 * z**9 + out10_27 * z**10 + out11_27 * z**11 && true;
ghost cvrted2_28@uint12: cvrted2_28 = out0_28 * z**0 + out1_28 * z**1 + out2_28 * z**2 + out3_28 * z**3 + out4_28 * z**4 + out5_28 * z**5 + out6_28 * z**6 + out7_28 * z**7 + out8_28 * z**8 + out9_28 * z**9 + out10_28 * z**10 + out11_28 * z**11 && true;
ghost cvrted2_29@uint12: cvrted2_29 = out0_29 * z**0 + out1_29 * z**1 + out2_29 * z**2 + out3_29 * z**3 + out4_29 * z**4 + out5_29 * z**5 + out6_29 * z**6 + out7_29 * z**7 + out8_29 * z**8 + out9_29 * z**9 + out10_29 * z**10 + out11_29 * z**11 && true;
ghost cvrted2_30@uint12: cvrted2_30 = out0_30 * z**0 + out1_30 * z**1 + out2_30 * z**2 + out3_30 * z**3 + out4_30 * z**4 + out5_30 * z**5 + out6_30 * z**6 + out7_30 * z**7 + out8_30 * z**8 + out9_30 * z**9 + out10_30 * z**10 + out11_30 * z**11 && true;
ghost cvrted2_31@uint12: cvrted2_31 = out0_31 * z**0 + out1_31 * z**1 + out2_31 * z**2 + out3_31 * z**3 + out4_31 * z**4 + out5_31 * z**5 + out6_31 * z**6 + out7_31 * z**7 + out8_31 * z**8 + out9_31 * z**9 + out10_31 * z**10 + out11_31 * z**11 && true;
ghost cvrted2_32@uint12: cvrted2_32 = out0_32 * z**0 + out1_32 * z**1 + out2_32 * z**2 + out3_32 * z**3 + out4_32 * z**4 + out5_32 * z**5 + out6_32 * z**6 + out7_32 * z**7 + out8_32 * z**8 + out9_32 * z**9 + out10_32 * z**10 + out11_32 * z**11 && true;
ghost cvrted2_33@uint12: cvrted2_33 = out0_33 * z**0 + out1_33 * z**1 + out2_33 * z**2 + out3_33 * z**3 + out4_33 * z**4 + out5_33 * z**5 + out6_33 * z**6 + out7_33 * z**7 + out8_33 * z**8 + out9_33 * z**9 + out10_33 * z**10 + out11_33 * z**11 && true;
ghost cvrted2_34@uint12: cvrted2_34 = out0_34 * z**0 + out1_34 * z**1 + out2_34 * z**2 + out3_34 * z**3 + out4_34 * z**4 + out5_34 * z**5 + out6_34 * z**6 + out7_34 * z**7 + out8_34 * z**8 + out9_34 * z**9 + out10_34 * z**10 + out11_34 * z**11 && true;
ghost cvrted2_35@uint12: cvrted2_35 = out0_35 * z**0 + out1_35 * z**1 + out2_35 * z**2 + out3_35 * z**3 + out4_35 * z**4 + out5_35 * z**5 + out6_35 * z**6 + out7_35 * z**7 + out8_35 * z**8 + out9_35 * z**9 + out10_35 * z**10 + out11_35 * z**11 && true;
ghost cvrted2_36@uint12: cvrted2_36 = out0_36 * z**0 + out1_36 * z**1 + out2_36 * z**2 + out3_36 * z**3 + out4_36 * z**4 + out5_36 * z**5 + out6_36 * z**6 + out7_36 * z**7 + out8_36 * z**8 + out9_36 * z**9 + out10_36 * z**10 + out11_36 * z**11 && true;
ghost cvrted2_37@uint12: cvrted2_37 = out0_37 * z**0 + out1_37 * z**1 + out2_37 * z**2 + out3_37 * z**3 + out4_37 * z**4 + out5_37 * z**5 + out6_37 * z**6 + out7_37 * z**7 + out8_37 * z**8 + out9_37 * z**9 + out10_37 * z**10 + out11_37 * z**11 && true;
ghost cvrted2_38@uint12: cvrted2_38 = out0_38 * z**0 + out1_38 * z**1 + out2_38 * z**2 + out3_38 * z**3 + out4_38 * z**4 + out5_38 * z**5 + out6_38 * z**6 + out7_38 * z**7 + out8_38 * z**8 + out9_38 * z**9 + out10_38 * z**10 + out11_38 * z**11 && true;
ghost cvrted2_39@uint12: cvrted2_39 = out0_39 * z**0 + out1_39 * z**1 + out2_39 * z**2 + out3_39 * z**3 + out4_39 * z**4 + out5_39 * z**5 + out6_39 * z**6 + out7_39 * z**7 + out8_39 * z**8 + out9_39 * z**9 + out10_39 * z**10 + out11_39 * z**11 && true;
ghost cvrted2_40@uint12: cvrted2_40 = out0_40 * z**0 + out1_40 * z**1 + out2_40 * z**2 + out3_40 * z**3 + out4_40 * z**4 + out5_40 * z**5 + out6_40 * z**6 + out7_40 * z**7 + out8_40 * z**8 + out9_40 * z**9 + out10_40 * z**10 + out11_40 * z**11 && true;
ghost cvrted2_41@uint12: cvrted2_41 = out0_41 * z**0 + out1_41 * z**1 + out2_41 * z**2 + out3_41 * z**3 + out4_41 * z**4 + out5_41 * z**5 + out6_41 * z**6 + out7_41 * z**7 + out8_41 * z**8 + out9_41 * z**9 + out10_41 * z**10 + out11_41 * z**11 && true;
ghost cvrted2_42@uint12: cvrted2_42 = out0_42 * z**0 + out1_42 * z**1 + out2_42 * z**2 + out3_42 * z**3 + out4_42 * z**4 + out5_42 * z**5 + out6_42 * z**6 + out7_42 * z**7 + out8_42 * z**8 + out9_42 * z**9 + out10_42 * z**10 + out11_42 * z**11 && true;
ghost cvrted2_43@uint12: cvrted2_43 = out0_43 * z**0 + out1_43 * z**1 + out2_43 * z**2 + out3_43 * z**3 + out4_43 * z**4 + out5_43 * z**5 + out6_43 * z**6 + out7_43 * z**7 + out8_43 * z**8 + out9_43 * z**9 + out10_43 * z**10 + out11_43 * z**11 && true;
ghost cvrted2_44@uint12: cvrted2_44 = out0_44 * z**0 + out1_44 * z**1 + out2_44 * z**2 + out3_44 * z**3 + out4_44 * z**4 + out5_44 * z**5 + out6_44 * z**6 + out7_44 * z**7 + out8_44 * z**8 + out9_44 * z**9 + out10_44 * z**10 + out11_44 * z**11 && true;
ghost cvrted2_45@uint12: cvrted2_45 = out0_45 * z**0 + out1_45 * z**1 + out2_45 * z**2 + out3_45 * z**3 + out4_45 * z**4 + out5_45 * z**5 + out6_45 * z**6 + out7_45 * z**7 + out8_45 * z**8 + out9_45 * z**9 + out10_45 * z**10 + out11_45 * z**11 && true;
ghost cvrted2_46@uint12: cvrted2_46 = out0_46 * z**0 + out1_46 * z**1 + out2_46 * z**2 + out3_46 * z**3 + out4_46 * z**4 + out5_46 * z**5 + out6_46 * z**6 + out7_46 * z**7 + out8_46 * z**8 + out9_46 * z**9 + out10_46 * z**10 + out11_46 * z**11 && true;
ghost cvrted2_47@uint12: cvrted2_47 = out0_47 * z**0 + out1_47 * z**1 + out2_47 * z**2 + out3_47 * z**3 + out4_47 * z**4 + out5_47 * z**5 + out6_47 * z**6 + out7_47 * z**7 + out8_47 * z**8 + out9_47 * z**9 + out10_47 * z**10 + out11_47 * z**11 && true;
ghost cvrted2_48@uint12: cvrted2_48 = out0_48 * z**0 + out1_48 * z**1 + out2_48 * z**2 + out3_48 * z**3 + out4_48 * z**4 + out5_48 * z**5 + out6_48 * z**6 + out7_48 * z**7 + out8_48 * z**8 + out9_48 * z**9 + out10_48 * z**10 + out11_48 * z**11 && true;
ghost cvrted2_49@uint12: cvrted2_49 = out0_49 * z**0 + out1_49 * z**1 + out2_49 * z**2 + out3_49 * z**3 + out4_49 * z**4 + out5_49 * z**5 + out6_49 * z**6 + out7_49 * z**7 + out8_49 * z**8 + out9_49 * z**9 + out10_49 * z**10 + out11_49 * z**11 && true;
ghost cvrted2_50@uint12: cvrted2_50 = out0_50 * z**0 + out1_50 * z**1 + out2_50 * z**2 + out3_50 * z**3 + out4_50 * z**4 + out5_50 * z**5 + out6_50 * z**6 + out7_50 * z**7 + out8_50 * z**8 + out9_50 * z**9 + out10_50 * z**10 + out11_50 * z**11 && true;
ghost cvrted2_51@uint12: cvrted2_51 = out0_51 * z**0 + out1_51 * z**1 + out2_51 * z**2 + out3_51 * z**3 + out4_51 * z**4 + out5_51 * z**5 + out6_51 * z**6 + out7_51 * z**7 + out8_51 * z**8 + out9_51 * z**9 + out10_51 * z**10 + out11_51 * z**11 && true;
ghost cvrted2_52@uint12: cvrted2_52 = out0_52 * z**0 + out1_52 * z**1 + out2_52 * z**2 + out3_52 * z**3 + out4_52 * z**4 + out5_52 * z**5 + out6_52 * z**6 + out7_52 * z**7 + out8_52 * z**8 + out9_52 * z**9 + out10_52 * z**10 + out11_52 * z**11 && true;
ghost cvrted2_53@uint12: cvrted2_53 = out0_53 * z**0 + out1_53 * z**1 + out2_53 * z**2 + out3_53 * z**3 + out4_53 * z**4 + out5_53 * z**5 + out6_53 * z**6 + out7_53 * z**7 + out8_53 * z**8 + out9_53 * z**9 + out10_53 * z**10 + out11_53 * z**11 && true;
ghost cvrted2_54@uint12: cvrted2_54 = out0_54 * z**0 + out1_54 * z**1 + out2_54 * z**2 + out3_54 * z**3 + out4_54 * z**4 + out5_54 * z**5 + out6_54 * z**6 + out7_54 * z**7 + out8_54 * z**8 + out9_54 * z**9 + out10_54 * z**10 + out11_54 * z**11 && true;
ghost cvrted2_55@uint12: cvrted2_55 = out0_55 * z**0 + out1_55 * z**1 + out2_55 * z**2 + out3_55 * z**3 + out4_55 * z**4 + out5_55 * z**5 + out6_55 * z**6 + out7_55 * z**7 + out8_55 * z**8 + out9_55 * z**9 + out10_55 * z**10 + out11_55 * z**11 && true;
ghost cvrted2_56@uint12: cvrted2_56 = out0_56 * z**0 + out1_56 * z**1 + out2_56 * z**2 + out3_56 * z**3 + out4_56 * z**4 + out5_56 * z**5 + out6_56 * z**6 + out7_56 * z**7 + out8_56 * z**8 + out9_56 * z**9 + out10_56 * z**10 + out11_56 * z**11 && true;
ghost cvrted2_57@uint12: cvrted2_57 = out0_57 * z**0 + out1_57 * z**1 + out2_57 * z**2 + out3_57 * z**3 + out4_57 * z**4 + out5_57 * z**5 + out6_57 * z**6 + out7_57 * z**7 + out8_57 * z**8 + out9_57 * z**9 + out10_57 * z**10 + out11_57 * z**11 && true;
ghost cvrted2_58@uint12: cvrted2_58 = out0_58 * z**0 + out1_58 * z**1 + out2_58 * z**2 + out3_58 * z**3 + out4_58 * z**4 + out5_58 * z**5 + out6_58 * z**6 + out7_58 * z**7 + out8_58 * z**8 + out9_58 * z**9 + out10_58 * z**10 + out11_58 * z**11 && true;
ghost cvrted2_59@uint12: cvrted2_59 = out0_59 * z**0 + out1_59 * z**1 + out2_59 * z**2 + out3_59 * z**3 + out4_59 * z**4 + out5_59 * z**5 + out6_59 * z**6 + out7_59 * z**7 + out8_59 * z**8 + out9_59 * z**9 + out10_59 * z**10 + out11_59 * z**11 && true;
ghost cvrted2_60@uint12: cvrted2_60 = out0_60 * z**0 + out1_60 * z**1 + out2_60 * z**2 + out3_60 * z**3 + out4_60 * z**4 + out5_60 * z**5 + out6_60 * z**6 + out7_60 * z**7 + out8_60 * z**8 + out9_60 * z**9 + out10_60 * z**10 + out11_60 * z**11 && true;
ghost cvrted2_61@uint12: cvrted2_61 = out0_61 * z**0 + out1_61 * z**1 + out2_61 * z**2 + out3_61 * z**3 + out4_61 * z**4 + out5_61 * z**5 + out6_61 * z**6 + out7_61 * z**7 + out8_61 * z**8 + out9_61 * z**9 + out10_61 * z**10 + out11_61 * z**11 && true;
ghost cvrted2_62@uint12: cvrted2_62 = out0_62 * z**0 + out1_62 * z**1 + out2_62 * z**2 + out3_62 * z**3 + out4_62 * z**4 + out5_62 * z**5 + out6_62 * z**6 + out7_62 * z**7 + out8_62 * z**8 + out9_62 * z**9 + out10_62 * z**10 + out11_62 * z**11 && true;
ghost cvrted2_63@uint12: cvrted2_63 = out0_63 * z**0 + out1_63 * z**1 + out2_63 * z**2 + out3_63 * z**3 + out4_63 * z**4 + out5_63 * z**5 + out6_63 * z**6 + out7_63 * z**7 + out8_63 * z**8 + out9_63 * z**9 + out10_63 * z**10 + out11_63 * z**11 && true;

ecut and [
  eqmod inp2_0 (
    (cvrted2_0 + x * cvrted2_4) * (x ** 2 + x) ** 0 +
    (cvrted2_8 + x * cvrted2_12) * (x ** 2 + x) ** 1 +
    (cvrted2_16 + x * cvrted2_20) * (x ** 2 + x) ** 2 +
    (cvrted2_24 + x * cvrted2_28) * (x ** 2 + x) ** 3 +
    (cvrted2_32 + x * cvrted2_36) * (x ** 2 + x) ** 4 +
    (cvrted2_40 + x * cvrted2_44) * (x ** 2 + x) ** 5 +
    (cvrted2_48 + x * cvrted2_52) * (x ** 2 + x) ** 6 +
    (cvrted2_56 + x * cvrted2_60) * (x ** 2 + x) ** 7
  ) 2,
  eqmod inp2_1 (
    (cvrted2_1 + x * cvrted2_5) * (x ** 2 + x) ** 0 +
    (cvrted2_9 + x * cvrted2_13) * (x ** 2 + x) ** 1 +
    (cvrted2_17 + x * cvrted2_21) * (x ** 2 + x) ** 2 +
    (cvrted2_25 + x * cvrted2_29) * (x ** 2 + x) ** 3 +
    (cvrted2_33 + x * cvrted2_37) * (x ** 2 + x) ** 4 +
    (cvrted2_41 + x * cvrted2_45) * (x ** 2 + x) ** 5 +
    (cvrted2_49 + x * cvrted2_53) * (x ** 2 + x) ** 6 +
    (cvrted2_57 + x * cvrted2_61) * (x ** 2 + x) ** 7
  ) 2,
  eqmod inp2_2 (
    (cvrted2_2 + x * cvrted2_6) * (x ** 2 + x) ** 0 +
    (cvrted2_10 + x * cvrted2_14) * (x ** 2 + x) ** 1 +
    (cvrted2_18 + x * cvrted2_22) * (x ** 2 + x) ** 2 +
    (cvrted2_26 + x * cvrted2_30) * (x ** 2 + x) ** 3 +
    (cvrted2_34 + x * cvrted2_38) * (x ** 2 + x) ** 4 +
    (cvrted2_42 + x * cvrted2_46) * (x ** 2 + x) ** 5 +
    (cvrted2_50 + x * cvrted2_54) * (x ** 2 + x) ** 6 +
    (cvrted2_58 + x * cvrted2_62) * (x ** 2 + x) ** 7
  ) 2,
  eqmod inp2_3 (
    (cvrted2_3 + x * cvrted2_7) * (x ** 2 + x) ** 0 +
    (cvrted2_11 + x * cvrted2_15) * (x ** 2 + x) ** 1 +
    (cvrted2_19 + x * cvrted2_23) * (x ** 2 + x) ** 2 +
    (cvrted2_27 + x * cvrted2_31) * (x ** 2 + x) ** 3 +
    (cvrted2_35 + x * cvrted2_39) * (x ** 2 + x) ** 4 +
    (cvrted2_43 + x * cvrted2_47) * (x ** 2 + x) ** 5 +
    (cvrted2_51 + x * cvrted2_55) * (x ** 2 + x) ** 6 +
    (cvrted2_59 + x * cvrted2_63) * (x ** 2 + x) ** 7
  ) 2
];


(* #call   0x555555555380 <_PQCLEAN_MCELIECE348864_AVX_vec_mul_asm>#! PC = 0x555555555352 *)
#call   0x555555555380 <_PQCLEAN_MCELIECE348864_AVX_vec_mul_asm>#! 0x555555555352 = 0x555555555352;
(* #! -> SP = 0x7fffffffd828 *)
#! 0x7fffffffd828 = 0x7fffffffd828;
(* #! <- SP = 0x7fffffffd828 *)
#! 0x7fffffffd828 = 0x7fffffffd828;
(* #ret                                            #! PC = 0x555555555380 *)
#ret                                            #! 0x555555555380 = 0x555555555380;

nondet res2_0@bit; nondet res2_1@bit; nondet res2_2@bit; nondet res2_3@bit;
nondet res2_4@bit; nondet res2_5@bit; nondet res2_6@bit; nondet res2_7@bit;
nondet res2_8@bit; nondet res2_9@bit; nondet res2_10@bit; nondet res2_11@bit;
nondet res2_12@bit; nondet res2_13@bit; nondet res2_14@bit; nondet res2_15@bit;
nondet res2_16@bit; nondet res2_17@bit; nondet res2_18@bit; nondet res2_19@bit;
nondet res2_20@bit; nondet res2_21@bit; nondet res2_22@bit; nondet res2_23@bit;
nondet res2_24@bit; nondet res2_25@bit; nondet res2_26@bit; nondet res2_27@bit;
nondet res2_28@bit; nondet res2_29@bit; nondet res2_30@bit; nondet res2_31@bit;
nondet res2_32@bit; nondet res2_33@bit; nondet res2_34@bit; nondet res2_35@bit;
nondet res2_36@bit; nondet res2_37@bit; nondet res2_38@bit; nondet res2_39@bit;
nondet res2_40@bit; nondet res2_41@bit; nondet res2_42@bit; nondet res2_43@bit;
nondet res2_44@bit; nondet res2_45@bit; nondet res2_46@bit; nondet res2_47@bit;
nondet res2_48@bit; nondet res2_49@bit; nondet res2_50@bit; nondet res2_51@bit;
nondet res2_52@bit; nondet res2_53@bit; nondet res2_54@bit; nondet res2_55@bit;
nondet res2_56@bit; nondet res2_57@bit; nondet res2_58@bit; nondet res2_59@bit;
nondet res2_60@bit; nondet res2_61@bit; nondet res2_62@bit; nondet res2_63@bit;

assume and [
  eqmod res2_0 (cvrted2_0 * ((* 2  0 *) 1)) [2, modulus],
  eqmod res2_1 (cvrted2_1 * ((* 2  1 *) 1)) [2, modulus],
  eqmod res2_2 (cvrted2_2 * ((* 2  2 *) 1)) [2, modulus],
  eqmod res2_3 (cvrted2_3 * ((* 2  3 *) 1)) [2, modulus],
  eqmod res2_4 (cvrted2_4 * ((* 2  4 *) 1)) [2, modulus],
  eqmod res2_5 (cvrted2_5 * ((* 2  5 *) 1)) [2, modulus],
  eqmod res2_6 (cvrted2_6 * ((* 2  6 *) 1)) [2, modulus],
  eqmod res2_7 (cvrted2_7 * ((* 2  7 *) 1)) [2, modulus],
  eqmod res2_8 (cvrted2_8 * ((* 2  8 *) z**8 + z)) [2, modulus],
  eqmod res2_9 (cvrted2_9 * ((* 2  9 *) z**8 + z)) [2, modulus],
  eqmod res2_10 (cvrted2_10 * ((* 2 10 *) z**8 + z)) [2, modulus],
  eqmod res2_11 (cvrted2_11 * ((* 2 11 *) z**8 + z)) [2, modulus],
  eqmod res2_12 (cvrted2_12 * ((* 2 12 *) z**8 + z)) [2, modulus],
  eqmod res2_13 (cvrted2_13 * ((* 2 13 *) z**8 + z)) [2, modulus],
  eqmod res2_14 (cvrted2_14 * ((* 2 14 *) z**8 + z)) [2, modulus],
  eqmod res2_15 (cvrted2_15 * ((* 2 15 *) z**8 + z)) [2, modulus],
  eqmod res2_16 (cvrted2_16 * ((* 2 16 *) z**7 + z**4 + z**2)) [2, modulus],
  eqmod res2_17 (cvrted2_17 * ((* 2 17 *) z**7 + z**4 + z**2)) [2, modulus],
  eqmod res2_18 (cvrted2_18 * ((* 2 18 *) z**7 + z**4 + z**2)) [2, modulus],
  eqmod res2_19 (cvrted2_19 * ((* 2 19 *) z**7 + z**4 + z**2)) [2, modulus],
  eqmod res2_20 (cvrted2_20 * ((* 2 20 *) z**7 + z**4 + z**2)) [2, modulus],
  eqmod res2_21 (cvrted2_21 * ((* 2 21 *) z**7 + z**4 + z**2)) [2, modulus],
  eqmod res2_22 (cvrted2_22 * ((* 2 22 *) z**7 + z**4 + z**2)) [2, modulus],
  eqmod res2_23 (cvrted2_23 * ((* 2 23 *) z**7 + z**4 + z**2)) [2, modulus],
  eqmod res2_24 (cvrted2_24 * ((* 2 24 *) z**10 + z**8 + z**6 + z**5 + z**3 + 1)) [2, modulus],
  eqmod res2_25 (cvrted2_25 * ((* 2 25 *) z**10 + z**8 + z**6 + z**5 + z**3 + 1)) [2, modulus],
  eqmod res2_26 (cvrted2_26 * ((* 2 26 *) z**10 + z**8 + z**6 + z**5 + z**3 + 1)) [2, modulus],
  eqmod res2_27 (cvrted2_27 * ((* 2 27 *) z**10 + z**8 + z**6 + z**5 + z**3 + 1)) [2, modulus],
  eqmod res2_28 (cvrted2_28 * ((* 2 28 *) z**10 + z**8 + z**6 + z**5 + z**3 + 1)) [2, modulus],
  eqmod res2_29 (cvrted2_29 * ((* 2 29 *) z**10 + z**8 + z**6 + z**5 + z**3 + 1)) [2, modulus],
  eqmod res2_30 (cvrted2_30 * ((* 2 30 *) z**10 + z**8 + z**6 + z**5 + z**3 + 1)) [2, modulus],
  eqmod res2_31 (cvrted2_31 * ((* 2 31 *) z**10 + z**8 + z**6 + z**5 + z**3 + 1)) [2, modulus],
  eqmod res2_32 (cvrted2_32 * ((* 2 32 *) z**8 + z**5 + z**4 + z**2)) [2, modulus],
  eqmod res2_33 (cvrted2_33 * ((* 2 33 *) z**8 + z**5 + z**4 + z**2)) [2, modulus],
  eqmod res2_34 (cvrted2_34 * ((* 2 34 *) z**8 + z**5 + z**4 + z**2)) [2, modulus],
  eqmod res2_35 (cvrted2_35 * ((* 2 35 *) z**8 + z**5 + z**4 + z**2)) [2, modulus],
  eqmod res2_36 (cvrted2_36 * ((* 2 36 *) z**8 + z**5 + z**4 + z**2)) [2, modulus],
  eqmod res2_37 (cvrted2_37 * ((* 2 37 *) z**8 + z**5 + z**4 + z**2)) [2, modulus],
  eqmod res2_38 (cvrted2_38 * ((* 2 38 *) z**8 + z**5 + z**4 + z**2)) [2, modulus],
  eqmod res2_39 (cvrted2_39 * ((* 2 39 *) z**8 + z**5 + z**4 + z**2)) [2, modulus],
  eqmod res2_40 (cvrted2_40 * ((* 2 40 *) z**10 + z**9 + z**7 + z**6 + z**5 + z + 1)) [2, modulus],
  eqmod res2_41 (cvrted2_41 * ((* 2 41 *) z**10 + z**9 + z**7 + z**6 + z**5 + z + 1)) [2, modulus],
  eqmod res2_42 (cvrted2_42 * ((* 2 42 *) z**10 + z**9 + z**7 + z**6 + z**5 + z + 1)) [2, modulus],
  eqmod res2_43 (cvrted2_43 * ((* 2 43 *) z**10 + z**9 + z**7 + z**6 + z**5 + z + 1)) [2, modulus],
  eqmod res2_44 (cvrted2_44 * ((* 2 44 *) z**10 + z**9 + z**7 + z**6 + z**5 + z + 1)) [2, modulus],
  eqmod res2_45 (cvrted2_45 * ((* 2 45 *) z**10 + z**9 + z**7 + z**6 + z**5 + z + 1)) [2, modulus],
  eqmod res2_46 (cvrted2_46 * ((* 2 46 *) z**10 + z**9 + z**7 + z**6 + z**5 + z + 1)) [2, modulus],
  eqmod res2_47 (cvrted2_47 * ((* 2 47 *) z**10 + z**9 + z**7 + z**6 + z**5 + z + 1)) [2, modulus],
  eqmod res2_48 (cvrted2_48 * ((* 2 48 *) z**11 + z**10 + z**8 + z**7 + z**6 + z**4 + z**3)) [2, modulus],
  eqmod res2_49 (cvrted2_49 * ((* 2 49 *) z**11 + z**10 + z**8 + z**7 + z**6 + z**4 + z**3)) [2, modulus],
  eqmod res2_50 (cvrted2_50 * ((* 2 50 *) z**11 + z**10 + z**8 + z**7 + z**6 + z**4 + z**3)) [2, modulus],
  eqmod res2_51 (cvrted2_51 * ((* 2 51 *) z**11 + z**10 + z**8 + z**7 + z**6 + z**4 + z**3)) [2, modulus],
  eqmod res2_52 (cvrted2_52 * ((* 2 52 *) z**11 + z**10 + z**8 + z**7 + z**6 + z**4 + z**3)) [2, modulus],
  eqmod res2_53 (cvrted2_53 * ((* 2 53 *) z**11 + z**10 + z**8 + z**7 + z**6 + z**4 + z**3)) [2, modulus],
  eqmod res2_54 (cvrted2_54 * ((* 2 54 *) z**11 + z**10 + z**8 + z**7 + z**6 + z**4 + z**3)) [2, modulus],
  eqmod res2_55 (cvrted2_55 * ((* 2 55 *) z**11 + z**10 + z**8 + z**7 + z**6 + z**4 + z**3)) [2, modulus],
  eqmod res2_56 (cvrted2_56 * ((* 2 56 *) z**10 + z**8 + z**7 + z**3 + z**2)) [2, modulus],
  eqmod res2_57 (cvrted2_57 * ((* 2 57 *) z**10 + z**8 + z**7 + z**3 + z**2)) [2, modulus],
  eqmod res2_58 (cvrted2_58 * ((* 2 58 *) z**10 + z**8 + z**7 + z**3 + z**2)) [2, modulus],
  eqmod res2_59 (cvrted2_59 * ((* 2 59 *) z**10 + z**8 + z**7 + z**3 + z**2)) [2, modulus],
  eqmod res2_60 (cvrted2_60 * ((* 2 60 *) z**10 + z**8 + z**7 + z**3 + z**2)) [2, modulus],
  eqmod res2_61 (cvrted2_61 * ((* 2 61 *) z**10 + z**8 + z**7 + z**3 + z**2)) [2, modulus],
  eqmod res2_62 (cvrted2_62 * ((* 2 62 *) z**10 + z**8 + z**7 + z**3 + z**2)) [2, modulus],
  eqmod res2_63 (cvrted2_63 * ((* 2 63 *) z**10 + z**8 + z**7 + z**3 + z**2)) [2, modulus]
] && true;

nondet x3@uint12;

ecut and [
  eqmod (
    (cvrted2_0 + x * cvrted2_4) * ((z ** 8 + z) * x3) ** 0 +
    (cvrted2_8 + x * cvrted2_12) * ((z ** 8 + z) * x3) ** 1 +
    (cvrted2_16 + x * cvrted2_20) * ((z ** 8 + z) * x3) ** 2 +
    (cvrted2_24 + x * cvrted2_28) * ((z ** 8 + z) * x3) ** 3 +
    (cvrted2_32 + x * cvrted2_36) * ((z ** 8 + z) * x3) ** 4 +
    (cvrted2_40 + x * cvrted2_44) * ((z ** 8 + z) * x3) ** 5 +
    (cvrted2_48 + x * cvrted2_52) * ((z ** 8 + z) * x3) ** 6 +
    (cvrted2_56 + x * cvrted2_60) * ((z ** 8 + z) * x3) ** 7
  ) (
    (res2_0 + x * res2_4) * x3 ** 0 +
    (res2_8 + x * res2_12) * x3 ** 1 +
    (res2_16 + x * res2_20) * x3 ** 2 +
    (res2_24 + x * res2_28) * x3 ** 3 +
    (res2_32 + x * res2_36) * x3 ** 4 +
    (res2_40 + x * res2_44) * x3 ** 5 +
    (res2_48 + x * res2_52) * x3 ** 6 +
    (res2_56 + x * res2_60) * x3 ** 7
  ) [2, modulus],
  eqmod (
    (cvrted2_1 + x * cvrted2_5) * ((z ** 8 + z) * x3) ** 0 +
    (cvrted2_9 + x * cvrted2_13) * ((z ** 8 + z) * x3) ** 1 +
    (cvrted2_17 + x * cvrted2_21) * ((z ** 8 + z) * x3) ** 2 +
    (cvrted2_25 + x * cvrted2_29) * ((z ** 8 + z) * x3) ** 3 +
    (cvrted2_33 + x * cvrted2_37) * ((z ** 8 + z) * x3) ** 4 +
    (cvrted2_41 + x * cvrted2_45) * ((z ** 8 + z) * x3) ** 5 +
    (cvrted2_49 + x * cvrted2_53) * ((z ** 8 + z) * x3) ** 6 +
    (cvrted2_57 + x * cvrted2_61) * ((z ** 8 + z) * x3) ** 7
  ) (
    (res2_1 + x * res2_5) * x3 ** 0 +
    (res2_9 + x * res2_13) * x3 ** 1 +
    (res2_17 + x * res2_21) * x3 ** 2 +
    (res2_25 + x * res2_29) * x3 ** 3 +
    (res2_33 + x * res2_37) * x3 ** 4 +
    (res2_41 + x * res2_45) * x3 ** 5 +
    (res2_49 + x * res2_53) * x3 ** 6 +
    (res2_57 + x * res2_61) * x3 ** 7
  ) [2, modulus],
  eqmod (
    (cvrted2_2 + x * cvrted2_6) * ((z ** 8 + z) * x3) ** 0 +
    (cvrted2_10 + x * cvrted2_14) * ((z ** 8 + z) * x3) ** 1 +
    (cvrted2_18 + x * cvrted2_22) * ((z ** 8 + z) * x3) ** 2 +
    (cvrted2_26 + x * cvrted2_30) * ((z ** 8 + z) * x3) ** 3 +
    (cvrted2_34 + x * cvrted2_38) * ((z ** 8 + z) * x3) ** 4 +
    (cvrted2_42 + x * cvrted2_46) * ((z ** 8 + z) * x3) ** 5 +
    (cvrted2_50 + x * cvrted2_54) * ((z ** 8 + z) * x3) ** 6 +
    (cvrted2_58 + x * cvrted2_62) * ((z ** 8 + z) * x3) ** 7
  ) (
    (res2_2 + x * res2_6) * x3 ** 0 +
    (res2_10 + x * res2_14) * x3 ** 1 +
    (res2_18 + x * res2_22) * x3 ** 2 +
    (res2_26 + x * res2_30) * x3 ** 3 +
    (res2_34 + x * res2_38) * x3 ** 4 +
    (res2_42 + x * res2_46) * x3 ** 5 +
    (res2_50 + x * res2_54) * x3 ** 6 +
    (res2_58 + x * res2_62) * x3 ** 7
  ) [2, modulus],
  eqmod (
    (cvrted2_3 + x * cvrted2_7) * ((z ** 8 + z) * x3) ** 0 +
    (cvrted2_11 + x * cvrted2_15) * ((z ** 8 + z) * x3) ** 1 +
    (cvrted2_19 + x * cvrted2_23) * ((z ** 8 + z) * x3) ** 2 +
    (cvrted2_27 + x * cvrted2_31) * ((z ** 8 + z) * x3) ** 3 +
    (cvrted2_35 + x * cvrted2_39) * ((z ** 8 + z) * x3) ** 4 +
    (cvrted2_43 + x * cvrted2_47) * ((z ** 8 + z) * x3) ** 5 +
    (cvrted2_51 + x * cvrted2_55) * ((z ** 8 + z) * x3) ** 6 +
    (cvrted2_59 + x * cvrted2_63) * ((z ** 8 + z) * x3) ** 7
  ) (
    (res2_3 + x * res2_7) * x3 ** 0 +
    (res2_11 + x * res2_15) * x3 ** 1 +
    (res2_19 + x * res2_23) * x3 ** 2 +
    (res2_27 + x * res2_31) * x3 ** 3 +
    (res2_35 + x * res2_39) * x3 ** 4 +
    (res2_43 + x * res2_47) * x3 ** 5 +
    (res2_51 + x * res2_55) * x3 ** 6 +
    (res2_59 + x * res2_63) * x3 ** 7
  ) [2, modulus]
] prove with [precondition];

mov x x3;

nondet inp3_0@uint12; assume inp3_0 =
  res2_0 * x**0 + res2_8 * x**1 + res2_16 * x**2 + res2_24 * x**3 +
  res2_32 * x**4 + res2_40 * x**5 + res2_48 * x**6 + res2_56 * x**7
&& true;
nondet inp3_1@uint12; assume inp3_1 =
  res2_1 * x**0 + res2_9 * x**1 + res2_17 * x**2 + res2_25 * x**3 +
  res2_33 * x**4 + res2_41 * x**5 + res2_49 * x**6 + res2_57 * x**7
&& true;
nondet inp3_2@uint12; assume inp3_2 =
  res2_2 * x**0 + res2_10 * x**1 + res2_18 * x**2 + res2_26 * x**3 +
  res2_34 * x**4 + res2_42 * x**5 + res2_50 * x**6 + res2_58 * x**7
&& true;
nondet inp3_3@uint12; assume inp3_3 =
  res2_3 * x**0 + res2_11 * x**1 + res2_19 * x**2 + res2_27 * x**3 +
  res2_35 * x**4 + res2_43 * x**5 + res2_51 * x**6 + res2_59 * x**7
&& true;
nondet inp3_4@uint12; assume inp3_4 =
  res2_4 * x**0 + res2_12 * x**1 + res2_20 * x**2 + res2_28 * x**3 +
  res2_36 * x**4 + res2_44 * x**5 + res2_52 * x**6 + res2_60 * x**7
&& true;
nondet inp3_5@uint12; assume inp3_5 =
  res2_5 * x**0 + res2_13 * x**1 + res2_21 * x**2 + res2_29 * x**3 +
  res2_37 * x**4 + res2_45 * x**5 + res2_53 * x**6 + res2_61 * x**7
&& true;
nondet inp3_6@uint12; assume inp3_6 =
  res2_6 * x**0 + res2_14 * x**1 + res2_22 * x**2 + res2_30 * x**3 +
  res2_38 * x**4 + res2_46 * x**5 + res2_54 * x**6 + res2_62 * x**7
&& true;
nondet inp3_7@uint12; assume inp3_7 =
  res2_7 * x**0 + res2_15 * x**1 + res2_23 * x**2 + res2_31 * x**3 +
  res2_39 * x**4 + res2_47 * x**5 + res2_55 * x**6 + res2_63 * x**7
&& true;

nondet rb0_0@bit; nondet rb0_1@bit; nondet rb0_2@bit; nondet rb0_3@bit; nondet rb0_4@bit; nondet rb0_5@bit; nondet rb0_6@bit; nondet rb0_7@bit; nondet rb0_8@bit; nondet rb0_9@bit; nondet rb0_10@bit; nondet rb0_11@bit; nondet rb0_12@bit; nondet rb0_13@bit; nondet rb0_14@bit; nondet rb0_15@bit; nondet rb0_16@bit; nondet rb0_17@bit; nondet rb0_18@bit; nondet rb0_19@bit; nondet rb0_20@bit; nondet rb0_21@bit; nondet rb0_22@bit; nondet rb0_23@bit; nondet rb0_24@bit; nondet rb0_25@bit; nondet rb0_26@bit; nondet rb0_27@bit; nondet rb0_28@bit; nondet rb0_29@bit; nondet rb0_30@bit; nondet rb0_31@bit; nondet rb0_32@bit; nondet rb0_33@bit; nondet rb0_34@bit; nondet rb0_35@bit; nondet rb0_36@bit; nondet rb0_37@bit; nondet rb0_38@bit; nondet rb0_39@bit; nondet rb0_40@bit; nondet rb0_41@bit; nondet rb0_42@bit; nondet rb0_43@bit; nondet rb0_44@bit; nondet rb0_45@bit; nondet rb0_46@bit; nondet rb0_47@bit; nondet rb0_48@bit; nondet rb0_49@bit; nondet rb0_50@bit; nondet rb0_51@bit; nondet rb0_52@bit; nondet rb0_53@bit; nondet rb0_54@bit; nondet rb0_55@bit; nondet rb0_56@bit; nondet rb0_57@bit; nondet rb0_58@bit; nondet rb0_59@bit; nondet rb0_60@bit; nondet rb0_61@bit; nondet rb0_62@bit; nondet rb0_63@bit;
nondet rb1_0@bit; nondet rb1_1@bit; nondet rb1_2@bit; nondet rb1_3@bit; nondet rb1_4@bit; nondet rb1_5@bit; nondet rb1_6@bit; nondet rb1_7@bit; nondet rb1_8@bit; nondet rb1_9@bit; nondet rb1_10@bit; nondet rb1_11@bit; nondet rb1_12@bit; nondet rb1_13@bit; nondet rb1_14@bit; nondet rb1_15@bit; nondet rb1_16@bit; nondet rb1_17@bit; nondet rb1_18@bit; nondet rb1_19@bit; nondet rb1_20@bit; nondet rb1_21@bit; nondet rb1_22@bit; nondet rb1_23@bit; nondet rb1_24@bit; nondet rb1_25@bit; nondet rb1_26@bit; nondet rb1_27@bit; nondet rb1_28@bit; nondet rb1_29@bit; nondet rb1_30@bit; nondet rb1_31@bit; nondet rb1_32@bit; nondet rb1_33@bit; nondet rb1_34@bit; nondet rb1_35@bit; nondet rb1_36@bit; nondet rb1_37@bit; nondet rb1_38@bit; nondet rb1_39@bit; nondet rb1_40@bit; nondet rb1_41@bit; nondet rb1_42@bit; nondet rb1_43@bit; nondet rb1_44@bit; nondet rb1_45@bit; nondet rb1_46@bit; nondet rb1_47@bit; nondet rb1_48@bit; nondet rb1_49@bit; nondet rb1_50@bit; nondet rb1_51@bit; nondet rb1_52@bit; nondet rb1_53@bit; nondet rb1_54@bit; nondet rb1_55@bit; nondet rb1_56@bit; nondet rb1_57@bit; nondet rb1_58@bit; nondet rb1_59@bit; nondet rb1_60@bit; nondet rb1_61@bit; nondet rb1_62@bit; nondet rb1_63@bit;
nondet rb2_0@bit; nondet rb2_1@bit; nondet rb2_2@bit; nondet rb2_3@bit; nondet rb2_4@bit; nondet rb2_5@bit; nondet rb2_6@bit; nondet rb2_7@bit; nondet rb2_8@bit; nondet rb2_9@bit; nondet rb2_10@bit; nondet rb2_11@bit; nondet rb2_12@bit; nondet rb2_13@bit; nondet rb2_14@bit; nondet rb2_15@bit; nondet rb2_16@bit; nondet rb2_17@bit; nondet rb2_18@bit; nondet rb2_19@bit; nondet rb2_20@bit; nondet rb2_21@bit; nondet rb2_22@bit; nondet rb2_23@bit; nondet rb2_24@bit; nondet rb2_25@bit; nondet rb2_26@bit; nondet rb2_27@bit; nondet rb2_28@bit; nondet rb2_29@bit; nondet rb2_30@bit; nondet rb2_31@bit; nondet rb2_32@bit; nondet rb2_33@bit; nondet rb2_34@bit; nondet rb2_35@bit; nondet rb2_36@bit; nondet rb2_37@bit; nondet rb2_38@bit; nondet rb2_39@bit; nondet rb2_40@bit; nondet rb2_41@bit; nondet rb2_42@bit; nondet rb2_43@bit; nondet rb2_44@bit; nondet rb2_45@bit; nondet rb2_46@bit; nondet rb2_47@bit; nondet rb2_48@bit; nondet rb2_49@bit; nondet rb2_50@bit; nondet rb2_51@bit; nondet rb2_52@bit; nondet rb2_53@bit; nondet rb2_54@bit; nondet rb2_55@bit; nondet rb2_56@bit; nondet rb2_57@bit; nondet rb2_58@bit; nondet rb2_59@bit; nondet rb2_60@bit; nondet rb2_61@bit; nondet rb2_62@bit; nondet rb2_63@bit;
nondet rb3_0@bit; nondet rb3_1@bit; nondet rb3_2@bit; nondet rb3_3@bit; nondet rb3_4@bit; nondet rb3_5@bit; nondet rb3_6@bit; nondet rb3_7@bit; nondet rb3_8@bit; nondet rb3_9@bit; nondet rb3_10@bit; nondet rb3_11@bit; nondet rb3_12@bit; nondet rb3_13@bit; nondet rb3_14@bit; nondet rb3_15@bit; nondet rb3_16@bit; nondet rb3_17@bit; nondet rb3_18@bit; nondet rb3_19@bit; nondet rb3_20@bit; nondet rb3_21@bit; nondet rb3_22@bit; nondet rb3_23@bit; nondet rb3_24@bit; nondet rb3_25@bit; nondet rb3_26@bit; nondet rb3_27@bit; nondet rb3_28@bit; nondet rb3_29@bit; nondet rb3_30@bit; nondet rb3_31@bit; nondet rb3_32@bit; nondet rb3_33@bit; nondet rb3_34@bit; nondet rb3_35@bit; nondet rb3_36@bit; nondet rb3_37@bit; nondet rb3_38@bit; nondet rb3_39@bit; nondet rb3_40@bit; nondet rb3_41@bit; nondet rb3_42@bit; nondet rb3_43@bit; nondet rb3_44@bit; nondet rb3_45@bit; nondet rb3_46@bit; nondet rb3_47@bit; nondet rb3_48@bit; nondet rb3_49@bit; nondet rb3_50@bit; nondet rb3_51@bit; nondet rb3_52@bit; nondet rb3_53@bit; nondet rb3_54@bit; nondet rb3_55@bit; nondet rb3_56@bit; nondet rb3_57@bit; nondet rb3_58@bit; nondet rb3_59@bit; nondet rb3_60@bit; nondet rb3_61@bit; nondet rb3_62@bit; nondet rb3_63@bit;
nondet rb4_0@bit; nondet rb4_1@bit; nondet rb4_2@bit; nondet rb4_3@bit; nondet rb4_4@bit; nondet rb4_5@bit; nondet rb4_6@bit; nondet rb4_7@bit; nondet rb4_8@bit; nondet rb4_9@bit; nondet rb4_10@bit; nondet rb4_11@bit; nondet rb4_12@bit; nondet rb4_13@bit; nondet rb4_14@bit; nondet rb4_15@bit; nondet rb4_16@bit; nondet rb4_17@bit; nondet rb4_18@bit; nondet rb4_19@bit; nondet rb4_20@bit; nondet rb4_21@bit; nondet rb4_22@bit; nondet rb4_23@bit; nondet rb4_24@bit; nondet rb4_25@bit; nondet rb4_26@bit; nondet rb4_27@bit; nondet rb4_28@bit; nondet rb4_29@bit; nondet rb4_30@bit; nondet rb4_31@bit; nondet rb4_32@bit; nondet rb4_33@bit; nondet rb4_34@bit; nondet rb4_35@bit; nondet rb4_36@bit; nondet rb4_37@bit; nondet rb4_38@bit; nondet rb4_39@bit; nondet rb4_40@bit; nondet rb4_41@bit; nondet rb4_42@bit; nondet rb4_43@bit; nondet rb4_44@bit; nondet rb4_45@bit; nondet rb4_46@bit; nondet rb4_47@bit; nondet rb4_48@bit; nondet rb4_49@bit; nondet rb4_50@bit; nondet rb4_51@bit; nondet rb4_52@bit; nondet rb4_53@bit; nondet rb4_54@bit; nondet rb4_55@bit; nondet rb4_56@bit; nondet rb4_57@bit; nondet rb4_58@bit; nondet rb4_59@bit; nondet rb4_60@bit; nondet rb4_61@bit; nondet rb4_62@bit; nondet rb4_63@bit;
nondet rb5_0@bit; nondet rb5_1@bit; nondet rb5_2@bit; nondet rb5_3@bit; nondet rb5_4@bit; nondet rb5_5@bit; nondet rb5_6@bit; nondet rb5_7@bit; nondet rb5_8@bit; nondet rb5_9@bit; nondet rb5_10@bit; nondet rb5_11@bit; nondet rb5_12@bit; nondet rb5_13@bit; nondet rb5_14@bit; nondet rb5_15@bit; nondet rb5_16@bit; nondet rb5_17@bit; nondet rb5_18@bit; nondet rb5_19@bit; nondet rb5_20@bit; nondet rb5_21@bit; nondet rb5_22@bit; nondet rb5_23@bit; nondet rb5_24@bit; nondet rb5_25@bit; nondet rb5_26@bit; nondet rb5_27@bit; nondet rb5_28@bit; nondet rb5_29@bit; nondet rb5_30@bit; nondet rb5_31@bit; nondet rb5_32@bit; nondet rb5_33@bit; nondet rb5_34@bit; nondet rb5_35@bit; nondet rb5_36@bit; nondet rb5_37@bit; nondet rb5_38@bit; nondet rb5_39@bit; nondet rb5_40@bit; nondet rb5_41@bit; nondet rb5_42@bit; nondet rb5_43@bit; nondet rb5_44@bit; nondet rb5_45@bit; nondet rb5_46@bit; nondet rb5_47@bit; nondet rb5_48@bit; nondet rb5_49@bit; nondet rb5_50@bit; nondet rb5_51@bit; nondet rb5_52@bit; nondet rb5_53@bit; nondet rb5_54@bit; nondet rb5_55@bit; nondet rb5_56@bit; nondet rb5_57@bit; nondet rb5_58@bit; nondet rb5_59@bit; nondet rb5_60@bit; nondet rb5_61@bit; nondet rb5_62@bit; nondet rb5_63@bit;
nondet rb6_0@bit; nondet rb6_1@bit; nondet rb6_2@bit; nondet rb6_3@bit; nondet rb6_4@bit; nondet rb6_5@bit; nondet rb6_6@bit; nondet rb6_7@bit; nondet rb6_8@bit; nondet rb6_9@bit; nondet rb6_10@bit; nondet rb6_11@bit; nondet rb6_12@bit; nondet rb6_13@bit; nondet rb6_14@bit; nondet rb6_15@bit; nondet rb6_16@bit; nondet rb6_17@bit; nondet rb6_18@bit; nondet rb6_19@bit; nondet rb6_20@bit; nondet rb6_21@bit; nondet rb6_22@bit; nondet rb6_23@bit; nondet rb6_24@bit; nondet rb6_25@bit; nondet rb6_26@bit; nondet rb6_27@bit; nondet rb6_28@bit; nondet rb6_29@bit; nondet rb6_30@bit; nondet rb6_31@bit; nondet rb6_32@bit; nondet rb6_33@bit; nondet rb6_34@bit; nondet rb6_35@bit; nondet rb6_36@bit; nondet rb6_37@bit; nondet rb6_38@bit; nondet rb6_39@bit; nondet rb6_40@bit; nondet rb6_41@bit; nondet rb6_42@bit; nondet rb6_43@bit; nondet rb6_44@bit; nondet rb6_45@bit; nondet rb6_46@bit; nondet rb6_47@bit; nondet rb6_48@bit; nondet rb6_49@bit; nondet rb6_50@bit; nondet rb6_51@bit; nondet rb6_52@bit; nondet rb6_53@bit; nondet rb6_54@bit; nondet rb6_55@bit; nondet rb6_56@bit; nondet rb6_57@bit; nondet rb6_58@bit; nondet rb6_59@bit; nondet rb6_60@bit; nondet rb6_61@bit; nondet rb6_62@bit; nondet rb6_63@bit;
nondet rb7_0@bit; nondet rb7_1@bit; nondet rb7_2@bit; nondet rb7_3@bit; nondet rb7_4@bit; nondet rb7_5@bit; nondet rb7_6@bit; nondet rb7_7@bit; nondet rb7_8@bit; nondet rb7_9@bit; nondet rb7_10@bit; nondet rb7_11@bit; nondet rb7_12@bit; nondet rb7_13@bit; nondet rb7_14@bit; nondet rb7_15@bit; nondet rb7_16@bit; nondet rb7_17@bit; nondet rb7_18@bit; nondet rb7_19@bit; nondet rb7_20@bit; nondet rb7_21@bit; nondet rb7_22@bit; nondet rb7_23@bit; nondet rb7_24@bit; nondet rb7_25@bit; nondet rb7_26@bit; nondet rb7_27@bit; nondet rb7_28@bit; nondet rb7_29@bit; nondet rb7_30@bit; nondet rb7_31@bit; nondet rb7_32@bit; nondet rb7_33@bit; nondet rb7_34@bit; nondet rb7_35@bit; nondet rb7_36@bit; nondet rb7_37@bit; nondet rb7_38@bit; nondet rb7_39@bit; nondet rb7_40@bit; nondet rb7_41@bit; nondet rb7_42@bit; nondet rb7_43@bit; nondet rb7_44@bit; nondet rb7_45@bit; nondet rb7_46@bit; nondet rb7_47@bit; nondet rb7_48@bit; nondet rb7_49@bit; nondet rb7_50@bit; nondet rb7_51@bit; nondet rb7_52@bit; nondet rb7_53@bit; nondet rb7_54@bit; nondet rb7_55@bit; nondet rb7_56@bit; nondet rb7_57@bit; nondet rb7_58@bit; nondet rb7_59@bit; nondet rb7_60@bit; nondet rb7_61@bit; nondet rb7_62@bit; nondet rb7_63@bit;
nondet rb8_0@bit; nondet rb8_1@bit; nondet rb8_2@bit; nondet rb8_3@bit; nondet rb8_4@bit; nondet rb8_5@bit; nondet rb8_6@bit; nondet rb8_7@bit; nondet rb8_8@bit; nondet rb8_9@bit; nondet rb8_10@bit; nondet rb8_11@bit; nondet rb8_12@bit; nondet rb8_13@bit; nondet rb8_14@bit; nondet rb8_15@bit; nondet rb8_16@bit; nondet rb8_17@bit; nondet rb8_18@bit; nondet rb8_19@bit; nondet rb8_20@bit; nondet rb8_21@bit; nondet rb8_22@bit; nondet rb8_23@bit; nondet rb8_24@bit; nondet rb8_25@bit; nondet rb8_26@bit; nondet rb8_27@bit; nondet rb8_28@bit; nondet rb8_29@bit; nondet rb8_30@bit; nondet rb8_31@bit; nondet rb8_32@bit; nondet rb8_33@bit; nondet rb8_34@bit; nondet rb8_35@bit; nondet rb8_36@bit; nondet rb8_37@bit; nondet rb8_38@bit; nondet rb8_39@bit; nondet rb8_40@bit; nondet rb8_41@bit; nondet rb8_42@bit; nondet rb8_43@bit; nondet rb8_44@bit; nondet rb8_45@bit; nondet rb8_46@bit; nondet rb8_47@bit; nondet rb8_48@bit; nondet rb8_49@bit; nondet rb8_50@bit; nondet rb8_51@bit; nondet rb8_52@bit; nondet rb8_53@bit; nondet rb8_54@bit; nondet rb8_55@bit; nondet rb8_56@bit; nondet rb8_57@bit; nondet rb8_58@bit; nondet rb8_59@bit; nondet rb8_60@bit; nondet rb8_61@bit; nondet rb8_62@bit; nondet rb8_63@bit;
nondet rb9_0@bit; nondet rb9_1@bit; nondet rb9_2@bit; nondet rb9_3@bit; nondet rb9_4@bit; nondet rb9_5@bit; nondet rb9_6@bit; nondet rb9_7@bit; nondet rb9_8@bit; nondet rb9_9@bit; nondet rb9_10@bit; nondet rb9_11@bit; nondet rb9_12@bit; nondet rb9_13@bit; nondet rb9_14@bit; nondet rb9_15@bit; nondet rb9_16@bit; nondet rb9_17@bit; nondet rb9_18@bit; nondet rb9_19@bit; nondet rb9_20@bit; nondet rb9_21@bit; nondet rb9_22@bit; nondet rb9_23@bit; nondet rb9_24@bit; nondet rb9_25@bit; nondet rb9_26@bit; nondet rb9_27@bit; nondet rb9_28@bit; nondet rb9_29@bit; nondet rb9_30@bit; nondet rb9_31@bit; nondet rb9_32@bit; nondet rb9_33@bit; nondet rb9_34@bit; nondet rb9_35@bit; nondet rb9_36@bit; nondet rb9_37@bit; nondet rb9_38@bit; nondet rb9_39@bit; nondet rb9_40@bit; nondet rb9_41@bit; nondet rb9_42@bit; nondet rb9_43@bit; nondet rb9_44@bit; nondet rb9_45@bit; nondet rb9_46@bit; nondet rb9_47@bit; nondet rb9_48@bit; nondet rb9_49@bit; nondet rb9_50@bit; nondet rb9_51@bit; nondet rb9_52@bit; nondet rb9_53@bit; nondet rb9_54@bit; nondet rb9_55@bit; nondet rb9_56@bit; nondet rb9_57@bit; nondet rb9_58@bit; nondet rb9_59@bit; nondet rb9_60@bit; nondet rb9_61@bit; nondet rb9_62@bit; nondet rb9_63@bit;
nondet rb10_0@bit; nondet rb10_1@bit; nondet rb10_2@bit; nondet rb10_3@bit; nondet rb10_4@bit; nondet rb10_5@bit; nondet rb10_6@bit; nondet rb10_7@bit; nondet rb10_8@bit; nondet rb10_9@bit; nondet rb10_10@bit; nondet rb10_11@bit; nondet rb10_12@bit; nondet rb10_13@bit; nondet rb10_14@bit; nondet rb10_15@bit; nondet rb10_16@bit; nondet rb10_17@bit; nondet rb10_18@bit; nondet rb10_19@bit; nondet rb10_20@bit; nondet rb10_21@bit; nondet rb10_22@bit; nondet rb10_23@bit; nondet rb10_24@bit; nondet rb10_25@bit; nondet rb10_26@bit; nondet rb10_27@bit; nondet rb10_28@bit; nondet rb10_29@bit; nondet rb10_30@bit; nondet rb10_31@bit; nondet rb10_32@bit; nondet rb10_33@bit; nondet rb10_34@bit; nondet rb10_35@bit; nondet rb10_36@bit; nondet rb10_37@bit; nondet rb10_38@bit; nondet rb10_39@bit; nondet rb10_40@bit; nondet rb10_41@bit; nondet rb10_42@bit; nondet rb10_43@bit; nondet rb10_44@bit; nondet rb10_45@bit; nondet rb10_46@bit; nondet rb10_47@bit; nondet rb10_48@bit; nondet rb10_49@bit; nondet rb10_50@bit; nondet rb10_51@bit; nondet rb10_52@bit; nondet rb10_53@bit; nondet rb10_54@bit; nondet rb10_55@bit; nondet rb10_56@bit; nondet rb10_57@bit; nondet rb10_58@bit; nondet rb10_59@bit; nondet rb10_60@bit; nondet rb10_61@bit; nondet rb10_62@bit; nondet rb10_63@bit;
nondet rb11_0@bit; nondet rb11_1@bit; nondet rb11_2@bit; nondet rb11_3@bit; nondet rb11_4@bit; nondet rb11_5@bit; nondet rb11_6@bit; nondet rb11_7@bit; nondet rb11_8@bit; nondet rb11_9@bit; nondet rb11_10@bit; nondet rb11_11@bit; nondet rb11_12@bit; nondet rb11_13@bit; nondet rb11_14@bit; nondet rb11_15@bit; nondet rb11_16@bit; nondet rb11_17@bit; nondet rb11_18@bit; nondet rb11_19@bit; nondet rb11_20@bit; nondet rb11_21@bit; nondet rb11_22@bit; nondet rb11_23@bit; nondet rb11_24@bit; nondet rb11_25@bit; nondet rb11_26@bit; nondet rb11_27@bit; nondet rb11_28@bit; nondet rb11_29@bit; nondet rb11_30@bit; nondet rb11_31@bit; nondet rb11_32@bit; nondet rb11_33@bit; nondet rb11_34@bit; nondet rb11_35@bit; nondet rb11_36@bit; nondet rb11_37@bit; nondet rb11_38@bit; nondet rb11_39@bit; nondet rb11_40@bit; nondet rb11_41@bit; nondet rb11_42@bit; nondet rb11_43@bit; nondet rb11_44@bit; nondet rb11_45@bit; nondet rb11_46@bit; nondet rb11_47@bit; nondet rb11_48@bit; nondet rb11_49@bit; nondet rb11_50@bit; nondet rb11_51@bit; nondet rb11_52@bit; nondet rb11_53@bit; nondet rb11_54@bit; nondet rb11_55@bit; nondet rb11_56@bit; nondet rb11_57@bit; nondet rb11_58@bit; nondet rb11_59@bit; nondet rb11_60@bit; nondet rb11_61@bit; nondet rb11_62@bit; nondet rb11_63@bit;
assume and [
  res2_0 = rb0_0 * z**0 + rb1_0 * z**1 + rb2_0 * z**2 + rb3_0 * z**3 + rb4_0 * z**4 + rb5_0 * z**5 + rb6_0 * z**6 + rb7_0 * z**7 + rb8_0 * z**8 + rb9_0 * z**9 + rb10_0 * z**10 + rb11_0 * z**11,
  res2_1 = rb0_1 * z**0 + rb1_1 * z**1 + rb2_1 * z**2 + rb3_1 * z**3 + rb4_1 * z**4 + rb5_1 * z**5 + rb6_1 * z**6 + rb7_1 * z**7 + rb8_1 * z**8 + rb9_1 * z**9 + rb10_1 * z**10 + rb11_1 * z**11,
  res2_2 = rb0_2 * z**0 + rb1_2 * z**1 + rb2_2 * z**2 + rb3_2 * z**3 + rb4_2 * z**4 + rb5_2 * z**5 + rb6_2 * z**6 + rb7_2 * z**7 + rb8_2 * z**8 + rb9_2 * z**9 + rb10_2 * z**10 + rb11_2 * z**11,
  res2_3 = rb0_3 * z**0 + rb1_3 * z**1 + rb2_3 * z**2 + rb3_3 * z**3 + rb4_3 * z**4 + rb5_3 * z**5 + rb6_3 * z**6 + rb7_3 * z**7 + rb8_3 * z**8 + rb9_3 * z**9 + rb10_3 * z**10 + rb11_3 * z**11,
  res2_4 = rb0_4 * z**0 + rb1_4 * z**1 + rb2_4 * z**2 + rb3_4 * z**3 + rb4_4 * z**4 + rb5_4 * z**5 + rb6_4 * z**6 + rb7_4 * z**7 + rb8_4 * z**8 + rb9_4 * z**9 + rb10_4 * z**10 + rb11_4 * z**11,
  res2_5 = rb0_5 * z**0 + rb1_5 * z**1 + rb2_5 * z**2 + rb3_5 * z**3 + rb4_5 * z**4 + rb5_5 * z**5 + rb6_5 * z**6 + rb7_5 * z**7 + rb8_5 * z**8 + rb9_5 * z**9 + rb10_5 * z**10 + rb11_5 * z**11,
  res2_6 = rb0_6 * z**0 + rb1_6 * z**1 + rb2_6 * z**2 + rb3_6 * z**3 + rb4_6 * z**4 + rb5_6 * z**5 + rb6_6 * z**6 + rb7_6 * z**7 + rb8_6 * z**8 + rb9_6 * z**9 + rb10_6 * z**10 + rb11_6 * z**11,
  res2_7 = rb0_7 * z**0 + rb1_7 * z**1 + rb2_7 * z**2 + rb3_7 * z**3 + rb4_7 * z**4 + rb5_7 * z**5 + rb6_7 * z**6 + rb7_7 * z**7 + rb8_7 * z**8 + rb9_7 * z**9 + rb10_7 * z**10 + rb11_7 * z**11,
  res2_8 = rb0_8 * z**0 + rb1_8 * z**1 + rb2_8 * z**2 + rb3_8 * z**3 + rb4_8 * z**4 + rb5_8 * z**5 + rb6_8 * z**6 + rb7_8 * z**7 + rb8_8 * z**8 + rb9_8 * z**9 + rb10_8 * z**10 + rb11_8 * z**11,
  res2_9 = rb0_9 * z**0 + rb1_9 * z**1 + rb2_9 * z**2 + rb3_9 * z**3 + rb4_9 * z**4 + rb5_9 * z**5 + rb6_9 * z**6 + rb7_9 * z**7 + rb8_9 * z**8 + rb9_9 * z**9 + rb10_9 * z**10 + rb11_9 * z**11,
  res2_10 = rb0_10 * z**0 + rb1_10 * z**1 + rb2_10 * z**2 + rb3_10 * z**3 + rb4_10 * z**4 + rb5_10 * z**5 + rb6_10 * z**6 + rb7_10 * z**7 + rb8_10 * z**8 + rb9_10 * z**9 + rb10_10 * z**10 + rb11_10 * z**11,
  res2_11 = rb0_11 * z**0 + rb1_11 * z**1 + rb2_11 * z**2 + rb3_11 * z**3 + rb4_11 * z**4 + rb5_11 * z**5 + rb6_11 * z**6 + rb7_11 * z**7 + rb8_11 * z**8 + rb9_11 * z**9 + rb10_11 * z**10 + rb11_11 * z**11,
  res2_12 = rb0_12 * z**0 + rb1_12 * z**1 + rb2_12 * z**2 + rb3_12 * z**3 + rb4_12 * z**4 + rb5_12 * z**5 + rb6_12 * z**6 + rb7_12 * z**7 + rb8_12 * z**8 + rb9_12 * z**9 + rb10_12 * z**10 + rb11_12 * z**11,
  res2_13 = rb0_13 * z**0 + rb1_13 * z**1 + rb2_13 * z**2 + rb3_13 * z**3 + rb4_13 * z**4 + rb5_13 * z**5 + rb6_13 * z**6 + rb7_13 * z**7 + rb8_13 * z**8 + rb9_13 * z**9 + rb10_13 * z**10 + rb11_13 * z**11,
  res2_14 = rb0_14 * z**0 + rb1_14 * z**1 + rb2_14 * z**2 + rb3_14 * z**3 + rb4_14 * z**4 + rb5_14 * z**5 + rb6_14 * z**6 + rb7_14 * z**7 + rb8_14 * z**8 + rb9_14 * z**9 + rb10_14 * z**10 + rb11_14 * z**11,
  res2_15 = rb0_15 * z**0 + rb1_15 * z**1 + rb2_15 * z**2 + rb3_15 * z**3 + rb4_15 * z**4 + rb5_15 * z**5 + rb6_15 * z**6 + rb7_15 * z**7 + rb8_15 * z**8 + rb9_15 * z**9 + rb10_15 * z**10 + rb11_15 * z**11,
  res2_16 = rb0_16 * z**0 + rb1_16 * z**1 + rb2_16 * z**2 + rb3_16 * z**3 + rb4_16 * z**4 + rb5_16 * z**5 + rb6_16 * z**6 + rb7_16 * z**7 + rb8_16 * z**8 + rb9_16 * z**9 + rb10_16 * z**10 + rb11_16 * z**11,
  res2_17 = rb0_17 * z**0 + rb1_17 * z**1 + rb2_17 * z**2 + rb3_17 * z**3 + rb4_17 * z**4 + rb5_17 * z**5 + rb6_17 * z**6 + rb7_17 * z**7 + rb8_17 * z**8 + rb9_17 * z**9 + rb10_17 * z**10 + rb11_17 * z**11,
  res2_18 = rb0_18 * z**0 + rb1_18 * z**1 + rb2_18 * z**2 + rb3_18 * z**3 + rb4_18 * z**4 + rb5_18 * z**5 + rb6_18 * z**6 + rb7_18 * z**7 + rb8_18 * z**8 + rb9_18 * z**9 + rb10_18 * z**10 + rb11_18 * z**11,
  res2_19 = rb0_19 * z**0 + rb1_19 * z**1 + rb2_19 * z**2 + rb3_19 * z**3 + rb4_19 * z**4 + rb5_19 * z**5 + rb6_19 * z**6 + rb7_19 * z**7 + rb8_19 * z**8 + rb9_19 * z**9 + rb10_19 * z**10 + rb11_19 * z**11,
  res2_20 = rb0_20 * z**0 + rb1_20 * z**1 + rb2_20 * z**2 + rb3_20 * z**3 + rb4_20 * z**4 + rb5_20 * z**5 + rb6_20 * z**6 + rb7_20 * z**7 + rb8_20 * z**8 + rb9_20 * z**9 + rb10_20 * z**10 + rb11_20 * z**11,
  res2_21 = rb0_21 * z**0 + rb1_21 * z**1 + rb2_21 * z**2 + rb3_21 * z**3 + rb4_21 * z**4 + rb5_21 * z**5 + rb6_21 * z**6 + rb7_21 * z**7 + rb8_21 * z**8 + rb9_21 * z**9 + rb10_21 * z**10 + rb11_21 * z**11,
  res2_22 = rb0_22 * z**0 + rb1_22 * z**1 + rb2_22 * z**2 + rb3_22 * z**3 + rb4_22 * z**4 + rb5_22 * z**5 + rb6_22 * z**6 + rb7_22 * z**7 + rb8_22 * z**8 + rb9_22 * z**9 + rb10_22 * z**10 + rb11_22 * z**11,
  res2_23 = rb0_23 * z**0 + rb1_23 * z**1 + rb2_23 * z**2 + rb3_23 * z**3 + rb4_23 * z**4 + rb5_23 * z**5 + rb6_23 * z**6 + rb7_23 * z**7 + rb8_23 * z**8 + rb9_23 * z**9 + rb10_23 * z**10 + rb11_23 * z**11,
  res2_24 = rb0_24 * z**0 + rb1_24 * z**1 + rb2_24 * z**2 + rb3_24 * z**3 + rb4_24 * z**4 + rb5_24 * z**5 + rb6_24 * z**6 + rb7_24 * z**7 + rb8_24 * z**8 + rb9_24 * z**9 + rb10_24 * z**10 + rb11_24 * z**11,
  res2_25 = rb0_25 * z**0 + rb1_25 * z**1 + rb2_25 * z**2 + rb3_25 * z**3 + rb4_25 * z**4 + rb5_25 * z**5 + rb6_25 * z**6 + rb7_25 * z**7 + rb8_25 * z**8 + rb9_25 * z**9 + rb10_25 * z**10 + rb11_25 * z**11,
  res2_26 = rb0_26 * z**0 + rb1_26 * z**1 + rb2_26 * z**2 + rb3_26 * z**3 + rb4_26 * z**4 + rb5_26 * z**5 + rb6_26 * z**6 + rb7_26 * z**7 + rb8_26 * z**8 + rb9_26 * z**9 + rb10_26 * z**10 + rb11_26 * z**11,
  res2_27 = rb0_27 * z**0 + rb1_27 * z**1 + rb2_27 * z**2 + rb3_27 * z**3 + rb4_27 * z**4 + rb5_27 * z**5 + rb6_27 * z**6 + rb7_27 * z**7 + rb8_27 * z**8 + rb9_27 * z**9 + rb10_27 * z**10 + rb11_27 * z**11,
  res2_28 = rb0_28 * z**0 + rb1_28 * z**1 + rb2_28 * z**2 + rb3_28 * z**3 + rb4_28 * z**4 + rb5_28 * z**5 + rb6_28 * z**6 + rb7_28 * z**7 + rb8_28 * z**8 + rb9_28 * z**9 + rb10_28 * z**10 + rb11_28 * z**11,
  res2_29 = rb0_29 * z**0 + rb1_29 * z**1 + rb2_29 * z**2 + rb3_29 * z**3 + rb4_29 * z**4 + rb5_29 * z**5 + rb6_29 * z**6 + rb7_29 * z**7 + rb8_29 * z**8 + rb9_29 * z**9 + rb10_29 * z**10 + rb11_29 * z**11,
  res2_30 = rb0_30 * z**0 + rb1_30 * z**1 + rb2_30 * z**2 + rb3_30 * z**3 + rb4_30 * z**4 + rb5_30 * z**5 + rb6_30 * z**6 + rb7_30 * z**7 + rb8_30 * z**8 + rb9_30 * z**9 + rb10_30 * z**10 + rb11_30 * z**11,
  res2_31 = rb0_31 * z**0 + rb1_31 * z**1 + rb2_31 * z**2 + rb3_31 * z**3 + rb4_31 * z**4 + rb5_31 * z**5 + rb6_31 * z**6 + rb7_31 * z**7 + rb8_31 * z**8 + rb9_31 * z**9 + rb10_31 * z**10 + rb11_31 * z**11,
  res2_32 = rb0_32 * z**0 + rb1_32 * z**1 + rb2_32 * z**2 + rb3_32 * z**3 + rb4_32 * z**4 + rb5_32 * z**5 + rb6_32 * z**6 + rb7_32 * z**7 + rb8_32 * z**8 + rb9_32 * z**9 + rb10_32 * z**10 + rb11_32 * z**11,
  res2_33 = rb0_33 * z**0 + rb1_33 * z**1 + rb2_33 * z**2 + rb3_33 * z**3 + rb4_33 * z**4 + rb5_33 * z**5 + rb6_33 * z**6 + rb7_33 * z**7 + rb8_33 * z**8 + rb9_33 * z**9 + rb10_33 * z**10 + rb11_33 * z**11,
  res2_34 = rb0_34 * z**0 + rb1_34 * z**1 + rb2_34 * z**2 + rb3_34 * z**3 + rb4_34 * z**4 + rb5_34 * z**5 + rb6_34 * z**6 + rb7_34 * z**7 + rb8_34 * z**8 + rb9_34 * z**9 + rb10_34 * z**10 + rb11_34 * z**11,
  res2_35 = rb0_35 * z**0 + rb1_35 * z**1 + rb2_35 * z**2 + rb3_35 * z**3 + rb4_35 * z**4 + rb5_35 * z**5 + rb6_35 * z**6 + rb7_35 * z**7 + rb8_35 * z**8 + rb9_35 * z**9 + rb10_35 * z**10 + rb11_35 * z**11,
  res2_36 = rb0_36 * z**0 + rb1_36 * z**1 + rb2_36 * z**2 + rb3_36 * z**3 + rb4_36 * z**4 + rb5_36 * z**5 + rb6_36 * z**6 + rb7_36 * z**7 + rb8_36 * z**8 + rb9_36 * z**9 + rb10_36 * z**10 + rb11_36 * z**11,
  res2_37 = rb0_37 * z**0 + rb1_37 * z**1 + rb2_37 * z**2 + rb3_37 * z**3 + rb4_37 * z**4 + rb5_37 * z**5 + rb6_37 * z**6 + rb7_37 * z**7 + rb8_37 * z**8 + rb9_37 * z**9 + rb10_37 * z**10 + rb11_37 * z**11,
  res2_38 = rb0_38 * z**0 + rb1_38 * z**1 + rb2_38 * z**2 + rb3_38 * z**3 + rb4_38 * z**4 + rb5_38 * z**5 + rb6_38 * z**6 + rb7_38 * z**7 + rb8_38 * z**8 + rb9_38 * z**9 + rb10_38 * z**10 + rb11_38 * z**11,
  res2_39 = rb0_39 * z**0 + rb1_39 * z**1 + rb2_39 * z**2 + rb3_39 * z**3 + rb4_39 * z**4 + rb5_39 * z**5 + rb6_39 * z**6 + rb7_39 * z**7 + rb8_39 * z**8 + rb9_39 * z**9 + rb10_39 * z**10 + rb11_39 * z**11,
  res2_40 = rb0_40 * z**0 + rb1_40 * z**1 + rb2_40 * z**2 + rb3_40 * z**3 + rb4_40 * z**4 + rb5_40 * z**5 + rb6_40 * z**6 + rb7_40 * z**7 + rb8_40 * z**8 + rb9_40 * z**9 + rb10_40 * z**10 + rb11_40 * z**11,
  res2_41 = rb0_41 * z**0 + rb1_41 * z**1 + rb2_41 * z**2 + rb3_41 * z**3 + rb4_41 * z**4 + rb5_41 * z**5 + rb6_41 * z**6 + rb7_41 * z**7 + rb8_41 * z**8 + rb9_41 * z**9 + rb10_41 * z**10 + rb11_41 * z**11,
  res2_42 = rb0_42 * z**0 + rb1_42 * z**1 + rb2_42 * z**2 + rb3_42 * z**3 + rb4_42 * z**4 + rb5_42 * z**5 + rb6_42 * z**6 + rb7_42 * z**7 + rb8_42 * z**8 + rb9_42 * z**9 + rb10_42 * z**10 + rb11_42 * z**11,
  res2_43 = rb0_43 * z**0 + rb1_43 * z**1 + rb2_43 * z**2 + rb3_43 * z**3 + rb4_43 * z**4 + rb5_43 * z**5 + rb6_43 * z**6 + rb7_43 * z**7 + rb8_43 * z**8 + rb9_43 * z**9 + rb10_43 * z**10 + rb11_43 * z**11,
  res2_44 = rb0_44 * z**0 + rb1_44 * z**1 + rb2_44 * z**2 + rb3_44 * z**3 + rb4_44 * z**4 + rb5_44 * z**5 + rb6_44 * z**6 + rb7_44 * z**7 + rb8_44 * z**8 + rb9_44 * z**9 + rb10_44 * z**10 + rb11_44 * z**11,
  res2_45 = rb0_45 * z**0 + rb1_45 * z**1 + rb2_45 * z**2 + rb3_45 * z**3 + rb4_45 * z**4 + rb5_45 * z**5 + rb6_45 * z**6 + rb7_45 * z**7 + rb8_45 * z**8 + rb9_45 * z**9 + rb10_45 * z**10 + rb11_45 * z**11,
  res2_46 = rb0_46 * z**0 + rb1_46 * z**1 + rb2_46 * z**2 + rb3_46 * z**3 + rb4_46 * z**4 + rb5_46 * z**5 + rb6_46 * z**6 + rb7_46 * z**7 + rb8_46 * z**8 + rb9_46 * z**9 + rb10_46 * z**10 + rb11_46 * z**11,
  res2_47 = rb0_47 * z**0 + rb1_47 * z**1 + rb2_47 * z**2 + rb3_47 * z**3 + rb4_47 * z**4 + rb5_47 * z**5 + rb6_47 * z**6 + rb7_47 * z**7 + rb8_47 * z**8 + rb9_47 * z**9 + rb10_47 * z**10 + rb11_47 * z**11,
  res2_48 = rb0_48 * z**0 + rb1_48 * z**1 + rb2_48 * z**2 + rb3_48 * z**3 + rb4_48 * z**4 + rb5_48 * z**5 + rb6_48 * z**6 + rb7_48 * z**7 + rb8_48 * z**8 + rb9_48 * z**9 + rb10_48 * z**10 + rb11_48 * z**11,
  res2_49 = rb0_49 * z**0 + rb1_49 * z**1 + rb2_49 * z**2 + rb3_49 * z**3 + rb4_49 * z**4 + rb5_49 * z**5 + rb6_49 * z**6 + rb7_49 * z**7 + rb8_49 * z**8 + rb9_49 * z**9 + rb10_49 * z**10 + rb11_49 * z**11,
  res2_50 = rb0_50 * z**0 + rb1_50 * z**1 + rb2_50 * z**2 + rb3_50 * z**3 + rb4_50 * z**4 + rb5_50 * z**5 + rb6_50 * z**6 + rb7_50 * z**7 + rb8_50 * z**8 + rb9_50 * z**9 + rb10_50 * z**10 + rb11_50 * z**11,
  res2_51 = rb0_51 * z**0 + rb1_51 * z**1 + rb2_51 * z**2 + rb3_51 * z**3 + rb4_51 * z**4 + rb5_51 * z**5 + rb6_51 * z**6 + rb7_51 * z**7 + rb8_51 * z**8 + rb9_51 * z**9 + rb10_51 * z**10 + rb11_51 * z**11,
  res2_52 = rb0_52 * z**0 + rb1_52 * z**1 + rb2_52 * z**2 + rb3_52 * z**3 + rb4_52 * z**4 + rb5_52 * z**5 + rb6_52 * z**6 + rb7_52 * z**7 + rb8_52 * z**8 + rb9_52 * z**9 + rb10_52 * z**10 + rb11_52 * z**11,
  res2_53 = rb0_53 * z**0 + rb1_53 * z**1 + rb2_53 * z**2 + rb3_53 * z**3 + rb4_53 * z**4 + rb5_53 * z**5 + rb6_53 * z**6 + rb7_53 * z**7 + rb8_53 * z**8 + rb9_53 * z**9 + rb10_53 * z**10 + rb11_53 * z**11,
  res2_54 = rb0_54 * z**0 + rb1_54 * z**1 + rb2_54 * z**2 + rb3_54 * z**3 + rb4_54 * z**4 + rb5_54 * z**5 + rb6_54 * z**6 + rb7_54 * z**7 + rb8_54 * z**8 + rb9_54 * z**9 + rb10_54 * z**10 + rb11_54 * z**11,
  res2_55 = rb0_55 * z**0 + rb1_55 * z**1 + rb2_55 * z**2 + rb3_55 * z**3 + rb4_55 * z**4 + rb5_55 * z**5 + rb6_55 * z**6 + rb7_55 * z**7 + rb8_55 * z**8 + rb9_55 * z**9 + rb10_55 * z**10 + rb11_55 * z**11,
  res2_56 = rb0_56 * z**0 + rb1_56 * z**1 + rb2_56 * z**2 + rb3_56 * z**3 + rb4_56 * z**4 + rb5_56 * z**5 + rb6_56 * z**6 + rb7_56 * z**7 + rb8_56 * z**8 + rb9_56 * z**9 + rb10_56 * z**10 + rb11_56 * z**11,
  res2_57 = rb0_57 * z**0 + rb1_57 * z**1 + rb2_57 * z**2 + rb3_57 * z**3 + rb4_57 * z**4 + rb5_57 * z**5 + rb6_57 * z**6 + rb7_57 * z**7 + rb8_57 * z**8 + rb9_57 * z**9 + rb10_57 * z**10 + rb11_57 * z**11,
  res2_58 = rb0_58 * z**0 + rb1_58 * z**1 + rb2_58 * z**2 + rb3_58 * z**3 + rb4_58 * z**4 + rb5_58 * z**5 + rb6_58 * z**6 + rb7_58 * z**7 + rb8_58 * z**8 + rb9_58 * z**9 + rb10_58 * z**10 + rb11_58 * z**11,
  res2_59 = rb0_59 * z**0 + rb1_59 * z**1 + rb2_59 * z**2 + rb3_59 * z**3 + rb4_59 * z**4 + rb5_59 * z**5 + rb6_59 * z**6 + rb7_59 * z**7 + rb8_59 * z**8 + rb9_59 * z**9 + rb10_59 * z**10 + rb11_59 * z**11,
  res2_60 = rb0_60 * z**0 + rb1_60 * z**1 + rb2_60 * z**2 + rb3_60 * z**3 + rb4_60 * z**4 + rb5_60 * z**5 + rb6_60 * z**6 + rb7_60 * z**7 + rb8_60 * z**8 + rb9_60 * z**9 + rb10_60 * z**10 + rb11_60 * z**11,
  res2_61 = rb0_61 * z**0 + rb1_61 * z**1 + rb2_61 * z**2 + rb3_61 * z**3 + rb4_61 * z**4 + rb5_61 * z**5 + rb6_61 * z**6 + rb7_61 * z**7 + rb8_61 * z**8 + rb9_61 * z**9 + rb10_61 * z**10 + rb11_61 * z**11,
  res2_62 = rb0_62 * z**0 + rb1_62 * z**1 + rb2_62 * z**2 + rb3_62 * z**3 + rb4_62 * z**4 + rb5_62 * z**5 + rb6_62 * z**6 + rb7_62 * z**7 + rb8_62 * z**8 + rb9_62 * z**9 + rb10_62 * z**10 + rb11_62 * z**11,
  res2_63 = rb0_63 * z**0 + rb1_63 * z**1 + rb2_63 * z**2 + rb3_63 * z**3 + rb4_63 * z**4 + rb5_63 * z**5 + rb6_63 * z**6 + rb7_63 * z**7 + rb8_63 * z**8 + rb9_63 * z**9 + rb10_63 * z**10 + rb11_63 * z**11
] && true;

mov %L0x7fffffffdaa0 [rb0_0, rb0_1, rb0_2, rb0_3, rb0_4, rb0_5, rb0_6, rb0_7, rb0_8, rb0_9, rb0_10, rb0_11, rb0_12, rb0_13, rb0_14, rb0_15, rb0_16, rb0_17, rb0_18, rb0_19, rb0_20, rb0_21, rb0_22, rb0_23, rb0_24, rb0_25, rb0_26, rb0_27, rb0_28, rb0_29, rb0_30, rb0_31, rb0_32, rb0_33, rb0_34, rb0_35, rb0_36, rb0_37, rb0_38, rb0_39, rb0_40, rb0_41, rb0_42, rb0_43, rb0_44, rb0_45, rb0_46, rb0_47, rb0_48, rb0_49, rb0_50, rb0_51, rb0_52, rb0_53, rb0_54, rb0_55, rb0_56, rb0_57, rb0_58, rb0_59, rb0_60, rb0_61, rb0_62, rb0_63];
mov %L0x7fffffffdaa8 [rb1_0, rb1_1, rb1_2, rb1_3, rb1_4, rb1_5, rb1_6, rb1_7, rb1_8, rb1_9, rb1_10, rb1_11, rb1_12, rb1_13, rb1_14, rb1_15, rb1_16, rb1_17, rb1_18, rb1_19, rb1_20, rb1_21, rb1_22, rb1_23, rb1_24, rb1_25, rb1_26, rb1_27, rb1_28, rb1_29, rb1_30, rb1_31, rb1_32, rb1_33, rb1_34, rb1_35, rb1_36, rb1_37, rb1_38, rb1_39, rb1_40, rb1_41, rb1_42, rb1_43, rb1_44, rb1_45, rb1_46, rb1_47, rb1_48, rb1_49, rb1_50, rb1_51, rb1_52, rb1_53, rb1_54, rb1_55, rb1_56, rb1_57, rb1_58, rb1_59, rb1_60, rb1_61, rb1_62, rb1_63];
mov %L0x7fffffffdab0 [rb2_0, rb2_1, rb2_2, rb2_3, rb2_4, rb2_5, rb2_6, rb2_7, rb2_8, rb2_9, rb2_10, rb2_11, rb2_12, rb2_13, rb2_14, rb2_15, rb2_16, rb2_17, rb2_18, rb2_19, rb2_20, rb2_21, rb2_22, rb2_23, rb2_24, rb2_25, rb2_26, rb2_27, rb2_28, rb2_29, rb2_30, rb2_31, rb2_32, rb2_33, rb2_34, rb2_35, rb2_36, rb2_37, rb2_38, rb2_39, rb2_40, rb2_41, rb2_42, rb2_43, rb2_44, rb2_45, rb2_46, rb2_47, rb2_48, rb2_49, rb2_50, rb2_51, rb2_52, rb2_53, rb2_54, rb2_55, rb2_56, rb2_57, rb2_58, rb2_59, rb2_60, rb2_61, rb2_62, rb2_63];
mov %L0x7fffffffdab8 [rb3_0, rb3_1, rb3_2, rb3_3, rb3_4, rb3_5, rb3_6, rb3_7, rb3_8, rb3_9, rb3_10, rb3_11, rb3_12, rb3_13, rb3_14, rb3_15, rb3_16, rb3_17, rb3_18, rb3_19, rb3_20, rb3_21, rb3_22, rb3_23, rb3_24, rb3_25, rb3_26, rb3_27, rb3_28, rb3_29, rb3_30, rb3_31, rb3_32, rb3_33, rb3_34, rb3_35, rb3_36, rb3_37, rb3_38, rb3_39, rb3_40, rb3_41, rb3_42, rb3_43, rb3_44, rb3_45, rb3_46, rb3_47, rb3_48, rb3_49, rb3_50, rb3_51, rb3_52, rb3_53, rb3_54, rb3_55, rb3_56, rb3_57, rb3_58, rb3_59, rb3_60, rb3_61, rb3_62, rb3_63];
mov %L0x7fffffffdac0 [rb4_0, rb4_1, rb4_2, rb4_3, rb4_4, rb4_5, rb4_6, rb4_7, rb4_8, rb4_9, rb4_10, rb4_11, rb4_12, rb4_13, rb4_14, rb4_15, rb4_16, rb4_17, rb4_18, rb4_19, rb4_20, rb4_21, rb4_22, rb4_23, rb4_24, rb4_25, rb4_26, rb4_27, rb4_28, rb4_29, rb4_30, rb4_31, rb4_32, rb4_33, rb4_34, rb4_35, rb4_36, rb4_37, rb4_38, rb4_39, rb4_40, rb4_41, rb4_42, rb4_43, rb4_44, rb4_45, rb4_46, rb4_47, rb4_48, rb4_49, rb4_50, rb4_51, rb4_52, rb4_53, rb4_54, rb4_55, rb4_56, rb4_57, rb4_58, rb4_59, rb4_60, rb4_61, rb4_62, rb4_63];
mov %L0x7fffffffdac8 [rb5_0, rb5_1, rb5_2, rb5_3, rb5_4, rb5_5, rb5_6, rb5_7, rb5_8, rb5_9, rb5_10, rb5_11, rb5_12, rb5_13, rb5_14, rb5_15, rb5_16, rb5_17, rb5_18, rb5_19, rb5_20, rb5_21, rb5_22, rb5_23, rb5_24, rb5_25, rb5_26, rb5_27, rb5_28, rb5_29, rb5_30, rb5_31, rb5_32, rb5_33, rb5_34, rb5_35, rb5_36, rb5_37, rb5_38, rb5_39, rb5_40, rb5_41, rb5_42, rb5_43, rb5_44, rb5_45, rb5_46, rb5_47, rb5_48, rb5_49, rb5_50, rb5_51, rb5_52, rb5_53, rb5_54, rb5_55, rb5_56, rb5_57, rb5_58, rb5_59, rb5_60, rb5_61, rb5_62, rb5_63];
mov %L0x7fffffffdad0 [rb6_0, rb6_1, rb6_2, rb6_3, rb6_4, rb6_5, rb6_6, rb6_7, rb6_8, rb6_9, rb6_10, rb6_11, rb6_12, rb6_13, rb6_14, rb6_15, rb6_16, rb6_17, rb6_18, rb6_19, rb6_20, rb6_21, rb6_22, rb6_23, rb6_24, rb6_25, rb6_26, rb6_27, rb6_28, rb6_29, rb6_30, rb6_31, rb6_32, rb6_33, rb6_34, rb6_35, rb6_36, rb6_37, rb6_38, rb6_39, rb6_40, rb6_41, rb6_42, rb6_43, rb6_44, rb6_45, rb6_46, rb6_47, rb6_48, rb6_49, rb6_50, rb6_51, rb6_52, rb6_53, rb6_54, rb6_55, rb6_56, rb6_57, rb6_58, rb6_59, rb6_60, rb6_61, rb6_62, rb6_63];
mov %L0x7fffffffdad8 [rb7_0, rb7_1, rb7_2, rb7_3, rb7_4, rb7_5, rb7_6, rb7_7, rb7_8, rb7_9, rb7_10, rb7_11, rb7_12, rb7_13, rb7_14, rb7_15, rb7_16, rb7_17, rb7_18, rb7_19, rb7_20, rb7_21, rb7_22, rb7_23, rb7_24, rb7_25, rb7_26, rb7_27, rb7_28, rb7_29, rb7_30, rb7_31, rb7_32, rb7_33, rb7_34, rb7_35, rb7_36, rb7_37, rb7_38, rb7_39, rb7_40, rb7_41, rb7_42, rb7_43, rb7_44, rb7_45, rb7_46, rb7_47, rb7_48, rb7_49, rb7_50, rb7_51, rb7_52, rb7_53, rb7_54, rb7_55, rb7_56, rb7_57, rb7_58, rb7_59, rb7_60, rb7_61, rb7_62, rb7_63];
mov %L0x7fffffffdae0 [rb8_0, rb8_1, rb8_2, rb8_3, rb8_4, rb8_5, rb8_6, rb8_7, rb8_8, rb8_9, rb8_10, rb8_11, rb8_12, rb8_13, rb8_14, rb8_15, rb8_16, rb8_17, rb8_18, rb8_19, rb8_20, rb8_21, rb8_22, rb8_23, rb8_24, rb8_25, rb8_26, rb8_27, rb8_28, rb8_29, rb8_30, rb8_31, rb8_32, rb8_33, rb8_34, rb8_35, rb8_36, rb8_37, rb8_38, rb8_39, rb8_40, rb8_41, rb8_42, rb8_43, rb8_44, rb8_45, rb8_46, rb8_47, rb8_48, rb8_49, rb8_50, rb8_51, rb8_52, rb8_53, rb8_54, rb8_55, rb8_56, rb8_57, rb8_58, rb8_59, rb8_60, rb8_61, rb8_62, rb8_63];
mov %L0x7fffffffdae8 [rb9_0, rb9_1, rb9_2, rb9_3, rb9_4, rb9_5, rb9_6, rb9_7, rb9_8, rb9_9, rb9_10, rb9_11, rb9_12, rb9_13, rb9_14, rb9_15, rb9_16, rb9_17, rb9_18, rb9_19, rb9_20, rb9_21, rb9_22, rb9_23, rb9_24, rb9_25, rb9_26, rb9_27, rb9_28, rb9_29, rb9_30, rb9_31, rb9_32, rb9_33, rb9_34, rb9_35, rb9_36, rb9_37, rb9_38, rb9_39, rb9_40, rb9_41, rb9_42, rb9_43, rb9_44, rb9_45, rb9_46, rb9_47, rb9_48, rb9_49, rb9_50, rb9_51, rb9_52, rb9_53, rb9_54, rb9_55, rb9_56, rb9_57, rb9_58, rb9_59, rb9_60, rb9_61, rb9_62, rb9_63];
mov %L0x7fffffffdaf0 [rb10_0, rb10_1, rb10_2, rb10_3, rb10_4, rb10_5, rb10_6, rb10_7, rb10_8, rb10_9, rb10_10, rb10_11, rb10_12, rb10_13, rb10_14, rb10_15, rb10_16, rb10_17, rb10_18, rb10_19, rb10_20, rb10_21, rb10_22, rb10_23, rb10_24, rb10_25, rb10_26, rb10_27, rb10_28, rb10_29, rb10_30, rb10_31, rb10_32, rb10_33, rb10_34, rb10_35, rb10_36, rb10_37, rb10_38, rb10_39, rb10_40, rb10_41, rb10_42, rb10_43, rb10_44, rb10_45, rb10_46, rb10_47, rb10_48, rb10_49, rb10_50, rb10_51, rb10_52, rb10_53, rb10_54, rb10_55, rb10_56, rb10_57, rb10_58, rb10_59, rb10_60, rb10_61, rb10_62, rb10_63];
mov %L0x7fffffffdaf8 [rb11_0, rb11_1, rb11_2, rb11_3, rb11_4, rb11_5, rb11_6, rb11_7, rb11_8, rb11_9, rb11_10, rb11_11, rb11_12, rb11_13, rb11_14, rb11_15, rb11_16, rb11_17, rb11_18, rb11_19, rb11_20, rb11_21, rb11_22, rb11_23, rb11_24, rb11_25, rb11_26, rb11_27, rb11_28, rb11_29, rb11_30, rb11_31, rb11_32, rb11_33, rb11_34, rb11_35, rb11_36, rb11_37, rb11_38, rb11_39, rb11_40, rb11_41, rb11_42, rb11_43, rb11_44, rb11_45, rb11_46, rb11_47, rb11_48, rb11_49, rb11_50, rb11_51, rb11_52, rb11_53, rb11_54, rb11_55, rb11_56, rb11_57, rb11_58, rb11_59, rb11_60, rb11_61, rb11_62, rb11_63];


(* #jne    0x5555555552e4 <radix_conversions+196>  #! PC = 0x55555555535b *)
#jne    0x5555555552e4 <radix_conversions+196>  #! 0x55555555535b = 0x55555555535b;
(* mov    %r15,%r9                                 #! PC = 0x5555555552e4 *)
mov r9 r15;
(* mov    $0x1,%r8d                                #! PC = 0x5555555552e7 *)
mov r8d 0x1@uint32;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaa0; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaa0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaa0; PC = 0x555555555330 *)
mov %L0x7fffffffdaa0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaa8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaa8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaa8; PC = 0x555555555330 *)
mov %L0x7fffffffdaa8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdab0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdab0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdab0; PC = 0x555555555330 *)
mov %L0x7fffffffdab0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdab8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdab8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdab8; PC = 0x555555555330 *)
mov %L0x7fffffffdab8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdac0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdac0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdac0; PC = 0x555555555330 *)
mov %L0x7fffffffdac0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdac8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdac8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdac8; PC = 0x555555555330 *)
mov %L0x7fffffffdac8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdad0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdad0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdad0; PC = 0x555555555330 *)
mov %L0x7fffffffdad0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdad8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdad8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdad8; PC = 0x555555555330 *)
mov %L0x7fffffffdad8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdae0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdae0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdae0; PC = 0x555555555330 *)
mov %L0x7fffffffdae0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdae8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdae8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdae8; PC = 0x555555555330 *)
mov %L0x7fffffffdae8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaf0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaf0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaf0; PC = 0x555555555330 *)
mov %L0x7fffffffdaf0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaf8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaf8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    0x48(%rdi),%rdx                          #! EA = L0x7fffffffd868; Value = 0x00ff000000ff0000; PC = 0x55555555532a *)
mov %rdx %L0x7fffffffd868;
(* #jmp    0x5555555552fe <radix_conversions+222>  #! PC = 0x55555555532e *)
#jmp    0x5555555552fe <radix_conversions+222>  #! 0x55555555532e = 0x55555555532e;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd860; Value = 0xff000000ff000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd860;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 3@8;
shl r11d r11d 3;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 8 *)
assert true && cl = 8@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 8 *)
assert true && cl = 8@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaf8; PC = 0x555555555330 *)
mov %L0x7fffffffdaf8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* add    $0x1,%rbp                                #! PC = 0x555555555347 *)
add ebp ebp 0x1@sint32;


mov [out0_0, out0_1, out0_2, out0_3, out0_4, out0_5, out0_6, out0_7, out0_8, out0_9, out0_10, out0_11, out0_12, out0_13, out0_14, out0_15, out0_16, out0_17, out0_18, out0_19, out0_20, out0_21, out0_22, out0_23, out0_24, out0_25, out0_26, out0_27, out0_28, out0_29, out0_30, out0_31, out0_32, out0_33, out0_34, out0_35, out0_36, out0_37, out0_38, out0_39, out0_40, out0_41, out0_42, out0_43, out0_44, out0_45, out0_46, out0_47, out0_48, out0_49, out0_50, out0_51, out0_52, out0_53, out0_54, out0_55, out0_56, out0_57, out0_58, out0_59, out0_60, out0_61, out0_62, out0_63] %L0x7fffffffdaa0;
mov [out1_0, out1_1, out1_2, out1_3, out1_4, out1_5, out1_6, out1_7, out1_8, out1_9, out1_10, out1_11, out1_12, out1_13, out1_14, out1_15, out1_16, out1_17, out1_18, out1_19, out1_20, out1_21, out1_22, out1_23, out1_24, out1_25, out1_26, out1_27, out1_28, out1_29, out1_30, out1_31, out1_32, out1_33, out1_34, out1_35, out1_36, out1_37, out1_38, out1_39, out1_40, out1_41, out1_42, out1_43, out1_44, out1_45, out1_46, out1_47, out1_48, out1_49, out1_50, out1_51, out1_52, out1_53, out1_54, out1_55, out1_56, out1_57, out1_58, out1_59, out1_60, out1_61, out1_62, out1_63] %L0x7fffffffdaa8;
mov [out2_0, out2_1, out2_2, out2_3, out2_4, out2_5, out2_6, out2_7, out2_8, out2_9, out2_10, out2_11, out2_12, out2_13, out2_14, out2_15, out2_16, out2_17, out2_18, out2_19, out2_20, out2_21, out2_22, out2_23, out2_24, out2_25, out2_26, out2_27, out2_28, out2_29, out2_30, out2_31, out2_32, out2_33, out2_34, out2_35, out2_36, out2_37, out2_38, out2_39, out2_40, out2_41, out2_42, out2_43, out2_44, out2_45, out2_46, out2_47, out2_48, out2_49, out2_50, out2_51, out2_52, out2_53, out2_54, out2_55, out2_56, out2_57, out2_58, out2_59, out2_60, out2_61, out2_62, out2_63] %L0x7fffffffdab0;
mov [out3_0, out3_1, out3_2, out3_3, out3_4, out3_5, out3_6, out3_7, out3_8, out3_9, out3_10, out3_11, out3_12, out3_13, out3_14, out3_15, out3_16, out3_17, out3_18, out3_19, out3_20, out3_21, out3_22, out3_23, out3_24, out3_25, out3_26, out3_27, out3_28, out3_29, out3_30, out3_31, out3_32, out3_33, out3_34, out3_35, out3_36, out3_37, out3_38, out3_39, out3_40, out3_41, out3_42, out3_43, out3_44, out3_45, out3_46, out3_47, out3_48, out3_49, out3_50, out3_51, out3_52, out3_53, out3_54, out3_55, out3_56, out3_57, out3_58, out3_59, out3_60, out3_61, out3_62, out3_63] %L0x7fffffffdab8;
mov [out4_0, out4_1, out4_2, out4_3, out4_4, out4_5, out4_6, out4_7, out4_8, out4_9, out4_10, out4_11, out4_12, out4_13, out4_14, out4_15, out4_16, out4_17, out4_18, out4_19, out4_20, out4_21, out4_22, out4_23, out4_24, out4_25, out4_26, out4_27, out4_28, out4_29, out4_30, out4_31, out4_32, out4_33, out4_34, out4_35, out4_36, out4_37, out4_38, out4_39, out4_40, out4_41, out4_42, out4_43, out4_44, out4_45, out4_46, out4_47, out4_48, out4_49, out4_50, out4_51, out4_52, out4_53, out4_54, out4_55, out4_56, out4_57, out4_58, out4_59, out4_60, out4_61, out4_62, out4_63] %L0x7fffffffdac0;
mov [out5_0, out5_1, out5_2, out5_3, out5_4, out5_5, out5_6, out5_7, out5_8, out5_9, out5_10, out5_11, out5_12, out5_13, out5_14, out5_15, out5_16, out5_17, out5_18, out5_19, out5_20, out5_21, out5_22, out5_23, out5_24, out5_25, out5_26, out5_27, out5_28, out5_29, out5_30, out5_31, out5_32, out5_33, out5_34, out5_35, out5_36, out5_37, out5_38, out5_39, out5_40, out5_41, out5_42, out5_43, out5_44, out5_45, out5_46, out5_47, out5_48, out5_49, out5_50, out5_51, out5_52, out5_53, out5_54, out5_55, out5_56, out5_57, out5_58, out5_59, out5_60, out5_61, out5_62, out5_63] %L0x7fffffffdac8;
mov [out6_0, out6_1, out6_2, out6_3, out6_4, out6_5, out6_6, out6_7, out6_8, out6_9, out6_10, out6_11, out6_12, out6_13, out6_14, out6_15, out6_16, out6_17, out6_18, out6_19, out6_20, out6_21, out6_22, out6_23, out6_24, out6_25, out6_26, out6_27, out6_28, out6_29, out6_30, out6_31, out6_32, out6_33, out6_34, out6_35, out6_36, out6_37, out6_38, out6_39, out6_40, out6_41, out6_42, out6_43, out6_44, out6_45, out6_46, out6_47, out6_48, out6_49, out6_50, out6_51, out6_52, out6_53, out6_54, out6_55, out6_56, out6_57, out6_58, out6_59, out6_60, out6_61, out6_62, out6_63] %L0x7fffffffdad0;
mov [out7_0, out7_1, out7_2, out7_3, out7_4, out7_5, out7_6, out7_7, out7_8, out7_9, out7_10, out7_11, out7_12, out7_13, out7_14, out7_15, out7_16, out7_17, out7_18, out7_19, out7_20, out7_21, out7_22, out7_23, out7_24, out7_25, out7_26, out7_27, out7_28, out7_29, out7_30, out7_31, out7_32, out7_33, out7_34, out7_35, out7_36, out7_37, out7_38, out7_39, out7_40, out7_41, out7_42, out7_43, out7_44, out7_45, out7_46, out7_47, out7_48, out7_49, out7_50, out7_51, out7_52, out7_53, out7_54, out7_55, out7_56, out7_57, out7_58, out7_59, out7_60, out7_61, out7_62, out7_63] %L0x7fffffffdad8;
mov [out8_0, out8_1, out8_2, out8_3, out8_4, out8_5, out8_6, out8_7, out8_8, out8_9, out8_10, out8_11, out8_12, out8_13, out8_14, out8_15, out8_16, out8_17, out8_18, out8_19, out8_20, out8_21, out8_22, out8_23, out8_24, out8_25, out8_26, out8_27, out8_28, out8_29, out8_30, out8_31, out8_32, out8_33, out8_34, out8_35, out8_36, out8_37, out8_38, out8_39, out8_40, out8_41, out8_42, out8_43, out8_44, out8_45, out8_46, out8_47, out8_48, out8_49, out8_50, out8_51, out8_52, out8_53, out8_54, out8_55, out8_56, out8_57, out8_58, out8_59, out8_60, out8_61, out8_62, out8_63] %L0x7fffffffdae0;
mov [out9_0, out9_1, out9_2, out9_3, out9_4, out9_5, out9_6, out9_7, out9_8, out9_9, out9_10, out9_11, out9_12, out9_13, out9_14, out9_15, out9_16, out9_17, out9_18, out9_19, out9_20, out9_21, out9_22, out9_23, out9_24, out9_25, out9_26, out9_27, out9_28, out9_29, out9_30, out9_31, out9_32, out9_33, out9_34, out9_35, out9_36, out9_37, out9_38, out9_39, out9_40, out9_41, out9_42, out9_43, out9_44, out9_45, out9_46, out9_47, out9_48, out9_49, out9_50, out9_51, out9_52, out9_53, out9_54, out9_55, out9_56, out9_57, out9_58, out9_59, out9_60, out9_61, out9_62, out9_63] %L0x7fffffffdae8;
mov [out10_0, out10_1, out10_2, out10_3, out10_4, out10_5, out10_6, out10_7, out10_8, out10_9, out10_10, out10_11, out10_12, out10_13, out10_14, out10_15, out10_16, out10_17, out10_18, out10_19, out10_20, out10_21, out10_22, out10_23, out10_24, out10_25, out10_26, out10_27, out10_28, out10_29, out10_30, out10_31, out10_32, out10_33, out10_34, out10_35, out10_36, out10_37, out10_38, out10_39, out10_40, out10_41, out10_42, out10_43, out10_44, out10_45, out10_46, out10_47, out10_48, out10_49, out10_50, out10_51, out10_52, out10_53, out10_54, out10_55, out10_56, out10_57, out10_58, out10_59, out10_60, out10_61, out10_62, out10_63] %L0x7fffffffdaf0;
mov [out11_0, out11_1, out11_2, out11_3, out11_4, out11_5, out11_6, out11_7, out11_8, out11_9, out11_10, out11_11, out11_12, out11_13, out11_14, out11_15, out11_16, out11_17, out11_18, out11_19, out11_20, out11_21, out11_22, out11_23, out11_24, out11_25, out11_26, out11_27, out11_28, out11_29, out11_30, out11_31, out11_32, out11_33, out11_34, out11_35, out11_36, out11_37, out11_38, out11_39, out11_40, out11_41, out11_42, out11_43, out11_44, out11_45, out11_46, out11_47, out11_48, out11_49, out11_50, out11_51, out11_52, out11_53, out11_54, out11_55, out11_56, out11_57, out11_58, out11_59, out11_60, out11_61, out11_62, out11_63] %L0x7fffffffdaf8;

ghost cvrted3_0@uint12: cvrted3_0 = out0_0 * z**0 + out1_0 * z**1 + out2_0 * z**2 + out3_0 * z**3 + out4_0 * z**4 + out5_0 * z**5 + out6_0 * z**6 + out7_0 * z**7 + out8_0 * z**8 + out9_0 * z**9 + out10_0 * z**10 + out11_0 * z**11 && true;
ghost cvrted3_1@uint12: cvrted3_1 = out0_1 * z**0 + out1_1 * z**1 + out2_1 * z**2 + out3_1 * z**3 + out4_1 * z**4 + out5_1 * z**5 + out6_1 * z**6 + out7_1 * z**7 + out8_1 * z**8 + out9_1 * z**9 + out10_1 * z**10 + out11_1 * z**11 && true;
ghost cvrted3_2@uint12: cvrted3_2 = out0_2 * z**0 + out1_2 * z**1 + out2_2 * z**2 + out3_2 * z**3 + out4_2 * z**4 + out5_2 * z**5 + out6_2 * z**6 + out7_2 * z**7 + out8_2 * z**8 + out9_2 * z**9 + out10_2 * z**10 + out11_2 * z**11 && true;
ghost cvrted3_3@uint12: cvrted3_3 = out0_3 * z**0 + out1_3 * z**1 + out2_3 * z**2 + out3_3 * z**3 + out4_3 * z**4 + out5_3 * z**5 + out6_3 * z**6 + out7_3 * z**7 + out8_3 * z**8 + out9_3 * z**9 + out10_3 * z**10 + out11_3 * z**11 && true;
ghost cvrted3_4@uint12: cvrted3_4 = out0_4 * z**0 + out1_4 * z**1 + out2_4 * z**2 + out3_4 * z**3 + out4_4 * z**4 + out5_4 * z**5 + out6_4 * z**6 + out7_4 * z**7 + out8_4 * z**8 + out9_4 * z**9 + out10_4 * z**10 + out11_4 * z**11 && true;
ghost cvrted3_5@uint12: cvrted3_5 = out0_5 * z**0 + out1_5 * z**1 + out2_5 * z**2 + out3_5 * z**3 + out4_5 * z**4 + out5_5 * z**5 + out6_5 * z**6 + out7_5 * z**7 + out8_5 * z**8 + out9_5 * z**9 + out10_5 * z**10 + out11_5 * z**11 && true;
ghost cvrted3_6@uint12: cvrted3_6 = out0_6 * z**0 + out1_6 * z**1 + out2_6 * z**2 + out3_6 * z**3 + out4_6 * z**4 + out5_6 * z**5 + out6_6 * z**6 + out7_6 * z**7 + out8_6 * z**8 + out9_6 * z**9 + out10_6 * z**10 + out11_6 * z**11 && true;
ghost cvrted3_7@uint12: cvrted3_7 = out0_7 * z**0 + out1_7 * z**1 + out2_7 * z**2 + out3_7 * z**3 + out4_7 * z**4 + out5_7 * z**5 + out6_7 * z**6 + out7_7 * z**7 + out8_7 * z**8 + out9_7 * z**9 + out10_7 * z**10 + out11_7 * z**11 && true;
ghost cvrted3_8@uint12: cvrted3_8 = out0_8 * z**0 + out1_8 * z**1 + out2_8 * z**2 + out3_8 * z**3 + out4_8 * z**4 + out5_8 * z**5 + out6_8 * z**6 + out7_8 * z**7 + out8_8 * z**8 + out9_8 * z**9 + out10_8 * z**10 + out11_8 * z**11 && true;
ghost cvrted3_9@uint12: cvrted3_9 = out0_9 * z**0 + out1_9 * z**1 + out2_9 * z**2 + out3_9 * z**3 + out4_9 * z**4 + out5_9 * z**5 + out6_9 * z**6 + out7_9 * z**7 + out8_9 * z**8 + out9_9 * z**9 + out10_9 * z**10 + out11_9 * z**11 && true;
ghost cvrted3_10@uint12: cvrted3_10 = out0_10 * z**0 + out1_10 * z**1 + out2_10 * z**2 + out3_10 * z**3 + out4_10 * z**4 + out5_10 * z**5 + out6_10 * z**6 + out7_10 * z**7 + out8_10 * z**8 + out9_10 * z**9 + out10_10 * z**10 + out11_10 * z**11 && true;
ghost cvrted3_11@uint12: cvrted3_11 = out0_11 * z**0 + out1_11 * z**1 + out2_11 * z**2 + out3_11 * z**3 + out4_11 * z**4 + out5_11 * z**5 + out6_11 * z**6 + out7_11 * z**7 + out8_11 * z**8 + out9_11 * z**9 + out10_11 * z**10 + out11_11 * z**11 && true;
ghost cvrted3_12@uint12: cvrted3_12 = out0_12 * z**0 + out1_12 * z**1 + out2_12 * z**2 + out3_12 * z**3 + out4_12 * z**4 + out5_12 * z**5 + out6_12 * z**6 + out7_12 * z**7 + out8_12 * z**8 + out9_12 * z**9 + out10_12 * z**10 + out11_12 * z**11 && true;
ghost cvrted3_13@uint12: cvrted3_13 = out0_13 * z**0 + out1_13 * z**1 + out2_13 * z**2 + out3_13 * z**3 + out4_13 * z**4 + out5_13 * z**5 + out6_13 * z**6 + out7_13 * z**7 + out8_13 * z**8 + out9_13 * z**9 + out10_13 * z**10 + out11_13 * z**11 && true;
ghost cvrted3_14@uint12: cvrted3_14 = out0_14 * z**0 + out1_14 * z**1 + out2_14 * z**2 + out3_14 * z**3 + out4_14 * z**4 + out5_14 * z**5 + out6_14 * z**6 + out7_14 * z**7 + out8_14 * z**8 + out9_14 * z**9 + out10_14 * z**10 + out11_14 * z**11 && true;
ghost cvrted3_15@uint12: cvrted3_15 = out0_15 * z**0 + out1_15 * z**1 + out2_15 * z**2 + out3_15 * z**3 + out4_15 * z**4 + out5_15 * z**5 + out6_15 * z**6 + out7_15 * z**7 + out8_15 * z**8 + out9_15 * z**9 + out10_15 * z**10 + out11_15 * z**11 && true;
ghost cvrted3_16@uint12: cvrted3_16 = out0_16 * z**0 + out1_16 * z**1 + out2_16 * z**2 + out3_16 * z**3 + out4_16 * z**4 + out5_16 * z**5 + out6_16 * z**6 + out7_16 * z**7 + out8_16 * z**8 + out9_16 * z**9 + out10_16 * z**10 + out11_16 * z**11 && true;
ghost cvrted3_17@uint12: cvrted3_17 = out0_17 * z**0 + out1_17 * z**1 + out2_17 * z**2 + out3_17 * z**3 + out4_17 * z**4 + out5_17 * z**5 + out6_17 * z**6 + out7_17 * z**7 + out8_17 * z**8 + out9_17 * z**9 + out10_17 * z**10 + out11_17 * z**11 && true;
ghost cvrted3_18@uint12: cvrted3_18 = out0_18 * z**0 + out1_18 * z**1 + out2_18 * z**2 + out3_18 * z**3 + out4_18 * z**4 + out5_18 * z**5 + out6_18 * z**6 + out7_18 * z**7 + out8_18 * z**8 + out9_18 * z**9 + out10_18 * z**10 + out11_18 * z**11 && true;
ghost cvrted3_19@uint12: cvrted3_19 = out0_19 * z**0 + out1_19 * z**1 + out2_19 * z**2 + out3_19 * z**3 + out4_19 * z**4 + out5_19 * z**5 + out6_19 * z**6 + out7_19 * z**7 + out8_19 * z**8 + out9_19 * z**9 + out10_19 * z**10 + out11_19 * z**11 && true;
ghost cvrted3_20@uint12: cvrted3_20 = out0_20 * z**0 + out1_20 * z**1 + out2_20 * z**2 + out3_20 * z**3 + out4_20 * z**4 + out5_20 * z**5 + out6_20 * z**6 + out7_20 * z**7 + out8_20 * z**8 + out9_20 * z**9 + out10_20 * z**10 + out11_20 * z**11 && true;
ghost cvrted3_21@uint12: cvrted3_21 = out0_21 * z**0 + out1_21 * z**1 + out2_21 * z**2 + out3_21 * z**3 + out4_21 * z**4 + out5_21 * z**5 + out6_21 * z**6 + out7_21 * z**7 + out8_21 * z**8 + out9_21 * z**9 + out10_21 * z**10 + out11_21 * z**11 && true;
ghost cvrted3_22@uint12: cvrted3_22 = out0_22 * z**0 + out1_22 * z**1 + out2_22 * z**2 + out3_22 * z**3 + out4_22 * z**4 + out5_22 * z**5 + out6_22 * z**6 + out7_22 * z**7 + out8_22 * z**8 + out9_22 * z**9 + out10_22 * z**10 + out11_22 * z**11 && true;
ghost cvrted3_23@uint12: cvrted3_23 = out0_23 * z**0 + out1_23 * z**1 + out2_23 * z**2 + out3_23 * z**3 + out4_23 * z**4 + out5_23 * z**5 + out6_23 * z**6 + out7_23 * z**7 + out8_23 * z**8 + out9_23 * z**9 + out10_23 * z**10 + out11_23 * z**11 && true;
ghost cvrted3_24@uint12: cvrted3_24 = out0_24 * z**0 + out1_24 * z**1 + out2_24 * z**2 + out3_24 * z**3 + out4_24 * z**4 + out5_24 * z**5 + out6_24 * z**6 + out7_24 * z**7 + out8_24 * z**8 + out9_24 * z**9 + out10_24 * z**10 + out11_24 * z**11 && true;
ghost cvrted3_25@uint12: cvrted3_25 = out0_25 * z**0 + out1_25 * z**1 + out2_25 * z**2 + out3_25 * z**3 + out4_25 * z**4 + out5_25 * z**5 + out6_25 * z**6 + out7_25 * z**7 + out8_25 * z**8 + out9_25 * z**9 + out10_25 * z**10 + out11_25 * z**11 && true;
ghost cvrted3_26@uint12: cvrted3_26 = out0_26 * z**0 + out1_26 * z**1 + out2_26 * z**2 + out3_26 * z**3 + out4_26 * z**4 + out5_26 * z**5 + out6_26 * z**6 + out7_26 * z**7 + out8_26 * z**8 + out9_26 * z**9 + out10_26 * z**10 + out11_26 * z**11 && true;
ghost cvrted3_27@uint12: cvrted3_27 = out0_27 * z**0 + out1_27 * z**1 + out2_27 * z**2 + out3_27 * z**3 + out4_27 * z**4 + out5_27 * z**5 + out6_27 * z**6 + out7_27 * z**7 + out8_27 * z**8 + out9_27 * z**9 + out10_27 * z**10 + out11_27 * z**11 && true;
ghost cvrted3_28@uint12: cvrted3_28 = out0_28 * z**0 + out1_28 * z**1 + out2_28 * z**2 + out3_28 * z**3 + out4_28 * z**4 + out5_28 * z**5 + out6_28 * z**6 + out7_28 * z**7 + out8_28 * z**8 + out9_28 * z**9 + out10_28 * z**10 + out11_28 * z**11 && true;
ghost cvrted3_29@uint12: cvrted3_29 = out0_29 * z**0 + out1_29 * z**1 + out2_29 * z**2 + out3_29 * z**3 + out4_29 * z**4 + out5_29 * z**5 + out6_29 * z**6 + out7_29 * z**7 + out8_29 * z**8 + out9_29 * z**9 + out10_29 * z**10 + out11_29 * z**11 && true;
ghost cvrted3_30@uint12: cvrted3_30 = out0_30 * z**0 + out1_30 * z**1 + out2_30 * z**2 + out3_30 * z**3 + out4_30 * z**4 + out5_30 * z**5 + out6_30 * z**6 + out7_30 * z**7 + out8_30 * z**8 + out9_30 * z**9 + out10_30 * z**10 + out11_30 * z**11 && true;
ghost cvrted3_31@uint12: cvrted3_31 = out0_31 * z**0 + out1_31 * z**1 + out2_31 * z**2 + out3_31 * z**3 + out4_31 * z**4 + out5_31 * z**5 + out6_31 * z**6 + out7_31 * z**7 + out8_31 * z**8 + out9_31 * z**9 + out10_31 * z**10 + out11_31 * z**11 && true;
ghost cvrted3_32@uint12: cvrted3_32 = out0_32 * z**0 + out1_32 * z**1 + out2_32 * z**2 + out3_32 * z**3 + out4_32 * z**4 + out5_32 * z**5 + out6_32 * z**6 + out7_32 * z**7 + out8_32 * z**8 + out9_32 * z**9 + out10_32 * z**10 + out11_32 * z**11 && true;
ghost cvrted3_33@uint12: cvrted3_33 = out0_33 * z**0 + out1_33 * z**1 + out2_33 * z**2 + out3_33 * z**3 + out4_33 * z**4 + out5_33 * z**5 + out6_33 * z**6 + out7_33 * z**7 + out8_33 * z**8 + out9_33 * z**9 + out10_33 * z**10 + out11_33 * z**11 && true;
ghost cvrted3_34@uint12: cvrted3_34 = out0_34 * z**0 + out1_34 * z**1 + out2_34 * z**2 + out3_34 * z**3 + out4_34 * z**4 + out5_34 * z**5 + out6_34 * z**6 + out7_34 * z**7 + out8_34 * z**8 + out9_34 * z**9 + out10_34 * z**10 + out11_34 * z**11 && true;
ghost cvrted3_35@uint12: cvrted3_35 = out0_35 * z**0 + out1_35 * z**1 + out2_35 * z**2 + out3_35 * z**3 + out4_35 * z**4 + out5_35 * z**5 + out6_35 * z**6 + out7_35 * z**7 + out8_35 * z**8 + out9_35 * z**9 + out10_35 * z**10 + out11_35 * z**11 && true;
ghost cvrted3_36@uint12: cvrted3_36 = out0_36 * z**0 + out1_36 * z**1 + out2_36 * z**2 + out3_36 * z**3 + out4_36 * z**4 + out5_36 * z**5 + out6_36 * z**6 + out7_36 * z**7 + out8_36 * z**8 + out9_36 * z**9 + out10_36 * z**10 + out11_36 * z**11 && true;
ghost cvrted3_37@uint12: cvrted3_37 = out0_37 * z**0 + out1_37 * z**1 + out2_37 * z**2 + out3_37 * z**3 + out4_37 * z**4 + out5_37 * z**5 + out6_37 * z**6 + out7_37 * z**7 + out8_37 * z**8 + out9_37 * z**9 + out10_37 * z**10 + out11_37 * z**11 && true;
ghost cvrted3_38@uint12: cvrted3_38 = out0_38 * z**0 + out1_38 * z**1 + out2_38 * z**2 + out3_38 * z**3 + out4_38 * z**4 + out5_38 * z**5 + out6_38 * z**6 + out7_38 * z**7 + out8_38 * z**8 + out9_38 * z**9 + out10_38 * z**10 + out11_38 * z**11 && true;
ghost cvrted3_39@uint12: cvrted3_39 = out0_39 * z**0 + out1_39 * z**1 + out2_39 * z**2 + out3_39 * z**3 + out4_39 * z**4 + out5_39 * z**5 + out6_39 * z**6 + out7_39 * z**7 + out8_39 * z**8 + out9_39 * z**9 + out10_39 * z**10 + out11_39 * z**11 && true;
ghost cvrted3_40@uint12: cvrted3_40 = out0_40 * z**0 + out1_40 * z**1 + out2_40 * z**2 + out3_40 * z**3 + out4_40 * z**4 + out5_40 * z**5 + out6_40 * z**6 + out7_40 * z**7 + out8_40 * z**8 + out9_40 * z**9 + out10_40 * z**10 + out11_40 * z**11 && true;
ghost cvrted3_41@uint12: cvrted3_41 = out0_41 * z**0 + out1_41 * z**1 + out2_41 * z**2 + out3_41 * z**3 + out4_41 * z**4 + out5_41 * z**5 + out6_41 * z**6 + out7_41 * z**7 + out8_41 * z**8 + out9_41 * z**9 + out10_41 * z**10 + out11_41 * z**11 && true;
ghost cvrted3_42@uint12: cvrted3_42 = out0_42 * z**0 + out1_42 * z**1 + out2_42 * z**2 + out3_42 * z**3 + out4_42 * z**4 + out5_42 * z**5 + out6_42 * z**6 + out7_42 * z**7 + out8_42 * z**8 + out9_42 * z**9 + out10_42 * z**10 + out11_42 * z**11 && true;
ghost cvrted3_43@uint12: cvrted3_43 = out0_43 * z**0 + out1_43 * z**1 + out2_43 * z**2 + out3_43 * z**3 + out4_43 * z**4 + out5_43 * z**5 + out6_43 * z**6 + out7_43 * z**7 + out8_43 * z**8 + out9_43 * z**9 + out10_43 * z**10 + out11_43 * z**11 && true;
ghost cvrted3_44@uint12: cvrted3_44 = out0_44 * z**0 + out1_44 * z**1 + out2_44 * z**2 + out3_44 * z**3 + out4_44 * z**4 + out5_44 * z**5 + out6_44 * z**6 + out7_44 * z**7 + out8_44 * z**8 + out9_44 * z**9 + out10_44 * z**10 + out11_44 * z**11 && true;
ghost cvrted3_45@uint12: cvrted3_45 = out0_45 * z**0 + out1_45 * z**1 + out2_45 * z**2 + out3_45 * z**3 + out4_45 * z**4 + out5_45 * z**5 + out6_45 * z**6 + out7_45 * z**7 + out8_45 * z**8 + out9_45 * z**9 + out10_45 * z**10 + out11_45 * z**11 && true;
ghost cvrted3_46@uint12: cvrted3_46 = out0_46 * z**0 + out1_46 * z**1 + out2_46 * z**2 + out3_46 * z**3 + out4_46 * z**4 + out5_46 * z**5 + out6_46 * z**6 + out7_46 * z**7 + out8_46 * z**8 + out9_46 * z**9 + out10_46 * z**10 + out11_46 * z**11 && true;
ghost cvrted3_47@uint12: cvrted3_47 = out0_47 * z**0 + out1_47 * z**1 + out2_47 * z**2 + out3_47 * z**3 + out4_47 * z**4 + out5_47 * z**5 + out6_47 * z**6 + out7_47 * z**7 + out8_47 * z**8 + out9_47 * z**9 + out10_47 * z**10 + out11_47 * z**11 && true;
ghost cvrted3_48@uint12: cvrted3_48 = out0_48 * z**0 + out1_48 * z**1 + out2_48 * z**2 + out3_48 * z**3 + out4_48 * z**4 + out5_48 * z**5 + out6_48 * z**6 + out7_48 * z**7 + out8_48 * z**8 + out9_48 * z**9 + out10_48 * z**10 + out11_48 * z**11 && true;
ghost cvrted3_49@uint12: cvrted3_49 = out0_49 * z**0 + out1_49 * z**1 + out2_49 * z**2 + out3_49 * z**3 + out4_49 * z**4 + out5_49 * z**5 + out6_49 * z**6 + out7_49 * z**7 + out8_49 * z**8 + out9_49 * z**9 + out10_49 * z**10 + out11_49 * z**11 && true;
ghost cvrted3_50@uint12: cvrted3_50 = out0_50 * z**0 + out1_50 * z**1 + out2_50 * z**2 + out3_50 * z**3 + out4_50 * z**4 + out5_50 * z**5 + out6_50 * z**6 + out7_50 * z**7 + out8_50 * z**8 + out9_50 * z**9 + out10_50 * z**10 + out11_50 * z**11 && true;
ghost cvrted3_51@uint12: cvrted3_51 = out0_51 * z**0 + out1_51 * z**1 + out2_51 * z**2 + out3_51 * z**3 + out4_51 * z**4 + out5_51 * z**5 + out6_51 * z**6 + out7_51 * z**7 + out8_51 * z**8 + out9_51 * z**9 + out10_51 * z**10 + out11_51 * z**11 && true;
ghost cvrted3_52@uint12: cvrted3_52 = out0_52 * z**0 + out1_52 * z**1 + out2_52 * z**2 + out3_52 * z**3 + out4_52 * z**4 + out5_52 * z**5 + out6_52 * z**6 + out7_52 * z**7 + out8_52 * z**8 + out9_52 * z**9 + out10_52 * z**10 + out11_52 * z**11 && true;
ghost cvrted3_53@uint12: cvrted3_53 = out0_53 * z**0 + out1_53 * z**1 + out2_53 * z**2 + out3_53 * z**3 + out4_53 * z**4 + out5_53 * z**5 + out6_53 * z**6 + out7_53 * z**7 + out8_53 * z**8 + out9_53 * z**9 + out10_53 * z**10 + out11_53 * z**11 && true;
ghost cvrted3_54@uint12: cvrted3_54 = out0_54 * z**0 + out1_54 * z**1 + out2_54 * z**2 + out3_54 * z**3 + out4_54 * z**4 + out5_54 * z**5 + out6_54 * z**6 + out7_54 * z**7 + out8_54 * z**8 + out9_54 * z**9 + out10_54 * z**10 + out11_54 * z**11 && true;
ghost cvrted3_55@uint12: cvrted3_55 = out0_55 * z**0 + out1_55 * z**1 + out2_55 * z**2 + out3_55 * z**3 + out4_55 * z**4 + out5_55 * z**5 + out6_55 * z**6 + out7_55 * z**7 + out8_55 * z**8 + out9_55 * z**9 + out10_55 * z**10 + out11_55 * z**11 && true;
ghost cvrted3_56@uint12: cvrted3_56 = out0_56 * z**0 + out1_56 * z**1 + out2_56 * z**2 + out3_56 * z**3 + out4_56 * z**4 + out5_56 * z**5 + out6_56 * z**6 + out7_56 * z**7 + out8_56 * z**8 + out9_56 * z**9 + out10_56 * z**10 + out11_56 * z**11 && true;
ghost cvrted3_57@uint12: cvrted3_57 = out0_57 * z**0 + out1_57 * z**1 + out2_57 * z**2 + out3_57 * z**3 + out4_57 * z**4 + out5_57 * z**5 + out6_57 * z**6 + out7_57 * z**7 + out8_57 * z**8 + out9_57 * z**9 + out10_57 * z**10 + out11_57 * z**11 && true;
ghost cvrted3_58@uint12: cvrted3_58 = out0_58 * z**0 + out1_58 * z**1 + out2_58 * z**2 + out3_58 * z**3 + out4_58 * z**4 + out5_58 * z**5 + out6_58 * z**6 + out7_58 * z**7 + out8_58 * z**8 + out9_58 * z**9 + out10_58 * z**10 + out11_58 * z**11 && true;
ghost cvrted3_59@uint12: cvrted3_59 = out0_59 * z**0 + out1_59 * z**1 + out2_59 * z**2 + out3_59 * z**3 + out4_59 * z**4 + out5_59 * z**5 + out6_59 * z**6 + out7_59 * z**7 + out8_59 * z**8 + out9_59 * z**9 + out10_59 * z**10 + out11_59 * z**11 && true;
ghost cvrted3_60@uint12: cvrted3_60 = out0_60 * z**0 + out1_60 * z**1 + out2_60 * z**2 + out3_60 * z**3 + out4_60 * z**4 + out5_60 * z**5 + out6_60 * z**6 + out7_60 * z**7 + out8_60 * z**8 + out9_60 * z**9 + out10_60 * z**10 + out11_60 * z**11 && true;
ghost cvrted3_61@uint12: cvrted3_61 = out0_61 * z**0 + out1_61 * z**1 + out2_61 * z**2 + out3_61 * z**3 + out4_61 * z**4 + out5_61 * z**5 + out6_61 * z**6 + out7_61 * z**7 + out8_61 * z**8 + out9_61 * z**9 + out10_61 * z**10 + out11_61 * z**11 && true;
ghost cvrted3_62@uint12: cvrted3_62 = out0_62 * z**0 + out1_62 * z**1 + out2_62 * z**2 + out3_62 * z**3 + out4_62 * z**4 + out5_62 * z**5 + out6_62 * z**6 + out7_62 * z**7 + out8_62 * z**8 + out9_62 * z**9 + out10_62 * z**10 + out11_62 * z**11 && true;
ghost cvrted3_63@uint12: cvrted3_63 = out0_63 * z**0 + out1_63 * z**1 + out2_63 * z**2 + out3_63 * z**3 + out4_63 * z**4 + out5_63 * z**5 + out6_63 * z**6 + out7_63 * z**7 + out8_63 * z**8 + out9_63 * z**9 + out10_63 * z**10 + out11_63 * z**11 && true;

ecut and [
  eqmod inp3_0 (
    (cvrted3_0 + x * cvrted3_8) * (x ** 2 + x) ** 0 +
    (cvrted3_16 + x * cvrted3_24) * (x ** 2 + x) ** 1 +
    (cvrted3_32 + x * cvrted3_40) * (x ** 2 + x) ** 2 +
    (cvrted3_48 + x * cvrted3_56) * (x ** 2 + x) ** 3
  ) 2,
  eqmod inp3_1 (
    (cvrted3_1 + x * cvrted3_9) * (x ** 2 + x) ** 0 +
    (cvrted3_17 + x * cvrted3_25) * (x ** 2 + x) ** 1 +
    (cvrted3_33 + x * cvrted3_41) * (x ** 2 + x) ** 2 +
    (cvrted3_49 + x * cvrted3_57) * (x ** 2 + x) ** 3
  ) 2,
  eqmod inp3_2 (
    (cvrted3_2 + x * cvrted3_10) * (x ** 2 + x) ** 0 +
    (cvrted3_18 + x * cvrted3_26) * (x ** 2 + x) ** 1 +
    (cvrted3_34 + x * cvrted3_42) * (x ** 2 + x) ** 2 +
    (cvrted3_50 + x * cvrted3_58) * (x ** 2 + x) ** 3
  ) 2,
  eqmod inp3_3 (
    (cvrted3_3 + x * cvrted3_11) * (x ** 2 + x) ** 0 +
    (cvrted3_19 + x * cvrted3_27) * (x ** 2 + x) ** 1 +
    (cvrted3_35 + x * cvrted3_43) * (x ** 2 + x) ** 2 +
    (cvrted3_51 + x * cvrted3_59) * (x ** 2 + x) ** 3
  ) 2,
  eqmod inp3_4 (
    (cvrted3_4 + x * cvrted3_12) * (x ** 2 + x) ** 0 +
    (cvrted3_20 + x * cvrted3_28) * (x ** 2 + x) ** 1 +
    (cvrted3_36 + x * cvrted3_44) * (x ** 2 + x) ** 2 +
    (cvrted3_52 + x * cvrted3_60) * (x ** 2 + x) ** 3
  ) 2,
  eqmod inp3_5 (
    (cvrted3_5 + x * cvrted3_13) * (x ** 2 + x) ** 0 +
    (cvrted3_21 + x * cvrted3_29) * (x ** 2 + x) ** 1 +
    (cvrted3_37 + x * cvrted3_45) * (x ** 2 + x) ** 2 +
    (cvrted3_53 + x * cvrted3_61) * (x ** 2 + x) ** 3
  ) 2,
  eqmod inp3_6 (
    (cvrted3_6 + x * cvrted3_14) * (x ** 2 + x) ** 0 +
    (cvrted3_22 + x * cvrted3_30) * (x ** 2 + x) ** 1 +
    (cvrted3_38 + x * cvrted3_46) * (x ** 2 + x) ** 2 +
    (cvrted3_54 + x * cvrted3_62) * (x ** 2 + x) ** 3
  ) 2,
  eqmod inp3_7 (
    (cvrted3_7 + x * cvrted3_15) * (x ** 2 + x) ** 0 +
    (cvrted3_23 + x * cvrted3_31) * (x ** 2 + x) ** 1 +
    (cvrted3_39 + x * cvrted3_47) * (x ** 2 + x) ** 2 +
    (cvrted3_55 + x * cvrted3_63) * (x ** 2 + x) ** 3
  ) 2
];

(* #call   0x555555555380 <_PQCLEAN_MCELIECE348864_AVX_vec_mul_asm>#! PC = 0x555555555352 *)
#call   0x555555555380 <_PQCLEAN_MCELIECE348864_AVX_vec_mul_asm>#! 0x555555555352 = 0x555555555352;
(* #! -> SP = 0x7fffffffd828 *)
#! 0x7fffffffd828 = 0x7fffffffd828;
(* #! <- SP = 0x7fffffffd828 *)
#! 0x7fffffffd828 = 0x7fffffffd828;
(* #ret                                            #! PC = 0x555555555380 *)
#ret                                            #! 0x555555555380 = 0x555555555380;

nondet res3_0@bit; nondet res3_1@bit; nondet res3_2@bit; nondet res3_3@bit;
nondet res3_4@bit; nondet res3_5@bit; nondet res3_6@bit; nondet res3_7@bit;
nondet res3_8@bit; nondet res3_9@bit; nondet res3_10@bit; nondet res3_11@bit;
nondet res3_12@bit; nondet res3_13@bit; nondet res3_14@bit; nondet res3_15@bit;
nondet res3_16@bit; nondet res3_17@bit; nondet res3_18@bit; nondet res3_19@bit;
nondet res3_20@bit; nondet res3_21@bit; nondet res3_22@bit; nondet res3_23@bit;
nondet res3_24@bit; nondet res3_25@bit; nondet res3_26@bit; nondet res3_27@bit;
nondet res3_28@bit; nondet res3_29@bit; nondet res3_30@bit; nondet res3_31@bit;
nondet res3_32@bit; nondet res3_33@bit; nondet res3_34@bit; nondet res3_35@bit;
nondet res3_36@bit; nondet res3_37@bit; nondet res3_38@bit; nondet res3_39@bit;
nondet res3_40@bit; nondet res3_41@bit; nondet res3_42@bit; nondet res3_43@bit;
nondet res3_44@bit; nondet res3_45@bit; nondet res3_46@bit; nondet res3_47@bit;
nondet res3_48@bit; nondet res3_49@bit; nondet res3_50@bit; nondet res3_51@bit;
nondet res3_52@bit; nondet res3_53@bit; nondet res3_54@bit; nondet res3_55@bit;
nondet res3_56@bit; nondet res3_57@bit; nondet res3_58@bit; nondet res3_59@bit;
nondet res3_60@bit; nondet res3_61@bit; nondet res3_62@bit; nondet res3_63@bit;

assume and [
  eqmod res3_0 (cvrted3_0 * ((* 3  0 *) 1)) [2, modulus],
  eqmod res3_1 (cvrted3_1 * ((* 3  1 *) 1)) [2, modulus],
  eqmod res3_2 (cvrted3_2 * ((* 3  2 *) 1)) [2, modulus],
  eqmod res3_3 (cvrted3_3 * ((* 3  3 *) 1)) [2, modulus],
  eqmod res3_4 (cvrted3_4 * ((* 3  4 *) 1)) [2, modulus],
  eqmod res3_5 (cvrted3_5 * ((* 3  5 *) 1)) [2, modulus],
  eqmod res3_6 (cvrted3_6 * ((* 3  6 *) 1)) [2, modulus],
  eqmod res3_7 (cvrted3_7 * ((* 3  7 *) 1)) [2, modulus],
  eqmod res3_8 (cvrted3_8 * ((* 3  8 *) 1)) [2, modulus],
  eqmod res3_9 (cvrted3_9 * ((* 3  9 *) 1)) [2, modulus],
  eqmod res3_10 (cvrted3_10 * ((* 3 10 *) 1)) [2, modulus],
  eqmod res3_11 (cvrted3_11 * ((* 3 11 *) 1)) [2, modulus],
  eqmod res3_12 (cvrted3_12 * ((* 3 12 *) 1)) [2, modulus],
  eqmod res3_13 (cvrted3_13 * ((* 3 13 *) 1)) [2, modulus],
  eqmod res3_14 (cvrted3_14 * ((* 3 14 *) 1)) [2, modulus],
  eqmod res3_15 (cvrted3_15 * ((* 3 15 *) 1)) [2, modulus],
  eqmod res3_16 (cvrted3_16 * ((* 3 16 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_17 (cvrted3_17 * ((* 3 17 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_18 (cvrted3_18 * ((* 3 18 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_19 (cvrted3_19 * ((* 3 19 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_20 (cvrted3_20 * ((* 3 20 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_21 (cvrted3_21 * ((* 3 21 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_22 (cvrted3_22 * ((* 3 22 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_23 (cvrted3_23 * ((* 3 23 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_24 (cvrted3_24 * ((* 3 24 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_25 (cvrted3_25 * ((* 3 25 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_26 (cvrted3_26 * ((* 3 26 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_27 (cvrted3_27 * ((* 3 27 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_28 (cvrted3_28 * ((* 3 28 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_29 (cvrted3_29 * ((* 3 29 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_30 (cvrted3_30 * ((* 3 30 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_31 (cvrted3_31 * ((* 3 31 *) z**7 + z**4 + z)) [2, modulus],
  eqmod res3_32 (cvrted3_32 * ((* 3 32 *) z**8 + z**5)) [2, modulus],
  eqmod res3_33 (cvrted3_33 * ((* 3 33 *) z**8 + z**5)) [2, modulus],
  eqmod res3_34 (cvrted3_34 * ((* 3 34 *) z**8 + z**5)) [2, modulus],
  eqmod res3_35 (cvrted3_35 * ((* 3 35 *) z**8 + z**5)) [2, modulus],
  eqmod res3_36 (cvrted3_36 * ((* 3 36 *) z**8 + z**5)) [2, modulus],
  eqmod res3_37 (cvrted3_37 * ((* 3 37 *) z**8 + z**5)) [2, modulus],
  eqmod res3_38 (cvrted3_38 * ((* 3 38 *) z**8 + z**5)) [2, modulus],
  eqmod res3_39 (cvrted3_39 * ((* 3 39 *) z**8 + z**5)) [2, modulus],
  eqmod res3_40 (cvrted3_40 * ((* 3 40 *) z**8 + z**5)) [2, modulus],
  eqmod res3_41 (cvrted3_41 * ((* 3 41 *) z**8 + z**5)) [2, modulus],
  eqmod res3_42 (cvrted3_42 * ((* 3 42 *) z**8 + z**5)) [2, modulus],
  eqmod res3_43 (cvrted3_43 * ((* 3 43 *) z**8 + z**5)) [2, modulus],
  eqmod res3_44 (cvrted3_44 * ((* 3 44 *) z**8 + z**5)) [2, modulus],
  eqmod res3_45 (cvrted3_45 * ((* 3 45 *) z**8 + z**5)) [2, modulus],
  eqmod res3_46 (cvrted3_46 * ((* 3 46 *) z**8 + z**5)) [2, modulus],
  eqmod res3_47 (cvrted3_47 * ((* 3 47 *) z**8 + z**5)) [2, modulus],
  eqmod res3_48 (cvrted3_48 * ((* 3 48 *) z**3)) [2, modulus],
  eqmod res3_49 (cvrted3_49 * ((* 3 49 *) z**3)) [2, modulus],
  eqmod res3_50 (cvrted3_50 * ((* 3 50 *) z**3)) [2, modulus],
  eqmod res3_51 (cvrted3_51 * ((* 3 51 *) z**3)) [2, modulus],
  eqmod res3_52 (cvrted3_52 * ((* 3 52 *) z**3)) [2, modulus],
  eqmod res3_53 (cvrted3_53 * ((* 3 53 *) z**3)) [2, modulus],
  eqmod res3_54 (cvrted3_54 * ((* 3 54 *) z**3)) [2, modulus],
  eqmod res3_55 (cvrted3_55 * ((* 3 55 *) z**3)) [2, modulus],
  eqmod res3_56 (cvrted3_56 * ((* 3 56 *) z**3)) [2, modulus],
  eqmod res3_57 (cvrted3_57 * ((* 3 57 *) z**3)) [2, modulus],
  eqmod res3_58 (cvrted3_58 * ((* 3 58 *) z**3)) [2, modulus],
  eqmod res3_59 (cvrted3_59 * ((* 3 59 *) z**3)) [2, modulus],
  eqmod res3_60 (cvrted3_60 * ((* 3 60 *) z**3)) [2, modulus],
  eqmod res3_61 (cvrted3_61 * ((* 3 61 *) z**3)) [2, modulus],
  eqmod res3_62 (cvrted3_62 * ((* 3 62 *) z**3)) [2, modulus],
  eqmod res3_63 (cvrted3_63 * ((* 3 63 *) z**3)) [2, modulus]
] && true;

nondet x4@uint12;

ecut and [
  eqmod (
    (cvrted3_0 + x * cvrted3_8) * ((z ** 16 + z) * x4) ** 0 +
    (cvrted3_16 + x * cvrted3_24) * ((z ** 16 + z) * x4) ** 1 +
    (cvrted3_32 + x * cvrted3_40) * ((z ** 16 + z) * x4) ** 2 +
    (cvrted3_48 + x * cvrted3_56) * ((z ** 16 + z) * x4) ** 3
  ) (
    (res3_0 + x * res3_8) * x4 ** 0 +
    (res3_16 + x * res3_24) * x4 ** 1 +
    (res3_32 + x * res3_40) * x4 ** 2 +
    (res3_48 + x * res3_56) * x4 ** 3
  ) [2, modulus],
  eqmod (
    (cvrted3_1 + x * cvrted3_9) * ((z ** 16 + z) * x4) ** 0 +
    (cvrted3_17 + x * cvrted3_25) * ((z ** 16 + z) * x4) ** 1 +
    (cvrted3_33 + x * cvrted3_41) * ((z ** 16 + z) * x4) ** 2 +
    (cvrted3_49 + x * cvrted3_57) * ((z ** 16 + z) * x4) ** 3
  ) (
    (res3_1 + x * res3_9) * x4 ** 0 +
    (res3_17 + x * res3_25) * x4 ** 1 +
    (res3_33 + x * res3_41) * x4 ** 2 +
    (res3_49 + x * res3_57) * x4 ** 3
  ) [2, modulus],
  eqmod (
    (cvrted3_2 + x * cvrted3_10) * ((z ** 16 + z) * x4) ** 0 +
    (cvrted3_18 + x * cvrted3_26) * ((z ** 16 + z) * x4) ** 1 +
    (cvrted3_34 + x * cvrted3_42) * ((z ** 16 + z) * x4) ** 2 +
    (cvrted3_50 + x * cvrted3_58) * ((z ** 16 + z) * x4) ** 3
  ) (
    (res3_2 + x * res3_10) * x4 ** 0 +
    (res3_18 + x * res3_26) * x4 ** 1 +
    (res3_34 + x * res3_42) * x4 ** 2 +
    (res3_50 + x * res3_58) * x4 ** 3
  ) [2, modulus],
  eqmod (
    (cvrted3_3 + x * cvrted3_11) * ((z ** 16 + z) * x4) ** 0 +
    (cvrted3_19 + x * cvrted3_27) * ((z ** 16 + z) * x4) ** 1 +
    (cvrted3_35 + x * cvrted3_43) * ((z ** 16 + z) * x4) ** 2 +
    (cvrted3_51 + x * cvrted3_59) * ((z ** 16 + z) * x4) ** 3
  ) (
    (res3_3 + x * res3_11) * x4 ** 0 +
    (res3_19 + x * res3_27) * x4 ** 1 +
    (res3_35 + x * res3_43) * x4 ** 2 +
    (res3_51 + x * res3_59) * x4 ** 3
  ) [2, modulus],
  eqmod (
    (cvrted3_4 + x * cvrted3_12) * ((z ** 16 + z) * x4) ** 0 +
    (cvrted3_20 + x * cvrted3_28) * ((z ** 16 + z) * x4) ** 1 +
    (cvrted3_36 + x * cvrted3_44) * ((z ** 16 + z) * x4) ** 2 +
    (cvrted3_52 + x * cvrted3_60) * ((z ** 16 + z) * x4) ** 3
  ) (
    (res3_4 + x * res3_12) * x4 ** 0 +
    (res3_20 + x * res3_28) * x4 ** 1 +
    (res3_36 + x * res3_44) * x4 ** 2 +
    (res3_52 + x * res3_60) * x4 ** 3
  ) [2, modulus],
  eqmod (
    (cvrted3_5 + x * cvrted3_13) * ((z ** 16 + z) * x4) ** 0 +
    (cvrted3_21 + x * cvrted3_29) * ((z ** 16 + z) * x4) ** 1 +
    (cvrted3_37 + x * cvrted3_45) * ((z ** 16 + z) * x4) ** 2 +
    (cvrted3_53 + x * cvrted3_61) * ((z ** 16 + z) * x4) ** 3
  ) (
    (res3_5 + x * res3_13) * x4 ** 0 +
    (res3_21 + x * res3_29) * x4 ** 1 +
    (res3_37 + x * res3_45) * x4 ** 2 +
    (res3_53 + x * res3_61) * x4 ** 3
  ) [2, modulus],
  eqmod (
    (cvrted3_6 + x * cvrted3_14) * ((z ** 16 + z) * x4) ** 0 +
    (cvrted3_22 + x * cvrted3_30) * ((z ** 16 + z) * x4) ** 1 +
    (cvrted3_38 + x * cvrted3_46) * ((z ** 16 + z) * x4) ** 2 +
    (cvrted3_54 + x * cvrted3_62) * ((z ** 16 + z) * x4) ** 3
  ) (
    (res3_6 + x * res3_14) * x4 ** 0 +
    (res3_22 + x * res3_30) * x4 ** 1 +
    (res3_38 + x * res3_46) * x4 ** 2 +
    (res3_54 + x * res3_62) * x4 ** 3
  ) [2, modulus],
  eqmod (
    (cvrted3_7 + x * cvrted3_15) * ((z ** 16 + z) * x4) ** 0 +
    (cvrted3_23 + x * cvrted3_31) * ((z ** 16 + z) * x4) ** 1 +
    (cvrted3_39 + x * cvrted3_47) * ((z ** 16 + z) * x4) ** 2 +
    (cvrted3_55 + x * cvrted3_63) * ((z ** 16 + z) * x4) ** 3
  ) (
    (res3_7 + x * res3_15) * x4 ** 0 +
    (res3_23 + x * res3_31) * x4 ** 1 +
    (res3_39 + x * res3_47) * x4 ** 2 +
    (res3_55 + x * res3_63) * x4 ** 3
  ) [2, modulus]
] prove with [precondition];

mov x x4;

nondet inp4_0@uint12; assume inp4_0 =
  res3_0 * x**0 + res3_16 * x**1 + res3_32 * x**2 + res3_48 * x**3
&& true;
nondet inp4_1@uint12; assume inp4_1 =
  res3_1 * x**0 + res3_17 * x**1 + res3_33 * x**2 + res3_49 * x**3
&& true;
nondet inp4_2@uint12; assume inp4_2 =
  res3_2 * x**0 + res3_18 * x**1 + res3_34 * x**2 + res3_50 * x**3
&& true;
nondet inp4_3@uint12; assume inp4_3 =
  res3_3 * x**0 + res3_19 * x**1 + res3_35 * x**2 + res3_51 * x**3
&& true;
nondet inp4_4@uint12; assume inp4_4 =
  res3_4 * x**0 + res3_20 * x**1 + res3_36 * x**2 + res3_52 * x**3
&& true;
nondet inp4_5@uint12; assume inp4_5 =
  res3_5 * x**0 + res3_21 * x**1 + res3_37 * x**2 + res3_53 * x**3
&& true;
nondet inp4_6@uint12; assume inp4_6 =
  res3_6 * x**0 + res3_22 * x**1 + res3_38 * x**2 + res3_54 * x**3
&& true;
nondet inp4_7@uint12; assume inp4_7 =
  res3_7 * x**0 + res3_23 * x**1 + res3_39 * x**2 + res3_55 * x**3
&& true;
nondet inp4_8@uint12; assume inp4_8 =
  res3_8 * x**0 + res3_24 * x**1 + res3_40 * x**2 + res3_56 * x**3
&& true;
nondet inp4_9@uint12; assume inp4_9 =
  res3_9 * x**0 + res3_25 * x**1 + res3_41 * x**2 + res3_57 * x**3
&& true;
nondet inp4_10@uint12; assume inp4_10 =
  res3_10 * x**0 + res3_26 * x**1 + res3_42 * x**2 + res3_58 * x**3
&& true;
nondet inp4_11@uint12; assume inp4_11 =
  res3_11 * x**0 + res3_27 * x**1 + res3_43 * x**2 + res3_59 * x**3
&& true;
nondet inp4_12@uint12; assume inp4_12 =
  res3_12 * x**0 + res3_28 * x**1 + res3_44 * x**2 + res3_60 * x**3
&& true;
nondet inp4_13@uint12; assume inp4_13 =
  res3_13 * x**0 + res3_29 * x**1 + res3_45 * x**2 + res3_61 * x**3
&& true;
nondet inp4_14@uint12; assume inp4_14 =
  res3_14 * x**0 + res3_30 * x**1 + res3_46 * x**2 + res3_62 * x**3
&& true;
nondet inp4_15@uint12; assume inp4_15 =
  res3_15 * x**0 + res3_31 * x**1 + res3_47 * x**2 + res3_63 * x**3
&& true;

nondet rb0_0@bit; nondet rb0_1@bit; nondet rb0_2@bit; nondet rb0_3@bit; nondet rb0_4@bit; nondet rb0_5@bit; nondet rb0_6@bit; nondet rb0_7@bit; nondet rb0_8@bit; nondet rb0_9@bit; nondet rb0_10@bit; nondet rb0_11@bit; nondet rb0_12@bit; nondet rb0_13@bit; nondet rb0_14@bit; nondet rb0_15@bit; nondet rb0_16@bit; nondet rb0_17@bit; nondet rb0_18@bit; nondet rb0_19@bit; nondet rb0_20@bit; nondet rb0_21@bit; nondet rb0_22@bit; nondet rb0_23@bit; nondet rb0_24@bit; nondet rb0_25@bit; nondet rb0_26@bit; nondet rb0_27@bit; nondet rb0_28@bit; nondet rb0_29@bit; nondet rb0_30@bit; nondet rb0_31@bit; nondet rb0_32@bit; nondet rb0_33@bit; nondet rb0_34@bit; nondet rb0_35@bit; nondet rb0_36@bit; nondet rb0_37@bit; nondet rb0_38@bit; nondet rb0_39@bit; nondet rb0_40@bit; nondet rb0_41@bit; nondet rb0_42@bit; nondet rb0_43@bit; nondet rb0_44@bit; nondet rb0_45@bit; nondet rb0_46@bit; nondet rb0_47@bit; nondet rb0_48@bit; nondet rb0_49@bit; nondet rb0_50@bit; nondet rb0_51@bit; nondet rb0_52@bit; nondet rb0_53@bit; nondet rb0_54@bit; nondet rb0_55@bit; nondet rb0_56@bit; nondet rb0_57@bit; nondet rb0_58@bit; nondet rb0_59@bit; nondet rb0_60@bit; nondet rb0_61@bit; nondet rb0_62@bit; nondet rb0_63@bit;
nondet rb1_0@bit; nondet rb1_1@bit; nondet rb1_2@bit; nondet rb1_3@bit; nondet rb1_4@bit; nondet rb1_5@bit; nondet rb1_6@bit; nondet rb1_7@bit; nondet rb1_8@bit; nondet rb1_9@bit; nondet rb1_10@bit; nondet rb1_11@bit; nondet rb1_12@bit; nondet rb1_13@bit; nondet rb1_14@bit; nondet rb1_15@bit; nondet rb1_16@bit; nondet rb1_17@bit; nondet rb1_18@bit; nondet rb1_19@bit; nondet rb1_20@bit; nondet rb1_21@bit; nondet rb1_22@bit; nondet rb1_23@bit; nondet rb1_24@bit; nondet rb1_25@bit; nondet rb1_26@bit; nondet rb1_27@bit; nondet rb1_28@bit; nondet rb1_29@bit; nondet rb1_30@bit; nondet rb1_31@bit; nondet rb1_32@bit; nondet rb1_33@bit; nondet rb1_34@bit; nondet rb1_35@bit; nondet rb1_36@bit; nondet rb1_37@bit; nondet rb1_38@bit; nondet rb1_39@bit; nondet rb1_40@bit; nondet rb1_41@bit; nondet rb1_42@bit; nondet rb1_43@bit; nondet rb1_44@bit; nondet rb1_45@bit; nondet rb1_46@bit; nondet rb1_47@bit; nondet rb1_48@bit; nondet rb1_49@bit; nondet rb1_50@bit; nondet rb1_51@bit; nondet rb1_52@bit; nondet rb1_53@bit; nondet rb1_54@bit; nondet rb1_55@bit; nondet rb1_56@bit; nondet rb1_57@bit; nondet rb1_58@bit; nondet rb1_59@bit; nondet rb1_60@bit; nondet rb1_61@bit; nondet rb1_62@bit; nondet rb1_63@bit;
nondet rb2_0@bit; nondet rb2_1@bit; nondet rb2_2@bit; nondet rb2_3@bit; nondet rb2_4@bit; nondet rb2_5@bit; nondet rb2_6@bit; nondet rb2_7@bit; nondet rb2_8@bit; nondet rb2_9@bit; nondet rb2_10@bit; nondet rb2_11@bit; nondet rb2_12@bit; nondet rb2_13@bit; nondet rb2_14@bit; nondet rb2_15@bit; nondet rb2_16@bit; nondet rb2_17@bit; nondet rb2_18@bit; nondet rb2_19@bit; nondet rb2_20@bit; nondet rb2_21@bit; nondet rb2_22@bit; nondet rb2_23@bit; nondet rb2_24@bit; nondet rb2_25@bit; nondet rb2_26@bit; nondet rb2_27@bit; nondet rb2_28@bit; nondet rb2_29@bit; nondet rb2_30@bit; nondet rb2_31@bit; nondet rb2_32@bit; nondet rb2_33@bit; nondet rb2_34@bit; nondet rb2_35@bit; nondet rb2_36@bit; nondet rb2_37@bit; nondet rb2_38@bit; nondet rb2_39@bit; nondet rb2_40@bit; nondet rb2_41@bit; nondet rb2_42@bit; nondet rb2_43@bit; nondet rb2_44@bit; nondet rb2_45@bit; nondet rb2_46@bit; nondet rb2_47@bit; nondet rb2_48@bit; nondet rb2_49@bit; nondet rb2_50@bit; nondet rb2_51@bit; nondet rb2_52@bit; nondet rb2_53@bit; nondet rb2_54@bit; nondet rb2_55@bit; nondet rb2_56@bit; nondet rb2_57@bit; nondet rb2_58@bit; nondet rb2_59@bit; nondet rb2_60@bit; nondet rb2_61@bit; nondet rb2_62@bit; nondet rb2_63@bit;
nondet rb3_0@bit; nondet rb3_1@bit; nondet rb3_2@bit; nondet rb3_3@bit; nondet rb3_4@bit; nondet rb3_5@bit; nondet rb3_6@bit; nondet rb3_7@bit; nondet rb3_8@bit; nondet rb3_9@bit; nondet rb3_10@bit; nondet rb3_11@bit; nondet rb3_12@bit; nondet rb3_13@bit; nondet rb3_14@bit; nondet rb3_15@bit; nondet rb3_16@bit; nondet rb3_17@bit; nondet rb3_18@bit; nondet rb3_19@bit; nondet rb3_20@bit; nondet rb3_21@bit; nondet rb3_22@bit; nondet rb3_23@bit; nondet rb3_24@bit; nondet rb3_25@bit; nondet rb3_26@bit; nondet rb3_27@bit; nondet rb3_28@bit; nondet rb3_29@bit; nondet rb3_30@bit; nondet rb3_31@bit; nondet rb3_32@bit; nondet rb3_33@bit; nondet rb3_34@bit; nondet rb3_35@bit; nondet rb3_36@bit; nondet rb3_37@bit; nondet rb3_38@bit; nondet rb3_39@bit; nondet rb3_40@bit; nondet rb3_41@bit; nondet rb3_42@bit; nondet rb3_43@bit; nondet rb3_44@bit; nondet rb3_45@bit; nondet rb3_46@bit; nondet rb3_47@bit; nondet rb3_48@bit; nondet rb3_49@bit; nondet rb3_50@bit; nondet rb3_51@bit; nondet rb3_52@bit; nondet rb3_53@bit; nondet rb3_54@bit; nondet rb3_55@bit; nondet rb3_56@bit; nondet rb3_57@bit; nondet rb3_58@bit; nondet rb3_59@bit; nondet rb3_60@bit; nondet rb3_61@bit; nondet rb3_62@bit; nondet rb3_63@bit;
nondet rb4_0@bit; nondet rb4_1@bit; nondet rb4_2@bit; nondet rb4_3@bit; nondet rb4_4@bit; nondet rb4_5@bit; nondet rb4_6@bit; nondet rb4_7@bit; nondet rb4_8@bit; nondet rb4_9@bit; nondet rb4_10@bit; nondet rb4_11@bit; nondet rb4_12@bit; nondet rb4_13@bit; nondet rb4_14@bit; nondet rb4_15@bit; nondet rb4_16@bit; nondet rb4_17@bit; nondet rb4_18@bit; nondet rb4_19@bit; nondet rb4_20@bit; nondet rb4_21@bit; nondet rb4_22@bit; nondet rb4_23@bit; nondet rb4_24@bit; nondet rb4_25@bit; nondet rb4_26@bit; nondet rb4_27@bit; nondet rb4_28@bit; nondet rb4_29@bit; nondet rb4_30@bit; nondet rb4_31@bit; nondet rb4_32@bit; nondet rb4_33@bit; nondet rb4_34@bit; nondet rb4_35@bit; nondet rb4_36@bit; nondet rb4_37@bit; nondet rb4_38@bit; nondet rb4_39@bit; nondet rb4_40@bit; nondet rb4_41@bit; nondet rb4_42@bit; nondet rb4_43@bit; nondet rb4_44@bit; nondet rb4_45@bit; nondet rb4_46@bit; nondet rb4_47@bit; nondet rb4_48@bit; nondet rb4_49@bit; nondet rb4_50@bit; nondet rb4_51@bit; nondet rb4_52@bit; nondet rb4_53@bit; nondet rb4_54@bit; nondet rb4_55@bit; nondet rb4_56@bit; nondet rb4_57@bit; nondet rb4_58@bit; nondet rb4_59@bit; nondet rb4_60@bit; nondet rb4_61@bit; nondet rb4_62@bit; nondet rb4_63@bit;
nondet rb5_0@bit; nondet rb5_1@bit; nondet rb5_2@bit; nondet rb5_3@bit; nondet rb5_4@bit; nondet rb5_5@bit; nondet rb5_6@bit; nondet rb5_7@bit; nondet rb5_8@bit; nondet rb5_9@bit; nondet rb5_10@bit; nondet rb5_11@bit; nondet rb5_12@bit; nondet rb5_13@bit; nondet rb5_14@bit; nondet rb5_15@bit; nondet rb5_16@bit; nondet rb5_17@bit; nondet rb5_18@bit; nondet rb5_19@bit; nondet rb5_20@bit; nondet rb5_21@bit; nondet rb5_22@bit; nondet rb5_23@bit; nondet rb5_24@bit; nondet rb5_25@bit; nondet rb5_26@bit; nondet rb5_27@bit; nondet rb5_28@bit; nondet rb5_29@bit; nondet rb5_30@bit; nondet rb5_31@bit; nondet rb5_32@bit; nondet rb5_33@bit; nondet rb5_34@bit; nondet rb5_35@bit; nondet rb5_36@bit; nondet rb5_37@bit; nondet rb5_38@bit; nondet rb5_39@bit; nondet rb5_40@bit; nondet rb5_41@bit; nondet rb5_42@bit; nondet rb5_43@bit; nondet rb5_44@bit; nondet rb5_45@bit; nondet rb5_46@bit; nondet rb5_47@bit; nondet rb5_48@bit; nondet rb5_49@bit; nondet rb5_50@bit; nondet rb5_51@bit; nondet rb5_52@bit; nondet rb5_53@bit; nondet rb5_54@bit; nondet rb5_55@bit; nondet rb5_56@bit; nondet rb5_57@bit; nondet rb5_58@bit; nondet rb5_59@bit; nondet rb5_60@bit; nondet rb5_61@bit; nondet rb5_62@bit; nondet rb5_63@bit;
nondet rb6_0@bit; nondet rb6_1@bit; nondet rb6_2@bit; nondet rb6_3@bit; nondet rb6_4@bit; nondet rb6_5@bit; nondet rb6_6@bit; nondet rb6_7@bit; nondet rb6_8@bit; nondet rb6_9@bit; nondet rb6_10@bit; nondet rb6_11@bit; nondet rb6_12@bit; nondet rb6_13@bit; nondet rb6_14@bit; nondet rb6_15@bit; nondet rb6_16@bit; nondet rb6_17@bit; nondet rb6_18@bit; nondet rb6_19@bit; nondet rb6_20@bit; nondet rb6_21@bit; nondet rb6_22@bit; nondet rb6_23@bit; nondet rb6_24@bit; nondet rb6_25@bit; nondet rb6_26@bit; nondet rb6_27@bit; nondet rb6_28@bit; nondet rb6_29@bit; nondet rb6_30@bit; nondet rb6_31@bit; nondet rb6_32@bit; nondet rb6_33@bit; nondet rb6_34@bit; nondet rb6_35@bit; nondet rb6_36@bit; nondet rb6_37@bit; nondet rb6_38@bit; nondet rb6_39@bit; nondet rb6_40@bit; nondet rb6_41@bit; nondet rb6_42@bit; nondet rb6_43@bit; nondet rb6_44@bit; nondet rb6_45@bit; nondet rb6_46@bit; nondet rb6_47@bit; nondet rb6_48@bit; nondet rb6_49@bit; nondet rb6_50@bit; nondet rb6_51@bit; nondet rb6_52@bit; nondet rb6_53@bit; nondet rb6_54@bit; nondet rb6_55@bit; nondet rb6_56@bit; nondet rb6_57@bit; nondet rb6_58@bit; nondet rb6_59@bit; nondet rb6_60@bit; nondet rb6_61@bit; nondet rb6_62@bit; nondet rb6_63@bit;
nondet rb7_0@bit; nondet rb7_1@bit; nondet rb7_2@bit; nondet rb7_3@bit; nondet rb7_4@bit; nondet rb7_5@bit; nondet rb7_6@bit; nondet rb7_7@bit; nondet rb7_8@bit; nondet rb7_9@bit; nondet rb7_10@bit; nondet rb7_11@bit; nondet rb7_12@bit; nondet rb7_13@bit; nondet rb7_14@bit; nondet rb7_15@bit; nondet rb7_16@bit; nondet rb7_17@bit; nondet rb7_18@bit; nondet rb7_19@bit; nondet rb7_20@bit; nondet rb7_21@bit; nondet rb7_22@bit; nondet rb7_23@bit; nondet rb7_24@bit; nondet rb7_25@bit; nondet rb7_26@bit; nondet rb7_27@bit; nondet rb7_28@bit; nondet rb7_29@bit; nondet rb7_30@bit; nondet rb7_31@bit; nondet rb7_32@bit; nondet rb7_33@bit; nondet rb7_34@bit; nondet rb7_35@bit; nondet rb7_36@bit; nondet rb7_37@bit; nondet rb7_38@bit; nondet rb7_39@bit; nondet rb7_40@bit; nondet rb7_41@bit; nondet rb7_42@bit; nondet rb7_43@bit; nondet rb7_44@bit; nondet rb7_45@bit; nondet rb7_46@bit; nondet rb7_47@bit; nondet rb7_48@bit; nondet rb7_49@bit; nondet rb7_50@bit; nondet rb7_51@bit; nondet rb7_52@bit; nondet rb7_53@bit; nondet rb7_54@bit; nondet rb7_55@bit; nondet rb7_56@bit; nondet rb7_57@bit; nondet rb7_58@bit; nondet rb7_59@bit; nondet rb7_60@bit; nondet rb7_61@bit; nondet rb7_62@bit; nondet rb7_63@bit;
nondet rb8_0@bit; nondet rb8_1@bit; nondet rb8_2@bit; nondet rb8_3@bit; nondet rb8_4@bit; nondet rb8_5@bit; nondet rb8_6@bit; nondet rb8_7@bit; nondet rb8_8@bit; nondet rb8_9@bit; nondet rb8_10@bit; nondet rb8_11@bit; nondet rb8_12@bit; nondet rb8_13@bit; nondet rb8_14@bit; nondet rb8_15@bit; nondet rb8_16@bit; nondet rb8_17@bit; nondet rb8_18@bit; nondet rb8_19@bit; nondet rb8_20@bit; nondet rb8_21@bit; nondet rb8_22@bit; nondet rb8_23@bit; nondet rb8_24@bit; nondet rb8_25@bit; nondet rb8_26@bit; nondet rb8_27@bit; nondet rb8_28@bit; nondet rb8_29@bit; nondet rb8_30@bit; nondet rb8_31@bit; nondet rb8_32@bit; nondet rb8_33@bit; nondet rb8_34@bit; nondet rb8_35@bit; nondet rb8_36@bit; nondet rb8_37@bit; nondet rb8_38@bit; nondet rb8_39@bit; nondet rb8_40@bit; nondet rb8_41@bit; nondet rb8_42@bit; nondet rb8_43@bit; nondet rb8_44@bit; nondet rb8_45@bit; nondet rb8_46@bit; nondet rb8_47@bit; nondet rb8_48@bit; nondet rb8_49@bit; nondet rb8_50@bit; nondet rb8_51@bit; nondet rb8_52@bit; nondet rb8_53@bit; nondet rb8_54@bit; nondet rb8_55@bit; nondet rb8_56@bit; nondet rb8_57@bit; nondet rb8_58@bit; nondet rb8_59@bit; nondet rb8_60@bit; nondet rb8_61@bit; nondet rb8_62@bit; nondet rb8_63@bit;
nondet rb9_0@bit; nondet rb9_1@bit; nondet rb9_2@bit; nondet rb9_3@bit; nondet rb9_4@bit; nondet rb9_5@bit; nondet rb9_6@bit; nondet rb9_7@bit; nondet rb9_8@bit; nondet rb9_9@bit; nondet rb9_10@bit; nondet rb9_11@bit; nondet rb9_12@bit; nondet rb9_13@bit; nondet rb9_14@bit; nondet rb9_15@bit; nondet rb9_16@bit; nondet rb9_17@bit; nondet rb9_18@bit; nondet rb9_19@bit; nondet rb9_20@bit; nondet rb9_21@bit; nondet rb9_22@bit; nondet rb9_23@bit; nondet rb9_24@bit; nondet rb9_25@bit; nondet rb9_26@bit; nondet rb9_27@bit; nondet rb9_28@bit; nondet rb9_29@bit; nondet rb9_30@bit; nondet rb9_31@bit; nondet rb9_32@bit; nondet rb9_33@bit; nondet rb9_34@bit; nondet rb9_35@bit; nondet rb9_36@bit; nondet rb9_37@bit; nondet rb9_38@bit; nondet rb9_39@bit; nondet rb9_40@bit; nondet rb9_41@bit; nondet rb9_42@bit; nondet rb9_43@bit; nondet rb9_44@bit; nondet rb9_45@bit; nondet rb9_46@bit; nondet rb9_47@bit; nondet rb9_48@bit; nondet rb9_49@bit; nondet rb9_50@bit; nondet rb9_51@bit; nondet rb9_52@bit; nondet rb9_53@bit; nondet rb9_54@bit; nondet rb9_55@bit; nondet rb9_56@bit; nondet rb9_57@bit; nondet rb9_58@bit; nondet rb9_59@bit; nondet rb9_60@bit; nondet rb9_61@bit; nondet rb9_62@bit; nondet rb9_63@bit;
nondet rb10_0@bit; nondet rb10_1@bit; nondet rb10_2@bit; nondet rb10_3@bit; nondet rb10_4@bit; nondet rb10_5@bit; nondet rb10_6@bit; nondet rb10_7@bit; nondet rb10_8@bit; nondet rb10_9@bit; nondet rb10_10@bit; nondet rb10_11@bit; nondet rb10_12@bit; nondet rb10_13@bit; nondet rb10_14@bit; nondet rb10_15@bit; nondet rb10_16@bit; nondet rb10_17@bit; nondet rb10_18@bit; nondet rb10_19@bit; nondet rb10_20@bit; nondet rb10_21@bit; nondet rb10_22@bit; nondet rb10_23@bit; nondet rb10_24@bit; nondet rb10_25@bit; nondet rb10_26@bit; nondet rb10_27@bit; nondet rb10_28@bit; nondet rb10_29@bit; nondet rb10_30@bit; nondet rb10_31@bit; nondet rb10_32@bit; nondet rb10_33@bit; nondet rb10_34@bit; nondet rb10_35@bit; nondet rb10_36@bit; nondet rb10_37@bit; nondet rb10_38@bit; nondet rb10_39@bit; nondet rb10_40@bit; nondet rb10_41@bit; nondet rb10_42@bit; nondet rb10_43@bit; nondet rb10_44@bit; nondet rb10_45@bit; nondet rb10_46@bit; nondet rb10_47@bit; nondet rb10_48@bit; nondet rb10_49@bit; nondet rb10_50@bit; nondet rb10_51@bit; nondet rb10_52@bit; nondet rb10_53@bit; nondet rb10_54@bit; nondet rb10_55@bit; nondet rb10_56@bit; nondet rb10_57@bit; nondet rb10_58@bit; nondet rb10_59@bit; nondet rb10_60@bit; nondet rb10_61@bit; nondet rb10_62@bit; nondet rb10_63@bit;
nondet rb11_0@bit; nondet rb11_1@bit; nondet rb11_2@bit; nondet rb11_3@bit; nondet rb11_4@bit; nondet rb11_5@bit; nondet rb11_6@bit; nondet rb11_7@bit; nondet rb11_8@bit; nondet rb11_9@bit; nondet rb11_10@bit; nondet rb11_11@bit; nondet rb11_12@bit; nondet rb11_13@bit; nondet rb11_14@bit; nondet rb11_15@bit; nondet rb11_16@bit; nondet rb11_17@bit; nondet rb11_18@bit; nondet rb11_19@bit; nondet rb11_20@bit; nondet rb11_21@bit; nondet rb11_22@bit; nondet rb11_23@bit; nondet rb11_24@bit; nondet rb11_25@bit; nondet rb11_26@bit; nondet rb11_27@bit; nondet rb11_28@bit; nondet rb11_29@bit; nondet rb11_30@bit; nondet rb11_31@bit; nondet rb11_32@bit; nondet rb11_33@bit; nondet rb11_34@bit; nondet rb11_35@bit; nondet rb11_36@bit; nondet rb11_37@bit; nondet rb11_38@bit; nondet rb11_39@bit; nondet rb11_40@bit; nondet rb11_41@bit; nondet rb11_42@bit; nondet rb11_43@bit; nondet rb11_44@bit; nondet rb11_45@bit; nondet rb11_46@bit; nondet rb11_47@bit; nondet rb11_48@bit; nondet rb11_49@bit; nondet rb11_50@bit; nondet rb11_51@bit; nondet rb11_52@bit; nondet rb11_53@bit; nondet rb11_54@bit; nondet rb11_55@bit; nondet rb11_56@bit; nondet rb11_57@bit; nondet rb11_58@bit; nondet rb11_59@bit; nondet rb11_60@bit; nondet rb11_61@bit; nondet rb11_62@bit; nondet rb11_63@bit;
assume and [
  res3_0 = rb0_0 * z**0 + rb1_0 * z**1 + rb2_0 * z**2 + rb3_0 * z**3 + rb4_0 * z**4 + rb5_0 * z**5 + rb6_0 * z**6 + rb7_0 * z**7 + rb8_0 * z**8 + rb9_0 * z**9 + rb10_0 * z**10 + rb11_0 * z**11,
  res3_1 = rb0_1 * z**0 + rb1_1 * z**1 + rb2_1 * z**2 + rb3_1 * z**3 + rb4_1 * z**4 + rb5_1 * z**5 + rb6_1 * z**6 + rb7_1 * z**7 + rb8_1 * z**8 + rb9_1 * z**9 + rb10_1 * z**10 + rb11_1 * z**11,
  res3_2 = rb0_2 * z**0 + rb1_2 * z**1 + rb2_2 * z**2 + rb3_2 * z**3 + rb4_2 * z**4 + rb5_2 * z**5 + rb6_2 * z**6 + rb7_2 * z**7 + rb8_2 * z**8 + rb9_2 * z**9 + rb10_2 * z**10 + rb11_2 * z**11,
  res3_3 = rb0_3 * z**0 + rb1_3 * z**1 + rb2_3 * z**2 + rb3_3 * z**3 + rb4_3 * z**4 + rb5_3 * z**5 + rb6_3 * z**6 + rb7_3 * z**7 + rb8_3 * z**8 + rb9_3 * z**9 + rb10_3 * z**10 + rb11_3 * z**11,
  res3_4 = rb0_4 * z**0 + rb1_4 * z**1 + rb2_4 * z**2 + rb3_4 * z**3 + rb4_4 * z**4 + rb5_4 * z**5 + rb6_4 * z**6 + rb7_4 * z**7 + rb8_4 * z**8 + rb9_4 * z**9 + rb10_4 * z**10 + rb11_4 * z**11,
  res3_5 = rb0_5 * z**0 + rb1_5 * z**1 + rb2_5 * z**2 + rb3_5 * z**3 + rb4_5 * z**4 + rb5_5 * z**5 + rb6_5 * z**6 + rb7_5 * z**7 + rb8_5 * z**8 + rb9_5 * z**9 + rb10_5 * z**10 + rb11_5 * z**11,
  res3_6 = rb0_6 * z**0 + rb1_6 * z**1 + rb2_6 * z**2 + rb3_6 * z**3 + rb4_6 * z**4 + rb5_6 * z**5 + rb6_6 * z**6 + rb7_6 * z**7 + rb8_6 * z**8 + rb9_6 * z**9 + rb10_6 * z**10 + rb11_6 * z**11,
  res3_7 = rb0_7 * z**0 + rb1_7 * z**1 + rb2_7 * z**2 + rb3_7 * z**3 + rb4_7 * z**4 + rb5_7 * z**5 + rb6_7 * z**6 + rb7_7 * z**7 + rb8_7 * z**8 + rb9_7 * z**9 + rb10_7 * z**10 + rb11_7 * z**11,
  res3_8 = rb0_8 * z**0 + rb1_8 * z**1 + rb2_8 * z**2 + rb3_8 * z**3 + rb4_8 * z**4 + rb5_8 * z**5 + rb6_8 * z**6 + rb7_8 * z**7 + rb8_8 * z**8 + rb9_8 * z**9 + rb10_8 * z**10 + rb11_8 * z**11,
  res3_9 = rb0_9 * z**0 + rb1_9 * z**1 + rb2_9 * z**2 + rb3_9 * z**3 + rb4_9 * z**4 + rb5_9 * z**5 + rb6_9 * z**6 + rb7_9 * z**7 + rb8_9 * z**8 + rb9_9 * z**9 + rb10_9 * z**10 + rb11_9 * z**11,
  res3_10 = rb0_10 * z**0 + rb1_10 * z**1 + rb2_10 * z**2 + rb3_10 * z**3 + rb4_10 * z**4 + rb5_10 * z**5 + rb6_10 * z**6 + rb7_10 * z**7 + rb8_10 * z**8 + rb9_10 * z**9 + rb10_10 * z**10 + rb11_10 * z**11,
  res3_11 = rb0_11 * z**0 + rb1_11 * z**1 + rb2_11 * z**2 + rb3_11 * z**3 + rb4_11 * z**4 + rb5_11 * z**5 + rb6_11 * z**6 + rb7_11 * z**7 + rb8_11 * z**8 + rb9_11 * z**9 + rb10_11 * z**10 + rb11_11 * z**11,
  res3_12 = rb0_12 * z**0 + rb1_12 * z**1 + rb2_12 * z**2 + rb3_12 * z**3 + rb4_12 * z**4 + rb5_12 * z**5 + rb6_12 * z**6 + rb7_12 * z**7 + rb8_12 * z**8 + rb9_12 * z**9 + rb10_12 * z**10 + rb11_12 * z**11,
  res3_13 = rb0_13 * z**0 + rb1_13 * z**1 + rb2_13 * z**2 + rb3_13 * z**3 + rb4_13 * z**4 + rb5_13 * z**5 + rb6_13 * z**6 + rb7_13 * z**7 + rb8_13 * z**8 + rb9_13 * z**9 + rb10_13 * z**10 + rb11_13 * z**11,
  res3_14 = rb0_14 * z**0 + rb1_14 * z**1 + rb2_14 * z**2 + rb3_14 * z**3 + rb4_14 * z**4 + rb5_14 * z**5 + rb6_14 * z**6 + rb7_14 * z**7 + rb8_14 * z**8 + rb9_14 * z**9 + rb10_14 * z**10 + rb11_14 * z**11,
  res3_15 = rb0_15 * z**0 + rb1_15 * z**1 + rb2_15 * z**2 + rb3_15 * z**3 + rb4_15 * z**4 + rb5_15 * z**5 + rb6_15 * z**6 + rb7_15 * z**7 + rb8_15 * z**8 + rb9_15 * z**9 + rb10_15 * z**10 + rb11_15 * z**11,
  res3_16 = rb0_16 * z**0 + rb1_16 * z**1 + rb2_16 * z**2 + rb3_16 * z**3 + rb4_16 * z**4 + rb5_16 * z**5 + rb6_16 * z**6 + rb7_16 * z**7 + rb8_16 * z**8 + rb9_16 * z**9 + rb10_16 * z**10 + rb11_16 * z**11,
  res3_17 = rb0_17 * z**0 + rb1_17 * z**1 + rb2_17 * z**2 + rb3_17 * z**3 + rb4_17 * z**4 + rb5_17 * z**5 + rb6_17 * z**6 + rb7_17 * z**7 + rb8_17 * z**8 + rb9_17 * z**9 + rb10_17 * z**10 + rb11_17 * z**11,
  res3_18 = rb0_18 * z**0 + rb1_18 * z**1 + rb2_18 * z**2 + rb3_18 * z**3 + rb4_18 * z**4 + rb5_18 * z**5 + rb6_18 * z**6 + rb7_18 * z**7 + rb8_18 * z**8 + rb9_18 * z**9 + rb10_18 * z**10 + rb11_18 * z**11,
  res3_19 = rb0_19 * z**0 + rb1_19 * z**1 + rb2_19 * z**2 + rb3_19 * z**3 + rb4_19 * z**4 + rb5_19 * z**5 + rb6_19 * z**6 + rb7_19 * z**7 + rb8_19 * z**8 + rb9_19 * z**9 + rb10_19 * z**10 + rb11_19 * z**11,
  res3_20 = rb0_20 * z**0 + rb1_20 * z**1 + rb2_20 * z**2 + rb3_20 * z**3 + rb4_20 * z**4 + rb5_20 * z**5 + rb6_20 * z**6 + rb7_20 * z**7 + rb8_20 * z**8 + rb9_20 * z**9 + rb10_20 * z**10 + rb11_20 * z**11,
  res3_21 = rb0_21 * z**0 + rb1_21 * z**1 + rb2_21 * z**2 + rb3_21 * z**3 + rb4_21 * z**4 + rb5_21 * z**5 + rb6_21 * z**6 + rb7_21 * z**7 + rb8_21 * z**8 + rb9_21 * z**9 + rb10_21 * z**10 + rb11_21 * z**11,
  res3_22 = rb0_22 * z**0 + rb1_22 * z**1 + rb2_22 * z**2 + rb3_22 * z**3 + rb4_22 * z**4 + rb5_22 * z**5 + rb6_22 * z**6 + rb7_22 * z**7 + rb8_22 * z**8 + rb9_22 * z**9 + rb10_22 * z**10 + rb11_22 * z**11,
  res3_23 = rb0_23 * z**0 + rb1_23 * z**1 + rb2_23 * z**2 + rb3_23 * z**3 + rb4_23 * z**4 + rb5_23 * z**5 + rb6_23 * z**6 + rb7_23 * z**7 + rb8_23 * z**8 + rb9_23 * z**9 + rb10_23 * z**10 + rb11_23 * z**11,
  res3_24 = rb0_24 * z**0 + rb1_24 * z**1 + rb2_24 * z**2 + rb3_24 * z**3 + rb4_24 * z**4 + rb5_24 * z**5 + rb6_24 * z**6 + rb7_24 * z**7 + rb8_24 * z**8 + rb9_24 * z**9 + rb10_24 * z**10 + rb11_24 * z**11,
  res3_25 = rb0_25 * z**0 + rb1_25 * z**1 + rb2_25 * z**2 + rb3_25 * z**3 + rb4_25 * z**4 + rb5_25 * z**5 + rb6_25 * z**6 + rb7_25 * z**7 + rb8_25 * z**8 + rb9_25 * z**9 + rb10_25 * z**10 + rb11_25 * z**11,
  res3_26 = rb0_26 * z**0 + rb1_26 * z**1 + rb2_26 * z**2 + rb3_26 * z**3 + rb4_26 * z**4 + rb5_26 * z**5 + rb6_26 * z**6 + rb7_26 * z**7 + rb8_26 * z**8 + rb9_26 * z**9 + rb10_26 * z**10 + rb11_26 * z**11,
  res3_27 = rb0_27 * z**0 + rb1_27 * z**1 + rb2_27 * z**2 + rb3_27 * z**3 + rb4_27 * z**4 + rb5_27 * z**5 + rb6_27 * z**6 + rb7_27 * z**7 + rb8_27 * z**8 + rb9_27 * z**9 + rb10_27 * z**10 + rb11_27 * z**11,
  res3_28 = rb0_28 * z**0 + rb1_28 * z**1 + rb2_28 * z**2 + rb3_28 * z**3 + rb4_28 * z**4 + rb5_28 * z**5 + rb6_28 * z**6 + rb7_28 * z**7 + rb8_28 * z**8 + rb9_28 * z**9 + rb10_28 * z**10 + rb11_28 * z**11,
  res3_29 = rb0_29 * z**0 + rb1_29 * z**1 + rb2_29 * z**2 + rb3_29 * z**3 + rb4_29 * z**4 + rb5_29 * z**5 + rb6_29 * z**6 + rb7_29 * z**7 + rb8_29 * z**8 + rb9_29 * z**9 + rb10_29 * z**10 + rb11_29 * z**11,
  res3_30 = rb0_30 * z**0 + rb1_30 * z**1 + rb2_30 * z**2 + rb3_30 * z**3 + rb4_30 * z**4 + rb5_30 * z**5 + rb6_30 * z**6 + rb7_30 * z**7 + rb8_30 * z**8 + rb9_30 * z**9 + rb10_30 * z**10 + rb11_30 * z**11,
  res3_31 = rb0_31 * z**0 + rb1_31 * z**1 + rb2_31 * z**2 + rb3_31 * z**3 + rb4_31 * z**4 + rb5_31 * z**5 + rb6_31 * z**6 + rb7_31 * z**7 + rb8_31 * z**8 + rb9_31 * z**9 + rb10_31 * z**10 + rb11_31 * z**11,
  res3_32 = rb0_32 * z**0 + rb1_32 * z**1 + rb2_32 * z**2 + rb3_32 * z**3 + rb4_32 * z**4 + rb5_32 * z**5 + rb6_32 * z**6 + rb7_32 * z**7 + rb8_32 * z**8 + rb9_32 * z**9 + rb10_32 * z**10 + rb11_32 * z**11,
  res3_33 = rb0_33 * z**0 + rb1_33 * z**1 + rb2_33 * z**2 + rb3_33 * z**3 + rb4_33 * z**4 + rb5_33 * z**5 + rb6_33 * z**6 + rb7_33 * z**7 + rb8_33 * z**8 + rb9_33 * z**9 + rb10_33 * z**10 + rb11_33 * z**11,
  res3_34 = rb0_34 * z**0 + rb1_34 * z**1 + rb2_34 * z**2 + rb3_34 * z**3 + rb4_34 * z**4 + rb5_34 * z**5 + rb6_34 * z**6 + rb7_34 * z**7 + rb8_34 * z**8 + rb9_34 * z**9 + rb10_34 * z**10 + rb11_34 * z**11,
  res3_35 = rb0_35 * z**0 + rb1_35 * z**1 + rb2_35 * z**2 + rb3_35 * z**3 + rb4_35 * z**4 + rb5_35 * z**5 + rb6_35 * z**6 + rb7_35 * z**7 + rb8_35 * z**8 + rb9_35 * z**9 + rb10_35 * z**10 + rb11_35 * z**11,
  res3_36 = rb0_36 * z**0 + rb1_36 * z**1 + rb2_36 * z**2 + rb3_36 * z**3 + rb4_36 * z**4 + rb5_36 * z**5 + rb6_36 * z**6 + rb7_36 * z**7 + rb8_36 * z**8 + rb9_36 * z**9 + rb10_36 * z**10 + rb11_36 * z**11,
  res3_37 = rb0_37 * z**0 + rb1_37 * z**1 + rb2_37 * z**2 + rb3_37 * z**3 + rb4_37 * z**4 + rb5_37 * z**5 + rb6_37 * z**6 + rb7_37 * z**7 + rb8_37 * z**8 + rb9_37 * z**9 + rb10_37 * z**10 + rb11_37 * z**11,
  res3_38 = rb0_38 * z**0 + rb1_38 * z**1 + rb2_38 * z**2 + rb3_38 * z**3 + rb4_38 * z**4 + rb5_38 * z**5 + rb6_38 * z**6 + rb7_38 * z**7 + rb8_38 * z**8 + rb9_38 * z**9 + rb10_38 * z**10 + rb11_38 * z**11,
  res3_39 = rb0_39 * z**0 + rb1_39 * z**1 + rb2_39 * z**2 + rb3_39 * z**3 + rb4_39 * z**4 + rb5_39 * z**5 + rb6_39 * z**6 + rb7_39 * z**7 + rb8_39 * z**8 + rb9_39 * z**9 + rb10_39 * z**10 + rb11_39 * z**11,
  res3_40 = rb0_40 * z**0 + rb1_40 * z**1 + rb2_40 * z**2 + rb3_40 * z**3 + rb4_40 * z**4 + rb5_40 * z**5 + rb6_40 * z**6 + rb7_40 * z**7 + rb8_40 * z**8 + rb9_40 * z**9 + rb10_40 * z**10 + rb11_40 * z**11,
  res3_41 = rb0_41 * z**0 + rb1_41 * z**1 + rb2_41 * z**2 + rb3_41 * z**3 + rb4_41 * z**4 + rb5_41 * z**5 + rb6_41 * z**6 + rb7_41 * z**7 + rb8_41 * z**8 + rb9_41 * z**9 + rb10_41 * z**10 + rb11_41 * z**11,
  res3_42 = rb0_42 * z**0 + rb1_42 * z**1 + rb2_42 * z**2 + rb3_42 * z**3 + rb4_42 * z**4 + rb5_42 * z**5 + rb6_42 * z**6 + rb7_42 * z**7 + rb8_42 * z**8 + rb9_42 * z**9 + rb10_42 * z**10 + rb11_42 * z**11,
  res3_43 = rb0_43 * z**0 + rb1_43 * z**1 + rb2_43 * z**2 + rb3_43 * z**3 + rb4_43 * z**4 + rb5_43 * z**5 + rb6_43 * z**6 + rb7_43 * z**7 + rb8_43 * z**8 + rb9_43 * z**9 + rb10_43 * z**10 + rb11_43 * z**11,
  res3_44 = rb0_44 * z**0 + rb1_44 * z**1 + rb2_44 * z**2 + rb3_44 * z**3 + rb4_44 * z**4 + rb5_44 * z**5 + rb6_44 * z**6 + rb7_44 * z**7 + rb8_44 * z**8 + rb9_44 * z**9 + rb10_44 * z**10 + rb11_44 * z**11,
  res3_45 = rb0_45 * z**0 + rb1_45 * z**1 + rb2_45 * z**2 + rb3_45 * z**3 + rb4_45 * z**4 + rb5_45 * z**5 + rb6_45 * z**6 + rb7_45 * z**7 + rb8_45 * z**8 + rb9_45 * z**9 + rb10_45 * z**10 + rb11_45 * z**11,
  res3_46 = rb0_46 * z**0 + rb1_46 * z**1 + rb2_46 * z**2 + rb3_46 * z**3 + rb4_46 * z**4 + rb5_46 * z**5 + rb6_46 * z**6 + rb7_46 * z**7 + rb8_46 * z**8 + rb9_46 * z**9 + rb10_46 * z**10 + rb11_46 * z**11,
  res3_47 = rb0_47 * z**0 + rb1_47 * z**1 + rb2_47 * z**2 + rb3_47 * z**3 + rb4_47 * z**4 + rb5_47 * z**5 + rb6_47 * z**6 + rb7_47 * z**7 + rb8_47 * z**8 + rb9_47 * z**9 + rb10_47 * z**10 + rb11_47 * z**11,
  res3_48 = rb0_48 * z**0 + rb1_48 * z**1 + rb2_48 * z**2 + rb3_48 * z**3 + rb4_48 * z**4 + rb5_48 * z**5 + rb6_48 * z**6 + rb7_48 * z**7 + rb8_48 * z**8 + rb9_48 * z**9 + rb10_48 * z**10 + rb11_48 * z**11,
  res3_49 = rb0_49 * z**0 + rb1_49 * z**1 + rb2_49 * z**2 + rb3_49 * z**3 + rb4_49 * z**4 + rb5_49 * z**5 + rb6_49 * z**6 + rb7_49 * z**7 + rb8_49 * z**8 + rb9_49 * z**9 + rb10_49 * z**10 + rb11_49 * z**11,
  res3_50 = rb0_50 * z**0 + rb1_50 * z**1 + rb2_50 * z**2 + rb3_50 * z**3 + rb4_50 * z**4 + rb5_50 * z**5 + rb6_50 * z**6 + rb7_50 * z**7 + rb8_50 * z**8 + rb9_50 * z**9 + rb10_50 * z**10 + rb11_50 * z**11,
  res3_51 = rb0_51 * z**0 + rb1_51 * z**1 + rb2_51 * z**2 + rb3_51 * z**3 + rb4_51 * z**4 + rb5_51 * z**5 + rb6_51 * z**6 + rb7_51 * z**7 + rb8_51 * z**8 + rb9_51 * z**9 + rb10_51 * z**10 + rb11_51 * z**11,
  res3_52 = rb0_52 * z**0 + rb1_52 * z**1 + rb2_52 * z**2 + rb3_52 * z**3 + rb4_52 * z**4 + rb5_52 * z**5 + rb6_52 * z**6 + rb7_52 * z**7 + rb8_52 * z**8 + rb9_52 * z**9 + rb10_52 * z**10 + rb11_52 * z**11,
  res3_53 = rb0_53 * z**0 + rb1_53 * z**1 + rb2_53 * z**2 + rb3_53 * z**3 + rb4_53 * z**4 + rb5_53 * z**5 + rb6_53 * z**6 + rb7_53 * z**7 + rb8_53 * z**8 + rb9_53 * z**9 + rb10_53 * z**10 + rb11_53 * z**11,
  res3_54 = rb0_54 * z**0 + rb1_54 * z**1 + rb2_54 * z**2 + rb3_54 * z**3 + rb4_54 * z**4 + rb5_54 * z**5 + rb6_54 * z**6 + rb7_54 * z**7 + rb8_54 * z**8 + rb9_54 * z**9 + rb10_54 * z**10 + rb11_54 * z**11,
  res3_55 = rb0_55 * z**0 + rb1_55 * z**1 + rb2_55 * z**2 + rb3_55 * z**3 + rb4_55 * z**4 + rb5_55 * z**5 + rb6_55 * z**6 + rb7_55 * z**7 + rb8_55 * z**8 + rb9_55 * z**9 + rb10_55 * z**10 + rb11_55 * z**11,
  res3_56 = rb0_56 * z**0 + rb1_56 * z**1 + rb2_56 * z**2 + rb3_56 * z**3 + rb4_56 * z**4 + rb5_56 * z**5 + rb6_56 * z**6 + rb7_56 * z**7 + rb8_56 * z**8 + rb9_56 * z**9 + rb10_56 * z**10 + rb11_56 * z**11,
  res3_57 = rb0_57 * z**0 + rb1_57 * z**1 + rb2_57 * z**2 + rb3_57 * z**3 + rb4_57 * z**4 + rb5_57 * z**5 + rb6_57 * z**6 + rb7_57 * z**7 + rb8_57 * z**8 + rb9_57 * z**9 + rb10_57 * z**10 + rb11_57 * z**11,
  res3_58 = rb0_58 * z**0 + rb1_58 * z**1 + rb2_58 * z**2 + rb3_58 * z**3 + rb4_58 * z**4 + rb5_58 * z**5 + rb6_58 * z**6 + rb7_58 * z**7 + rb8_58 * z**8 + rb9_58 * z**9 + rb10_58 * z**10 + rb11_58 * z**11,
  res3_59 = rb0_59 * z**0 + rb1_59 * z**1 + rb2_59 * z**2 + rb3_59 * z**3 + rb4_59 * z**4 + rb5_59 * z**5 + rb6_59 * z**6 + rb7_59 * z**7 + rb8_59 * z**8 + rb9_59 * z**9 + rb10_59 * z**10 + rb11_59 * z**11,
  res3_60 = rb0_60 * z**0 + rb1_60 * z**1 + rb2_60 * z**2 + rb3_60 * z**3 + rb4_60 * z**4 + rb5_60 * z**5 + rb6_60 * z**6 + rb7_60 * z**7 + rb8_60 * z**8 + rb9_60 * z**9 + rb10_60 * z**10 + rb11_60 * z**11,
  res3_61 = rb0_61 * z**0 + rb1_61 * z**1 + rb2_61 * z**2 + rb3_61 * z**3 + rb4_61 * z**4 + rb5_61 * z**5 + rb6_61 * z**6 + rb7_61 * z**7 + rb8_61 * z**8 + rb9_61 * z**9 + rb10_61 * z**10 + rb11_61 * z**11,
  res3_62 = rb0_62 * z**0 + rb1_62 * z**1 + rb2_62 * z**2 + rb3_62 * z**3 + rb4_62 * z**4 + rb5_62 * z**5 + rb6_62 * z**6 + rb7_62 * z**7 + rb8_62 * z**8 + rb9_62 * z**9 + rb10_62 * z**10 + rb11_62 * z**11,
  res3_63 = rb0_63 * z**0 + rb1_63 * z**1 + rb2_63 * z**2 + rb3_63 * z**3 + rb4_63 * z**4 + rb5_63 * z**5 + rb6_63 * z**6 + rb7_63 * z**7 + rb8_63 * z**8 + rb9_63 * z**9 + rb10_63 * z**10 + rb11_63 * z**11
] && true;

mov %L0x7fffffffdaa0 [rb0_0, rb0_1, rb0_2, rb0_3, rb0_4, rb0_5, rb0_6, rb0_7, rb0_8, rb0_9, rb0_10, rb0_11, rb0_12, rb0_13, rb0_14, rb0_15, rb0_16, rb0_17, rb0_18, rb0_19, rb0_20, rb0_21, rb0_22, rb0_23, rb0_24, rb0_25, rb0_26, rb0_27, rb0_28, rb0_29, rb0_30, rb0_31, rb0_32, rb0_33, rb0_34, rb0_35, rb0_36, rb0_37, rb0_38, rb0_39, rb0_40, rb0_41, rb0_42, rb0_43, rb0_44, rb0_45, rb0_46, rb0_47, rb0_48, rb0_49, rb0_50, rb0_51, rb0_52, rb0_53, rb0_54, rb0_55, rb0_56, rb0_57, rb0_58, rb0_59, rb0_60, rb0_61, rb0_62, rb0_63];
mov %L0x7fffffffdaa8 [rb1_0, rb1_1, rb1_2, rb1_3, rb1_4, rb1_5, rb1_6, rb1_7, rb1_8, rb1_9, rb1_10, rb1_11, rb1_12, rb1_13, rb1_14, rb1_15, rb1_16, rb1_17, rb1_18, rb1_19, rb1_20, rb1_21, rb1_22, rb1_23, rb1_24, rb1_25, rb1_26, rb1_27, rb1_28, rb1_29, rb1_30, rb1_31, rb1_32, rb1_33, rb1_34, rb1_35, rb1_36, rb1_37, rb1_38, rb1_39, rb1_40, rb1_41, rb1_42, rb1_43, rb1_44, rb1_45, rb1_46, rb1_47, rb1_48, rb1_49, rb1_50, rb1_51, rb1_52, rb1_53, rb1_54, rb1_55, rb1_56, rb1_57, rb1_58, rb1_59, rb1_60, rb1_61, rb1_62, rb1_63];
mov %L0x7fffffffdab0 [rb2_0, rb2_1, rb2_2, rb2_3, rb2_4, rb2_5, rb2_6, rb2_7, rb2_8, rb2_9, rb2_10, rb2_11, rb2_12, rb2_13, rb2_14, rb2_15, rb2_16, rb2_17, rb2_18, rb2_19, rb2_20, rb2_21, rb2_22, rb2_23, rb2_24, rb2_25, rb2_26, rb2_27, rb2_28, rb2_29, rb2_30, rb2_31, rb2_32, rb2_33, rb2_34, rb2_35, rb2_36, rb2_37, rb2_38, rb2_39, rb2_40, rb2_41, rb2_42, rb2_43, rb2_44, rb2_45, rb2_46, rb2_47, rb2_48, rb2_49, rb2_50, rb2_51, rb2_52, rb2_53, rb2_54, rb2_55, rb2_56, rb2_57, rb2_58, rb2_59, rb2_60, rb2_61, rb2_62, rb2_63];
mov %L0x7fffffffdab8 [rb3_0, rb3_1, rb3_2, rb3_3, rb3_4, rb3_5, rb3_6, rb3_7, rb3_8, rb3_9, rb3_10, rb3_11, rb3_12, rb3_13, rb3_14, rb3_15, rb3_16, rb3_17, rb3_18, rb3_19, rb3_20, rb3_21, rb3_22, rb3_23, rb3_24, rb3_25, rb3_26, rb3_27, rb3_28, rb3_29, rb3_30, rb3_31, rb3_32, rb3_33, rb3_34, rb3_35, rb3_36, rb3_37, rb3_38, rb3_39, rb3_40, rb3_41, rb3_42, rb3_43, rb3_44, rb3_45, rb3_46, rb3_47, rb3_48, rb3_49, rb3_50, rb3_51, rb3_52, rb3_53, rb3_54, rb3_55, rb3_56, rb3_57, rb3_58, rb3_59, rb3_60, rb3_61, rb3_62, rb3_63];
mov %L0x7fffffffdac0 [rb4_0, rb4_1, rb4_2, rb4_3, rb4_4, rb4_5, rb4_6, rb4_7, rb4_8, rb4_9, rb4_10, rb4_11, rb4_12, rb4_13, rb4_14, rb4_15, rb4_16, rb4_17, rb4_18, rb4_19, rb4_20, rb4_21, rb4_22, rb4_23, rb4_24, rb4_25, rb4_26, rb4_27, rb4_28, rb4_29, rb4_30, rb4_31, rb4_32, rb4_33, rb4_34, rb4_35, rb4_36, rb4_37, rb4_38, rb4_39, rb4_40, rb4_41, rb4_42, rb4_43, rb4_44, rb4_45, rb4_46, rb4_47, rb4_48, rb4_49, rb4_50, rb4_51, rb4_52, rb4_53, rb4_54, rb4_55, rb4_56, rb4_57, rb4_58, rb4_59, rb4_60, rb4_61, rb4_62, rb4_63];
mov %L0x7fffffffdac8 [rb5_0, rb5_1, rb5_2, rb5_3, rb5_4, rb5_5, rb5_6, rb5_7, rb5_8, rb5_9, rb5_10, rb5_11, rb5_12, rb5_13, rb5_14, rb5_15, rb5_16, rb5_17, rb5_18, rb5_19, rb5_20, rb5_21, rb5_22, rb5_23, rb5_24, rb5_25, rb5_26, rb5_27, rb5_28, rb5_29, rb5_30, rb5_31, rb5_32, rb5_33, rb5_34, rb5_35, rb5_36, rb5_37, rb5_38, rb5_39, rb5_40, rb5_41, rb5_42, rb5_43, rb5_44, rb5_45, rb5_46, rb5_47, rb5_48, rb5_49, rb5_50, rb5_51, rb5_52, rb5_53, rb5_54, rb5_55, rb5_56, rb5_57, rb5_58, rb5_59, rb5_60, rb5_61, rb5_62, rb5_63];
mov %L0x7fffffffdad0 [rb6_0, rb6_1, rb6_2, rb6_3, rb6_4, rb6_5, rb6_6, rb6_7, rb6_8, rb6_9, rb6_10, rb6_11, rb6_12, rb6_13, rb6_14, rb6_15, rb6_16, rb6_17, rb6_18, rb6_19, rb6_20, rb6_21, rb6_22, rb6_23, rb6_24, rb6_25, rb6_26, rb6_27, rb6_28, rb6_29, rb6_30, rb6_31, rb6_32, rb6_33, rb6_34, rb6_35, rb6_36, rb6_37, rb6_38, rb6_39, rb6_40, rb6_41, rb6_42, rb6_43, rb6_44, rb6_45, rb6_46, rb6_47, rb6_48, rb6_49, rb6_50, rb6_51, rb6_52, rb6_53, rb6_54, rb6_55, rb6_56, rb6_57, rb6_58, rb6_59, rb6_60, rb6_61, rb6_62, rb6_63];
mov %L0x7fffffffdad8 [rb7_0, rb7_1, rb7_2, rb7_3, rb7_4, rb7_5, rb7_6, rb7_7, rb7_8, rb7_9, rb7_10, rb7_11, rb7_12, rb7_13, rb7_14, rb7_15, rb7_16, rb7_17, rb7_18, rb7_19, rb7_20, rb7_21, rb7_22, rb7_23, rb7_24, rb7_25, rb7_26, rb7_27, rb7_28, rb7_29, rb7_30, rb7_31, rb7_32, rb7_33, rb7_34, rb7_35, rb7_36, rb7_37, rb7_38, rb7_39, rb7_40, rb7_41, rb7_42, rb7_43, rb7_44, rb7_45, rb7_46, rb7_47, rb7_48, rb7_49, rb7_50, rb7_51, rb7_52, rb7_53, rb7_54, rb7_55, rb7_56, rb7_57, rb7_58, rb7_59, rb7_60, rb7_61, rb7_62, rb7_63];
mov %L0x7fffffffdae0 [rb8_0, rb8_1, rb8_2, rb8_3, rb8_4, rb8_5, rb8_6, rb8_7, rb8_8, rb8_9, rb8_10, rb8_11, rb8_12, rb8_13, rb8_14, rb8_15, rb8_16, rb8_17, rb8_18, rb8_19, rb8_20, rb8_21, rb8_22, rb8_23, rb8_24, rb8_25, rb8_26, rb8_27, rb8_28, rb8_29, rb8_30, rb8_31, rb8_32, rb8_33, rb8_34, rb8_35, rb8_36, rb8_37, rb8_38, rb8_39, rb8_40, rb8_41, rb8_42, rb8_43, rb8_44, rb8_45, rb8_46, rb8_47, rb8_48, rb8_49, rb8_50, rb8_51, rb8_52, rb8_53, rb8_54, rb8_55, rb8_56, rb8_57, rb8_58, rb8_59, rb8_60, rb8_61, rb8_62, rb8_63];
mov %L0x7fffffffdae8 [rb9_0, rb9_1, rb9_2, rb9_3, rb9_4, rb9_5, rb9_6, rb9_7, rb9_8, rb9_9, rb9_10, rb9_11, rb9_12, rb9_13, rb9_14, rb9_15, rb9_16, rb9_17, rb9_18, rb9_19, rb9_20, rb9_21, rb9_22, rb9_23, rb9_24, rb9_25, rb9_26, rb9_27, rb9_28, rb9_29, rb9_30, rb9_31, rb9_32, rb9_33, rb9_34, rb9_35, rb9_36, rb9_37, rb9_38, rb9_39, rb9_40, rb9_41, rb9_42, rb9_43, rb9_44, rb9_45, rb9_46, rb9_47, rb9_48, rb9_49, rb9_50, rb9_51, rb9_52, rb9_53, rb9_54, rb9_55, rb9_56, rb9_57, rb9_58, rb9_59, rb9_60, rb9_61, rb9_62, rb9_63];
mov %L0x7fffffffdaf0 [rb10_0, rb10_1, rb10_2, rb10_3, rb10_4, rb10_5, rb10_6, rb10_7, rb10_8, rb10_9, rb10_10, rb10_11, rb10_12, rb10_13, rb10_14, rb10_15, rb10_16, rb10_17, rb10_18, rb10_19, rb10_20, rb10_21, rb10_22, rb10_23, rb10_24, rb10_25, rb10_26, rb10_27, rb10_28, rb10_29, rb10_30, rb10_31, rb10_32, rb10_33, rb10_34, rb10_35, rb10_36, rb10_37, rb10_38, rb10_39, rb10_40, rb10_41, rb10_42, rb10_43, rb10_44, rb10_45, rb10_46, rb10_47, rb10_48, rb10_49, rb10_50, rb10_51, rb10_52, rb10_53, rb10_54, rb10_55, rb10_56, rb10_57, rb10_58, rb10_59, rb10_60, rb10_61, rb10_62, rb10_63];
mov %L0x7fffffffdaf8 [rb11_0, rb11_1, rb11_2, rb11_3, rb11_4, rb11_5, rb11_6, rb11_7, rb11_8, rb11_9, rb11_10, rb11_11, rb11_12, rb11_13, rb11_14, rb11_15, rb11_16, rb11_17, rb11_18, rb11_19, rb11_20, rb11_21, rb11_22, rb11_23, rb11_24, rb11_25, rb11_26, rb11_27, rb11_28, rb11_29, rb11_30, rb11_31, rb11_32, rb11_33, rb11_34, rb11_35, rb11_36, rb11_37, rb11_38, rb11_39, rb11_40, rb11_41, rb11_42, rb11_43, rb11_44, rb11_45, rb11_46, rb11_47, rb11_48, rb11_49, rb11_50, rb11_51, rb11_52, rb11_53, rb11_54, rb11_55, rb11_56, rb11_57, rb11_58, rb11_59, rb11_60, rb11_61, rb11_62, rb11_63];


(* #jne    0x5555555552e4 <radix_conversions+196>  #! PC = 0x55555555535b *)
#jne    0x5555555552e4 <radix_conversions+196>  #! 0x55555555535b = 0x55555555535b;
(* mov    %r15,%r9                                 #! PC = 0x5555555552e4 *)
mov r9 r15;
(* mov    $0x1,%r8d                                #! PC = 0x5555555552e7 *)
mov r8d 0x1@uint32;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaa0; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaa0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaa0; PC = 0x555555555330 *)
mov %L0x7fffffffdaa0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaa8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaa8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaa8; PC = 0x555555555330 *)
mov %L0x7fffffffdaa8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdab0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdab0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdab0; PC = 0x555555555330 *)
mov %L0x7fffffffdab0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdab8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdab8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdab8; PC = 0x555555555330 *)
mov %L0x7fffffffdab8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdac0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdac0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdac0; PC = 0x555555555330 *)
mov %L0x7fffffffdac0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdac8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdac8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdac8; PC = 0x555555555330 *)
mov %L0x7fffffffdac8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdad0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdad0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdad0; PC = 0x555555555330 *)
mov %L0x7fffffffdad0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdad8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdad8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdad8; PC = 0x555555555330 *)
mov %L0x7fffffffdad8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdae0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdae0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdae0; PC = 0x555555555330 *)
mov %L0x7fffffffdae0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdae8; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdae8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdae8; PC = 0x555555555330 *)
mov %L0x7fffffffdae8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaf0; Value = 0x0000000000000000; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaf0;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaf0; PC = 0x555555555330 *)
mov %L0x7fffffffdaf0 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* mov    (%r9),%rax                               #! EA = L0x7fffffffdaf8; Value = 0x0000000000000002; PC = 0x5555555552f0 *)
mov %rax %L0x7fffffffdaf8;
(* mov    %r14,%rdi                                #! PC = 0x5555555552f3 *)
mov rdi r14;
(* mov    %r12,%rdx                                #! PC = 0x5555555552f6 *)
mov %rdx %r12;
(* mov    $0x4,%esi                                #! PC = 0x5555555552f9 *)
mov esi 0x4@sint32;
(* mov    %esi,%ecx                                #! PC = 0x5555555552fe *)
mov ecx esi; vpc cl@uint8 esi;
(* mov    0x40(%rdi),%r10                          #! EA = L0x7fffffffd870; Value = 0xffff000000000000; PC = 0x555555555300 *)
mov %r10 %L0x7fffffffd870;
(* mov    %r8d,%r11d                               #! PC = 0x555555555304 *)
mov r11d r8d;
(* sub    $0x1,%esi                                #! PC = 0x555555555307 *)
sub esi esi 0x1@sint32;
(* shl    %cl,%r11d                                #! PC = 0x55555555530a *)
assert true && cl = 4@8;
shl r11d r11d 4;
(* sub    $0x10,%rdi                               #! PC = 0x55555555530d *)
sub rdi rdi 0x10@uint64;
(* mov    %r11d,%ecx                               #! PC = 0x555555555311 *)
mov ecx r11d; vpc cl@uint8 r11d;
(* and    %rax,%r10                                #! PC = 0x555555555314 *)
mul %r10 %r10 %rax;
(* shr    %cl,%r10                                 #! PC = 0x555555555317 *)
(* shr r10 for 16 *)
assert true && cl = 16@8;
mov [r10_0, r10_1, r10_2, r10_3, r10_4, r10_5, r10_6, r10_7, r10_8, r10_9, r10_10, r10_11, r10_12, r10_13, r10_14, r10_15, r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63] %r10;
mov %r10 [r10_16, r10_17, r10_18, r10_19, r10_20, r10_21, r10_22, r10_23, r10_24, r10_25, r10_26, r10_27, r10_28, r10_29, r10_30, r10_31, r10_32, r10_33, r10_34, r10_35, r10_36, r10_37, r10_38, r10_39, r10_40, r10_41, r10_42, r10_43, r10_44, r10_45, r10_46, r10_47, r10_48, r10_49, r10_50, r10_51, r10_52, r10_53, r10_54, r10_55, r10_56, r10_57, r10_58, r10_59, r10_60, r10_61, r10_62, r10_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %r10,%rax                                #! PC = 0x55555555531a *)
adds %dc %rax %rax %r10;
(* and    %rax,%rdx                                #! PC = 0x55555555531d *)
mul %rdx %rdx %rax;
(* shr    %cl,%rdx                                 #! PC = 0x555555555320 *)
(* shr rdx for 16 *)
assert true && cl = 16@8;
mov [rdx_0, rdx_1, rdx_2, rdx_3, rdx_4, rdx_5, rdx_6, rdx_7, rdx_8, rdx_9, rdx_10, rdx_11, rdx_12, rdx_13, rdx_14, rdx_15, rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63] %rdx;
mov %rdx [rdx_16, rdx_17, rdx_18, rdx_19, rdx_20, rdx_21, rdx_22, rdx_23, rdx_24, rdx_25, rdx_26, rdx_27, rdx_28, rdx_29, rdx_30, rdx_31, rdx_32, rdx_33, rdx_34, rdx_35, rdx_36, rdx_37, rdx_38, rdx_39, rdx_40, rdx_41, rdx_42, rdx_43, rdx_44, rdx_45, rdx_46, rdx_47, rdx_48, rdx_49, rdx_50, rdx_51, rdx_52, rdx_53, rdx_54, rdx_55, rdx_56, rdx_57, rdx_58, rdx_59, rdx_60, rdx_61, rdx_62, rdx_63, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit, 0@bit];
(* xor    %rdx,%rax                                #! PC = 0x555555555323 *)
adds %dc %rax %rax %rdx;
(* #jl     0x555555555330 <radix_conversions+272>  #! PC = 0x555555555328 *)
#jl     0x555555555330 <radix_conversions+272>  #! 0x555555555328 = 0x555555555328;
(* mov    %rax,(%r9)                               #! EA = L0x7fffffffdaf8; PC = 0x555555555330 *)
mov %L0x7fffffffdaf8 %rax;
(* add    $0x8,%r9                                 #! PC = 0x555555555333 *)
add r9 r9 0x8@uint64;
(* #jne    0x5555555552f0 <radix_conversions+208>  #! PC = 0x55555555533a *)
#jne    0x5555555552f0 <radix_conversions+208>  #! 0x55555555533a = 0x55555555533a;
(* add    $0x1,%rbp                                #! PC = 0x555555555347 *)
add ebp ebp 0x1@sint32;


mov [out0_0, out0_1, out0_2, out0_3, out0_4, out0_5, out0_6, out0_7, out0_8, out0_9, out0_10, out0_11, out0_12, out0_13, out0_14, out0_15, out0_16, out0_17, out0_18, out0_19, out0_20, out0_21, out0_22, out0_23, out0_24, out0_25, out0_26, out0_27, out0_28, out0_29, out0_30, out0_31, out0_32, out0_33, out0_34, out0_35, out0_36, out0_37, out0_38, out0_39, out0_40, out0_41, out0_42, out0_43, out0_44, out0_45, out0_46, out0_47, out0_48, out0_49, out0_50, out0_51, out0_52, out0_53, out0_54, out0_55, out0_56, out0_57, out0_58, out0_59, out0_60, out0_61, out0_62, out0_63] %L0x7fffffffdaa0;
mov [out1_0, out1_1, out1_2, out1_3, out1_4, out1_5, out1_6, out1_7, out1_8, out1_9, out1_10, out1_11, out1_12, out1_13, out1_14, out1_15, out1_16, out1_17, out1_18, out1_19, out1_20, out1_21, out1_22, out1_23, out1_24, out1_25, out1_26, out1_27, out1_28, out1_29, out1_30, out1_31, out1_32, out1_33, out1_34, out1_35, out1_36, out1_37, out1_38, out1_39, out1_40, out1_41, out1_42, out1_43, out1_44, out1_45, out1_46, out1_47, out1_48, out1_49, out1_50, out1_51, out1_52, out1_53, out1_54, out1_55, out1_56, out1_57, out1_58, out1_59, out1_60, out1_61, out1_62, out1_63] %L0x7fffffffdaa8;
mov [out2_0, out2_1, out2_2, out2_3, out2_4, out2_5, out2_6, out2_7, out2_8, out2_9, out2_10, out2_11, out2_12, out2_13, out2_14, out2_15, out2_16, out2_17, out2_18, out2_19, out2_20, out2_21, out2_22, out2_23, out2_24, out2_25, out2_26, out2_27, out2_28, out2_29, out2_30, out2_31, out2_32, out2_33, out2_34, out2_35, out2_36, out2_37, out2_38, out2_39, out2_40, out2_41, out2_42, out2_43, out2_44, out2_45, out2_46, out2_47, out2_48, out2_49, out2_50, out2_51, out2_52, out2_53, out2_54, out2_55, out2_56, out2_57, out2_58, out2_59, out2_60, out2_61, out2_62, out2_63] %L0x7fffffffdab0;
mov [out3_0, out3_1, out3_2, out3_3, out3_4, out3_5, out3_6, out3_7, out3_8, out3_9, out3_10, out3_11, out3_12, out3_13, out3_14, out3_15, out3_16, out3_17, out3_18, out3_19, out3_20, out3_21, out3_22, out3_23, out3_24, out3_25, out3_26, out3_27, out3_28, out3_29, out3_30, out3_31, out3_32, out3_33, out3_34, out3_35, out3_36, out3_37, out3_38, out3_39, out3_40, out3_41, out3_42, out3_43, out3_44, out3_45, out3_46, out3_47, out3_48, out3_49, out3_50, out3_51, out3_52, out3_53, out3_54, out3_55, out3_56, out3_57, out3_58, out3_59, out3_60, out3_61, out3_62, out3_63] %L0x7fffffffdab8;
mov [out4_0, out4_1, out4_2, out4_3, out4_4, out4_5, out4_6, out4_7, out4_8, out4_9, out4_10, out4_11, out4_12, out4_13, out4_14, out4_15, out4_16, out4_17, out4_18, out4_19, out4_20, out4_21, out4_22, out4_23, out4_24, out4_25, out4_26, out4_27, out4_28, out4_29, out4_30, out4_31, out4_32, out4_33, out4_34, out4_35, out4_36, out4_37, out4_38, out4_39, out4_40, out4_41, out4_42, out4_43, out4_44, out4_45, out4_46, out4_47, out4_48, out4_49, out4_50, out4_51, out4_52, out4_53, out4_54, out4_55, out4_56, out4_57, out4_58, out4_59, out4_60, out4_61, out4_62, out4_63] %L0x7fffffffdac0;
mov [out5_0, out5_1, out5_2, out5_3, out5_4, out5_5, out5_6, out5_7, out5_8, out5_9, out5_10, out5_11, out5_12, out5_13, out5_14, out5_15, out5_16, out5_17, out5_18, out5_19, out5_20, out5_21, out5_22, out5_23, out5_24, out5_25, out5_26, out5_27, out5_28, out5_29, out5_30, out5_31, out5_32, out5_33, out5_34, out5_35, out5_36, out5_37, out5_38, out5_39, out5_40, out5_41, out5_42, out5_43, out5_44, out5_45, out5_46, out5_47, out5_48, out5_49, out5_50, out5_51, out5_52, out5_53, out5_54, out5_55, out5_56, out5_57, out5_58, out5_59, out5_60, out5_61, out5_62, out5_63] %L0x7fffffffdac8;
mov [out6_0, out6_1, out6_2, out6_3, out6_4, out6_5, out6_6, out6_7, out6_8, out6_9, out6_10, out6_11, out6_12, out6_13, out6_14, out6_15, out6_16, out6_17, out6_18, out6_19, out6_20, out6_21, out6_22, out6_23, out6_24, out6_25, out6_26, out6_27, out6_28, out6_29, out6_30, out6_31, out6_32, out6_33, out6_34, out6_35, out6_36, out6_37, out6_38, out6_39, out6_40, out6_41, out6_42, out6_43, out6_44, out6_45, out6_46, out6_47, out6_48, out6_49, out6_50, out6_51, out6_52, out6_53, out6_54, out6_55, out6_56, out6_57, out6_58, out6_59, out6_60, out6_61, out6_62, out6_63] %L0x7fffffffdad0;
mov [out7_0, out7_1, out7_2, out7_3, out7_4, out7_5, out7_6, out7_7, out7_8, out7_9, out7_10, out7_11, out7_12, out7_13, out7_14, out7_15, out7_16, out7_17, out7_18, out7_19, out7_20, out7_21, out7_22, out7_23, out7_24, out7_25, out7_26, out7_27, out7_28, out7_29, out7_30, out7_31, out7_32, out7_33, out7_34, out7_35, out7_36, out7_37, out7_38, out7_39, out7_40, out7_41, out7_42, out7_43, out7_44, out7_45, out7_46, out7_47, out7_48, out7_49, out7_50, out7_51, out7_52, out7_53, out7_54, out7_55, out7_56, out7_57, out7_58, out7_59, out7_60, out7_61, out7_62, out7_63] %L0x7fffffffdad8;
mov [out8_0, out8_1, out8_2, out8_3, out8_4, out8_5, out8_6, out8_7, out8_8, out8_9, out8_10, out8_11, out8_12, out8_13, out8_14, out8_15, out8_16, out8_17, out8_18, out8_19, out8_20, out8_21, out8_22, out8_23, out8_24, out8_25, out8_26, out8_27, out8_28, out8_29, out8_30, out8_31, out8_32, out8_33, out8_34, out8_35, out8_36, out8_37, out8_38, out8_39, out8_40, out8_41, out8_42, out8_43, out8_44, out8_45, out8_46, out8_47, out8_48, out8_49, out8_50, out8_51, out8_52, out8_53, out8_54, out8_55, out8_56, out8_57, out8_58, out8_59, out8_60, out8_61, out8_62, out8_63] %L0x7fffffffdae0;
mov [out9_0, out9_1, out9_2, out9_3, out9_4, out9_5, out9_6, out9_7, out9_8, out9_9, out9_10, out9_11, out9_12, out9_13, out9_14, out9_15, out9_16, out9_17, out9_18, out9_19, out9_20, out9_21, out9_22, out9_23, out9_24, out9_25, out9_26, out9_27, out9_28, out9_29, out9_30, out9_31, out9_32, out9_33, out9_34, out9_35, out9_36, out9_37, out9_38, out9_39, out9_40, out9_41, out9_42, out9_43, out9_44, out9_45, out9_46, out9_47, out9_48, out9_49, out9_50, out9_51, out9_52, out9_53, out9_54, out9_55, out9_56, out9_57, out9_58, out9_59, out9_60, out9_61, out9_62, out9_63] %L0x7fffffffdae8;
mov [out10_0, out10_1, out10_2, out10_3, out10_4, out10_5, out10_6, out10_7, out10_8, out10_9, out10_10, out10_11, out10_12, out10_13, out10_14, out10_15, out10_16, out10_17, out10_18, out10_19, out10_20, out10_21, out10_22, out10_23, out10_24, out10_25, out10_26, out10_27, out10_28, out10_29, out10_30, out10_31, out10_32, out10_33, out10_34, out10_35, out10_36, out10_37, out10_38, out10_39, out10_40, out10_41, out10_42, out10_43, out10_44, out10_45, out10_46, out10_47, out10_48, out10_49, out10_50, out10_51, out10_52, out10_53, out10_54, out10_55, out10_56, out10_57, out10_58, out10_59, out10_60, out10_61, out10_62, out10_63] %L0x7fffffffdaf0;
mov [out11_0, out11_1, out11_2, out11_3, out11_4, out11_5, out11_6, out11_7, out11_8, out11_9, out11_10, out11_11, out11_12, out11_13, out11_14, out11_15, out11_16, out11_17, out11_18, out11_19, out11_20, out11_21, out11_22, out11_23, out11_24, out11_25, out11_26, out11_27, out11_28, out11_29, out11_30, out11_31, out11_32, out11_33, out11_34, out11_35, out11_36, out11_37, out11_38, out11_39, out11_40, out11_41, out11_42, out11_43, out11_44, out11_45, out11_46, out11_47, out11_48, out11_49, out11_50, out11_51, out11_52, out11_53, out11_54, out11_55, out11_56, out11_57, out11_58, out11_59, out11_60, out11_61, out11_62, out11_63] %L0x7fffffffdaf8;

ghost cvrted4_0@uint12: cvrted4_0 = out0_0 * z**0 + out1_0 * z**1 + out2_0 * z**2 + out3_0 * z**3 + out4_0 * z**4 + out5_0 * z**5 + out6_0 * z**6 + out7_0 * z**7 + out8_0 * z**8 + out9_0 * z**9 + out10_0 * z**10 + out11_0 * z**11 && true;
ghost cvrted4_1@uint12: cvrted4_1 = out0_1 * z**0 + out1_1 * z**1 + out2_1 * z**2 + out3_1 * z**3 + out4_1 * z**4 + out5_1 * z**5 + out6_1 * z**6 + out7_1 * z**7 + out8_1 * z**8 + out9_1 * z**9 + out10_1 * z**10 + out11_1 * z**11 && true;
ghost cvrted4_2@uint12: cvrted4_2 = out0_2 * z**0 + out1_2 * z**1 + out2_2 * z**2 + out3_2 * z**3 + out4_2 * z**4 + out5_2 * z**5 + out6_2 * z**6 + out7_2 * z**7 + out8_2 * z**8 + out9_2 * z**9 + out10_2 * z**10 + out11_2 * z**11 && true;
ghost cvrted4_3@uint12: cvrted4_3 = out0_3 * z**0 + out1_3 * z**1 + out2_3 * z**2 + out3_3 * z**3 + out4_3 * z**4 + out5_3 * z**5 + out6_3 * z**6 + out7_3 * z**7 + out8_3 * z**8 + out9_3 * z**9 + out10_3 * z**10 + out11_3 * z**11 && true;
ghost cvrted4_4@uint12: cvrted4_4 = out0_4 * z**0 + out1_4 * z**1 + out2_4 * z**2 + out3_4 * z**3 + out4_4 * z**4 + out5_4 * z**5 + out6_4 * z**6 + out7_4 * z**7 + out8_4 * z**8 + out9_4 * z**9 + out10_4 * z**10 + out11_4 * z**11 && true;
ghost cvrted4_5@uint12: cvrted4_5 = out0_5 * z**0 + out1_5 * z**1 + out2_5 * z**2 + out3_5 * z**3 + out4_5 * z**4 + out5_5 * z**5 + out6_5 * z**6 + out7_5 * z**7 + out8_5 * z**8 + out9_5 * z**9 + out10_5 * z**10 + out11_5 * z**11 && true;
ghost cvrted4_6@uint12: cvrted4_6 = out0_6 * z**0 + out1_6 * z**1 + out2_6 * z**2 + out3_6 * z**3 + out4_6 * z**4 + out5_6 * z**5 + out6_6 * z**6 + out7_6 * z**7 + out8_6 * z**8 + out9_6 * z**9 + out10_6 * z**10 + out11_6 * z**11 && true;
ghost cvrted4_7@uint12: cvrted4_7 = out0_7 * z**0 + out1_7 * z**1 + out2_7 * z**2 + out3_7 * z**3 + out4_7 * z**4 + out5_7 * z**5 + out6_7 * z**6 + out7_7 * z**7 + out8_7 * z**8 + out9_7 * z**9 + out10_7 * z**10 + out11_7 * z**11 && true;
ghost cvrted4_8@uint12: cvrted4_8 = out0_8 * z**0 + out1_8 * z**1 + out2_8 * z**2 + out3_8 * z**3 + out4_8 * z**4 + out5_8 * z**5 + out6_8 * z**6 + out7_8 * z**7 + out8_8 * z**8 + out9_8 * z**9 + out10_8 * z**10 + out11_8 * z**11 && true;
ghost cvrted4_9@uint12: cvrted4_9 = out0_9 * z**0 + out1_9 * z**1 + out2_9 * z**2 + out3_9 * z**3 + out4_9 * z**4 + out5_9 * z**5 + out6_9 * z**6 + out7_9 * z**7 + out8_9 * z**8 + out9_9 * z**9 + out10_9 * z**10 + out11_9 * z**11 && true;
ghost cvrted4_10@uint12: cvrted4_10 = out0_10 * z**0 + out1_10 * z**1 + out2_10 * z**2 + out3_10 * z**3 + out4_10 * z**4 + out5_10 * z**5 + out6_10 * z**6 + out7_10 * z**7 + out8_10 * z**8 + out9_10 * z**9 + out10_10 * z**10 + out11_10 * z**11 && true;
ghost cvrted4_11@uint12: cvrted4_11 = out0_11 * z**0 + out1_11 * z**1 + out2_11 * z**2 + out3_11 * z**3 + out4_11 * z**4 + out5_11 * z**5 + out6_11 * z**6 + out7_11 * z**7 + out8_11 * z**8 + out9_11 * z**9 + out10_11 * z**10 + out11_11 * z**11 && true;
ghost cvrted4_12@uint12: cvrted4_12 = out0_12 * z**0 + out1_12 * z**1 + out2_12 * z**2 + out3_12 * z**3 + out4_12 * z**4 + out5_12 * z**5 + out6_12 * z**6 + out7_12 * z**7 + out8_12 * z**8 + out9_12 * z**9 + out10_12 * z**10 + out11_12 * z**11 && true;
ghost cvrted4_13@uint12: cvrted4_13 = out0_13 * z**0 + out1_13 * z**1 + out2_13 * z**2 + out3_13 * z**3 + out4_13 * z**4 + out5_13 * z**5 + out6_13 * z**6 + out7_13 * z**7 + out8_13 * z**8 + out9_13 * z**9 + out10_13 * z**10 + out11_13 * z**11 && true;
ghost cvrted4_14@uint12: cvrted4_14 = out0_14 * z**0 + out1_14 * z**1 + out2_14 * z**2 + out3_14 * z**3 + out4_14 * z**4 + out5_14 * z**5 + out6_14 * z**6 + out7_14 * z**7 + out8_14 * z**8 + out9_14 * z**9 + out10_14 * z**10 + out11_14 * z**11 && true;
ghost cvrted4_15@uint12: cvrted4_15 = out0_15 * z**0 + out1_15 * z**1 + out2_15 * z**2 + out3_15 * z**3 + out4_15 * z**4 + out5_15 * z**5 + out6_15 * z**6 + out7_15 * z**7 + out8_15 * z**8 + out9_15 * z**9 + out10_15 * z**10 + out11_15 * z**11 && true;
ghost cvrted4_16@uint12: cvrted4_16 = out0_16 * z**0 + out1_16 * z**1 + out2_16 * z**2 + out3_16 * z**3 + out4_16 * z**4 + out5_16 * z**5 + out6_16 * z**6 + out7_16 * z**7 + out8_16 * z**8 + out9_16 * z**9 + out10_16 * z**10 + out11_16 * z**11 && true;
ghost cvrted4_17@uint12: cvrted4_17 = out0_17 * z**0 + out1_17 * z**1 + out2_17 * z**2 + out3_17 * z**3 + out4_17 * z**4 + out5_17 * z**5 + out6_17 * z**6 + out7_17 * z**7 + out8_17 * z**8 + out9_17 * z**9 + out10_17 * z**10 + out11_17 * z**11 && true;
ghost cvrted4_18@uint12: cvrted4_18 = out0_18 * z**0 + out1_18 * z**1 + out2_18 * z**2 + out3_18 * z**3 + out4_18 * z**4 + out5_18 * z**5 + out6_18 * z**6 + out7_18 * z**7 + out8_18 * z**8 + out9_18 * z**9 + out10_18 * z**10 + out11_18 * z**11 && true;
ghost cvrted4_19@uint12: cvrted4_19 = out0_19 * z**0 + out1_19 * z**1 + out2_19 * z**2 + out3_19 * z**3 + out4_19 * z**4 + out5_19 * z**5 + out6_19 * z**6 + out7_19 * z**7 + out8_19 * z**8 + out9_19 * z**9 + out10_19 * z**10 + out11_19 * z**11 && true;
ghost cvrted4_20@uint12: cvrted4_20 = out0_20 * z**0 + out1_20 * z**1 + out2_20 * z**2 + out3_20 * z**3 + out4_20 * z**4 + out5_20 * z**5 + out6_20 * z**6 + out7_20 * z**7 + out8_20 * z**8 + out9_20 * z**9 + out10_20 * z**10 + out11_20 * z**11 && true;
ghost cvrted4_21@uint12: cvrted4_21 = out0_21 * z**0 + out1_21 * z**1 + out2_21 * z**2 + out3_21 * z**3 + out4_21 * z**4 + out5_21 * z**5 + out6_21 * z**6 + out7_21 * z**7 + out8_21 * z**8 + out9_21 * z**9 + out10_21 * z**10 + out11_21 * z**11 && true;
ghost cvrted4_22@uint12: cvrted4_22 = out0_22 * z**0 + out1_22 * z**1 + out2_22 * z**2 + out3_22 * z**3 + out4_22 * z**4 + out5_22 * z**5 + out6_22 * z**6 + out7_22 * z**7 + out8_22 * z**8 + out9_22 * z**9 + out10_22 * z**10 + out11_22 * z**11 && true;
ghost cvrted4_23@uint12: cvrted4_23 = out0_23 * z**0 + out1_23 * z**1 + out2_23 * z**2 + out3_23 * z**3 + out4_23 * z**4 + out5_23 * z**5 + out6_23 * z**6 + out7_23 * z**7 + out8_23 * z**8 + out9_23 * z**9 + out10_23 * z**10 + out11_23 * z**11 && true;
ghost cvrted4_24@uint12: cvrted4_24 = out0_24 * z**0 + out1_24 * z**1 + out2_24 * z**2 + out3_24 * z**3 + out4_24 * z**4 + out5_24 * z**5 + out6_24 * z**6 + out7_24 * z**7 + out8_24 * z**8 + out9_24 * z**9 + out10_24 * z**10 + out11_24 * z**11 && true;
ghost cvrted4_25@uint12: cvrted4_25 = out0_25 * z**0 + out1_25 * z**1 + out2_25 * z**2 + out3_25 * z**3 + out4_25 * z**4 + out5_25 * z**5 + out6_25 * z**6 + out7_25 * z**7 + out8_25 * z**8 + out9_25 * z**9 + out10_25 * z**10 + out11_25 * z**11 && true;
ghost cvrted4_26@uint12: cvrted4_26 = out0_26 * z**0 + out1_26 * z**1 + out2_26 * z**2 + out3_26 * z**3 + out4_26 * z**4 + out5_26 * z**5 + out6_26 * z**6 + out7_26 * z**7 + out8_26 * z**8 + out9_26 * z**9 + out10_26 * z**10 + out11_26 * z**11 && true;
ghost cvrted4_27@uint12: cvrted4_27 = out0_27 * z**0 + out1_27 * z**1 + out2_27 * z**2 + out3_27 * z**3 + out4_27 * z**4 + out5_27 * z**5 + out6_27 * z**6 + out7_27 * z**7 + out8_27 * z**8 + out9_27 * z**9 + out10_27 * z**10 + out11_27 * z**11 && true;
ghost cvrted4_28@uint12: cvrted4_28 = out0_28 * z**0 + out1_28 * z**1 + out2_28 * z**2 + out3_28 * z**3 + out4_28 * z**4 + out5_28 * z**5 + out6_28 * z**6 + out7_28 * z**7 + out8_28 * z**8 + out9_28 * z**9 + out10_28 * z**10 + out11_28 * z**11 && true;
ghost cvrted4_29@uint12: cvrted4_29 = out0_29 * z**0 + out1_29 * z**1 + out2_29 * z**2 + out3_29 * z**3 + out4_29 * z**4 + out5_29 * z**5 + out6_29 * z**6 + out7_29 * z**7 + out8_29 * z**8 + out9_29 * z**9 + out10_29 * z**10 + out11_29 * z**11 && true;
ghost cvrted4_30@uint12: cvrted4_30 = out0_30 * z**0 + out1_30 * z**1 + out2_30 * z**2 + out3_30 * z**3 + out4_30 * z**4 + out5_30 * z**5 + out6_30 * z**6 + out7_30 * z**7 + out8_30 * z**8 + out9_30 * z**9 + out10_30 * z**10 + out11_30 * z**11 && true;
ghost cvrted4_31@uint12: cvrted4_31 = out0_31 * z**0 + out1_31 * z**1 + out2_31 * z**2 + out3_31 * z**3 + out4_31 * z**4 + out5_31 * z**5 + out6_31 * z**6 + out7_31 * z**7 + out8_31 * z**8 + out9_31 * z**9 + out10_31 * z**10 + out11_31 * z**11 && true;
ghost cvrted4_32@uint12: cvrted4_32 = out0_32 * z**0 + out1_32 * z**1 + out2_32 * z**2 + out3_32 * z**3 + out4_32 * z**4 + out5_32 * z**5 + out6_32 * z**6 + out7_32 * z**7 + out8_32 * z**8 + out9_32 * z**9 + out10_32 * z**10 + out11_32 * z**11 && true;
ghost cvrted4_33@uint12: cvrted4_33 = out0_33 * z**0 + out1_33 * z**1 + out2_33 * z**2 + out3_33 * z**3 + out4_33 * z**4 + out5_33 * z**5 + out6_33 * z**6 + out7_33 * z**7 + out8_33 * z**8 + out9_33 * z**9 + out10_33 * z**10 + out11_33 * z**11 && true;
ghost cvrted4_34@uint12: cvrted4_34 = out0_34 * z**0 + out1_34 * z**1 + out2_34 * z**2 + out3_34 * z**3 + out4_34 * z**4 + out5_34 * z**5 + out6_34 * z**6 + out7_34 * z**7 + out8_34 * z**8 + out9_34 * z**9 + out10_34 * z**10 + out11_34 * z**11 && true;
ghost cvrted4_35@uint12: cvrted4_35 = out0_35 * z**0 + out1_35 * z**1 + out2_35 * z**2 + out3_35 * z**3 + out4_35 * z**4 + out5_35 * z**5 + out6_35 * z**6 + out7_35 * z**7 + out8_35 * z**8 + out9_35 * z**9 + out10_35 * z**10 + out11_35 * z**11 && true;
ghost cvrted4_36@uint12: cvrted4_36 = out0_36 * z**0 + out1_36 * z**1 + out2_36 * z**2 + out3_36 * z**3 + out4_36 * z**4 + out5_36 * z**5 + out6_36 * z**6 + out7_36 * z**7 + out8_36 * z**8 + out9_36 * z**9 + out10_36 * z**10 + out11_36 * z**11 && true;
ghost cvrted4_37@uint12: cvrted4_37 = out0_37 * z**0 + out1_37 * z**1 + out2_37 * z**2 + out3_37 * z**3 + out4_37 * z**4 + out5_37 * z**5 + out6_37 * z**6 + out7_37 * z**7 + out8_37 * z**8 + out9_37 * z**9 + out10_37 * z**10 + out11_37 * z**11 && true;
ghost cvrted4_38@uint12: cvrted4_38 = out0_38 * z**0 + out1_38 * z**1 + out2_38 * z**2 + out3_38 * z**3 + out4_38 * z**4 + out5_38 * z**5 + out6_38 * z**6 + out7_38 * z**7 + out8_38 * z**8 + out9_38 * z**9 + out10_38 * z**10 + out11_38 * z**11 && true;
ghost cvrted4_39@uint12: cvrted4_39 = out0_39 * z**0 + out1_39 * z**1 + out2_39 * z**2 + out3_39 * z**3 + out4_39 * z**4 + out5_39 * z**5 + out6_39 * z**6 + out7_39 * z**7 + out8_39 * z**8 + out9_39 * z**9 + out10_39 * z**10 + out11_39 * z**11 && true;
ghost cvrted4_40@uint12: cvrted4_40 = out0_40 * z**0 + out1_40 * z**1 + out2_40 * z**2 + out3_40 * z**3 + out4_40 * z**4 + out5_40 * z**5 + out6_40 * z**6 + out7_40 * z**7 + out8_40 * z**8 + out9_40 * z**9 + out10_40 * z**10 + out11_40 * z**11 && true;
ghost cvrted4_41@uint12: cvrted4_41 = out0_41 * z**0 + out1_41 * z**1 + out2_41 * z**2 + out3_41 * z**3 + out4_41 * z**4 + out5_41 * z**5 + out6_41 * z**6 + out7_41 * z**7 + out8_41 * z**8 + out9_41 * z**9 + out10_41 * z**10 + out11_41 * z**11 && true;
ghost cvrted4_42@uint12: cvrted4_42 = out0_42 * z**0 + out1_42 * z**1 + out2_42 * z**2 + out3_42 * z**3 + out4_42 * z**4 + out5_42 * z**5 + out6_42 * z**6 + out7_42 * z**7 + out8_42 * z**8 + out9_42 * z**9 + out10_42 * z**10 + out11_42 * z**11 && true;
ghost cvrted4_43@uint12: cvrted4_43 = out0_43 * z**0 + out1_43 * z**1 + out2_43 * z**2 + out3_43 * z**3 + out4_43 * z**4 + out5_43 * z**5 + out6_43 * z**6 + out7_43 * z**7 + out8_43 * z**8 + out9_43 * z**9 + out10_43 * z**10 + out11_43 * z**11 && true;
ghost cvrted4_44@uint12: cvrted4_44 = out0_44 * z**0 + out1_44 * z**1 + out2_44 * z**2 + out3_44 * z**3 + out4_44 * z**4 + out5_44 * z**5 + out6_44 * z**6 + out7_44 * z**7 + out8_44 * z**8 + out9_44 * z**9 + out10_44 * z**10 + out11_44 * z**11 && true;
ghost cvrted4_45@uint12: cvrted4_45 = out0_45 * z**0 + out1_45 * z**1 + out2_45 * z**2 + out3_45 * z**3 + out4_45 * z**4 + out5_45 * z**5 + out6_45 * z**6 + out7_45 * z**7 + out8_45 * z**8 + out9_45 * z**9 + out10_45 * z**10 + out11_45 * z**11 && true;
ghost cvrted4_46@uint12: cvrted4_46 = out0_46 * z**0 + out1_46 * z**1 + out2_46 * z**2 + out3_46 * z**3 + out4_46 * z**4 + out5_46 * z**5 + out6_46 * z**6 + out7_46 * z**7 + out8_46 * z**8 + out9_46 * z**9 + out10_46 * z**10 + out11_46 * z**11 && true;
ghost cvrted4_47@uint12: cvrted4_47 = out0_47 * z**0 + out1_47 * z**1 + out2_47 * z**2 + out3_47 * z**3 + out4_47 * z**4 + out5_47 * z**5 + out6_47 * z**6 + out7_47 * z**7 + out8_47 * z**8 + out9_47 * z**9 + out10_47 * z**10 + out11_47 * z**11 && true;
ghost cvrted4_48@uint12: cvrted4_48 = out0_48 * z**0 + out1_48 * z**1 + out2_48 * z**2 + out3_48 * z**3 + out4_48 * z**4 + out5_48 * z**5 + out6_48 * z**6 + out7_48 * z**7 + out8_48 * z**8 + out9_48 * z**9 + out10_48 * z**10 + out11_48 * z**11 && true;
ghost cvrted4_49@uint12: cvrted4_49 = out0_49 * z**0 + out1_49 * z**1 + out2_49 * z**2 + out3_49 * z**3 + out4_49 * z**4 + out5_49 * z**5 + out6_49 * z**6 + out7_49 * z**7 + out8_49 * z**8 + out9_49 * z**9 + out10_49 * z**10 + out11_49 * z**11 && true;
ghost cvrted4_50@uint12: cvrted4_50 = out0_50 * z**0 + out1_50 * z**1 + out2_50 * z**2 + out3_50 * z**3 + out4_50 * z**4 + out5_50 * z**5 + out6_50 * z**6 + out7_50 * z**7 + out8_50 * z**8 + out9_50 * z**9 + out10_50 * z**10 + out11_50 * z**11 && true;
ghost cvrted4_51@uint12: cvrted4_51 = out0_51 * z**0 + out1_51 * z**1 + out2_51 * z**2 + out3_51 * z**3 + out4_51 * z**4 + out5_51 * z**5 + out6_51 * z**6 + out7_51 * z**7 + out8_51 * z**8 + out9_51 * z**9 + out10_51 * z**10 + out11_51 * z**11 && true;
ghost cvrted4_52@uint12: cvrted4_52 = out0_52 * z**0 + out1_52 * z**1 + out2_52 * z**2 + out3_52 * z**3 + out4_52 * z**4 + out5_52 * z**5 + out6_52 * z**6 + out7_52 * z**7 + out8_52 * z**8 + out9_52 * z**9 + out10_52 * z**10 + out11_52 * z**11 && true;
ghost cvrted4_53@uint12: cvrted4_53 = out0_53 * z**0 + out1_53 * z**1 + out2_53 * z**2 + out3_53 * z**3 + out4_53 * z**4 + out5_53 * z**5 + out6_53 * z**6 + out7_53 * z**7 + out8_53 * z**8 + out9_53 * z**9 + out10_53 * z**10 + out11_53 * z**11 && true;
ghost cvrted4_54@uint12: cvrted4_54 = out0_54 * z**0 + out1_54 * z**1 + out2_54 * z**2 + out3_54 * z**3 + out4_54 * z**4 + out5_54 * z**5 + out6_54 * z**6 + out7_54 * z**7 + out8_54 * z**8 + out9_54 * z**9 + out10_54 * z**10 + out11_54 * z**11 && true;
ghost cvrted4_55@uint12: cvrted4_55 = out0_55 * z**0 + out1_55 * z**1 + out2_55 * z**2 + out3_55 * z**3 + out4_55 * z**4 + out5_55 * z**5 + out6_55 * z**6 + out7_55 * z**7 + out8_55 * z**8 + out9_55 * z**9 + out10_55 * z**10 + out11_55 * z**11 && true;
ghost cvrted4_56@uint12: cvrted4_56 = out0_56 * z**0 + out1_56 * z**1 + out2_56 * z**2 + out3_56 * z**3 + out4_56 * z**4 + out5_56 * z**5 + out6_56 * z**6 + out7_56 * z**7 + out8_56 * z**8 + out9_56 * z**9 + out10_56 * z**10 + out11_56 * z**11 && true;
ghost cvrted4_57@uint12: cvrted4_57 = out0_57 * z**0 + out1_57 * z**1 + out2_57 * z**2 + out3_57 * z**3 + out4_57 * z**4 + out5_57 * z**5 + out6_57 * z**6 + out7_57 * z**7 + out8_57 * z**8 + out9_57 * z**9 + out10_57 * z**10 + out11_57 * z**11 && true;
ghost cvrted4_58@uint12: cvrted4_58 = out0_58 * z**0 + out1_58 * z**1 + out2_58 * z**2 + out3_58 * z**3 + out4_58 * z**4 + out5_58 * z**5 + out6_58 * z**6 + out7_58 * z**7 + out8_58 * z**8 + out9_58 * z**9 + out10_58 * z**10 + out11_58 * z**11 && true;
ghost cvrted4_59@uint12: cvrted4_59 = out0_59 * z**0 + out1_59 * z**1 + out2_59 * z**2 + out3_59 * z**3 + out4_59 * z**4 + out5_59 * z**5 + out6_59 * z**6 + out7_59 * z**7 + out8_59 * z**8 + out9_59 * z**9 + out10_59 * z**10 + out11_59 * z**11 && true;
ghost cvrted4_60@uint12: cvrted4_60 = out0_60 * z**0 + out1_60 * z**1 + out2_60 * z**2 + out3_60 * z**3 + out4_60 * z**4 + out5_60 * z**5 + out6_60 * z**6 + out7_60 * z**7 + out8_60 * z**8 + out9_60 * z**9 + out10_60 * z**10 + out11_60 * z**11 && true;
ghost cvrted4_61@uint12: cvrted4_61 = out0_61 * z**0 + out1_61 * z**1 + out2_61 * z**2 + out3_61 * z**3 + out4_61 * z**4 + out5_61 * z**5 + out6_61 * z**6 + out7_61 * z**7 + out8_61 * z**8 + out9_61 * z**9 + out10_61 * z**10 + out11_61 * z**11 && true;
ghost cvrted4_62@uint12: cvrted4_62 = out0_62 * z**0 + out1_62 * z**1 + out2_62 * z**2 + out3_62 * z**3 + out4_62 * z**4 + out5_62 * z**5 + out6_62 * z**6 + out7_62 * z**7 + out8_62 * z**8 + out9_62 * z**9 + out10_62 * z**10 + out11_62 * z**11 && true;
ghost cvrted4_63@uint12: cvrted4_63 = out0_63 * z**0 + out1_63 * z**1 + out2_63 * z**2 + out3_63 * z**3 + out4_63 * z**4 + out5_63 * z**5 + out6_63 * z**6 + out7_63 * z**7 + out8_63 * z**8 + out9_63 * z**9 + out10_63 * z**10 + out11_63 * z**11 && true;

ecut and [
  eqmod inp4_0 (
    (cvrted4_0 + x * cvrted4_16) * (x ** 2 + x) ** 0 +
    (cvrted4_32 + x * cvrted4_48) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_1 (
    (cvrted4_1 + x * cvrted4_17) * (x ** 2 + x) ** 0 +
    (cvrted4_33 + x * cvrted4_49) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_2 (
    (cvrted4_2 + x * cvrted4_18) * (x ** 2 + x) ** 0 +
    (cvrted4_34 + x * cvrted4_50) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_3 (
    (cvrted4_3 + x * cvrted4_19) * (x ** 2 + x) ** 0 +
    (cvrted4_35 + x * cvrted4_51) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_4 (
    (cvrted4_4 + x * cvrted4_20) * (x ** 2 + x) ** 0 +
    (cvrted4_36 + x * cvrted4_52) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_5 (
    (cvrted4_5 + x * cvrted4_21) * (x ** 2 + x) ** 0 +
    (cvrted4_37 + x * cvrted4_53) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_6 (
    (cvrted4_6 + x * cvrted4_22) * (x ** 2 + x) ** 0 +
    (cvrted4_38 + x * cvrted4_54) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_7 (
    (cvrted4_7 + x * cvrted4_23) * (x ** 2 + x) ** 0 +
    (cvrted4_39 + x * cvrted4_55) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_8 (
    (cvrted4_8 + x * cvrted4_24) * (x ** 2 + x) ** 0 +
    (cvrted4_40 + x * cvrted4_56) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_9 (
    (cvrted4_9 + x * cvrted4_25) * (x ** 2 + x) ** 0 +
    (cvrted4_41 + x * cvrted4_57) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_10 (
    (cvrted4_10 + x * cvrted4_26) * (x ** 2 + x) ** 0 +
    (cvrted4_42 + x * cvrted4_58) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_11 (
    (cvrted4_11 + x * cvrted4_27) * (x ** 2 + x) ** 0 +
    (cvrted4_43 + x * cvrted4_59) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_12 (
    (cvrted4_12 + x * cvrted4_28) * (x ** 2 + x) ** 0 +
    (cvrted4_44 + x * cvrted4_60) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_13 (
    (cvrted4_13 + x * cvrted4_29) * (x ** 2 + x) ** 0 +
    (cvrted4_45 + x * cvrted4_61) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_14 (
    (cvrted4_14 + x * cvrted4_30) * (x ** 2 + x) ** 0 +
    (cvrted4_46 + x * cvrted4_62) * (x ** 2 + x) ** 1
  ) 2,
  eqmod inp4_15 (
    (cvrted4_15 + x * cvrted4_31) * (x ** 2 + x) ** 0 +
    (cvrted4_47 + x * cvrted4_63) * (x ** 2 + x) ** 1
  ) 2
];

(* #call   0x555555555380 <_PQCLEAN_MCELIECE348864_AVX_vec_mul_asm>#! PC = 0x555555555352 *)
#call   0x555555555380 <_PQCLEAN_MCELIECE348864_AVX_vec_mul_asm>#! 0x555555555352 = 0x555555555352;
(* #! -> SP = 0x7fffffffd828 *)
#! 0x7fffffffd828 = 0x7fffffffd828;
(* #! <- SP = 0x7fffffffd828 *)
#! 0x7fffffffd828 = 0x7fffffffd828;
(* #ret                                            #! PC = 0x555555555380 *)
#ret                                            #! 0x555555555380 = 0x555555555380;

nondet res4_0@bit; nondet res4_1@bit; nondet res4_2@bit; nondet res4_3@bit;
nondet res4_4@bit; nondet res4_5@bit; nondet res4_6@bit; nondet res4_7@bit;
nondet res4_8@bit; nondet res4_9@bit; nondet res4_10@bit; nondet res4_11@bit;
nondet res4_12@bit; nondet res4_13@bit; nondet res4_14@bit; nondet res4_15@bit;
nondet res4_16@bit; nondet res4_17@bit; nondet res4_18@bit; nondet res4_19@bit;
nondet res4_20@bit; nondet res4_21@bit; nondet res4_22@bit; nondet res4_23@bit;
nondet res4_24@bit; nondet res4_25@bit; nondet res4_26@bit; nondet res4_27@bit;
nondet res4_28@bit; nondet res4_29@bit; nondet res4_30@bit; nondet res4_31@bit;
nondet res4_32@bit; nondet res4_33@bit; nondet res4_34@bit; nondet res4_35@bit;
nondet res4_36@bit; nondet res4_37@bit; nondet res4_38@bit; nondet res4_39@bit;
nondet res4_40@bit; nondet res4_41@bit; nondet res4_42@bit; nondet res4_43@bit;
nondet res4_44@bit; nondet res4_45@bit; nondet res4_46@bit; nondet res4_47@bit;
nondet res4_48@bit; nondet res4_49@bit; nondet res4_50@bit; nondet res4_51@bit;
nondet res4_52@bit; nondet res4_53@bit; nondet res4_54@bit; nondet res4_55@bit;
nondet res4_56@bit; nondet res4_57@bit; nondet res4_58@bit; nondet res4_59@bit;
nondet res4_60@bit; nondet res4_61@bit; nondet res4_62@bit; nondet res4_63@bit;

assume and [
  eqmod res4_0 (cvrted4_0 * ((* 4  0 *) 1)) [2, modulus],
  eqmod res4_1 (cvrted4_1 * ((* 4  1 *) 1)) [2, modulus],
  eqmod res4_2 (cvrted4_2 * ((* 4  2 *) 1)) [2, modulus],
  eqmod res4_3 (cvrted4_3 * ((* 4  3 *) 1)) [2, modulus],
  eqmod res4_4 (cvrted4_4 * ((* 4  4 *) 1)) [2, modulus],
  eqmod res4_5 (cvrted4_5 * ((* 4  5 *) 1)) [2, modulus],
  eqmod res4_6 (cvrted4_6 * ((* 4  6 *) 1)) [2, modulus],
  eqmod res4_7 (cvrted4_7 * ((* 4  7 *) 1)) [2, modulus],
  eqmod res4_8 (cvrted4_8 * ((* 4  8 *) 1)) [2, modulus],
  eqmod res4_9 (cvrted4_9 * ((* 4  9 *) 1)) [2, modulus],
  eqmod res4_10 (cvrted4_10 * ((* 4 10 *) 1)) [2, modulus],
  eqmod res4_11 (cvrted4_11 * ((* 4 11 *) 1)) [2, modulus],
  eqmod res4_12 (cvrted4_12 * ((* 4 12 *) 1)) [2, modulus],
  eqmod res4_13 (cvrted4_13 * ((* 4 13 *) 1)) [2, modulus],
  eqmod res4_14 (cvrted4_14 * ((* 4 14 *) 1)) [2, modulus],
  eqmod res4_15 (cvrted4_15 * ((* 4 15 *) 1)) [2, modulus],
  eqmod res4_16 (cvrted4_16 * ((* 4 16 *) 1)) [2, modulus],
  eqmod res4_17 (cvrted4_17 * ((* 4 17 *) 1)) [2, modulus],
  eqmod res4_18 (cvrted4_18 * ((* 4 18 *) 1)) [2, modulus],
  eqmod res4_19 (cvrted4_19 * ((* 4 19 *) 1)) [2, modulus],
  eqmod res4_20 (cvrted4_20 * ((* 4 20 *) 1)) [2, modulus],
  eqmod res4_21 (cvrted4_21 * ((* 4 21 *) 1)) [2, modulus],
  eqmod res4_22 (cvrted4_22 * ((* 4 22 *) 1)) [2, modulus],
  eqmod res4_23 (cvrted4_23 * ((* 4 23 *) 1)) [2, modulus],
  eqmod res4_24 (cvrted4_24 * ((* 4 24 *) 1)) [2, modulus],
  eqmod res4_25 (cvrted4_25 * ((* 4 25 *) 1)) [2, modulus],
  eqmod res4_26 (cvrted4_26 * ((* 4 26 *) 1)) [2, modulus],
  eqmod res4_27 (cvrted4_27 * ((* 4 27 *) 1)) [2, modulus],
  eqmod res4_28 (cvrted4_28 * ((* 4 28 *) 1)) [2, modulus],
  eqmod res4_29 (cvrted4_29 * ((* 4 29 *) 1)) [2, modulus],
  eqmod res4_30 (cvrted4_30 * ((* 4 30 *) 1)) [2, modulus],
  eqmod res4_31 (cvrted4_31 * ((* 4 31 *) 1)) [2, modulus],
  eqmod res4_32 (cvrted4_32 * ((* 4 32 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_33 (cvrted4_33 * ((* 4 33 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_34 (cvrted4_34 * ((* 4 34 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_35 (cvrted4_35 * ((* 4 35 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_36 (cvrted4_36 * ((* 4 36 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_37 (cvrted4_37 * ((* 4 37 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_38 (cvrted4_38 * ((* 4 38 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_39 (cvrted4_39 * ((* 4 39 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_40 (cvrted4_40 * ((* 4 40 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_41 (cvrted4_41 * ((* 4 41 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_42 (cvrted4_42 * ((* 4 42 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_43 (cvrted4_43 * ((* 4 43 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_44 (cvrted4_44 * ((* 4 44 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_45 (cvrted4_45 * ((* 4 45 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_46 (cvrted4_46 * ((* 4 46 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_47 (cvrted4_47 * ((* 4 47 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_48 (cvrted4_48 * ((* 4 48 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_49 (cvrted4_49 * ((* 4 49 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_50 (cvrted4_50 * ((* 4 50 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_51 (cvrted4_51 * ((* 4 51 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_52 (cvrted4_52 * ((* 4 52 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_53 (cvrted4_53 * ((* 4 53 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_54 (cvrted4_54 * ((* 4 54 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_55 (cvrted4_55 * ((* 4 55 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_56 (cvrted4_56 * ((* 4 56 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_57 (cvrted4_57 * ((* 4 57 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_58 (cvrted4_58 * ((* 4 58 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_59 (cvrted4_59 * ((* 4 59 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_60 (cvrted4_60 * ((* 4 60 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_61 (cvrted4_61 * ((* 4 61 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_62 (cvrted4_62 * ((* 4 62 *) z**8 + z**5 + z**2 + z)) [2, modulus],
  eqmod res4_63 (cvrted4_63 * ((* 4 63 *) z**8 + z**5 + z**2 + z)) [2, modulus]
] && true;

nondet x5@uint12;

ecut and [
  eqmod (
    (cvrted4_0 + x * cvrted4_16) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_32 + x * cvrted4_48) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_0 + x * res4_16) * x5 ** 0 +
    (res4_32 + x * res4_48) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_1 + x * cvrted4_17) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_33 + x * cvrted4_49) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_1 + x * res4_17) * x5 ** 0 +
    (res4_33 + x * res4_49) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_2 + x * cvrted4_18) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_34 + x * cvrted4_50) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_2 + x * res4_18) * x5 ** 0 +
    (res4_34 + x * res4_50) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_3 + x * cvrted4_19) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_35 + x * cvrted4_51) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_3 + x * res4_19) * x5 ** 0 +
    (res4_35 + x * res4_51) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_4 + x * cvrted4_20) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_36 + x * cvrted4_52) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_4 + x * res4_20) * x5 ** 0 +
    (res4_36 + x * res4_52) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_5 + x * cvrted4_21) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_37 + x * cvrted4_53) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_5 + x * res4_21) * x5 ** 0 +
    (res4_37 + x * res4_53) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_6 + x * cvrted4_22) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_38 + x * cvrted4_54) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_6 + x * res4_22) * x5 ** 0 +
    (res4_38 + x * res4_54) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_7 + x * cvrted4_23) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_39 + x * cvrted4_55) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_7 + x * res4_23) * x5 ** 0 +
    (res4_39 + x * res4_55) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_8 + x * cvrted4_24) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_40 + x * cvrted4_56) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_8 + x * res4_24) * x5 ** 0 +
    (res4_40 + x * res4_56) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_9 + x * cvrted4_25) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_41 + x * cvrted4_57) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_9 + x * res4_25) * x5 ** 0 +
    (res4_41 + x * res4_57) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_10 + x * cvrted4_26) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_42 + x * cvrted4_58) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_10 + x * res4_26) * x5 ** 0 +
    (res4_42 + x * res4_58) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_11 + x * cvrted4_27) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_43 + x * cvrted4_59) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_11 + x * res4_27) * x5 ** 0 +
    (res4_43 + x * res4_59) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_12 + x * cvrted4_28) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_44 + x * cvrted4_60) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_12 + x * res4_28) * x5 ** 0 +
    (res4_44 + x * res4_60) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_13 + x * cvrted4_29) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_45 + x * cvrted4_61) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_13 + x * res4_29) * x5 ** 0 +
    (res4_45 + x * res4_61) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_14 + x * cvrted4_30) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_46 + x * cvrted4_62) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_14 + x * res4_30) * x5 ** 0 +
    (res4_46 + x * res4_62) * x5 ** 1
  ) [2, modulus],
  eqmod (
    (cvrted4_15 + x * cvrted4_31) * ((z ** 32 + z) * x5) ** 0 +
    (cvrted4_47 + x * cvrted4_63) * ((z ** 32 + z) * x5) ** 1
  ) (
    (res4_15 + x * res4_31) * x5 ** 0 +
    (res4_47 + x * res4_63) * x5 ** 1
  ) [2, modulus]
] prove with [precondition];

(* #jne    0x5555555552e4 <radix_conversions+196>  #! PC = 0x55555555535b *)
#jne    0x5555555552e4 <radix_conversions+196>  #! 0x55555555535b = 0x55555555535b;
(* #! <- SP = 0x7fffffffda98 *)
#! 0x7fffffffda98 = 0x7fffffffda98;
(* #ret                                            #! PC = 0x55555555536e *)
#ret                                            #! 0x55555555536e = 0x55555555536e;

# dummy cut to prevent the previous conditions being the pre-condition
ecut true;

# now, "remind" these conditions...
mov x x1;

assume inp1_0 =
  res0_0 * x**0 + res0_2 * x**1 + res0_4 * x**2 + res0_6 * x**3 +
  res0_8 * x**4 + res0_10 * x**5 + res0_12 * x**6 + res0_14 * x**7 +
  res0_16 * x**8 + res0_18 * x**9 + res0_20 * x**10 + res0_22 * x**11 +
  res0_24 * x**12 + res0_26 * x**13 + res0_28 * x**14 + res0_30 * x**15 +
  res0_32 * x**16 + res0_34 * x**17 + res0_36 * x**18 + res0_38 * x**19 +
  res0_40 * x**20 + res0_42 * x**21 + res0_44 * x**22 + res0_46 * x**23 +
  res0_48 * x**24 + res0_50 * x**25 + res0_52 * x**26 + res0_54 * x**27 +
  res0_56 * x**28 + res0_58 * x**29 + res0_60 * x**30 + res0_62 * x**31
&& true;
assume inp1_1 =
  res0_1 * x**0 + res0_3 * x**1 + res0_5 * x**2 + res0_7 * x**3 +
  res0_9 * x**4 + res0_11 * x**5 + res0_13 * x**6 + res0_15 * x**7 +
  res0_17 * x**8 + res0_19 * x**9 + res0_21 * x**10 + res0_23 * x**11 +
  res0_25 * x**12 + res0_27 * x**13 + res0_29 * x**14 + res0_31 * x**15 +
  res0_33 * x**16 + res0_35 * x**17 + res0_37 * x**18 + res0_39 * x**19 +
  res0_41 * x**20 + res0_43 * x**21 + res0_45 * x**22 + res0_47 * x**23 +
  res0_49 * x**24 + res0_51 * x**25 + res0_53 * x**26 + res0_55 * x**27 +
  res0_57 * x**28 + res0_59 * x**29 + res0_61 * x**30 + res0_63 * x**31
&& true;

mov x x2;

assume inp2_0 =
  res1_0 * x**0 + res1_4 * x**1 + res1_8 * x**2 + res1_12 * x**3 +
  res1_16 * x**4 + res1_20 * x**5 + res1_24 * x**6 + res1_28 * x**7 +
  res1_32 * x**8 + res1_36 * x**9 + res1_40 * x**10 + res1_44 * x**11 +
  res1_48 * x**12 + res1_52 * x**13 + res1_56 * x**14 + res1_60 * x**15
&& true;
assume inp2_1 =
  res1_1 * x**0 + res1_5 * x**1 + res1_9 * x**2 + res1_13 * x**3 +
  res1_17 * x**4 + res1_21 * x**5 + res1_25 * x**6 + res1_29 * x**7 +
  res1_33 * x**8 + res1_37 * x**9 + res1_41 * x**10 + res1_45 * x**11 +
  res1_49 * x**12 + res1_53 * x**13 + res1_57 * x**14 + res1_61 * x**15
&& true;
assume inp2_2 =
  res1_2 * x**0 + res1_6 * x**1 + res1_10 * x**2 + res1_14 * x**3 +
  res1_18 * x**4 + res1_22 * x**5 + res1_26 * x**6 + res1_30 * x**7 +
  res1_34 * x**8 + res1_38 * x**9 + res1_42 * x**10 + res1_46 * x**11 +
  res1_50 * x**12 + res1_54 * x**13 + res1_58 * x**14 + res1_62 * x**15
&& true;
assume inp2_3 =
  res1_3 * x**0 + res1_7 * x**1 + res1_11 * x**2 + res1_15 * x**3 +
  res1_19 * x**4 + res1_23 * x**5 + res1_27 * x**6 + res1_31 * x**7 +
  res1_35 * x**8 + res1_39 * x**9 + res1_43 * x**10 + res1_47 * x**11 +
  res1_51 * x**12 + res1_55 * x**13 + res1_59 * x**14 + res1_63 * x**15
&& true;

mov x x3;

assume inp3_0 =
  res2_0 * x**0 + res2_8 * x**1 + res2_16 * x**2 + res2_24 * x**3 +
  res2_32 * x**4 + res2_40 * x**5 + res2_48 * x**6 + res2_56 * x**7
&& true;
assume inp3_1 =
  res2_1 * x**0 + res2_9 * x**1 + res2_17 * x**2 + res2_25 * x**3 +
  res2_33 * x**4 + res2_41 * x**5 + res2_49 * x**6 + res2_57 * x**7
&& true;
assume inp3_2 =
  res2_2 * x**0 + res2_10 * x**1 + res2_18 * x**2 + res2_26 * x**3 +
  res2_34 * x**4 + res2_42 * x**5 + res2_50 * x**6 + res2_58 * x**7
&& true;
assume inp3_3 =
  res2_3 * x**0 + res2_11 * x**1 + res2_19 * x**2 + res2_27 * x**3 +
  res2_35 * x**4 + res2_43 * x**5 + res2_51 * x**6 + res2_59 * x**7
&& true;
assume inp3_4 =
  res2_4 * x**0 + res2_12 * x**1 + res2_20 * x**2 + res2_28 * x**3 +
  res2_36 * x**4 + res2_44 * x**5 + res2_52 * x**6 + res2_60 * x**7
&& true;
assume inp3_5 =
  res2_5 * x**0 + res2_13 * x**1 + res2_21 * x**2 + res2_29 * x**3 +
  res2_37 * x**4 + res2_45 * x**5 + res2_53 * x**6 + res2_61 * x**7
&& true;
assume inp3_6 =
  res2_6 * x**0 + res2_14 * x**1 + res2_22 * x**2 + res2_30 * x**3 +
  res2_38 * x**4 + res2_46 * x**5 + res2_54 * x**6 + res2_62 * x**7
&& true;
assume inp3_7 =
  res2_7 * x**0 + res2_15 * x**1 + res2_23 * x**2 + res2_31 * x**3 +
  res2_39 * x**4 + res2_47 * x**5 + res2_55 * x**6 + res2_63 * x**7
&& true;

mov x x4;

assume inp4_0 =
  res3_0 * x**0 + res3_16 * x**1 + res3_32 * x**2 + res3_48 * x**3
&& true;
assume inp4_1 =
  res3_1 * x**0 + res3_17 * x**1 + res3_33 * x**2 + res3_49 * x**3
&& true;
assume inp4_2 =
  res3_2 * x**0 + res3_18 * x**1 + res3_34 * x**2 + res3_50 * x**3
&& true;
assume inp4_3 =
  res3_3 * x**0 + res3_19 * x**1 + res3_35 * x**2 + res3_51 * x**3
&& true;
assume inp4_4 =
  res3_4 * x**0 + res3_20 * x**1 + res3_36 * x**2 + res3_52 * x**3
&& true;
assume inp4_5 =
  res3_5 * x**0 + res3_21 * x**1 + res3_37 * x**2 + res3_53 * x**3
&& true;
assume inp4_6 =
  res3_6 * x**0 + res3_22 * x**1 + res3_38 * x**2 + res3_54 * x**3
&& true;
assume inp4_7 =
  res3_7 * x**0 + res3_23 * x**1 + res3_39 * x**2 + res3_55 * x**3
&& true;
assume inp4_8 =
  res3_8 * x**0 + res3_24 * x**1 + res3_40 * x**2 + res3_56 * x**3
&& true;
assume inp4_9 =
  res3_9 * x**0 + res3_25 * x**1 + res3_41 * x**2 + res3_57 * x**3
&& true;
assume inp4_10 =
  res3_10 * x**0 + res3_26 * x**1 + res3_42 * x**2 + res3_58 * x**3
&& true;
assume inp4_11 =
  res3_11 * x**0 + res3_27 * x**1 + res3_43 * x**2 + res3_59 * x**3
&& true;
assume inp4_12 =
  res3_12 * x**0 + res3_28 * x**1 + res3_44 * x**2 + res3_60 * x**3
&& true;
assume inp4_13 =
  res3_13 * x**0 + res3_29 * x**1 + res3_45 * x**2 + res3_61 * x**3
&& true;
assume inp4_14 =
  res3_14 * x**0 + res3_30 * x**1 + res3_46 * x**2 + res3_62 * x**3
&& true;
assume inp4_15 =
  res3_15 * x**0 + res3_31 * x**1 + res3_47 * x**2 + res3_63 * x**3
&& true;

mov c0 res4_0; mov c1 res4_1; mov c2 res4_2; mov c3 res4_3;
mov c4 res4_4; mov c5 res4_5; mov c6 res4_6; mov c7 res4_7;
mov c8 res4_8; mov c9 res4_9; mov c10 res4_10; mov c11 res4_11;
mov c12 res4_12; mov c13 res4_13; mov c14 res4_14; mov c15 res4_15;
mov c16 res4_16; mov c17 res4_17; mov c18 res4_18; mov c19 res4_19;
mov c20 res4_20; mov c21 res4_21; mov c22 res4_22; mov c23 res4_23;
mov c24 res4_24; mov c25 res4_25; mov c26 res4_26; mov c27 res4_27;
mov c28 res4_28; mov c29 res4_29; mov c30 res4_30; mov c31 res4_31;
mov c32 res4_32; mov c33 res4_33; mov c34 res4_34; mov c35 res4_35;
mov c36 res4_36; mov c37 res4_37; mov c38 res4_38; mov c39 res4_39;
mov c40 res4_40; mov c41 res4_41; mov c42 res4_42; mov c43 res4_43;
mov c44 res4_44; mov c45 res4_45; mov c46 res4_46; mov c47 res4_47;
mov c48 res4_48; mov c49 res4_49; mov c50 res4_50; mov c51 res4_51;
mov c52 res4_52; mov c53 res4_53; mov c54 res4_54; mov c55 res4_55;
mov c56 res4_56; mov c57 res4_57; mov c58 res4_58; mov c59 res4_59;
mov c60 res4_60; mov c61 res4_61; mov c62 res4_62; mov c63 res4_63;

ghost be00@bit: be00 = z**2+z && true;

ghost be10@bit: be10 = z**4+z**3+z**2 && true;
ghost be11@bit: be11 = z**4+z**2+z && true;

ghost be20@bit: be20 = z**6+z**5+z**4+z**3 && true;
ghost be21@bit: be21 = z**8+z**6+z**5+z**4+z**3+z**2 && true;
ghost be22@bit: be22 = z**8+z**4+z**2+z && true;

ghost be30@bit: be30 = z**8+z**7+z**6+z**5+z**4 && true;
ghost be31@bit: be31 = z**10+z**9+z**8+z**7+z**5+z**4+1 && true;
ghost be32@bit: be32 = z**10+z**9+z**8+z**7+z**6+z**5+z**2+1 && true;
ghost be33@bit: be33 = z**8+z**7+z**2+z && true;

{
  and [
    eqmod input_poly
      c0
      [2, modulus, x0, x1, x2, x3, x4, x5],
    eqmod input_poly
      c0 + c1
      [2, modulus, x0 - 1, x1, x2, x3, x4, x5],
    eqmod input_poly
      (c0 + c2) + z * (c1 + c3)
      [2, modulus, x0 - z, x1 - 1, x2, x3, x4, x5],
    eqmod input_poly
      (c0 + c4 +
       be00 * (c2 + c6)) +
      z**2 * (c1 + c5 +
       be00 * (c3 + c7))
      [2, modulus, x0 - z**2, x1 - be00, x2 - 1, x3, x4, x5],
    eqmod input_poly
      (c0 + c8 +
       be11 * (c4 + c12) +
       be10 * (c2 + c10 +
        be11 * (c6 + c14)) +
      z**3 * (c1 + c9 +
       be11 * (c5 + c13) +
       be10 * (c3 + c11 +
        be11 * (c7 + c15))))
      [2, modulus, x0 - z**3, x1 - be10, x2 - be11, x3 - 1, x4, x5],
    eqmod input_poly
      (c0 + c16 +
       be22 * (c8 + c24) +
       be21 * (c4 + c20 +
        be22 * (c12 + c28)) +
       be20 * (c2 + c18 +
        be22 * (c10 + c26) +
        be21 * (c6 + c22 +
         be22 * (c14 + c30))) +
      z**4 * (c1 + c17 +
       be22 * (c9 + c25) +
       be21 * (c5 + c21 +
        be22 * (c13 + c29)) +
       be20 * (c3 + c19 +
        be22 * (c11 + c27) +
        be21 * (c7 + c23 +
         be22 * (c15 + c31)))))
      [2, modulus, x0 - z**4, x1 - be20, x2 - be21, x3 - be22, x4 - 1, x5],
    eqmod input_poly
      (c0 + c32 +
       be33 * (c16 + c48) +
       be32 * (c8 + c40 +
        be33 * (c24 + c56)) +
       be31 * (c4 + c36 +
        be33 * (c20 + c52) +
        be32 * (c12 + c44 +
         be33 * (c28 + c60))) +
       be30 * (c2 + c34 +
        be33 * (c18 + c50) +
        be32 * (c10 + c42 +
         be33 * (c26 + c58)) +
        be31 * (c6 + c38 +
         be33 * (c22 + c54) +
         be32 * (c14 + c46 +
          be33 * (c30 + c62)))) +
      z**5 * (c1 + c33 +
       be33 * (c17 + c49) +
       be32 * (c9 + c41 +
        be33 * (c25 + c57)) +
       be31 * (c5 + c37 +
        be33 * (c21 + c53) +
        be32 * (c13 + c45 +
         be33 * (c29 + c61))) +
       be30 * (c3 + c35 +
        be33 * (c19 + c51) +
        be32 * (c11 + c43 +
         be33 * (c27 + c59)) +
        be31 * (c7 + c39 +
         be33 * (c23 + c55) +
         be32 * (c15 + c47 +
          be33 * (c31 + c63))))))
      [2, modulus, x0 - z**5, x1 - be30, x2 - be31, x3 - be32, x4 - be33, x5 - 1]
  ] prove with [precondition, cuts [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]

  &&
  true
}

