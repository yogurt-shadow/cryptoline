(* frege: -v -isafety -jobs 42 -slicing -no_carry_constraint -vo lex asm_Good_3x2.cl.new
Parsing Cryptoline file:                [OK]            0.492083 seconds
Checking well-formedness:               [OK]            0.410408 seconds
Transforming to SSA form:               [OK]            0.061947 seconds
Normalizing specification:              [OK]            0.001368 seconds
Rewriting assignments:                  [OK]            0.040232 seconds
Verifying program safety:               [OK]            1247.233482 seconds
Verifying range assertions:             [OK]            2.767104 seconds
Verifying range specification:          [OK]            1405.861823 seconds
Rewriting value-preserved casting:      [OK]            0.001753 seconds
Verifying algebraic assertions:         [OK]            73.621739 seconds
Verifying algebraic specification:      [OK]            3140.892401 seconds
Verification result:                    [OK]            5871.412938 seconds
*)                      

proc main (
sint32 f0000, sint32 f0001, sint32 f0002, sint32 f0003, sint32 f0004,
sint32 f0005, sint32 f0006, sint32 f0007, sint32 f0008, sint32 f0009,
sint32 f0010, sint32 f0011, sint32 f0012, sint32 f0013, sint32 f0014,
sint32 f0015, sint32 f0016, sint32 f0017, sint32 f0018, sint32 f0019,
sint32 f0020, sint32 f0021, sint32 f0022, sint32 f0023, sint32 f0024,
sint32 f0025, sint32 f0026, sint32 f0027, sint32 f0028, sint32 f0029,
sint32 f0030, sint32 f0031, sint32 f0100, sint32 f0101, sint32 f0102,
sint32 f0103, sint32 f0104, sint32 f0105, sint32 f0106, sint32 f0107,
sint32 f0108, sint32 f0109, sint32 f0110, sint32 f0111, sint32 f0112,
sint32 f0113, sint32 f0114, sint32 f0115, sint32 f0116, sint32 f0117,
sint32 f0118, sint32 f0119, sint32 f0120, sint32 f0121, sint32 f0122,
sint32 f0123, sint32 f0124, sint32 f0125, sint32 f0126, sint32 f0127,
sint32 f0128, sint32 f0129, sint32 f0130, sint32 f0131, sint32 f0200,
sint32 f0201, sint32 f0202, sint32 f0203, sint32 f0204, sint32 f0205,
sint32 f0206, sint32 f0207, sint32 f0208, sint32 f0209, sint32 f0210,
sint32 f0211, sint32 f0212, sint32 f0213, sint32 f0214, sint32 f0215,
sint32 f0216, sint32 f0217, sint32 f0218, sint32 f0219, sint32 f0220,
sint32 f0221, sint32 f0222, sint32 f0223, sint32 f0224, sint32 f0225,
sint32 f0226, sint32 f0227, sint32 f0228, sint32 f0229, sint32 f0230,
sint32 f0231, sint32 f0300, sint32 f0301, sint32 f0302, sint32 f0303,
sint32 f0304, sint32 f0305, sint32 f0306, sint32 f0307, sint32 f0308,
sint32 f0309, sint32 f0310, sint32 f0311, sint32 f0312, sint32 f0313,
sint32 f0314, sint32 f0315, sint32 f0316, sint32 f0317, sint32 f0318,
sint32 f0319, sint32 f0320, sint32 f0321, sint32 f0322, sint32 f0323,
sint32 f0324, sint32 f0325, sint32 f0326, sint32 f0327, sint32 f0328,
sint32 f0329, sint32 f0330, sint32 f0331, sint32 f0400, sint32 f0401,
sint32 f0402, sint32 f0403, sint32 f0404, sint32 f0405, sint32 f0406,
sint32 f0407, sint32 f0408, sint32 f0409, sint32 f0410, sint32 f0411,
sint32 f0412, sint32 f0413, sint32 f0414, sint32 f0415, sint32 f0416,
sint32 f0417, sint32 f0418, sint32 f0419, sint32 f0420, sint32 f0421,
sint32 f0422, sint32 f0423, sint32 f0424, sint32 f0425, sint32 f0426,
sint32 f0427, sint32 f0428, sint32 f0429, sint32 f0430, sint32 f0431,
sint32 f0500, sint32 f0501, sint32 f0502, sint32 f0503, sint32 f0504,
sint32 f0505, sint32 f0506, sint32 f0507, sint32 f0508, sint32 f0509,
sint32 f0510, sint32 f0511, sint32 f0512, sint32 f0513, sint32 f0514,
sint32 f0515, sint32 f0516, sint32 f0517, sint32 f0518, sint32 f0519,
sint32 f0520, sint32 f0521, sint32 f0522, sint32 f0523, sint32 f0524,
sint32 f0525, sint32 f0526, sint32 f0527, sint32 f0528, sint32 f0529,
sint32 f0530, sint32 f0531, sint32 f0600, sint32 f0601, sint32 f0602,
sint32 f0603, sint32 f0604, sint32 f0605, sint32 f0606, sint32 f0607,
sint32 f0608, sint32 f0609, sint32 f0610, sint32 f0611, sint32 f0612,
sint32 f0613, sint32 f0614, sint32 f0615, sint32 f0616, sint32 f0617,
sint32 f0618, sint32 f0619, sint32 f0620, sint32 f0621, sint32 f0622,
sint32 f0623, sint32 f0624, sint32 f0625, sint32 f0626, sint32 f0627,
sint32 f0628, sint32 f0629, sint32 f0630, sint32 f0631, sint32 f0700,
sint32 f0701, sint32 f0702, sint32 f0703, sint32 f0704, sint32 f0705,
sint32 f0706, sint32 f0707, sint32 f0708, sint32 f0709, sint32 f0710,
sint32 f0711, sint32 f0712, sint32 f0713, sint32 f0714, sint32 f0715,
sint32 f0716, sint32 f0717, sint32 f0718, sint32 f0719, sint32 f0720,
sint32 f0721, sint32 f0722, sint32 f0723, sint32 f0724, sint32 f0725,
sint32 f0726, sint32 f0727, sint32 f0728, sint32 f0729, sint32 f0730,
sint32 f0731, sint32 f0800, sint32 f0801, sint32 f0802, sint32 f0803,
sint32 f0804, sint32 f0805, sint32 f0806, sint32 f0807, sint32 f0808,
sint32 f0809, sint32 f0810, sint32 f0811, sint32 f0812, sint32 f0813,
sint32 f0814, sint32 f0815, sint32 f0816, sint32 f0817, sint32 f0818,
sint32 f0819, sint32 f0820, sint32 f0821, sint32 f0822, sint32 f0823,
sint32 f0824, sint32 f0825, sint32 f0826, sint32 f0827, sint32 f0828,
sint32 f0829, sint32 f0830, sint32 f0831, sint32 f1000, sint32 f1001,
sint32 f1002, sint32 f1003, sint32 f1004, sint32 f1005, sint32 f1006,
sint32 f1007, sint32 f1008, sint32 f1009, sint32 f1010, sint32 f1011,
sint32 f1012, sint32 f1013, sint32 f1014, sint32 f1015, sint32 f1016,
sint32 f1017, sint32 f1018, sint32 f1019, sint32 f1020, sint32 f1021,
sint32 f1022, sint32 f1023, sint32 f1024, sint32 f1025, sint32 f1026,
sint32 f1027, sint32 f1028, sint32 f1029, sint32 f1030, sint32 f1031,
sint32 f1100, sint32 f1101, sint32 f1102, sint32 f1103, sint32 f1104,
sint32 f1105, sint32 f1106, sint32 f1107, sint32 f1108, sint32 f1109,
sint32 f1110, sint32 f1111, sint32 f1112, sint32 f1113, sint32 f1114,
sint32 f1115, sint32 f1116, sint32 f1117, sint32 f1118, sint32 f1119,
sint32 f1120, sint32 f1121, sint32 f1122, sint32 f1123, sint32 f1124,
sint32 f1125, sint32 f1126, sint32 f1127, sint32 f1128, sint32 f1129,
sint32 f1130, sint32 f1131, sint32 f1200, sint32 f1201, sint32 f1202,
sint32 f1203, sint32 f1204, sint32 f1205, sint32 f1206, sint32 f1207,
sint32 f1208, sint32 f1209, sint32 f1210, sint32 f1211, sint32 f1212,
sint32 f1213, sint32 f1214, sint32 f1215, sint32 f1216, sint32 f1217,
sint32 f1218, sint32 f1219, sint32 f1220, sint32 f1221, sint32 f1222,
sint32 f1223, sint32 f1224, sint32 f1225, sint32 f1226, sint32 f1227,
sint32 f1228, sint32 f1229, sint32 f1230, sint32 f1231, sint32 f1300,
sint32 f1301, sint32 f1302, sint32 f1303, sint32 f1304, sint32 f1305,
sint32 f1306, sint32 f1307, sint32 f1308, sint32 f1309, sint32 f1310,
sint32 f1311, sint32 f1312, sint32 f1313, sint32 f1314, sint32 f1315,
sint32 f1316, sint32 f1317, sint32 f1318, sint32 f1319, sint32 f1320,
sint32 f1321, sint32 f1322, sint32 f1323, sint32 f1324, sint32 f1325,
sint32 f1326, sint32 f1327, sint32 f1328, sint32 f1329, sint32 f1330,
sint32 f1331, sint32 f1400, sint32 f1401, sint32 f1402, sint32 f1403,
sint32 f1404, sint32 f1405, sint32 f1406, sint32 f1407, sint32 f1408,
sint32 f1409, sint32 f1410, sint32 f1411, sint32 f1412, sint32 f1413,
sint32 f1414, sint32 f1415, sint32 f1416, sint32 f1417, sint32 f1418,
sint32 f1419, sint32 f1420, sint32 f1421, sint32 f1422, sint32 f1423,
sint32 f1424, sint32 f1425, sint32 f1426, sint32 f1427, sint32 f1428,
sint32 f1429, sint32 f1430, sint32 f1431, sint32 f1500, sint32 f1501,
sint32 f1502, sint32 f1503, sint32 f1504, sint32 f1505, sint32 f1506,
sint32 f1507, sint32 f1508, sint32 f1509, sint32 f1510, sint32 f1511,
sint32 f1512, sint32 f1513, sint32 f1514, sint32 f1515, sint32 f1516,
sint32 f1517, sint32 f1518, sint32 f1519, sint32 f1520, sint32 f1521,
sint32 f1522, sint32 f1523, sint32 f1524, sint32 f1525, sint32 f1526,
sint32 f1527, sint32 f1528, sint32 f1529, sint32 f1530, sint32 f1531,
sint32 f1600, sint32 f1601, sint32 f1602, sint32 f1603, sint32 f1604,
sint32 f1605, sint32 f1606, sint32 f1607, sint32 f1608, sint32 f1609,
sint32 f1610, sint32 f1611, sint32 f1612, sint32 f1613, sint32 f1614,
sint32 f1615, sint32 f1616, sint32 f1617, sint32 f1618, sint32 f1619,
sint32 f1620, sint32 f1621, sint32 f1622, sint32 f1623, sint32 f1624,
sint32 f1625, sint32 f1626, sint32 f1627, sint32 f1628, sint32 f1629,
sint32 f1630, sint32 f1631, sint32 f1700, sint32 f1701, sint32 f1702,
sint32 f1703, sint32 f1704, sint32 f1705, sint32 f1706, sint32 f1707,
sint32 f1708, sint32 f1709, sint32 f1710, sint32 f1711, sint32 f1712,
sint32 f1713, sint32 f1714, sint32 f1715, sint32 f1716, sint32 f1717,
sint32 f1718, sint32 f1719, sint32 f1720, sint32 f1721, sint32 f1722,
sint32 f1723, sint32 f1724, sint32 f1725, sint32 f1726, sint32 f1727,
sint32 f1728, sint32 f1729, sint32 f1730, sint32 f1731, sint32 f1800,
sint32 f1801, sint32 f1802, sint32 f1803, sint32 f1804, sint32 f1805,
sint32 f1806, sint32 f1807, sint32 f1808, sint32 f1809, sint32 f1810,
sint32 f1811, sint32 f1812, sint32 f1813, sint32 f1814, sint32 f1815,
sint32 f1816, sint32 f1817, sint32 f1818, sint32 f1819, sint32 f1820,
sint32 f1821, sint32 f1822, sint32 f1823, sint32 f1824, sint32 f1825,
sint32 f1826, sint32 f1827, sint32 f1828, sint32 f1829, sint32 f1830,
sint32 f1831, sint32 f2000, sint32 f2001, sint32 f2002, sint32 f2003,
sint32 f2004, sint32 f2005, sint32 f2006, sint32 f2007, sint32 f2008,
sint32 f2009, sint32 f2010, sint32 f2011, sint32 f2012, sint32 f2013,
sint32 f2014, sint32 f2015, sint32 f2016, sint32 f2017, sint32 f2018,
sint32 f2019, sint32 f2020, sint32 f2021, sint32 f2022, sint32 f2023,
sint32 f2024, sint32 f2025, sint32 f2026, sint32 f2027, sint32 f2028,
sint32 f2029, sint32 f2030, sint32 f2031, sint32 f2100, sint32 f2101,
sint32 f2102, sint32 f2103, sint32 f2104, sint32 f2105, sint32 f2106,
sint32 f2107, sint32 f2108, sint32 f2109, sint32 f2110, sint32 f2111,
sint32 f2112, sint32 f2113, sint32 f2114, sint32 f2115, sint32 f2116,
sint32 f2117, sint32 f2118, sint32 f2119, sint32 f2120, sint32 f2121,
sint32 f2122, sint32 f2123, sint32 f2124, sint32 f2125, sint32 f2126,
sint32 f2127, sint32 f2128, sint32 f2129, sint32 f2130, sint32 f2131,
sint32 f2200, sint32 f2201, sint32 f2202, sint32 f2203, sint32 f2204,
sint32 f2205, sint32 f2206, sint32 f2207, sint32 f2208, sint32 f2209,
sint32 f2210, sint32 f2211, sint32 f2212, sint32 f2213, sint32 f2214,
sint32 f2215, sint32 f2216, sint32 f2217, sint32 f2218, sint32 f2219,
sint32 f2220, sint32 f2221, sint32 f2222, sint32 f2223, sint32 f2224,
sint32 f2225, sint32 f2226, sint32 f2227, sint32 f2228, sint32 f2229,
sint32 f2230, sint32 f2231, sint32 f2300, sint32 f2301, sint32 f2302,
sint32 f2303, sint32 f2304, sint32 f2305, sint32 f2306, sint32 f2307,
sint32 f2308, sint32 f2309, sint32 f2310, sint32 f2311, sint32 f2312,
sint32 f2313, sint32 f2314, sint32 f2315, sint32 f2316, sint32 f2317,
sint32 f2318, sint32 f2319, sint32 f2320, sint32 f2321, sint32 f2322,
sint32 f2323, sint32 f2324, sint32 f2325, sint32 f2326, sint32 f2327,
sint32 f2328, sint32 f2329, sint32 f2330, sint32 f2331, sint32 f2400,
sint32 f2401, sint32 f2402, sint32 f2403, sint32 f2404, sint32 f2405,
sint32 f2406, sint32 f2407, sint32 f2408, sint32 f2409, sint32 f2410,
sint32 f2411, sint32 f2412, sint32 f2413, sint32 f2414, sint32 f2415,
sint32 f2416, sint32 f2417, sint32 f2418, sint32 f2419, sint32 f2420,
sint32 f2421, sint32 f2422, sint32 f2423, sint32 f2424, sint32 f2425,
sint32 f2426, sint32 f2427, sint32 f2428, sint32 f2429, sint32 f2430,
sint32 f2431, sint32 f2500, sint32 f2501, sint32 f2502, sint32 f2503,
sint32 f2504, sint32 f2505, sint32 f2506, sint32 f2507, sint32 f2508,
sint32 f2509, sint32 f2510, sint32 f2511, sint32 f2512, sint32 f2513,
sint32 f2514, sint32 f2515, sint32 f2516, sint32 f2517, sint32 f2518,
sint32 f2519, sint32 f2520, sint32 f2521, sint32 f2522, sint32 f2523,
sint32 f2524, sint32 f2525, sint32 f2526, sint32 f2527, sint32 f2528,
sint32 f2529, sint32 f2530, sint32 f2531, sint32 f2600, sint32 f2601,
sint32 f2602, sint32 f2603, sint32 f2604, sint32 f2605, sint32 f2606,
sint32 f2607, sint32 f2608, sint32 f2609, sint32 f2610, sint32 f2611,
sint32 f2612, sint32 f2613, sint32 f2614, sint32 f2615, sint32 f2616,
sint32 f2617, sint32 f2618, sint32 f2619, sint32 f2620, sint32 f2621,
sint32 f2622, sint32 f2623, sint32 f2624, sint32 f2625, sint32 f2626,
sint32 f2627, sint32 f2628, sint32 f2629, sint32 f2630, sint32 f2631,
sint32 f2700, sint32 f2701, sint32 f2702, sint32 f2703, sint32 f2704,
sint32 f2705, sint32 f2706, sint32 f2707, sint32 f2708, sint32 f2709,
sint32 f2710, sint32 f2711, sint32 f2712, sint32 f2713, sint32 f2714,
sint32 f2715, sint32 f2716, sint32 f2717, sint32 f2718, sint32 f2719,
sint32 f2720, sint32 f2721, sint32 f2722, sint32 f2723, sint32 f2724,
sint32 f2725, sint32 f2726, sint32 f2727, sint32 f2728, sint32 f2729,
sint32 f2730, sint32 f2731, sint32 f2800, sint32 f2801, sint32 f2802,
sint32 f2803, sint32 f2804, sint32 f2805, sint32 f2806, sint32 f2807,
sint32 f2808, sint32 f2809, sint32 f2810, sint32 f2811, sint32 f2812,
sint32 f2813, sint32 f2814, sint32 f2815, sint32 f2816, sint32 f2817,
sint32 f2818, sint32 f2819, sint32 f2820, sint32 f2821, sint32 f2822,
sint32 f2823, sint32 f2824, sint32 f2825, sint32 f2826, sint32 f2827,
sint32 f2828, sint32 f2829, sint32 f2830, sint32 f2831
) =
{
(******************** precondition ********************)

true && and [
0@32<=sf0000,f0000<4096@32,0@32<=sf0001,f0001<4096@32,0@32<=sf0002,f0002<4096@32,
0@32<=sf0003,f0003<4096@32,0@32<=sf0004,f0004<4096@32,0@32<=sf0005,f0005<4096@32,
0@32<=sf0006,f0006<4096@32,0@32<=sf0007,f0007<4096@32,0@32<=sf0008,f0008<4096@32,
0@32<=sf0009,f0009<4096@32,0@32<=sf0010,f0010<4096@32,0@32<=sf0011,f0011<4096@32,
0@32<=sf0012,f0012<4096@32,0@32<=sf0013,f0013<4096@32,0@32<=sf0014,f0014<4096@32,
0@32<=sf0015,f0015<4096@32,0@32<=sf0016,f0016<4096@32,0@32<=sf0017,f0017<4096@32,
0@32<=sf0018,f0018<4096@32,0@32<=sf0019,f0019<4096@32,0@32<=sf0020,f0020<4096@32,
0@32<=sf0021,f0021<4096@32,0@32<=sf0022,f0022<4096@32,0@32<=sf0023,f0023<4096@32,
0@32<=sf0024,f0024<4096@32,0@32<=sf0025,f0025<4096@32,0@32<=sf0026,f0026<4096@32,
0@32<=sf0027,f0027<4096@32,0@32<=sf0028,f0028<4096@32,0@32<=sf0029,f0029<4096@32,
0@32<=sf0030,f0030<4096@32,0@32<=sf0031,f0031<4096@32,0@32<=sf0100,f0100<4096@32,
0@32<=sf0101,f0101<4096@32,0@32<=sf0102,f0102<4096@32,0@32<=sf0103,f0103<4096@32,
0@32<=sf0104,f0104<4096@32,0@32<=sf0105,f0105<4096@32,0@32<=sf0106,f0106<4096@32,
0@32<=sf0107,f0107<4096@32,0@32<=sf0108,f0108<4096@32,0@32<=sf0109,f0109<4096@32,
0@32<=sf0110,f0110<4096@32,0@32<=sf0111,f0111<4096@32,0@32<=sf0112,f0112<4096@32,
0@32<=sf0113,f0113<4096@32,0@32<=sf0114,f0114<4096@32,0@32<=sf0115,f0115<4096@32,
0@32<=sf0116,f0116<4096@32,0@32<=sf0117,f0117<4096@32,0@32<=sf0118,f0118<4096@32,
0@32<=sf0119,f0119<4096@32,0@32<=sf0120,f0120<4096@32,0@32<=sf0121,f0121<4096@32,
0@32<=sf0122,f0122<4096@32,0@32<=sf0123,f0123<4096@32,0@32<=sf0124,f0124<4096@32,
0@32<=sf0125,f0125<4096@32,0@32<=sf0126,f0126<4096@32,0@32<=sf0127,f0127<4096@32,
0@32<=sf0128,f0128<4096@32,0@32<=sf0129,f0129<4096@32,0@32<=sf0130,f0130<4096@32,
0@32<=sf0131,f0131<4096@32,0@32<=sf0200,f0200<4096@32,0@32<=sf0201,f0201<4096@32,
0@32<=sf0202,f0202<4096@32,0@32<=sf0203,f0203<4096@32,0@32<=sf0204,f0204<4096@32,
0@32<=sf0205,f0205<4096@32,0@32<=sf0206,f0206<4096@32,0@32<=sf0207,f0207<4096@32,
0@32<=sf0208,f0208<4096@32,0@32<=sf0209,f0209<4096@32,0@32<=sf0210,f0210<4096@32,
0@32<=sf0211,f0211<4096@32,0@32<=sf0212,f0212<4096@32,0@32<=sf0213,f0213<4096@32,
0@32<=sf0214,f0214<4096@32,0@32<=sf0215,f0215<4096@32,0@32<=sf0216,f0216<4096@32,
0@32<=sf0217,f0217<4096@32,0@32<=sf0218,f0218<4096@32,0@32<=sf0219,f0219<4096@32,
0@32<=sf0220,f0220<4096@32,0@32<=sf0221,f0221<4096@32,0@32<=sf0222,f0222<4096@32,
0@32<=sf0223,f0223<4096@32,0@32<=sf0224,f0224<4096@32,0@32<=sf0225,f0225<4096@32,
0@32<=sf0226,f0226<4096@32,0@32<=sf0227,f0227<4096@32,0@32<=sf0228,f0228<4096@32,
0@32<=sf0229,f0229<4096@32,0@32<=sf0230,f0230<4096@32,0@32<=sf0231,f0231<4096@32,
0@32<=sf0300,f0300<4096@32,0@32<=sf0301,f0301<4096@32,0@32<=sf0302,f0302<4096@32,
0@32<=sf0303,f0303<4096@32,0@32<=sf0304,f0304<4096@32,0@32<=sf0305,f0305<4096@32,
0@32<=sf0306,f0306<4096@32,0@32<=sf0307,f0307<4096@32,0@32<=sf0308,f0308<4096@32,
0@32<=sf0309,f0309<4096@32,0@32<=sf0310,f0310<4096@32,0@32<=sf0311,f0311<4096@32,
0@32<=sf0312,f0312<4096@32,0@32<=sf0313,f0313<4096@32,0@32<=sf0314,f0314<4096@32,
0@32<=sf0315,f0315<4096@32,0@32<=sf0316,f0316<4096@32,0@32<=sf0317,f0317<4096@32,
0@32<=sf0318,f0318<4096@32,0@32<=sf0319,f0319<4096@32,0@32<=sf0320,f0320<4096@32,
0@32<=sf0321,f0321<4096@32,0@32<=sf0322,f0322<4096@32,0@32<=sf0323,f0323<4096@32,
0@32<=sf0324,f0324<4096@32,0@32<=sf0325,f0325<4096@32,0@32<=sf0326,f0326<4096@32,
0@32<=sf0327,f0327<4096@32,0@32<=sf0328,f0328<4096@32,0@32<=sf0329,f0329<4096@32,
0@32<=sf0330,f0330<4096@32,0@32<=sf0331,f0331<4096@32,0@32<=sf0400,f0400<4096@32,
0@32<=sf0401,f0401<4096@32,0@32<=sf0402,f0402<4096@32,0@32<=sf0403,f0403<4096@32,
0@32<=sf0404,f0404<4096@32,0@32<=sf0405,f0405<4096@32,0@32<=sf0406,f0406<4096@32,
0@32<=sf0407,f0407<4096@32,0@32<=sf0408,f0408<4096@32,0@32<=sf0409,f0409<4096@32,
0@32<=sf0410,f0410<4096@32,0@32<=sf0411,f0411<4096@32,0@32<=sf0412,f0412<4096@32,
0@32<=sf0413,f0413<4096@32,0@32<=sf0414,f0414<4096@32,0@32<=sf0415,f0415<4096@32,
0@32<=sf0416,f0416<4096@32,0@32<=sf0417,f0417<4096@32,0@32<=sf0418,f0418<4096@32,
0@32<=sf0419,f0419<4096@32,0@32<=sf0420,f0420<4096@32,0@32<=sf0421,f0421<4096@32,
0@32<=sf0422,f0422<4096@32,0@32<=sf0423,f0423<4096@32,0@32<=sf0424,f0424<4096@32,
0@32<=sf0425,f0425<4096@32,0@32<=sf0426,f0426<4096@32,0@32<=sf0427,f0427<4096@32,
0@32<=sf0428,f0428<4096@32,0@32<=sf0429,f0429<4096@32,0@32<=sf0430,f0430<4096@32,
0@32<=sf0431,f0431<4096@32,0@32<=sf0500,f0500<4096@32,0@32<=sf0501,f0501<4096@32,
0@32<=sf0502,f0502<4096@32,0@32<=sf0503,f0503<4096@32,0@32<=sf0504,f0504<4096@32,
0@32<=sf0505,f0505<4096@32,0@32<=sf0506,f0506<4096@32,0@32<=sf0507,f0507<4096@32,
0@32<=sf0508,f0508<4096@32,0@32<=sf0509,f0509<4096@32,0@32<=sf0510,f0510<4096@32,
0@32<=sf0511,f0511<4096@32,0@32<=sf0512,f0512<4096@32,0@32<=sf0513,f0513<4096@32,
0@32<=sf0514,f0514<4096@32,0@32<=sf0515,f0515<4096@32,0@32<=sf0516,f0516<4096@32,
0@32<=sf0517,f0517<4096@32,0@32<=sf0518,f0518<4096@32,0@32<=sf0519,f0519<4096@32,
0@32<=sf0520,f0520<4096@32,0@32<=sf0521,f0521<4096@32,0@32<=sf0522,f0522<4096@32,
0@32<=sf0523,f0523<4096@32,0@32<=sf0524,f0524<4096@32,0@32<=sf0525,f0525<4096@32,
0@32<=sf0526,f0526<4096@32,0@32<=sf0527,f0527<4096@32,0@32<=sf0528,f0528<4096@32,
0@32<=sf0529,f0529<4096@32,0@32<=sf0530,f0530<4096@32,0@32<=sf0531,f0531<4096@32,
0@32<=sf0600,f0600<4096@32,0@32<=sf0601,f0601<4096@32,0@32<=sf0602,f0602<4096@32,
0@32<=sf0603,f0603<4096@32,0@32<=sf0604,f0604<4096@32,0@32<=sf0605,f0605<4096@32,
0@32<=sf0606,f0606<4096@32,0@32<=sf0607,f0607<4096@32,0@32<=sf0608,f0608<4096@32,
0@32<=sf0609,f0609<4096@32,0@32<=sf0610,f0610<4096@32,0@32<=sf0611,f0611<4096@32,
0@32<=sf0612,f0612<4096@32,0@32<=sf0613,f0613<4096@32,0@32<=sf0614,f0614<4096@32,
0@32<=sf0615,f0615<4096@32,0@32<=sf0616,f0616<4096@32,0@32<=sf0617,f0617<4096@32,
0@32<=sf0618,f0618<4096@32,0@32<=sf0619,f0619<4096@32,0@32<=sf0620,f0620<4096@32,
0@32<=sf0621,f0621<4096@32,0@32<=sf0622,f0622<4096@32,0@32<=sf0623,f0623<4096@32,
0@32<=sf0624,f0624<4096@32,0@32<=sf0625,f0625<4096@32,0@32<=sf0626,f0626<4096@32,
0@32<=sf0627,f0627<4096@32,0@32<=sf0628,f0628<4096@32,0@32<=sf0629,f0629<4096@32,
0@32<=sf0630,f0630<4096@32,0@32<=sf0631,f0631<4096@32,0@32<=sf0700,f0700<4096@32,
0@32<=sf0701,f0701<4096@32,0@32<=sf0702,f0702<4096@32,0@32<=sf0703,f0703<4096@32,
0@32<=sf0704,f0704<4096@32,0@32<=sf0705,f0705<4096@32,0@32<=sf0706,f0706<4096@32,
0@32<=sf0707,f0707<4096@32,0@32<=sf0708,f0708<4096@32,0@32<=sf0709,f0709<4096@32,
0@32<=sf0710,f0710<4096@32,0@32<=sf0711,f0711<4096@32,0@32<=sf0712,f0712<4096@32,
0@32<=sf0713,f0713<4096@32,0@32<=sf0714,f0714<4096@32,0@32<=sf0715,f0715<4096@32,
0@32<=sf0716,f0716<4096@32,0@32<=sf0717,f0717<4096@32,0@32<=sf0718,f0718<4096@32,
0@32<=sf0719,f0719<4096@32,0@32<=sf0720,f0720<4096@32,0@32<=sf0721,f0721<4096@32,
0@32<=sf0722,f0722<4096@32,0@32<=sf0723,f0723<4096@32,0@32<=sf0724,f0724<4096@32,
0@32<=sf0725,f0725<4096@32,0@32<=sf0726,f0726<4096@32,0@32<=sf0727,f0727<4096@32,
0@32<=sf0728,f0728<4096@32,0@32<=sf0729,f0729<4096@32,0@32<=sf0730,f0730<4096@32,
0@32<=sf0731,f0731<4096@32,0@32<=sf0800,f0800<4096@32,0@32<=sf0801,f0801<4096@32,
0@32<=sf0802,f0802<4096@32,0@32<=sf0803,f0803<4096@32,0@32<=sf0804,f0804<4096@32,
0@32<=sf0805,f0805<4096@32,0@32<=sf0806,f0806<4096@32,0@32<=sf0807,f0807<4096@32,
0@32<=sf0808,f0808<4096@32,0@32<=sf0809,f0809<4096@32,0@32<=sf0810,f0810<4096@32,
0@32<=sf0811,f0811<4096@32,0@32<=sf0812,f0812<4096@32,0@32<=sf0813,f0813<4096@32,
0@32<=sf0814,f0814<4096@32,0@32<=sf0815,f0815<4096@32,0@32<=sf0816,f0816<4096@32,
0@32<=sf0817,f0817<4096@32,0@32<=sf0818,f0818<4096@32,0@32<=sf0819,f0819<4096@32,
0@32<=sf0820,f0820<4096@32,0@32<=sf0821,f0821<4096@32,0@32<=sf0822,f0822<4096@32,
0@32<=sf0823,f0823<4096@32,0@32<=sf0824,f0824<4096@32,0@32<=sf0825,f0825<4096@32,
0@32<=sf0826,f0826<4096@32,0@32<=sf0827,f0827<4096@32,0@32<=sf0828,f0828<4096@32,
0@32<=sf0829,f0829<4096@32,0@32<=sf0830,f0830<4096@32,0@32<=sf0831,f0831<4096@32,
0@32<=sf1000,f1000<4096@32,0@32<=sf1001,f1001<4096@32,0@32<=sf1002,f1002<4096@32,
0@32<=sf1003,f1003<4096@32,0@32<=sf1004,f1004<4096@32,0@32<=sf1005,f1005<4096@32,
0@32<=sf1006,f1006<4096@32,0@32<=sf1007,f1007<4096@32,0@32<=sf1008,f1008<4096@32,
0@32<=sf1009,f1009<4096@32,0@32<=sf1010,f1010<4096@32,0@32<=sf1011,f1011<4096@32,
0@32<=sf1012,f1012<4096@32,0@32<=sf1013,f1013<4096@32,0@32<=sf1014,f1014<4096@32,
0@32<=sf1015,f1015<4096@32,0@32<=sf1016,f1016<4096@32,0@32<=sf1017,f1017<4096@32,
0@32<=sf1018,f1018<4096@32,0@32<=sf1019,f1019<4096@32,0@32<=sf1020,f1020<4096@32,
0@32<=sf1021,f1021<4096@32,0@32<=sf1022,f1022<4096@32,0@32<=sf1023,f1023<4096@32,
0@32<=sf1024,f1024<4096@32,0@32<=sf1025,f1025<4096@32,0@32<=sf1026,f1026<4096@32,
0@32<=sf1027,f1027<4096@32,0@32<=sf1028,f1028<4096@32,0@32<=sf1029,f1029<4096@32,
0@32<=sf1030,f1030<4096@32,0@32<=sf1031,f1031<4096@32,0@32<=sf1100,f1100<4096@32,
0@32<=sf1101,f1101<4096@32,0@32<=sf1102,f1102<4096@32,0@32<=sf1103,f1103<4096@32,
0@32<=sf1104,f1104<4096@32,0@32<=sf1105,f1105<4096@32,0@32<=sf1106,f1106<4096@32,
0@32<=sf1107,f1107<4096@32,0@32<=sf1108,f1108<4096@32,0@32<=sf1109,f1109<4096@32,
0@32<=sf1110,f1110<4096@32,0@32<=sf1111,f1111<4096@32,0@32<=sf1112,f1112<4096@32,
0@32<=sf1113,f1113<4096@32,0@32<=sf1114,f1114<4096@32,0@32<=sf1115,f1115<4096@32,
0@32<=sf1116,f1116<4096@32,0@32<=sf1117,f1117<4096@32,0@32<=sf1118,f1118<4096@32,
0@32<=sf1119,f1119<4096@32,0@32<=sf1120,f1120<4096@32,0@32<=sf1121,f1121<4096@32,
0@32<=sf1122,f1122<4096@32,0@32<=sf1123,f1123<4096@32,0@32<=sf1124,f1124<4096@32,
0@32<=sf1125,f1125<4096@32,0@32<=sf1126,f1126<4096@32,0@32<=sf1127,f1127<4096@32,
0@32<=sf1128,f1128<4096@32,0@32<=sf1129,f1129<4096@32,0@32<=sf1130,f1130<4096@32,
0@32<=sf1131,f1131<4096@32,0@32<=sf1200,f1200<4096@32,0@32<=sf1201,f1201<4096@32,
0@32<=sf1202,f1202<4096@32,0@32<=sf1203,f1203<4096@32,0@32<=sf1204,f1204<4096@32,
0@32<=sf1205,f1205<4096@32,0@32<=sf1206,f1206<4096@32,0@32<=sf1207,f1207<4096@32,
0@32<=sf1208,f1208<4096@32,0@32<=sf1209,f1209<4096@32,0@32<=sf1210,f1210<4096@32,
0@32<=sf1211,f1211<4096@32,0@32<=sf1212,f1212<4096@32,0@32<=sf1213,f1213<4096@32,
0@32<=sf1214,f1214<4096@32,0@32<=sf1215,f1215<4096@32,0@32<=sf1216,f1216<4096@32,
0@32<=sf1217,f1217<4096@32,0@32<=sf1218,f1218<4096@32,0@32<=sf1219,f1219<4096@32,
0@32<=sf1220,f1220<4096@32,0@32<=sf1221,f1221<4096@32,0@32<=sf1222,f1222<4096@32,
0@32<=sf1223,f1223<4096@32,0@32<=sf1224,f1224<4096@32,0@32<=sf1225,f1225<4096@32,
0@32<=sf1226,f1226<4096@32,0@32<=sf1227,f1227<4096@32,0@32<=sf1228,f1228<4096@32,
0@32<=sf1229,f1229<4096@32,0@32<=sf1230,f1230<4096@32,0@32<=sf1231,f1231<4096@32,
0@32<=sf1300,f1300<4096@32,0@32<=sf1301,f1301<4096@32,0@32<=sf1302,f1302<4096@32,
0@32<=sf1303,f1303<4096@32,0@32<=sf1304,f1304<4096@32,0@32<=sf1305,f1305<4096@32,
0@32<=sf1306,f1306<4096@32,0@32<=sf1307,f1307<4096@32,0@32<=sf1308,f1308<4096@32,
0@32<=sf1309,f1309<4096@32,0@32<=sf1310,f1310<4096@32,0@32<=sf1311,f1311<4096@32,
0@32<=sf1312,f1312<4096@32,0@32<=sf1313,f1313<4096@32,0@32<=sf1314,f1314<4096@32,
0@32<=sf1315,f1315<4096@32,0@32<=sf1316,f1316<4096@32,0@32<=sf1317,f1317<4096@32,
0@32<=sf1318,f1318<4096@32,0@32<=sf1319,f1319<4096@32,0@32<=sf1320,f1320<4096@32,
0@32<=sf1321,f1321<4096@32,0@32<=sf1322,f1322<4096@32,0@32<=sf1323,f1323<4096@32,
0@32<=sf1324,f1324<4096@32,0@32<=sf1325,f1325<4096@32,0@32<=sf1326,f1326<4096@32,
0@32<=sf1327,f1327<4096@32,0@32<=sf1328,f1328<4096@32,0@32<=sf1329,f1329<4096@32,
0@32<=sf1330,f1330<4096@32,0@32<=sf1331,f1331<4096@32,0@32<=sf1400,f1400<4096@32,
0@32<=sf1401,f1401<4096@32,0@32<=sf1402,f1402<4096@32,0@32<=sf1403,f1403<4096@32,
0@32<=sf1404,f1404<4096@32,0@32<=sf1405,f1405<4096@32,0@32<=sf1406,f1406<4096@32,
0@32<=sf1407,f1407<4096@32,0@32<=sf1408,f1408<4096@32,0@32<=sf1409,f1409<4096@32,
0@32<=sf1410,f1410<4096@32,0@32<=sf1411,f1411<4096@32,0@32<=sf1412,f1412<4096@32,
0@32<=sf1413,f1413<4096@32,0@32<=sf1414,f1414<4096@32,0@32<=sf1415,f1415<4096@32,
0@32<=sf1416,f1416<4096@32,0@32<=sf1417,f1417<4096@32,0@32<=sf1418,f1418<4096@32,
0@32<=sf1419,f1419<4096@32,0@32<=sf1420,f1420<4096@32,0@32<=sf1421,f1421<4096@32,
0@32<=sf1422,f1422<4096@32,0@32<=sf1423,f1423<4096@32,0@32<=sf1424,f1424<4096@32,
0@32<=sf1425,f1425<4096@32,0@32<=sf1426,f1426<4096@32,0@32<=sf1427,f1427<4096@32,
0@32<=sf1428,f1428<4096@32,0@32<=sf1429,f1429<4096@32,0@32<=sf1430,f1430<4096@32,
0@32<=sf1431,f1431<4096@32,0@32<=sf1500,f1500<4096@32,0@32<=sf1501,f1501<4096@32,
0@32<=sf1502,f1502<4096@32,0@32<=sf1503,f1503<4096@32,0@32<=sf1504,f1504<4096@32,
0@32<=sf1505,f1505<4096@32,0@32<=sf1506,f1506<4096@32,0@32<=sf1507,f1507<4096@32,
0@32<=sf1508,f1508<4096@32,0@32<=sf1509,f1509<4096@32,0@32<=sf1510,f1510<4096@32,
0@32<=sf1511,f1511<4096@32,0@32<=sf1512,f1512<4096@32,0@32<=sf1513,f1513<4096@32,
0@32<=sf1514,f1514<4096@32,0@32<=sf1515,f1515<4096@32,0@32<=sf1516,f1516<4096@32,
0@32<=sf1517,f1517<4096@32,0@32<=sf1518,f1518<4096@32,0@32<=sf1519,f1519<4096@32,
0@32<=sf1520,f1520<4096@32,0@32<=sf1521,f1521<4096@32,0@32<=sf1522,f1522<4096@32,
0@32<=sf1523,f1523<4096@32,0@32<=sf1524,f1524<4096@32,0@32<=sf1525,f1525<4096@32,
0@32<=sf1526,f1526<4096@32,0@32<=sf1527,f1527<4096@32,0@32<=sf1528,f1528<4096@32,
0@32<=sf1529,f1529<4096@32,0@32<=sf1530,f1530<4096@32,0@32<=sf1531,f1531<4096@32,
0@32<=sf1600,f1600<4096@32,0@32<=sf1601,f1601<4096@32,0@32<=sf1602,f1602<4096@32,
0@32<=sf1603,f1603<4096@32,0@32<=sf1604,f1604<4096@32,0@32<=sf1605,f1605<4096@32,
0@32<=sf1606,f1606<4096@32,0@32<=sf1607,f1607<4096@32,0@32<=sf1608,f1608<4096@32,
0@32<=sf1609,f1609<4096@32,0@32<=sf1610,f1610<4096@32,0@32<=sf1611,f1611<4096@32,
0@32<=sf1612,f1612<4096@32,0@32<=sf1613,f1613<4096@32,0@32<=sf1614,f1614<4096@32,
0@32<=sf1615,f1615<4096@32,0@32<=sf1616,f1616<4096@32,0@32<=sf1617,f1617<4096@32,
0@32<=sf1618,f1618<4096@32,0@32<=sf1619,f1619<4096@32,0@32<=sf1620,f1620<4096@32,
0@32<=sf1621,f1621<4096@32,0@32<=sf1622,f1622<4096@32,0@32<=sf1623,f1623<4096@32,
0@32<=sf1624,f1624<4096@32,0@32<=sf1625,f1625<4096@32,0@32<=sf1626,f1626<4096@32,
0@32<=sf1627,f1627<4096@32,0@32<=sf1628,f1628<4096@32,0@32<=sf1629,f1629<4096@32,
0@32<=sf1630,f1630<4096@32,0@32<=sf1631,f1631<4096@32,0@32<=sf1700,f1700<4096@32,
0@32<=sf1701,f1701<4096@32,0@32<=sf1702,f1702<4096@32,0@32<=sf1703,f1703<4096@32,
0@32<=sf1704,f1704<4096@32,0@32<=sf1705,f1705<4096@32,0@32<=sf1706,f1706<4096@32,
0@32<=sf1707,f1707<4096@32,0@32<=sf1708,f1708<4096@32,0@32<=sf1709,f1709<4096@32,
0@32<=sf1710,f1710<4096@32,0@32<=sf1711,f1711<4096@32,0@32<=sf1712,f1712<4096@32,
0@32<=sf1713,f1713<4096@32,0@32<=sf1714,f1714<4096@32,0@32<=sf1715,f1715<4096@32,
0@32<=sf1716,f1716<4096@32,0@32<=sf1717,f1717<4096@32,0@32<=sf1718,f1718<4096@32,
0@32<=sf1719,f1719<4096@32,0@32<=sf1720,f1720<4096@32,0@32<=sf1721,f1721<4096@32,
0@32<=sf1722,f1722<4096@32,0@32<=sf1723,f1723<4096@32,0@32<=sf1724,f1724<4096@32,
0@32<=sf1725,f1725<4096@32,0@32<=sf1726,f1726<4096@32,0@32<=sf1727,f1727<4096@32,
0@32<=sf1728,f1728<4096@32,0@32<=sf1729,f1729<4096@32,0@32<=sf1730,f1730<4096@32,
0@32<=sf1731,f1731<4096@32,0@32<=sf1800,f1800<4096@32,0@32<=sf1801,f1801<4096@32,
0@32<=sf1802,f1802<4096@32,0@32<=sf1803,f1803<4096@32,0@32<=sf1804,f1804<4096@32,
0@32<=sf1805,f1805<4096@32,0@32<=sf1806,f1806<4096@32,0@32<=sf1807,f1807<4096@32,
0@32<=sf1808,f1808<4096@32,0@32<=sf1809,f1809<4096@32,0@32<=sf1810,f1810<4096@32,
0@32<=sf1811,f1811<4096@32,0@32<=sf1812,f1812<4096@32,0@32<=sf1813,f1813<4096@32,
0@32<=sf1814,f1814<4096@32,0@32<=sf1815,f1815<4096@32,0@32<=sf1816,f1816<4096@32,
0@32<=sf1817,f1817<4096@32,0@32<=sf1818,f1818<4096@32,0@32<=sf1819,f1819<4096@32,
0@32<=sf1820,f1820<4096@32,0@32<=sf1821,f1821<4096@32,0@32<=sf1822,f1822<4096@32,
0@32<=sf1823,f1823<4096@32,0@32<=sf1824,f1824<4096@32,0@32<=sf1825,f1825<4096@32,
0@32<=sf1826,f1826<4096@32,0@32<=sf1827,f1827<4096@32,0@32<=sf1828,f1828<4096@32,
0@32<=sf1829,f1829<4096@32,0@32<=sf1830,f1830<4096@32,0@32<=sf1831,f1831<4096@32,
0@32<=sf2000,f2000<4096@32,0@32<=sf2001,f2001<4096@32,0@32<=sf2002,f2002<4096@32,
0@32<=sf2003,f2003<4096@32,0@32<=sf2004,f2004<4096@32,0@32<=sf2005,f2005<4096@32,
0@32<=sf2006,f2006<4096@32,0@32<=sf2007,f2007<4096@32,0@32<=sf2008,f2008<4096@32,
0@32<=sf2009,f2009<4096@32,0@32<=sf2010,f2010<4096@32,0@32<=sf2011,f2011<4096@32,
0@32<=sf2012,f2012<4096@32,0@32<=sf2013,f2013<4096@32,0@32<=sf2014,f2014<4096@32,
0@32<=sf2015,f2015<4096@32,0@32<=sf2016,f2016<4096@32,0@32<=sf2017,f2017<4096@32,
0@32<=sf2018,f2018<4096@32,0@32<=sf2019,f2019<4096@32,0@32<=sf2020,f2020<4096@32,
0@32<=sf2021,f2021<4096@32,0@32<=sf2022,f2022<4096@32,0@32<=sf2023,f2023<4096@32,
0@32<=sf2024,f2024<4096@32,0@32<=sf2025,f2025<4096@32,0@32<=sf2026,f2026<4096@32,
0@32<=sf2027,f2027<4096@32,0@32<=sf2028,f2028<4096@32,0@32<=sf2029,f2029<4096@32,
0@32<=sf2030,f2030<4096@32,0@32<=sf2031,f2031<4096@32,0@32<=sf2100,f2100<4096@32,
0@32<=sf2101,f2101<4096@32,0@32<=sf2102,f2102<4096@32,0@32<=sf2103,f2103<4096@32,
0@32<=sf2104,f2104<4096@32,0@32<=sf2105,f2105<4096@32,0@32<=sf2106,f2106<4096@32,
0@32<=sf2107,f2107<4096@32,0@32<=sf2108,f2108<4096@32,0@32<=sf2109,f2109<4096@32,
0@32<=sf2110,f2110<4096@32,0@32<=sf2111,f2111<4096@32,0@32<=sf2112,f2112<4096@32,
0@32<=sf2113,f2113<4096@32,0@32<=sf2114,f2114<4096@32,0@32<=sf2115,f2115<4096@32,
0@32<=sf2116,f2116<4096@32,0@32<=sf2117,f2117<4096@32,0@32<=sf2118,f2118<4096@32,
0@32<=sf2119,f2119<4096@32,0@32<=sf2120,f2120<4096@32,0@32<=sf2121,f2121<4096@32,
0@32<=sf2122,f2122<4096@32,0@32<=sf2123,f2123<4096@32,0@32<=sf2124,f2124<4096@32,
0@32<=sf2125,f2125<4096@32,0@32<=sf2126,f2126<4096@32,0@32<=sf2127,f2127<4096@32,
0@32<=sf2128,f2128<4096@32,0@32<=sf2129,f2129<4096@32,0@32<=sf2130,f2130<4096@32,
0@32<=sf2131,f2131<4096@32,0@32<=sf2200,f2200<4096@32,0@32<=sf2201,f2201<4096@32,
0@32<=sf2202,f2202<4096@32,0@32<=sf2203,f2203<4096@32,0@32<=sf2204,f2204<4096@32,
0@32<=sf2205,f2205<4096@32,0@32<=sf2206,f2206<4096@32,0@32<=sf2207,f2207<4096@32,
0@32<=sf2208,f2208<4096@32,0@32<=sf2209,f2209<4096@32,0@32<=sf2210,f2210<4096@32,
0@32<=sf2211,f2211<4096@32,0@32<=sf2212,f2212<4096@32,0@32<=sf2213,f2213<4096@32,
0@32<=sf2214,f2214<4096@32,0@32<=sf2215,f2215<4096@32,0@32<=sf2216,f2216<4096@32,
0@32<=sf2217,f2217<4096@32,0@32<=sf2218,f2218<4096@32,0@32<=sf2219,f2219<4096@32,
0@32<=sf2220,f2220<4096@32,0@32<=sf2221,f2221<4096@32,0@32<=sf2222,f2222<4096@32,
0@32<=sf2223,f2223<4096@32,0@32<=sf2224,f2224<4096@32,0@32<=sf2225,f2225<4096@32,
0@32<=sf2226,f2226<4096@32,0@32<=sf2227,f2227<4096@32,0@32<=sf2228,f2228<4096@32,
0@32<=sf2229,f2229<4096@32,0@32<=sf2230,f2230<4096@32,0@32<=sf2231,f2231<4096@32,
0@32<=sf2300,f2300<4096@32,0@32<=sf2301,f2301<4096@32,0@32<=sf2302,f2302<4096@32,
0@32<=sf2303,f2303<4096@32,0@32<=sf2304,f2304<4096@32,0@32<=sf2305,f2305<4096@32,
0@32<=sf2306,f2306<4096@32,0@32<=sf2307,f2307<4096@32,0@32<=sf2308,f2308<4096@32,
0@32<=sf2309,f2309<4096@32,0@32<=sf2310,f2310<4096@32,0@32<=sf2311,f2311<4096@32,
0@32<=sf2312,f2312<4096@32,0@32<=sf2313,f2313<4096@32,0@32<=sf2314,f2314<4096@32,
0@32<=sf2315,f2315<4096@32,0@32<=sf2316,f2316<4096@32,0@32<=sf2317,f2317<4096@32,
0@32<=sf2318,f2318<4096@32,0@32<=sf2319,f2319<4096@32,0@32<=sf2320,f2320<4096@32,
0@32<=sf2321,f2321<4096@32,0@32<=sf2322,f2322<4096@32,0@32<=sf2323,f2323<4096@32,
0@32<=sf2324,f2324<4096@32,0@32<=sf2325,f2325<4096@32,0@32<=sf2326,f2326<4096@32,
0@32<=sf2327,f2327<4096@32,0@32<=sf2328,f2328<4096@32,0@32<=sf2329,f2329<4096@32,
0@32<=sf2330,f2330<4096@32,0@32<=sf2331,f2331<4096@32,0@32<=sf2400,f2400<4096@32,
0@32<=sf2401,f2401<4096@32,0@32<=sf2402,f2402<4096@32,0@32<=sf2403,f2403<4096@32,
0@32<=sf2404,f2404<4096@32,0@32<=sf2405,f2405<4096@32,0@32<=sf2406,f2406<4096@32,
0@32<=sf2407,f2407<4096@32,0@32<=sf2408,f2408<4096@32,0@32<=sf2409,f2409<4096@32,
0@32<=sf2410,f2410<4096@32,0@32<=sf2411,f2411<4096@32,0@32<=sf2412,f2412<4096@32,
0@32<=sf2413,f2413<4096@32,0@32<=sf2414,f2414<4096@32,0@32<=sf2415,f2415<4096@32,
0@32<=sf2416,f2416<4096@32,0@32<=sf2417,f2417<4096@32,0@32<=sf2418,f2418<4096@32,
0@32<=sf2419,f2419<4096@32,0@32<=sf2420,f2420<4096@32,0@32<=sf2421,f2421<4096@32,
0@32<=sf2422,f2422<4096@32,0@32<=sf2423,f2423<4096@32,0@32<=sf2424,f2424<4096@32,
0@32<=sf2425,f2425<4096@32,0@32<=sf2426,f2426<4096@32,0@32<=sf2427,f2427<4096@32,
0@32<=sf2428,f2428<4096@32,0@32<=sf2429,f2429<4096@32,0@32<=sf2430,f2430<4096@32,
0@32<=sf2431,f2431<4096@32,0@32<=sf2500,f2500<4096@32,0@32<=sf2501,f2501<4096@32,
0@32<=sf2502,f2502<4096@32,0@32<=sf2503,f2503<4096@32,0@32<=sf2504,f2504<4096@32,
0@32<=sf2505,f2505<4096@32,0@32<=sf2506,f2506<4096@32,0@32<=sf2507,f2507<4096@32,
0@32<=sf2508,f2508<4096@32,0@32<=sf2509,f2509<4096@32,0@32<=sf2510,f2510<4096@32,
0@32<=sf2511,f2511<4096@32,0@32<=sf2512,f2512<4096@32,0@32<=sf2513,f2513<4096@32,
0@32<=sf2514,f2514<4096@32,0@32<=sf2515,f2515<4096@32,0@32<=sf2516,f2516<4096@32,
0@32<=sf2517,f2517<4096@32,0@32<=sf2518,f2518<4096@32,0@32<=sf2519,f2519<4096@32,
0@32<=sf2520,f2520<4096@32,0@32<=sf2521,f2521<4096@32,0@32<=sf2522,f2522<4096@32,
0@32<=sf2523,f2523<4096@32,0@32<=sf2524,f2524<4096@32,0@32<=sf2525,f2525<4096@32,
0@32<=sf2526,f2526<4096@32,0@32<=sf2527,f2527<4096@32,0@32<=sf2528,f2528<4096@32,
0@32<=sf2529,f2529<4096@32,0@32<=sf2530,f2530<4096@32,0@32<=sf2531,f2531<4096@32,
0@32<=sf2600,f2600<4096@32,0@32<=sf2601,f2601<4096@32,0@32<=sf2602,f2602<4096@32,
0@32<=sf2603,f2603<4096@32,0@32<=sf2604,f2604<4096@32,0@32<=sf2605,f2605<4096@32,
0@32<=sf2606,f2606<4096@32,0@32<=sf2607,f2607<4096@32,0@32<=sf2608,f2608<4096@32,
0@32<=sf2609,f2609<4096@32,0@32<=sf2610,f2610<4096@32,0@32<=sf2611,f2611<4096@32,
0@32<=sf2612,f2612<4096@32,0@32<=sf2613,f2613<4096@32,0@32<=sf2614,f2614<4096@32,
0@32<=sf2615,f2615<4096@32,0@32<=sf2616,f2616<4096@32,0@32<=sf2617,f2617<4096@32,
0@32<=sf2618,f2618<4096@32,0@32<=sf2619,f2619<4096@32,0@32<=sf2620,f2620<4096@32,
0@32<=sf2621,f2621<4096@32,0@32<=sf2622,f2622<4096@32,0@32<=sf2623,f2623<4096@32,
0@32<=sf2624,f2624<4096@32,0@32<=sf2625,f2625<4096@32,0@32<=sf2626,f2626<4096@32,
0@32<=sf2627,f2627<4096@32,0@32<=sf2628,f2628<4096@32,0@32<=sf2629,f2629<4096@32,
0@32<=sf2630,f2630<4096@32,0@32<=sf2631,f2631<4096@32,0@32<=sf2700,f2700<4096@32,
0@32<=sf2701,f2701<4096@32,0@32<=sf2702,f2702<4096@32,0@32<=sf2703,f2703<4096@32,
0@32<=sf2704,f2704<4096@32,0@32<=sf2705,f2705<4096@32,0@32<=sf2706,f2706<4096@32,
0@32<=sf2707,f2707<4096@32,0@32<=sf2708,f2708<4096@32,0@32<=sf2709,f2709<4096@32,
0@32<=sf2710,f2710<4096@32,0@32<=sf2711,f2711<4096@32,0@32<=sf2712,f2712<4096@32,
0@32<=sf2713,f2713<4096@32,0@32<=sf2714,f2714<4096@32,0@32<=sf2715,f2715<4096@32,
0@32<=sf2716,f2716<4096@32,0@32<=sf2717,f2717<4096@32,0@32<=sf2718,f2718<4096@32,
0@32<=sf2719,f2719<4096@32,0@32<=sf2720,f2720<4096@32,0@32<=sf2721,f2721<4096@32,
0@32<=sf2722,f2722<4096@32,0@32<=sf2723,f2723<4096@32,0@32<=sf2724,f2724<4096@32,
0@32<=sf2725,f2725<4096@32,0@32<=sf2726,f2726<4096@32,0@32<=sf2727,f2727<4096@32,
0@32<=sf2728,f2728<4096@32,0@32<=sf2729,f2729<4096@32,0@32<=sf2730,f2730<4096@32,
0@32<=sf2731,f2731<4096@32,0@32<=sf2800,f2800<4096@32,0@32<=sf2801,f2801<4096@32,
0@32<=sf2802,f2802<4096@32,0@32<=sf2803,f2803<4096@32,0@32<=sf2804,f2804<4096@32,
0@32<=sf2805,f2805<4096@32,0@32<=sf2806,f2806<4096@32,0@32<=sf2807,f2807<4096@32,
0@32<=sf2808,f2808<4096@32,0@32<=sf2809,f2809<4096@32,0@32<=sf2810,f2810<4096@32,
0@32<=sf2811,f2811<4096@32,0@32<=sf2812,f2812<4096@32,0@32<=sf2813,f2813<4096@32,
0@32<=sf2814,f2814<4096@32,0@32<=sf2815,f2815<4096@32,0@32<=sf2816,f2816<4096@32,
0@32<=sf2817,f2817<4096@32,0@32<=sf2818,f2818<4096@32,0@32<=sf2819,f2819<4096@32,
0@32<=sf2820,f2820<4096@32,0@32<=sf2821,f2821<4096@32,0@32<=sf2822,f2822<4096@32,
0@32<=sf2823,f2823<4096@32,0@32<=sf2824,f2824<4096@32,0@32<=sf2825,f2825<4096@32,
0@32<=sf2826,f2826<4096@32,0@32<=sf2827,f2827<4096@32,0@32<=sf2828,f2828<4096@32,
0@32<=sf2829,f2829<4096@32,0@32<=sf2830,f2830<4096@32,0@32<=sf2831,f2831<4096@32
]
}

(******************** initialization ********************)

nondet r0@uint32; nondet lr@uint32;
mov L0x20018570 f0000; mov L0x20018572 f1000; mov L0x20018574 f2000;
mov L0x20018576 f0101; mov L0x20018578 f1101; mov L0x2001857a f2101;
mov L0x2001857c f0202; mov L0x2001857e f1202; mov L0x20018580 f2202;
mov L0x20018582 f0303; mov L0x20018584 f1303; mov L0x20018586 f2303;
mov L0x20018588 f0404; mov L0x2001858a f1404; mov L0x2001858c f2404;
mov L0x2001858e f0505; mov L0x20018590 f1505; mov L0x20018592 f2505;
mov L0x20018594 f0606; mov L0x20018596 f1606; mov L0x20018598 f2606;
mov L0x2001859a f0707; mov L0x2001859c f1707; mov L0x2001859e f2707;
mov L0x200185a0 f0808; mov L0x200185a2 f1808; mov L0x200185a4 f2808;
mov L0x200185a6 f0009; mov L0x200185a8 f1009; mov L0x200185aa f2009;
mov L0x200185ac f0110; mov L0x200185ae f1110; mov L0x200185b0 f2110;
mov L0x200185b2 f0211; mov L0x200185b4 f1211; mov L0x200185b6 f2211;
mov L0x200185b8 f0312; mov L0x200185ba f1312; mov L0x200185bc f2312;
mov L0x200185be f0413; mov L0x200185c0 f1413; mov L0x200185c2 f2413;
mov L0x200185c4 f0514; mov L0x200185c6 f1514; mov L0x200185c8 f2514;
mov L0x200185ca f0615; mov L0x200185cc f1615; mov L0x200185ce f2615;
mov L0x200185d0 f0716; mov L0x200185d2 f1716; mov L0x200185d4 f2716;
mov L0x200185d6 f0817; mov L0x200185d8 f1817; mov L0x200185da f2817;
mov L0x200185dc f0018; mov L0x200185de f1018; mov L0x200185e0 f2018;
mov L0x200185e2 f0119; mov L0x200185e4 f1119; mov L0x200185e6 f2119;
mov L0x200185e8 f0220; mov L0x200185ea f1220; mov L0x200185ec f2220;
mov L0x200185ee f0321; mov L0x200185f0 f1321; mov L0x200185f2 f2321;
mov L0x200185f4 f0422; mov L0x200185f6 f1422; mov L0x200185f8 f2422;
mov L0x200185fa f0523; mov L0x200185fc f1523; mov L0x200185fe f2523;
mov L0x20018600 f0624; mov L0x20018602 f1624; mov L0x20018604 f2624;
mov L0x20018606 f0725; mov L0x20018608 f1725; mov L0x2001860a f2725;
mov L0x2001860c f0826; mov L0x2001860e f1826; mov L0x20018610 f2826;
mov L0x20018612 f0027; mov L0x20018614 f1027; mov L0x20018616 f2027;
mov L0x20018618 f0128; mov L0x2001861a f1128; mov L0x2001861c f2128;
mov L0x2001861e f0229; mov L0x20018620 f1229; mov L0x20018622 f2229;
mov L0x20018624 f0330; mov L0x20018626 f1330; mov L0x20018628 f2330;
mov L0x2001862a f0431; mov L0x2001862c f1431; mov L0x2001862e f2431;
mov L0x20018630 f0500; mov L0x20018632 f1500; mov L0x20018634 f2500;
mov L0x20018636 f0601; mov L0x20018638 f1601; mov L0x2001863a f2601;
mov L0x2001863c f0702; mov L0x2001863e f1702; mov L0x20018640 f2702;
mov L0x20018642 f0803; mov L0x20018644 f1803; mov L0x20018646 f2803;
mov L0x20018648 f0004; mov L0x2001864a f1004; mov L0x2001864c f2004;
mov L0x2001864e f0105; mov L0x20018650 f1105; mov L0x20018652 f2105;
mov L0x20018654 f0206; mov L0x20018656 f1206; mov L0x20018658 f2206;
mov L0x2001865a f0307; mov L0x2001865c f1307; mov L0x2001865e f2307;
mov L0x20018660 f0408; mov L0x20018662 f1408; mov L0x20018664 f2408;
mov L0x20018666 f0509; mov L0x20018668 f1509; mov L0x2001866a f2509;
mov L0x2001866c f0610; mov L0x2001866e f1610; mov L0x20018670 f2610;
mov L0x20018672 f0711; mov L0x20018674 f1711; mov L0x20018676 f2711;
mov L0x20018678 f0812; mov L0x2001867a f1812; mov L0x2001867c f2812;
mov L0x2001867e f0013; mov L0x20018680 f1013; mov L0x20018682 f2013;
mov L0x20018684 f0114; mov L0x20018686 f1114; mov L0x20018688 f2114;
mov L0x2001868a f0215; mov L0x2001868c f1215; mov L0x2001868e f2215;
mov L0x20018690 f0316; mov L0x20018692 f1316; mov L0x20018694 f2316;
mov L0x20018696 f0417; mov L0x20018698 f1417; mov L0x2001869a f2417;
mov L0x2001869c f0518; mov L0x2001869e f1518; mov L0x200186a0 f2518;
mov L0x200186a2 f0619; mov L0x200186a4 f1619; mov L0x200186a6 f2619;
mov L0x200186a8 f0720; mov L0x200186aa f1720; mov L0x200186ac f2720;
mov L0x200186ae f0821; mov L0x200186b0 f1821; mov L0x200186b2 f2821;
mov L0x200186b4 f0022; mov L0x200186b6 f1022; mov L0x200186b8 f2022;
mov L0x200186ba f0123; mov L0x200186bc f1123; mov L0x200186be f2123;
mov L0x200186c0 f0224; mov L0x200186c2 f1224; mov L0x200186c4 f2224;
mov L0x200186c6 f0325; mov L0x200186c8 f1325; mov L0x200186ca f2325;
mov L0x200186cc f0426; mov L0x200186ce f1426; mov L0x200186d0 f2426;
mov L0x200186d2 f0527; mov L0x200186d4 f1527; mov L0x200186d6 f2527;
mov L0x200186d8 f0628; mov L0x200186da f1628; mov L0x200186dc f2628;
mov L0x200186de f0729; mov L0x200186e0 f1729; mov L0x200186e2 f2729;
mov L0x200186e4 f0830; mov L0x200186e6 f1830; mov L0x200186e8 f2830;
mov L0x200186ea f0031; mov L0x200186ec f1031; mov L0x200186ee f2031;
mov L0x200186f0 f0100; mov L0x200186f2 f1100; mov L0x200186f4 f2100;
mov L0x200186f6 f0201; mov L0x200186f8 f1201; mov L0x200186fa f2201;
mov L0x200186fc f0302; mov L0x200186fe f1302; mov L0x20018700 f2302;
mov L0x20018702 f0403; mov L0x20018704 f1403; mov L0x20018706 f2403;
mov L0x20018708 f0504; mov L0x2001870a f1504; mov L0x2001870c f2504;
mov L0x2001870e f0605; mov L0x20018710 f1605; mov L0x20018712 f2605;
mov L0x20018714 f0706; mov L0x20018716 f1706; mov L0x20018718 f2706;
mov L0x2001871a f0807; mov L0x2001871c f1807; mov L0x2001871e f2807;
mov L0x20018720 f0008; mov L0x20018722 f1008; mov L0x20018724 f2008;
mov L0x20018726 f0109; mov L0x20018728 f1109; mov L0x2001872a f2109;
mov L0x2001872c f0210; mov L0x2001872e f1210; mov L0x20018730 f2210;
mov L0x20018732 f0311; mov L0x20018734 f1311; mov L0x20018736 f2311;
mov L0x20018738 f0412; mov L0x2001873a f1412; mov L0x2001873c f2412;
mov L0x2001873e f0513; mov L0x20018740 f1513; mov L0x20018742 f2513;
mov L0x20018744 f0614; mov L0x20018746 f1614; mov L0x20018748 f2614;
mov L0x2001874a f0715; mov L0x2001874c f1715; mov L0x2001874e f2715;
mov L0x20018750 f0816; mov L0x20018752 f1816; mov L0x20018754 f2816;
mov L0x20018756 f0017; mov L0x20018758 f1017; mov L0x2001875a f2017;
mov L0x2001875c f0118; mov L0x2001875e f1118; mov L0x20018760 f2118;
mov L0x20018762 f0219; mov L0x20018764 f1219; mov L0x20018766 f2219;
mov L0x20018768 f0320; mov L0x2001876a f1320; mov L0x2001876c f2320;
mov L0x2001876e f0421; mov L0x20018770 f1421; mov L0x20018772 f2421;
mov L0x20018774 f0522; mov L0x20018776 f1522; mov L0x20018778 f2522;
mov L0x2001877a f0623; mov L0x2001877c f1623; mov L0x2001877e f2623;
mov L0x20018780 f0724; mov L0x20018782 f1724; mov L0x20018784 f2724;
mov L0x20018786 f0825; mov L0x20018788 f1825; mov L0x2001878a f2825;
mov L0x2001878c f0026; mov L0x2001878e f1026; mov L0x20018790 f2026;
mov L0x20018792 f0127; mov L0x20018794 f1127; mov L0x20018796 f2127;
mov L0x20018798 f0228; mov L0x2001879a f1228; mov L0x2001879c f2228;
mov L0x2001879e f0329; mov L0x200187a0 f1329; mov L0x200187a2 f2329;
mov L0x200187a4 f0430; mov L0x200187a6 f1430; mov L0x200187a8 f2430;
mov L0x200187aa f0531; mov L0x200187ac f1531; mov L0x200187ae f2531;
mov L0x200187b0 f0600; mov L0x200187b2 f1600; mov L0x200187b4 f2600;
mov L0x200187b6 f0701; mov L0x200187b8 f1701; mov L0x200187ba f2701;
mov L0x200187bc f0802; mov L0x200187be f1802; mov L0x200187c0 f2802;
mov L0x200187c2 f0003; mov L0x200187c4 f1003; mov L0x200187c6 f2003;
mov L0x200187c8 f0104; mov L0x200187ca f1104; mov L0x200187cc f2104;
mov L0x200187ce f0205; mov L0x200187d0 f1205; mov L0x200187d2 f2205;
mov L0x200187d4 f0306; mov L0x200187d6 f1306; mov L0x200187d8 f2306;
mov L0x200187da f0407; mov L0x200187dc f1407; mov L0x200187de f2407;
mov L0x200187e0 f0508; mov L0x200187e2 f1508; mov L0x200187e4 f2508;
mov L0x200187e6 f0609; mov L0x200187e8 f1609; mov L0x200187ea f2609;
mov L0x200187ec f0710; mov L0x200187ee f1710; mov L0x200187f0 f2710;
mov L0x200187f2 f0811; mov L0x200187f4 f1811; mov L0x200187f6 f2811;
mov L0x200187f8 f0012; mov L0x200187fa f1012; mov L0x200187fc f2012;
mov L0x200187fe f0113; mov L0x20018800 f1113; mov L0x20018802 f2113;
mov L0x20018804 f0214; mov L0x20018806 f1214; mov L0x20018808 f2214;
mov L0x2001880a f0315; mov L0x2001880c f1315; mov L0x2001880e f2315;
mov L0x20018810 f0416; mov L0x20018812 f1416; mov L0x20018814 f2416;
mov L0x20018816 f0517; mov L0x20018818 f1517; mov L0x2001881a f2517;
mov L0x2001881c f0618; mov L0x2001881e f1618; mov L0x20018820 f2618;
mov L0x20018822 f0719; mov L0x20018824 f1719; mov L0x20018826 f2719;
mov L0x20018828 f0820; mov L0x2001882a f1820; mov L0x2001882c f2820;
mov L0x2001882e f0021; mov L0x20018830 f1021; mov L0x20018832 f2021;
mov L0x20018834 f0122; mov L0x20018836 f1122; mov L0x20018838 f2122;
mov L0x2001883a f0223; mov L0x2001883c f1223; mov L0x2001883e f2223;
mov L0x20018840 f0324; mov L0x20018842 f1324; mov L0x20018844 f2324;
mov L0x20018846 f0425; mov L0x20018848 f1425; mov L0x2001884a f2425;
mov L0x2001884c f0526; mov L0x2001884e f1526; mov L0x20018850 f2526;
mov L0x20018852 f0627; mov L0x20018854 f1627; mov L0x20018856 f2627;
mov L0x20018858 f0728; mov L0x2001885a f1728; mov L0x2001885c f2728;
mov L0x2001885e f0829; mov L0x20018860 f1829; mov L0x20018862 f2829;
mov L0x20018864 f0030; mov L0x20018866 f1030; mov L0x20018868 f2030;
mov L0x2001886a f0131; mov L0x2001886c f1131; mov L0x2001886e f2131;
mov L0x20018870 f0200; mov L0x20018872 f1200; mov L0x20018874 f2200;
mov L0x20018876 f0301; mov L0x20018878 f1301; mov L0x2001887a f2301;
mov L0x2001887c f0402; mov L0x2001887e f1402; mov L0x20018880 f2402;
mov L0x20018882 f0503; mov L0x20018884 f1503; mov L0x20018886 f2503;
mov L0x20018888 f0604; mov L0x2001888a f1604; mov L0x2001888c f2604;
mov L0x2001888e f0705; mov L0x20018890 f1705; mov L0x20018892 f2705;
mov L0x20018894 f0806; mov L0x20018896 f1806; mov L0x20018898 f2806;
mov L0x2001889a f0007; mov L0x2001889c f1007; mov L0x2001889e f2007;
mov L0x200188a0 f0108; mov L0x200188a2 f1108; mov L0x200188a4 f2108;
mov L0x200188a6 f0209; mov L0x200188a8 f1209; mov L0x200188aa f2209;
mov L0x200188ac f0310; mov L0x200188ae f1310; mov L0x200188b0 f2310;
mov L0x200188b2 f0411; mov L0x200188b4 f1411; mov L0x200188b6 f2411;
mov L0x200188b8 f0512; mov L0x200188ba f1512; mov L0x200188bc f2512;
mov L0x200188be f0613; mov L0x200188c0 f1613; mov L0x200188c2 f2613;
mov L0x200188c4 f0714; mov L0x200188c6 f1714; mov L0x200188c8 f2714;
mov L0x200188ca f0815; mov L0x200188cc f1815; mov L0x200188ce f2815;
mov L0x200188d0 f0016; mov L0x200188d2 f1016; mov L0x200188d4 f2016;
mov L0x200188d6 f0117; mov L0x200188d8 f1117; mov L0x200188da f2117;
mov L0x200188dc f0218; mov L0x200188de f1218; mov L0x200188e0 f2218;
mov L0x200188e2 f0319; mov L0x200188e4 f1319; mov L0x200188e6 f2319;
mov L0x200188e8 f0420; mov L0x200188ea f1420; mov L0x200188ec f2420;
mov L0x200188ee f0521; mov L0x200188f0 f1521; mov L0x200188f2 f2521;
mov L0x200188f4 f0622; mov L0x200188f6 f1622; mov L0x200188f8 f2622;
mov L0x200188fa f0723; mov L0x200188fc f1723; mov L0x200188fe f2723;
mov L0x20018900 f0824; mov L0x20018902 f1824; mov L0x20018904 f2824;
mov L0x20018906 f0025; mov L0x20018908 f1025; mov L0x2001890a f2025;
mov L0x2001890c f0126; mov L0x2001890e f1126; mov L0x20018910 f2126;
mov L0x20018912 f0227; mov L0x20018914 f1227; mov L0x20018916 f2227;
mov L0x20018918 f0328; mov L0x2001891a f1328; mov L0x2001891c f2328;
mov L0x2001891e f0429; mov L0x20018920 f1429; mov L0x20018922 f2429;
mov L0x20018924 f0530; mov L0x20018926 f1530; mov L0x20018928 f2530;
mov L0x2001892a f0631; mov L0x2001892c f1631; mov L0x2001892e f2631;
mov L0x20018930 f0700; mov L0x20018932 f1700; mov L0x20018934 f2700;
mov L0x20018936 f0801; mov L0x20018938 f1801; mov L0x2001893a f2801;
mov L0x2001893c f0002; mov L0x2001893e f1002; mov L0x20018940 f2002;
mov L0x20018942 f0103; mov L0x20018944 f1103; mov L0x20018946 f2103;
mov L0x20018948 f0204; mov L0x2001894a f1204; mov L0x2001894c f2204;
mov L0x2001894e f0305; mov L0x20018950 f1305; mov L0x20018952 f2305;
mov L0x20018954 f0406; mov L0x20018956 f1406; mov L0x20018958 f2406;
mov L0x2001895a f0507; mov L0x2001895c f1507; mov L0x2001895e f2507;
mov L0x20018960 f0608; mov L0x20018962 f1608; mov L0x20018964 f2608;
mov L0x20018966 f0709; mov L0x20018968 f1709; mov L0x2001896a f2709;
mov L0x2001896c f0810; mov L0x2001896e f1810; mov L0x20018970 f2810;
mov L0x20018972 f0011; mov L0x20018974 f1011; mov L0x20018976 f2011;
mov L0x20018978 f0112; mov L0x2001897a f1112; mov L0x2001897c f2112;
mov L0x2001897e f0213; mov L0x20018980 f1213; mov L0x20018982 f2213;
mov L0x20018984 f0314; mov L0x20018986 f1314; mov L0x20018988 f2314;
mov L0x2001898a f0415; mov L0x2001898c f1415; mov L0x2001898e f2415;
mov L0x20018990 f0516; mov L0x20018992 f1516; mov L0x20018994 f2516;
mov L0x20018996 f0617; mov L0x20018998 f1617; mov L0x2001899a f2617;
mov L0x2001899c f0718; mov L0x2001899e f1718; mov L0x200189a0 f2718;
mov L0x200189a2 f0819; mov L0x200189a4 f1819; mov L0x200189a6 f2819;
mov L0x200189a8 f0020; mov L0x200189aa f1020; mov L0x200189ac f2020;
mov L0x200189ae f0121; mov L0x200189b0 f1121; mov L0x200189b2 f2121;
mov L0x200189b4 f0222; mov L0x200189b6 f1222; mov L0x200189b8 f2222;
mov L0x200189ba f0323; mov L0x200189bc f1323; mov L0x200189be f2323;
mov L0x200189c0 f0424; mov L0x200189c2 f1424; mov L0x200189c4 f2424;
mov L0x200189c6 f0525; mov L0x200189c8 f1525; mov L0x200189ca f2525;
mov L0x200189cc f0626; mov L0x200189ce f1626; mov L0x200189d0 f2626;
mov L0x200189d2 f0727; mov L0x200189d4 f1727; mov L0x200189d6 f2727;
mov L0x200189d8 f0828; mov L0x200189da f1828; mov L0x200189dc f2828;
mov L0x200189de f0029; mov L0x200189e0 f1029; mov L0x200189e2 f2029;
mov L0x200189e4 f0130; mov L0x200189e6 f1130; mov L0x200189e8 f2130;
mov L0x200189ea f0231; mov L0x200189ec f1231; mov L0x200189ee f2231;
mov L0x200189f0 f0300; mov L0x200189f2 f1300; mov L0x200189f4 f2300;
mov L0x200189f6 f0401; mov L0x200189f8 f1401; mov L0x200189fa f2401;
mov L0x200189fc f0502; mov L0x200189fe f1502; mov L0x20018a00 f2502;
mov L0x20018a02 f0603; mov L0x20018a04 f1603; mov L0x20018a06 f2603;
mov L0x20018a08 f0704; mov L0x20018a0a f1704; mov L0x20018a0c f2704;
mov L0x20018a0e f0805; mov L0x20018a10 f1805; mov L0x20018a12 f2805;
mov L0x20018a14 f0006; mov L0x20018a16 f1006; mov L0x20018a18 f2006;
mov L0x20018a1a f0107; mov L0x20018a1c f1107; mov L0x20018a1e f2107;
mov L0x20018a20 f0208; mov L0x20018a22 f1208; mov L0x20018a24 f2208;
mov L0x20018a26 f0309; mov L0x20018a28 f1309; mov L0x20018a2a f2309;
mov L0x20018a2c f0410; mov L0x20018a2e f1410; mov L0x20018a30 f2410;
mov L0x20018a32 f0511; mov L0x20018a34 f1511; mov L0x20018a36 f2511;
mov L0x20018a38 f0612; mov L0x20018a3a f1612; mov L0x20018a3c f2612;
mov L0x20018a3e f0713; mov L0x20018a40 f1713; mov L0x20018a42 f2713;
mov L0x20018a44 f0814; mov L0x20018a46 f1814; mov L0x20018a48 f2814;
mov L0x20018a4a f0015; mov L0x20018a4c f1015; mov L0x20018a4e f2015;
mov L0x20018a50 f0116; mov L0x20018a52 f1116; mov L0x20018a54 f2116;
mov L0x20018a56 f0217; mov L0x20018a58 f1217; mov L0x20018a5a f2217;
mov L0x20018a5c f0318; mov L0x20018a5e f1318; mov L0x20018a60 f2318;
mov L0x20018a62 f0419; mov L0x20018a64 f1419; mov L0x20018a66 f2419;
mov L0x20018a68 f0520; mov L0x20018a6a f1520; mov L0x20018a6c f2520;
mov L0x20018a6e f0621; mov L0x20018a70 f1621; mov L0x20018a72 f2621;
mov L0x20018a74 f0722; mov L0x20018a76 f1722; mov L0x20018a78 f2722;
mov L0x20018a7a f0823; mov L0x20018a7c f1823; mov L0x20018a7e f2823;
mov L0x20018a80 f0024; mov L0x20018a82 f1024; mov L0x20018a84 f2024;
mov L0x20018a86 f0125; mov L0x20018a88 f1125; mov L0x20018a8a f2125;
mov L0x20018a8c f0226; mov L0x20018a8e f1226; mov L0x20018a90 f2226;
mov L0x20018a92 f0327; mov L0x20018a94 f1327; mov L0x20018a96 f2327;
mov L0x20018a98 f0428; mov L0x20018a9a f1428; mov L0x20018a9c f2428;
mov L0x20018a9e f0529; mov L0x20018aa0 f1529; mov L0x20018aa2 f2529;
mov L0x20018aa4 f0630; mov L0x20018aa6 f1630; mov L0x20018aa8 f2630;
mov L0x20018aaa f0731; mov L0x20018aac f1731; mov L0x20018aae f2731;
mov L0x20018ab0 f0800; mov L0x20018ab2 f1800; mov L0x20018ab4 f2800;
mov L0x20018ab6 f0001; mov L0x20018ab8 f1001; mov L0x20018aba f2001;
mov L0x20018abc f0102; mov L0x20018abe f1102; mov L0x20018ac0 f2102;
mov L0x20018ac2 f0203; mov L0x20018ac4 f1203; mov L0x20018ac6 f2203;
mov L0x20018ac8 f0304; mov L0x20018aca f1304; mov L0x20018acc f2304;
mov L0x20018ace f0405; mov L0x20018ad0 f1405; mov L0x20018ad2 f2405;
mov L0x20018ad4 f0506; mov L0x20018ad6 f1506; mov L0x20018ad8 f2506;
mov L0x20018ada f0607; mov L0x20018adc f1607; mov L0x20018ade f2607;
mov L0x20018ae0 f0708; mov L0x20018ae2 f1708; mov L0x20018ae4 f2708;
mov L0x20018ae6 f0809; mov L0x20018ae8 f1809; mov L0x20018aea f2809;
mov L0x20018aec f0010; mov L0x20018aee f1010; mov L0x20018af0 f2010;
mov L0x20018af2 f0111; mov L0x20018af4 f1111; mov L0x20018af6 f2111;
mov L0x20018af8 f0212; mov L0x20018afa f1212; mov L0x20018afc f2212;
mov L0x20018afe f0313; mov L0x20018b00 f1313; mov L0x20018b02 f2313;
mov L0x20018b04 f0414; mov L0x20018b06 f1414; mov L0x20018b08 f2414;
mov L0x20018b0a f0515; mov L0x20018b0c f1515; mov L0x20018b0e f2515;
mov L0x20018b10 f0616; mov L0x20018b12 f1616; mov L0x20018b14 f2616;
mov L0x20018b16 f0717; mov L0x20018b18 f1717; mov L0x20018b1a f2717;
mov L0x20018b1c f0818; mov L0x20018b1e f1818; mov L0x20018b20 f2818;
mov L0x20018b22 f0019; mov L0x20018b24 f1019; mov L0x20018b26 f2019;
mov L0x20018b28 f0120; mov L0x20018b2a f1120; mov L0x20018b2c f2120;
mov L0x20018b2e f0221; mov L0x20018b30 f1221; mov L0x20018b32 f2221;
mov L0x20018b34 f0322; mov L0x20018b36 f1322; mov L0x20018b38 f2322;
mov L0x20018b3a f0423; mov L0x20018b3c f1423; mov L0x20018b3e f2423;
mov L0x20018b40 f0524; mov L0x20018b42 f1524; mov L0x20018b44 f2524;
mov L0x20018b46 f0625; mov L0x20018b48 f1625; mov L0x20018b4a f2625;
mov L0x20018b4c f0726; mov L0x20018b4e f1726; mov L0x20018b50 f2726;
mov L0x20018b52 f0827; mov L0x20018b54 f1827; mov L0x20018b56 f2827;
mov L0x20018b58 f0028; mov L0x20018b5a f1028; mov L0x20018b5c f2028;
mov L0x20018b5e f0129; mov L0x20018b60 f1129; mov L0x20018b62 f2129;
mov L0x20018b64 f0230; mov L0x20018b66 f1230; mov L0x20018b68 f2230;
mov L0x20018b6a f0331; mov L0x20018b6c f1331; mov L0x20018b6e f2331;
mov L0x20018b70 f0400; mov L0x20018b72 f1400; mov L0x20018b74 f2400;
mov L0x20018b76 f0501; mov L0x20018b78 f1501; mov L0x20018b7a f2501;
mov L0x20018b7c f0602; mov L0x20018b7e f1602; mov L0x20018b80 f2602;
mov L0x20018b82 f0703; mov L0x20018b84 f1703; mov L0x20018b86 f2703;
mov L0x20018b88 f0804; mov L0x20018b8a f1804; mov L0x20018b8c f2804;
mov L0x20018b8e f0005; mov L0x20018b90 f1005; mov L0x20018b92 f2005;
mov L0x20018b94 f0106; mov L0x20018b96 f1106; mov L0x20018b98 f2106;
mov L0x20018b9a f0207; mov L0x20018b9c f1207; mov L0x20018b9e f2207;
mov L0x20018ba0 f0308; mov L0x20018ba2 f1308; mov L0x20018ba4 f2308;
mov L0x20018ba6 f0409; mov L0x20018ba8 f1409; mov L0x20018baa f2409;
mov L0x20018bac f0510; mov L0x20018bae f1510; mov L0x20018bb0 f2510;
mov L0x20018bb2 f0611; mov L0x20018bb4 f1611; mov L0x20018bb6 f2611;
mov L0x20018bb8 f0712; mov L0x20018bba f1712; mov L0x20018bbc f2712;
mov L0x20018bbe f0813; mov L0x20018bc0 f1813; mov L0x20018bc2 f2813;
mov L0x20018bc4 f0014; mov L0x20018bc6 f1014; mov L0x20018bc8 f2014;
mov L0x20018bca f0115; mov L0x20018bcc f1115; mov L0x20018bce f2115;
mov L0x20018bd0 f0216; mov L0x20018bd2 f1216; mov L0x20018bd4 f2216;
mov L0x20018bd6 f0317; mov L0x20018bd8 f1317; mov L0x20018bda f2317;
mov L0x20018bdc f0418; mov L0x20018bde f1418; mov L0x20018be0 f2418;
mov L0x20018be2 f0519; mov L0x20018be4 f1519; mov L0x20018be6 f2519;
mov L0x20018be8 f0620; mov L0x20018bea f1620; mov L0x20018bec f2620;
mov L0x20018bee f0721; mov L0x20018bf0 f1721; mov L0x20018bf2 f2721;
mov L0x20018bf4 f0822; mov L0x20018bf6 f1822; mov L0x20018bf8 f2822;
mov L0x20018bfa f0023; mov L0x20018bfc f1023; mov L0x20018bfe f2023;
mov L0x20018c00 f0124; mov L0x20018c02 f1124; mov L0x20018c04 f2124;
mov L0x20018c06 f0225; mov L0x20018c08 f1225; mov L0x20018c0a f2225;
mov L0x20018c0c f0326; mov L0x20018c0e f1326; mov L0x20018c10 f2326;
mov L0x20018c12 f0427; mov L0x20018c14 f1427; mov L0x20018c16 f2427;
mov L0x20018c18 f0528; mov L0x20018c1a f1528; mov L0x20018c1c f2528;
mov L0x20018c1e f0629; mov L0x20018c20 f1629; mov L0x20018c22 f2629;
mov L0x20018c24 f0730; mov L0x20018c26 f1730; mov L0x20018c28 f2730;
mov L0x20018c2a f0831; mov L0x20018c2c f1831; mov L0x20018c2e f2831;



(******************** constants ********************)

mov r2 1205062335@uint32; mov r3    3365569@sint32;
mov L0x8063178 (1883665)@sint32; mov L0x806317c ( 980652)@sint32;
mov L0x8063180 ( 501252)@sint32; mov L0x8063184 ( 501252)@sint32;
mov L0x8063188 ( 501252)@sint32; mov L0x806318c ( 501252)@sint32;
mov L0x8063190 (3346293)@sint32; mov L0x8063194 (2811707)@sint32;
mov L0x8063198 ( 501252)@sint32; mov L0x806319c (2811707)@sint32;
mov L0x80631a0 (1639017)@sint32; mov L0x80631a4 ( 813816)@sint32;
mov L0x80631a8 ( 813816)@sint32; mov L0x80631ac ( 813816)@sint32;
mov L0x80631b0 ( 813816)@sint32; mov L0x80631b4 (2997402)@sint32;
mov L0x80631b8 (1833606)@sint32; mov L0x80631bc ( 813816)@sint32;
mov L0x80631c0 (1833606)@sint32; mov L0x80631c4 (2087623)@sint32;



(******************* polynomial variables ********************)
ghost x@sint32, y@sint32, z@sint32 : true && true;


(* #! -> SP = 0x20014890 *)
#! 0x20014890 = 0x20014890;
(* #stmdb	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, lr}#! EA = L0x20014890; PC = 0x8055e24 *)
#stmdb	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, %%lr}#! L0x20014890 = L0x20014890; 0x8055e24 = 0x8055e24;
(* #ldr.w	lr, [sp, #40]	; 0x28                      #! EA = L0x20014890; Value = 0x20018570; PC = 0x8055e28 *)
#ldr.w	%%lr, %%L0x20014890	; 0x28                      #! L0x20014890 = L0x20014890; 0x20018570 = 0x20018570; 0x8055e28 = 0x8055e28;
(* ldmia.w	r1!, {r10, r11}                          #! EA = L0x8063178; Value = 0xffe96350; PC = 0x8055e2c *)
mov r10 L0x8063178;
mov r11 L0x806317c;
(* add.w	r12, r0, #12                              #! PC = 0x8055e30 *)
adds dc r12 r0 12@uint32;
(* addw	r1, r0, #2916	; 0xb64                      #! PC = 0x8055e34 *)
adds dc r1 r0 2916@uint32;
(* vmov	s3, r1                                     #! PC = 0x8055e38 *)
mov s3 r1;
(* ldrsh.w	r4, [lr]                                #! EA = L0x20018570; Value = 0x00030003; PC = 0x8055e3c *)
mov r4 L0x20018570;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x200189f0; Value = 0x0ffd0ffd; PC = 0x8055e40 *)
mov r6 L0x200189f0;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x200187b0; Value = 0x0ffd0000; PC = 0x8055e44 *)
mov r9 L0x200187b0;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0000@sint32 : and [cf0000 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0300@sint32 : and [cf0300 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0600@sint32 : and [cf0600 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014898; PC = 0x8055e90 *)
mov L0x20014898 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200148bc; PC = 0x8055e94 *)
mov L0x200148bc r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x200148e0; PC = 0x8055e98 *)
mov L0x200148e0 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015618; PC = 0x8055e9c *)
mov L0x20015618 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x2001563c; PC = 0x8055ea0 *)
mov L0x2001563c r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015660; PC = 0x8055ea4 *)
mov L0x20015660 r9;



(******************** offset 0, 0,  0 ********************)


(**************** CUT   0, - *****************)

ecut and [
eqmod cf0000 f0000 2048, eqmod cf0300 f0300 2048, eqmod cf0600 f0600 2048,
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x20014898*x**0*y**0*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x200148bc*x**0*y**0*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x200148e0*x**0*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x20015618*x**0*y**0*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x2001563c*x**0*y**0*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x20015660*x**0*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018ab6; Value = 0x00000000; PC = 0x8055ea8 *)
mov r5 L0x20018ab6;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x20018876; Value = 0x0ffd0000; PC = 0x8055eac *)
mov r6 L0x20018876;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x20018636; Value = 0x00000000; PC = 0x8055eb0 *)
mov r9 L0x20018636;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0001@sint32 : and [cf0001 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0301@sint32 : and [cf0301 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0601@sint32 : and [cf0601 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014904; PC = 0x8055f04 *)
mov L0x20014904 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014928; PC = 0x8055f08 *)
mov L0x20014928 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x2001494c; PC = 0x8055f0c *)
mov L0x2001494c r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015684; PC = 0x8055f10 *)
mov L0x20015684 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200156a8; PC = 0x8055f14 *)
mov L0x200156a8 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200156cc; PC = 0x8055f18 *)
mov L0x200156cc r9;



(******************** offset 0, 0,  1 ********************)


(**************** CUT   1, - *****************)

ecut and [
eqmod cf0001 f0001 2048, eqmod cf0301 f0301 2048, eqmod cf0601 f0601 2048,
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x20014904*x**0*y**0*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x20014928*x**0*y**0*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x2001494c*x**0*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x20015684*x**0*y**0*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x200156a8*x**0*y**0*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x200156cc*x**0*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x2001893c; Value = 0x00000ffd; PC = 0x8055f1c *)
mov r5 L0x2001893c;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x200186fc; Value = 0x00000003; PC = 0x8055f20 *)
mov r6 L0x200186fc;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018b7c; Value = 0x00000ffd; PC = 0x8055f24 *)
mov r8 L0x20018b7c;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0002@sint32 : and [cf0002 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0302@sint32 : and [cf0302 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0602@sint32 : and [cf0602 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014970; PC = 0x8055f64 *)
mov L0x20014970 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014994; PC = 0x8055f68 *)
mov L0x20014994 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200149b8; PC = 0x8055f6c *)
mov L0x200149b8 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x200156f0; PC = 0x8055f70 *)
mov L0x200156f0 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015714; PC = 0x8055f74 *)
mov L0x20015714 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015738; PC = 0x8055f78 *)
mov L0x20015738 r9;



(******************** offset 0, 0,  2 ********************)


(**************** CUT   2, - *****************)

ecut and [
eqmod cf0002 f0002 2048, eqmod cf0302 f0302 2048, eqmod cf0602 f0602 2048,
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x20014970*x**0*y**0*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x20014994*x**0*y**0*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x200149b8*x**0*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x200156f0*x**0*y**0*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x20015714*x**0*y**0*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x20015738*x**0*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x200187c2; Value = 0x00030ffd; PC = 0x8055f7c *)
mov r5 L0x200187c2;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x20018582; Value = 0x00000003; PC = 0x8055f80 *)
mov r6 L0x20018582;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018a02; Value = 0x00000000; PC = 0x8055f84 *)
mov r8 L0x20018a02;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0003@sint32 : and [cf0003 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0303@sint32 : and [cf0303 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0603@sint32 : and [cf0603 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x200149dc; PC = 0x8055fc4 *)
mov L0x200149dc r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014a00; PC = 0x8055fc8 *)
mov L0x20014a00 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014a24; PC = 0x8055fcc *)
mov L0x20014a24 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x2001575c; PC = 0x8055fd0 *)
mov L0x2001575c r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015780; PC = 0x8055fd4 *)
mov L0x20015780 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x200157a4; PC = 0x8055fd8 *)
mov L0x200157a4 r9;



(******************** offset 0, 0,  3 ********************)


(**************** CUT   3, - *****************)

ecut and [
eqmod cf0003 f0003 2048, eqmod cf0303 f0303 2048, eqmod cf0603 f0603 2048,
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x200149dc*x**0*y**0*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x20014a00*x**0*y**0*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x20014a24*x**0*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x2001575c*x**0*y**0*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x20015780*x**0*y**0*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x200157a4*x**0*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x20018648; Value = 0x00000000; PC = 0x8055fdc *)
mov r5 L0x20018648;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018ac8; Value = 0x0ffd0ffd; PC = 0x8055fe0 *)
mov r7 L0x20018ac8;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x20018888; Value = 0x00000ffd; PC = 0x8055fe4 *)
mov r8 L0x20018888;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0004@sint32 : and [cf0004 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0304@sint32 : and [cf0304 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0604@sint32 : and [cf0604 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014a48; PC = 0x8056038 *)
mov L0x20014a48 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014a6c; PC = 0x805603c *)
mov L0x20014a6c r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014a90; PC = 0x8056040 *)
mov L0x20014a90 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x200157c8; PC = 0x8056044 *)
mov L0x200157c8 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x200157ec; PC = 0x8056048 *)
mov L0x200157ec r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015810; PC = 0x805604c *)
mov L0x20015810 r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;



(******************** offset 0, 0,  4 ********************)


(**************** CUT   4, - *****************)

ecut and [
eqmod cf0004 f0004 2048, eqmod cf0304 f0304 2048, eqmod cf0604 f0604 2048,
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x20014a48*x**0*y**0*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x20014a6c*x**0*y**0*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x20014a90*x**0*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x200157c8*x**0*y**0*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x200157ec*x**0*y**0*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x20015810*x**0*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1566]	; 0x61e                 #! EA = L0x20018b8e; Value = 0x0ffd0000; PC = 0x805605c *)
mov r4 L0x20018b8e;
(* ldrsh.w	r7, [lr, #990]	; 0x3de                  #! EA = L0x2001894e; Value = 0x00030ffd; PC = 0x8056060 *)
mov r7 L0x2001894e;
(* ldrsh.w	r8, [lr, #414]	; 0x19e                  #! EA = L0x2001870e; Value = 0x0ffd0ffd; PC = 0x8056064 *)
mov r8 L0x2001870e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056068 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0005@sint32 : and [cf0005 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805606c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0305@sint32 : and [cf0305 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056070 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0605@sint32 : and [cf0605 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056074 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056078 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x805607c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x8056080 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056084 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056088 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x805608a *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x805608c *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056090 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056094 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056098 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805609c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805609e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80560a0 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80560a4 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80560a8 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80560ac *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014ab4; PC = 0x80560b0 *)
mov L0x20014ab4 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014ad8; PC = 0x80560b4 *)
mov L0x20014ad8 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014afc; PC = 0x80560b8 *)
mov L0x20014afc r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015834; PC = 0x80560bc *)
mov L0x20015834 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015858; PC = 0x80560c0 *)
mov L0x20015858 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x2001587c; PC = 0x80560c4 *)
mov L0x2001587c r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80560c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 0, 0,  5 ********************)


(**************** CUT   5, - *****************)

ecut and [
eqmod cf0005 f0005 2048, eqmod cf0305 f0305 2048, eqmod cf0605 f0605 2048,
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x20014ab4*x**0*y**0*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x20014ad8*x**0*y**0*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x20014afc*x**0*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x20015834*x**0*y**0*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x20015858*x**0*y**0*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x2001587c*x**0*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1188]	; 0x4a4                 #! EA = L0x20018a14; Value = 0x00030003; PC = 0x80560cc *)
mov r4 L0x20018a14;
(* ldrsh.w	r7, [lr, #612]	; 0x264                  #! EA = L0x200187d4; Value = 0x00030000; PC = 0x80560d0 *)
mov r7 L0x200187d4;
(* ldrsh.w	r8, [lr, #36]	; 0x24                    #! EA = L0x20018594; Value = 0x00000003; PC = 0x80560d4 *)
mov r8 L0x20018594;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80560d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0006@sint32 : and [cf0006 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80560dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0306@sint32 : and [cf0306 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80560e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0606@sint32 : and [cf0606 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80560e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80560e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80560ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80560f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80560f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80560f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80560fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80560fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056100 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056104 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056108 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805610c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805610e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056110 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056114 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056118 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805611c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014b20; PC = 0x8056120 *)
mov L0x20014b20 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014b44; PC = 0x8056124 *)
mov L0x20014b44 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014b68; PC = 0x8056128 *)
mov L0x20014b68 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200158a0; PC = 0x805612c *)
mov L0x200158a0 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200158c4; PC = 0x8056130 *)
mov L0x200158c4 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200158e8; PC = 0x8056134 *)
mov L0x200158e8 r9;



(******************** offset 0, 0,  6 ********************)


(**************** CUT   6, - *****************)

ecut and [
eqmod cf0006 f0006 2048, eqmod cf0306 f0306 2048, eqmod cf0606 f0606 2048,
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x20014b20*x**0*y**0*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x20014b44*x**0*y**0*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x20014b68*x**0*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x200158a0*x**0*y**0*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x200158c4*x**0*y**0*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x200158e8*x**0*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #810]	; 0x32a                  #! EA = L0x2001889a; Value = 0x0ffd0ffd; PC = 0x8056138 *)
mov r4 L0x2001889a;
(* ldrsh.w	r7, [lr, #234]	; 0xea                   #! EA = L0x2001865a; Value = 0x00030000; PC = 0x805613c *)
mov r7 L0x2001865a;
(* ldrsh.w	r9, [lr, #1386]	; 0x56a                 #! EA = L0x20018ada; Value = 0x00030000; PC = 0x8056140 *)
mov r9 L0x20018ada;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056144 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0007@sint32 : and [cf0007 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056148 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0307@sint32 : and [cf0307 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x805614c *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0607@sint32 : and [cf0607 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056150 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056154 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x8056158 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x805615c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056160 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056164 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x8056168 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x805616c *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056170 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056174 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x8056176 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056178 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014b8c; PC = 0x805617c *)
mov L0x20014b8c r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014bb0; PC = 0x8056180 *)
mov L0x20014bb0 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014bd4; PC = 0x8056184 *)
mov L0x20014bd4 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x2001590c; PC = 0x8056188 *)
mov L0x2001590c r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015930; PC = 0x805618c *)
mov L0x20015930 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015954; PC = 0x8056190 *)
mov L0x20015954 r9;



(******************** offset 0, 0,  7 ********************)


(**************** CUT   7, - *****************)

ecut and [
eqmod cf0007 f0007 2048, eqmod cf0307 f0307 2048, eqmod cf0607 f0607 2048,
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x20014b8c*x**0*y**0*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x20014bb0*x**0*y**0*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x20014bd4*x**0*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x2001590c*x**0*y**0*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x20015930*x**0*y**0*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x20015954*x**0*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #432]	; 0x1b0                  #! EA = L0x20018720; Value = 0x0ffd0003; PC = 0x8056194 *)
mov r4 L0x20018720;
(* ldrsh.w	r6, [lr, #1584]	; 0x630                 #! EA = L0x20018ba0; Value = 0x00030003; PC = 0x8056198 *)
mov r6 L0x20018ba0;
(* ldrsh.w	r9, [lr, #1008]	; 0x3f0                 #! EA = L0x20018960; Value = 0x0ffd0003; PC = 0x805619c *)
mov r9 L0x20018960;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80561a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0008@sint32 : and [cf0008 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80561a4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0308@sint32 : and [cf0308 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80561a8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0608@sint32 : and [cf0608 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80561ac *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80561b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80561b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80561b8 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80561bc *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80561c0 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80561c2 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80561c4 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80561c8 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80561cc *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80561d0 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80561d4 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80561d6 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80561d8 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80561dc *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80561e0 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80561e4 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014bf8; PC = 0x80561e8 *)
mov L0x20014bf8 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014c1c; PC = 0x80561ec *)
mov L0x20014c1c r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014c40; PC = 0x80561f0 *)
mov L0x20014c40 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015978; PC = 0x80561f4 *)
mov L0x20015978 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x2001599c; PC = 0x80561f8 *)
mov L0x2001599c r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200159c0; PC = 0x80561fc *)
mov L0x200159c0 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056200 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056204 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x8055e3c <_Good_loop0_inner>              #! PC = 0x8056208 *)
#b.w	0x8055e3c <_Good_loop0_inner>              #! 0x8056208 = 0x8056208;



(******************** offset 0, 0,  8 ********************)


(**************** CUT   8, - *****************)

ecut and [
eqmod cf0008 f0008 2048, eqmod cf0308 f0308 2048, eqmod cf0608 f0608 2048,
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x20014bf8*x**0*y**0*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x20014c1c*x**0*y**0*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x20014c40*x**0*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x20015978*x**0*y**0*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x2001599c*x**0*y**0*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x200159c0*x**0*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr]                                #! EA = L0x200185a6; Value = 0x00030ffd; PC = 0x8055e3c *)
mov r4 L0x200185a6;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x20018a26; Value = 0x0ffd0ffd; PC = 0x8055e40 *)
mov r6 L0x20018a26;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x200187e6; Value = 0x00030003; PC = 0x8055e44 *)
mov r9 L0x200187e6;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0009@sint32 : and [cf0009 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0309@sint32 : and [cf0309 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0609@sint32 : and [cf0609 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014c64; PC = 0x8055e90 *)
mov L0x20014c64 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014c88; PC = 0x8055e94 *)
mov L0x20014c88 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014cac; PC = 0x8055e98 *)
mov L0x20014cac r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200159e4; PC = 0x8055e9c *)
mov L0x200159e4 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015a08; PC = 0x8055ea0 *)
mov L0x20015a08 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015a2c; PC = 0x8055ea4 *)
mov L0x20015a2c r9;



(******************** offset 0, 0,  9 ********************)


(**************** CUT   9, - *****************)

ecut and [
eqmod cf0009 f0009 2048, eqmod cf0309 f0309 2048, eqmod cf0609 f0609 2048,
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x20014c64*x**0*y**0*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x20014c88*x**0*y**0*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x20014cac*x**0*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x200159e4*x**0*y**0*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x20015a08*x**0*y**0*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x20015a2c*x**0*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018aec; Value = 0x00000000; PC = 0x8055ea8 *)
mov r5 L0x20018aec;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x200188ac; Value = 0x00000000; PC = 0x8055eac *)
mov r6 L0x200188ac;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x2001866c; Value = 0x0ffd0000; PC = 0x8055eb0 *)
mov r9 L0x2001866c;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0010@sint32 : and [cf0010 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0310@sint32 : and [cf0310 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0610@sint32 : and [cf0610 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014cd0; PC = 0x8055f04 *)
mov L0x20014cd0 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014cf4; PC = 0x8055f08 *)
mov L0x20014cf4 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014d18; PC = 0x8055f0c *)
mov L0x20014d18 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015a50; PC = 0x8055f10 *)
mov L0x20015a50 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015a74; PC = 0x8055f14 *)
mov L0x20015a74 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015a98; PC = 0x8055f18 *)
mov L0x20015a98 r9;



(******************** offset 0, 0, 10 ********************)


(**************** CUT  10, - *****************)

ecut and [
eqmod cf0010 f0010 2048, eqmod cf0310 f0310 2048, eqmod cf0610 f0610 2048,
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20014cd0*x**0*y**0*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20014cf4*x**0*y**0*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20014d18*x**0*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20015a50*x**0*y**0*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20015a74*x**0*y**0*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20015a98*x**0*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x20018972; Value = 0x00000003; PC = 0x8055f1c *)
mov r5 L0x20018972;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x20018732; Value = 0x00000003; PC = 0x8055f20 *)
mov r6 L0x20018732;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018bb2; Value = 0x0ffd0ffd; PC = 0x8055f24 *)
mov r8 L0x20018bb2;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0011@sint32 : and [cf0011 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0311@sint32 : and [cf0311 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0611@sint32 : and [cf0611 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014d3c; PC = 0x8055f64 *)
mov L0x20014d3c r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014d60; PC = 0x8055f68 *)
mov L0x20014d60 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014d84; PC = 0x8055f6c *)
mov L0x20014d84 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015abc; PC = 0x8055f70 *)
mov L0x20015abc r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015ae0; PC = 0x8055f74 *)
mov L0x20015ae0 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015b04; PC = 0x8055f78 *)
mov L0x20015b04 r9;



(******************** offset 0, 0, 11 ********************)


(**************** CUT  11, - *****************)

ecut and [
eqmod cf0011 f0011 2048, eqmod cf0311 f0311 2048, eqmod cf0611 f0611 2048,
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20014d3c*x**0*y**0*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20014d60*x**0*y**0*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20014d84*x**0*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20015abc*x**0*y**0*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20015ae0*x**0*y**0*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20015b04*x**0*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x200187f8; Value = 0x00030ffd; PC = 0x8055f7c *)
mov r5 L0x200187f8;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x200185b8; Value = 0x0ffd0000; PC = 0x8055f80 *)
mov r6 L0x200185b8;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018a38; Value = 0x00000000; PC = 0x8055f84 *)
mov r8 L0x20018a38;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0012@sint32 : and [cf0012 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0312@sint32 : and [cf0312 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0612@sint32 : and [cf0612 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20014da8; PC = 0x8055fc4 *)
mov L0x20014da8 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014dcc; PC = 0x8055fc8 *)
mov L0x20014dcc r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014df0; PC = 0x8055fcc *)
mov L0x20014df0 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015b28; PC = 0x8055fd0 *)
mov L0x20015b28 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015b4c; PC = 0x8055fd4 *)
mov L0x20015b4c r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015b70; PC = 0x8055fd8 *)
mov L0x20015b70 r9;



(******************** offset 0, 0, 12 ********************)


(**************** CUT  12, - *****************)

ecut and [
eqmod cf0012 f0012 2048, eqmod cf0312 f0312 2048, eqmod cf0612 f0612 2048,
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20014da8*x**0*y**0*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20014dcc*x**0*y**0*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20014df0*x**0*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20015b28*x**0*y**0*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20015b4c*x**0*y**0*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20015b70*x**0*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x2001867e; Value = 0x00000ffd; PC = 0x8055fdc *)
mov r5 L0x2001867e;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018afe; Value = 0x00030000; PC = 0x8055fe0 *)
mov r7 L0x20018afe;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x200188be; Value = 0x00030000; PC = 0x8055fe4 *)
mov r8 L0x200188be;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0013@sint32 : and [cf0013 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0313@sint32 : and [cf0313 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0613@sint32 : and [cf0613 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014e14; PC = 0x8056038 *)
mov L0x20014e14 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014e38; PC = 0x805603c *)
mov L0x20014e38 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014e5c; PC = 0x8056040 *)
mov L0x20014e5c r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015b94; PC = 0x8056044 *)
mov L0x20015b94 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015bb8; PC = 0x8056048 *)
mov L0x20015bb8 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015bdc; PC = 0x805604c *)
mov L0x20015bdc r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;



(******************** offset 0, 0, 13 ********************)


(**************** CUT  13, - *****************)

ecut and [
eqmod cf0013 f0013 2048, eqmod cf0313 f0313 2048, eqmod cf0613 f0613 2048,
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20014e14*x**0*y**0*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20014e38*x**0*y**0*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20014e5c*x**0*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20015b94*x**0*y**0*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20015bb8*x**0*y**0*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20015bdc*x**0*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1566]	; 0x61e                 #! EA = L0x20018bc4; Value = 0x00000003; PC = 0x805605c *)
mov r4 L0x20018bc4;
(* ldrsh.w	r7, [lr, #990]	; 0x3de                  #! EA = L0x20018984; Value = 0x00030ffd; PC = 0x8056060 *)
mov r7 L0x20018984;
(* ldrsh.w	r8, [lr, #414]	; 0x19e                  #! EA = L0x20018744; Value = 0x00030000; PC = 0x8056064 *)
mov r8 L0x20018744;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056068 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0014@sint32 : and [cf0014 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805606c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0314@sint32 : and [cf0314 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056070 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0614@sint32 : and [cf0614 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056074 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056078 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x805607c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x8056080 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056084 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056088 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x805608a *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x805608c *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056090 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056094 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056098 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805609c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805609e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80560a0 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80560a4 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80560a8 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80560ac *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014e80; PC = 0x80560b0 *)
mov L0x20014e80 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014ea4; PC = 0x80560b4 *)
mov L0x20014ea4 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014ec8; PC = 0x80560b8 *)
mov L0x20014ec8 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015c00; PC = 0x80560bc *)
mov L0x20015c00 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015c24; PC = 0x80560c0 *)
mov L0x20015c24 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015c48; PC = 0x80560c4 *)
mov L0x20015c48 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80560c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 0, 0, 14 ********************)


(**************** CUT  14, - *****************)

ecut and [
eqmod cf0014 f0014 2048, eqmod cf0314 f0314 2048, eqmod cf0614 f0614 2048,
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20014e80*x**0*y**0*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20014ea4*x**0*y**0*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20014ec8*x**0*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20015c00*x**0*y**0*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20015c24*x**0*y**0*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20015c48*x**0*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1188]	; 0x4a4                 #! EA = L0x20018a4a; Value = 0x00030003; PC = 0x80560cc *)
mov r4 L0x20018a4a;
(* ldrsh.w	r7, [lr, #612]	; 0x264                  #! EA = L0x2001880a; Value = 0x00000000; PC = 0x80560d0 *)
mov r7 L0x2001880a;
(* ldrsh.w	r8, [lr, #36]	; 0x24                    #! EA = L0x200185ca; Value = 0x00000003; PC = 0x80560d4 *)
mov r8 L0x200185ca;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80560d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0015@sint32 : and [cf0015 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80560dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0315@sint32 : and [cf0315 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80560e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0615@sint32 : and [cf0615 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80560e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80560e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80560ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80560f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80560f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80560f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80560fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80560fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056100 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056104 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056108 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805610c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805610e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056110 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056114 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056118 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805611c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014eec; PC = 0x8056120 *)
mov L0x20014eec r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014f10; PC = 0x8056124 *)
mov L0x20014f10 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014f34; PC = 0x8056128 *)
mov L0x20014f34 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015c6c; PC = 0x805612c *)
mov L0x20015c6c r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015c90; PC = 0x8056130 *)
mov L0x20015c90 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015cb4; PC = 0x8056134 *)
mov L0x20015cb4 r9;



(******************** offset 0, 0, 15 ********************)


(**************** CUT  15, - *****************)

ecut and [
eqmod cf0015 f0015 2048, eqmod cf0315 f0315 2048, eqmod cf0615 f0615 2048,
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20014eec*x**0*y**0*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20014f10*x**0*y**0*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20014f34*x**0*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20015c6c*x**0*y**0*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20015c90*x**0*y**0*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20015cb4*x**0*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #810]	; 0x32a                  #! EA = L0x200188d0; Value = 0x0ffd0000; PC = 0x8056138 *)
mov r4 L0x200188d0;
(* ldrsh.w	r7, [lr, #234]	; 0xea                   #! EA = L0x20018690; Value = 0x00030000; PC = 0x805613c *)
mov r7 L0x20018690;
(* ldrsh.w	r9, [lr, #1386]	; 0x56a                 #! EA = L0x20018b10; Value = 0x00030ffd; PC = 0x8056140 *)
mov r9 L0x20018b10;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056144 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0016@sint32 : and [cf0016 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056148 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0316@sint32 : and [cf0316 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x805614c *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0616@sint32 : and [cf0616 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056150 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056154 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x8056158 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x805615c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056160 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056164 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x8056168 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x805616c *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056170 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056174 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x8056176 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056178 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014f58; PC = 0x805617c *)
mov L0x20014f58 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014f7c; PC = 0x8056180 *)
mov L0x20014f7c r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014fa0; PC = 0x8056184 *)
mov L0x20014fa0 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015cd8; PC = 0x8056188 *)
mov L0x20015cd8 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015cfc; PC = 0x805618c *)
mov L0x20015cfc r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015d20; PC = 0x8056190 *)
mov L0x20015d20 r9;



(******************** offset 0, 0, 16 ********************)


(**************** CUT  16, - *****************)

ecut and [
eqmod cf0016 f0016 2048, eqmod cf0316 f0316 2048, eqmod cf0616 f0616 2048,
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20014f58*x**0*y**0*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20014f7c*x**0*y**0*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20014fa0*x**0*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20015cd8*x**0*y**0*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20015cfc*x**0*y**0*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20015d20*x**0*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #432]	; 0x1b0                  #! EA = L0x20018756; Value = 0x0ffd0000; PC = 0x8056194 *)
mov r4 L0x20018756;
(* ldrsh.w	r6, [lr, #1584]	; 0x630                 #! EA = L0x20018bd6; Value = 0x00000000; PC = 0x8056198 *)
mov r6 L0x20018bd6;
(* ldrsh.w	r9, [lr, #1008]	; 0x3f0                 #! EA = L0x20018996; Value = 0x00030ffd; PC = 0x805619c *)
mov r9 L0x20018996;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80561a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0017@sint32 : and [cf0017 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80561a4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0317@sint32 : and [cf0317 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80561a8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0617@sint32 : and [cf0617 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80561ac *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80561b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80561b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80561b8 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80561bc *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80561c0 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80561c2 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80561c4 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80561c8 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80561cc *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80561d0 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80561d4 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80561d6 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80561d8 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80561dc *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80561e0 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80561e4 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014fc4; PC = 0x80561e8 *)
mov L0x20014fc4 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014fe8; PC = 0x80561ec *)
mov L0x20014fe8 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x2001500c; PC = 0x80561f0 *)
mov L0x2001500c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015d44; PC = 0x80561f4 *)
mov L0x20015d44 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015d68; PC = 0x80561f8 *)
mov L0x20015d68 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015d8c; PC = 0x80561fc *)
mov L0x20015d8c r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056200 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056204 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x8055e3c <_Good_loop0_inner>              #! PC = 0x8056208 *)
#b.w	0x8055e3c <_Good_loop0_inner>              #! 0x8056208 = 0x8056208;



(******************** offset 0, 0, 17 ********************)


(**************** CUT  17, - *****************)

ecut and [
eqmod cf0017 f0017 2048, eqmod cf0317 f0317 2048, eqmod cf0617 f0617 2048,
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x20014fc4*x**0*y**0*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x20014fe8*x**0*y**0*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x2001500c*x**0*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x20015d44*x**0*y**0*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x20015d68*x**0*y**0*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x20015d8c*x**0*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr]                                #! EA = L0x200185dc; Value = 0x00030000; PC = 0x8055e3c *)
mov r4 L0x200185dc;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x20018a5c; Value = 0x00030000; PC = 0x8055e40 *)
mov r6 L0x20018a5c;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x2001881c; Value = 0x00000ffd; PC = 0x8055e44 *)
mov r9 L0x2001881c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0018@sint32 : and [cf0018 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0318@sint32 : and [cf0318 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0618@sint32 : and [cf0618 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015030; PC = 0x8055e90 *)
mov L0x20015030 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015054; PC = 0x8055e94 *)
mov L0x20015054 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015078; PC = 0x8055e98 *)
mov L0x20015078 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015db0; PC = 0x8055e9c *)
mov L0x20015db0 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015dd4; PC = 0x8055ea0 *)
mov L0x20015dd4 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015df8; PC = 0x8055ea4 *)
mov L0x20015df8 r9;



(******************** offset 0, 0, 18 ********************)


(**************** CUT  18, - *****************)

ecut and [
eqmod cf0018 f0018 2048, eqmod cf0318 f0318 2048, eqmod cf0618 f0618 2048,
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015030*x**0*y**0*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015054*x**0*y**0*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015078*x**0*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015db0*x**0*y**0*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015dd4*x**0*y**0*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015df8*x**0*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018b22; Value = 0x0ffd0000; PC = 0x8055ea8 *)
mov r5 L0x20018b22;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x200188e2; Value = 0x00030003; PC = 0x8055eac *)
mov r6 L0x200188e2;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x200186a2; Value = 0x00000ffd; PC = 0x8055eb0 *)
mov r9 L0x200186a2;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0019@sint32 : and [cf0019 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0319@sint32 : and [cf0319 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0619@sint32 : and [cf0619 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x2001509c; PC = 0x8055f04 *)
mov L0x2001509c r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200150c0; PC = 0x8055f08 *)
mov L0x200150c0 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200150e4; PC = 0x8055f0c *)
mov L0x200150e4 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015e1c; PC = 0x8055f10 *)
mov L0x20015e1c r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015e40; PC = 0x8055f14 *)
mov L0x20015e40 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015e64; PC = 0x8055f18 *)
mov L0x20015e64 r9;



(******************** offset 0, 0, 19 ********************)


(**************** CUT  19, - *****************)

ecut and [
eqmod cf0019 f0019 2048, eqmod cf0319 f0319 2048, eqmod cf0619 f0619 2048,
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x2001509c*x**0*y**0*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x200150c0*x**0*y**0*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x200150e4*x**0*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x20015e1c*x**0*y**0*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x20015e40*x**0*y**0*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x20015e64*x**0*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x200189a8; Value = 0x00030003; PC = 0x8055f1c *)
mov r5 L0x200189a8;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x20018768; Value = 0x00030000; PC = 0x8055f20 *)
mov r6 L0x20018768;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018be8; Value = 0x00000000; PC = 0x8055f24 *)
mov r8 L0x20018be8;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0020@sint32 : and [cf0020 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0320@sint32 : and [cf0320 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0620@sint32 : and [cf0620 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20015108; PC = 0x8055f64 *)
mov L0x20015108 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x2001512c; PC = 0x8055f68 *)
mov L0x2001512c r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015150; PC = 0x8055f6c *)
mov L0x20015150 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015e88; PC = 0x8055f70 *)
mov L0x20015e88 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015eac; PC = 0x8055f74 *)
mov L0x20015eac r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015ed0; PC = 0x8055f78 *)
mov L0x20015ed0 r9;



(******************** offset 0, 0, 20 ********************)


(**************** CUT  20, - *****************)

ecut and [
eqmod cf0020 f0020 2048, eqmod cf0320 f0320 2048, eqmod cf0620 f0620 2048,
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x20015108*x**0*y**0*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x2001512c*x**0*y**0*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x20015150*x**0*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x20015e88*x**0*y**0*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x20015eac*x**0*y**0*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x20015ed0*x**0*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x2001882e; Value = 0x00030ffd; PC = 0x8055f7c *)
mov r5 L0x2001882e;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x200185ee; Value = 0x0ffd0ffd; PC = 0x8055f80 *)
mov r6 L0x200185ee;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018a6e; Value = 0x0ffd0000; PC = 0x8055f84 *)
mov r8 L0x20018a6e;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0021@sint32 : and [cf0021 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0321@sint32 : and [cf0321 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0621@sint32 : and [cf0621 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015174; PC = 0x8055fc4 *)
mov L0x20015174 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20015198; PC = 0x8055fc8 *)
mov L0x20015198 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200151bc; PC = 0x8055fcc *)
mov L0x200151bc r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015ef4; PC = 0x8055fd0 *)
mov L0x20015ef4 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015f18; PC = 0x8055fd4 *)
mov L0x20015f18 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015f3c; PC = 0x8055fd8 *)
mov L0x20015f3c r9;



(******************** offset 0, 0, 21 ********************)


(**************** CUT  21, - *****************)

ecut and [
eqmod cf0021 f0021 2048, eqmod cf0321 f0321 2048, eqmod cf0621 f0621 2048,
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x20015174*x**0*y**0*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x20015198*x**0*y**0*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x200151bc*x**0*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x20015ef4*x**0*y**0*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x20015f18*x**0*y**0*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x20015f3c*x**0*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x200186b4; Value = 0x0ffd0003; PC = 0x8055fdc *)
mov r5 L0x200186b4;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018b34; Value = 0x00000000; PC = 0x8055fe0 *)
mov r7 L0x20018b34;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x200188f4; Value = 0x00030000; PC = 0x8055fe4 *)
mov r8 L0x200188f4;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0022@sint32 : and [cf0022 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0322@sint32 : and [cf0322 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0622@sint32 : and [cf0622 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200151e0; PC = 0x8056038 *)
mov L0x200151e0 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20015204; PC = 0x805603c *)
mov L0x20015204 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015228; PC = 0x8056040 *)
mov L0x20015228 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015f60; PC = 0x8056044 *)
mov L0x20015f60 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015f84; PC = 0x8056048 *)
mov L0x20015f84 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015fa8; PC = 0x805604c *)
mov L0x20015fa8 r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;



(******************** offset 0, 0, 22 ********************)


(**************** CUT  22, - *****************)

ecut and [
eqmod cf0022 f0022 2048, eqmod cf0322 f0322 2048, eqmod cf0622 f0622 2048,
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x200151e0*x**0*y**0*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x20015204*x**0*y**0*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x20015228*x**0*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x20015f60*x**0*y**0*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x20015f84*x**0*y**0*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x20015fa8*x**0*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1566]	; 0x61e                 #! EA = L0x20018bfa; Value = 0x00000000; PC = 0x805605c *)
mov r4 L0x20018bfa;
(* ldrsh.w	r7, [lr, #990]	; 0x3de                  #! EA = L0x200189ba; Value = 0x00030ffd; PC = 0x8056060 *)
mov r7 L0x200189ba;
(* ldrsh.w	r8, [lr, #414]	; 0x19e                  #! EA = L0x2001877a; Value = 0x00000ffd; PC = 0x8056064 *)
mov r8 L0x2001877a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056068 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0023@sint32 : and [cf0023 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805606c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0323@sint32 : and [cf0323 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056070 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0623@sint32 : and [cf0623 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056074 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056078 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x805607c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x8056080 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056084 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056088 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x805608a *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x805608c *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056090 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056094 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056098 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805609c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805609e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80560a0 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80560a4 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80560a8 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80560ac *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x2001524c; PC = 0x80560b0 *)
mov L0x2001524c r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20015270; PC = 0x80560b4 *)
mov L0x20015270 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20015294; PC = 0x80560b8 *)
mov L0x20015294 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015fcc; PC = 0x80560bc *)
mov L0x20015fcc r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015ff0; PC = 0x80560c0 *)
mov L0x20015ff0 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20016014; PC = 0x80560c4 *)
mov L0x20016014 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80560c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 0, 0, 23 ********************)


(**************** CUT  23, - *****************)

ecut and [
eqmod cf0023 f0023 2048, eqmod cf0323 f0323 2048, eqmod cf0623 f0623 2048,
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x2001524c*x**0*y**0*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x20015270*x**0*y**0*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x20015294*x**0*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x20015fcc*x**0*y**0*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x20015ff0*x**0*y**0*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x20016014*x**0*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1188]	; 0x4a4                 #! EA = L0x20018a80; Value = 0x00000ffd; PC = 0x80560cc *)
mov r4 L0x20018a80;
(* ldrsh.w	r7, [lr, #612]	; 0x264                  #! EA = L0x20018840; Value = 0x00000000; PC = 0x80560d0 *)
mov r7 L0x20018840;
(* ldrsh.w	r8, [lr, #36]	; 0x24                    #! EA = L0x20018600; Value = 0x00030ffd; PC = 0x80560d4 *)
mov r8 L0x20018600;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80560d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0024@sint32 : and [cf0024 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80560dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0324@sint32 : and [cf0324 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80560e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0624@sint32 : and [cf0624 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80560e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80560e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80560ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80560f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80560f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80560f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80560fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80560fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056100 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056104 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056108 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805610c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805610e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056110 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056114 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056118 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805611c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200152b8; PC = 0x8056120 *)
mov L0x200152b8 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200152dc; PC = 0x8056124 *)
mov L0x200152dc r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015300; PC = 0x8056128 *)
mov L0x20015300 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016038; PC = 0x805612c *)
mov L0x20016038 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x2001605c; PC = 0x8056130 *)
mov L0x2001605c r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20016080; PC = 0x8056134 *)
mov L0x20016080 r9;



(******************** offset 0, 0, 24 ********************)


(**************** CUT  24, - *****************)

ecut and [
eqmod cf0024 f0024 2048, eqmod cf0324 f0324 2048, eqmod cf0624 f0624 2048,
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x200152b8*x**0*y**0*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x200152dc*x**0*y**0*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x20015300*x**0*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x20016038*x**0*y**0*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x2001605c*x**0*y**0*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x20016080*x**0*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #810]	; 0x32a                  #! EA = L0x20018906; Value = 0x00030ffd; PC = 0x8056138 *)
mov r4 L0x20018906;
(* ldrsh.w	r7, [lr, #234]	; 0xea                   #! EA = L0x200186c6; Value = 0x00000ffd; PC = 0x805613c *)
mov r7 L0x200186c6;
(* ldrsh.w	r9, [lr, #1386]	; 0x56a                 #! EA = L0x20018b46; Value = 0x0ffd0000; PC = 0x8056140 *)
mov r9 L0x20018b46;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056144 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0025@sint32 : and [cf0025 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056148 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0325@sint32 : and [cf0325 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x805614c *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0625@sint32 : and [cf0625 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056150 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056154 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x8056158 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x805615c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056160 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056164 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x8056168 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x805616c *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056170 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056174 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x8056176 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056178 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015324; PC = 0x805617c *)
mov L0x20015324 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20015348; PC = 0x8056180 *)
mov L0x20015348 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x2001536c; PC = 0x8056184 *)
mov L0x2001536c r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200160a4; PC = 0x8056188 *)
mov L0x200160a4 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200160c8; PC = 0x805618c *)
mov L0x200160c8 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200160ec; PC = 0x8056190 *)
mov L0x200160ec r9;



(******************** offset 0, 0, 25 ********************)


(**************** CUT  25, - *****************)

ecut and [
eqmod cf0025 f0025 2048, eqmod cf0325 f0325 2048, eqmod cf0625 f0625 2048,
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x20015324*x**0*y**0*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x20015348*x**0*y**0*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x2001536c*x**0*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x200160a4*x**0*y**0*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x200160c8*x**0*y**0*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x200160ec*x**0*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #432]	; 0x1b0                  #! EA = L0x2001878c; Value = 0x00000ffd; PC = 0x8056194 *)
mov r4 L0x2001878c;
(* ldrsh.w	r6, [lr, #1584]	; 0x630                 #! EA = L0x20018c0c; Value = 0x00000000; PC = 0x8056198 *)
mov r6 L0x20018c0c;
(* ldrsh.w	r9, [lr, #1008]	; 0x3f0                 #! EA = L0x200189cc; Value = 0x0ffd0ffd; PC = 0x805619c *)
mov r9 L0x200189cc;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80561a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0026@sint32 : and [cf0026 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80561a4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0326@sint32 : and [cf0326 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80561a8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0626@sint32 : and [cf0626 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80561ac *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80561b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80561b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80561b8 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80561bc *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80561c0 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80561c2 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80561c4 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80561c8 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80561cc *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80561d0 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80561d4 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80561d6 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80561d8 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80561dc *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80561e0 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80561e4 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20015390; PC = 0x80561e8 *)
mov L0x20015390 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200153b4; PC = 0x80561ec *)
mov L0x200153b4 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200153d8; PC = 0x80561f0 *)
mov L0x200153d8 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016110; PC = 0x80561f4 *)
mov L0x20016110 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016134; PC = 0x80561f8 *)
mov L0x20016134 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20016158; PC = 0x80561fc *)
mov L0x20016158 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056200 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056204 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x8055e3c <_Good_loop0_inner>              #! PC = 0x8056208 *)
#b.w	0x8055e3c <_Good_loop0_inner>              #! 0x8056208 = 0x8056208;



(******************** offset 0, 0, 26 ********************)


(**************** CUT  26, - *****************)

ecut and [
eqmod cf0026 f0026 2048, eqmod cf0326 f0326 2048, eqmod cf0626 f0626 2048,
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x20015390*x**0*y**0*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x200153b4*x**0*y**0*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x200153d8*x**0*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x20016110*x**0*y**0*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x20016134*x**0*y**0*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x20016158*x**0*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr]                                #! EA = L0x20018612; Value = 0x00030003; PC = 0x8055e3c *)
mov r4 L0x20018612;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x20018a92; Value = 0x00030ffd; PC = 0x8055e40 *)
mov r6 L0x20018a92;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x20018852; Value = 0x00030ffd; PC = 0x8055e44 *)
mov r9 L0x20018852;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0027@sint32 : and [cf0027 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0327@sint32 : and [cf0327 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0627@sint32 : and [cf0627 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200153fc; PC = 0x8055e90 *)
mov L0x200153fc r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015420; PC = 0x8055e94 *)
mov L0x20015420 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015444; PC = 0x8055e98 *)
mov L0x20015444 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x2001617c; PC = 0x8055e9c *)
mov L0x2001617c r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200161a0; PC = 0x8055ea0 *)
mov L0x200161a0 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200161c4; PC = 0x8055ea4 *)
mov L0x200161c4 r9;



(******************** offset 0, 0, 27 ********************)


(**************** CUT  27, - *****************)

ecut and [
eqmod cf0027 f0027 2048, eqmod cf0327 f0327 2048, eqmod cf0627 f0627 2048,
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x200153fc*x**0*y**0*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x20015420*x**0*y**0*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x20015444*x**0*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x2001617c*x**0*y**0*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x200161a0*x**0*y**0*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x200161c4*x**0*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018b58; Value = 0x0ffd0003; PC = 0x8055ea8 *)
mov r5 L0x20018b58;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x20018918; Value = 0x00000003; PC = 0x8055eac *)
mov r6 L0x20018918;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x200186d8; Value = 0x00030003; PC = 0x8055eb0 *)
mov r9 L0x200186d8;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0028@sint32 : and [cf0028 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0328@sint32 : and [cf0328 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0628@sint32 : and [cf0628 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015468; PC = 0x8055f04 *)
mov L0x20015468 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x2001548c; PC = 0x8055f08 *)
mov L0x2001548c r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200154b0; PC = 0x8055f0c *)
mov L0x200154b0 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200161e8; PC = 0x8055f10 *)
mov L0x200161e8 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x2001620c; PC = 0x8055f14 *)
mov L0x2001620c r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20016230; PC = 0x8055f18 *)
mov L0x20016230 r9;



(******************** offset 0, 0, 28 ********************)


(**************** CUT  28, - *****************)

ecut and [
eqmod cf0028 f0028 2048, eqmod cf0328 f0328 2048, eqmod cf0628 f0628 2048,
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x20015468*x**0*y**0*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x2001548c*x**0*y**0*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x200154b0*x**0*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x200161e8*x**0*y**0*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x2001620c*x**0*y**0*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x20016230*x**0*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x200189de; Value = 0x00030000; PC = 0x8055f1c *)
mov r5 L0x200189de;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x2001879e; Value = 0x0ffd0000; PC = 0x8055f20 *)
mov r6 L0x2001879e;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018c1e; Value = 0x00000000; PC = 0x8055f24 *)
mov r8 L0x20018c1e;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0029@sint32 : and [cf0029 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0329@sint32 : and [cf0329 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0629@sint32 : and [cf0629 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200154d4; PC = 0x8055f64 *)
mov L0x200154d4 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200154f8; PC = 0x8055f68 *)
mov L0x200154f8 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x2001551c; PC = 0x8055f6c *)
mov L0x2001551c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016254; PC = 0x8055f70 *)
mov L0x20016254 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016278; PC = 0x8055f74 *)
mov L0x20016278 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x2001629c; PC = 0x8055f78 *)
mov L0x2001629c r9;



(******************** offset 0, 0, 29 ********************)


(**************** CUT  29, - *****************)

ecut and [
eqmod cf0029 f0029 2048, eqmod cf0329 f0329 2048, eqmod cf0629 f0629 2048,
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x200154d4*x**0*y**0*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x200154f8*x**0*y**0*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x2001551c*x**0*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x20016254*x**0*y**0*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x20016278*x**0*y**0*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x2001629c*x**0*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x20018864; Value = 0x0ffd0ffd; PC = 0x8055f7c *)
mov r5 L0x20018864;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x20018624; Value = 0x00030000; PC = 0x8055f80 *)
mov r6 L0x20018624;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018aa4; Value = 0x0ffd0ffd; PC = 0x8055f84 *)
mov r8 L0x20018aa4;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0030@sint32 : and [cf0030 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0330@sint32 : and [cf0330 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0630@sint32 : and [cf0630 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015540; PC = 0x8055fc4 *)
mov L0x20015540 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20015564; PC = 0x8055fc8 *)
mov L0x20015564 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20015588; PC = 0x8055fcc *)
mov L0x20015588 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x200162c0; PC = 0x8055fd0 *)
mov L0x200162c0 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x200162e4; PC = 0x8055fd4 *)
mov L0x200162e4 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20016308; PC = 0x8055fd8 *)
mov L0x20016308 r9;



(******************** offset 0, 0, 30 ********************)


(**************** CUT  30, - *****************)

ecut and [
eqmod cf0030 f0030 2048, eqmod cf0330 f0330 2048, eqmod cf0630 f0630 2048,
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x20015540*x**0*y**0*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x20015564*x**0*y**0*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x20015588*x**0*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x200162c0*x**0*y**0*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x200162e4*x**0*y**0*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x20016308*x**0*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x200186ea; Value = 0x00000003; PC = 0x8055fdc *)
mov r5 L0x200186ea;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018b6a; Value = 0x00000000; PC = 0x8055fe0 *)
mov r7 L0x20018b6a;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x2001892a; Value = 0x00030003; PC = 0x8055fe4 *)
mov r8 L0x2001892a;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0031@sint32 : and [cf0031 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0331@sint32 : and [cf0331 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0631@sint32 : and [cf0631 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200155ac; PC = 0x8056038 *)
mov L0x200155ac r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x200155d0; PC = 0x805603c *)
mov L0x200155d0 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x200155f4; PC = 0x8056040 *)
mov L0x200155f4 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x2001632c; PC = 0x8056044 *)
mov L0x2001632c r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20016350; PC = 0x8056048 *)
mov L0x20016350 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20016374; PC = 0x805604c *)
mov L0x20016374 r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;
(* sub.w	lr, lr, #162	; 0xa2                       #! PC = 0x805620c *)
subs dc lr lr 162@uint32;
(* subw	r0, r0, #2916	; 0xb64                      #! PC = 0x8056210 *)
subc dc r0 r0 2916@uint32;
(* add.w	lr, lr, #2                                #! PC = 0x8056214 *)
adds dc lr lr 2@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8056218 *)
adds dc r0 r0 4@uint32;
(* cmp.w	r0, r12                                   #! PC = 0x805621c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8055e34 <_Good_loop0>                  #! PC = 0x8056220 *)
#bne.w	0x8055e34 <_Good_loop0>                  #! 0x8056220 = 0x8056220;
(* addw	r1, r0, #2916	; 0xb64                      #! PC = 0x8055e34 *)
adds dc r1 r0 2916@uint32;
(* vmov	s3, r1                                     #! PC = 0x8055e38 *)
mov s3 r1;



(******************** offset 0, 0, 31 ********************)


(**************** CUT  31, - *****************)

ecut and [
eqmod cf0031 f0031 2048, eqmod cf0331 f0331 2048, eqmod cf0631 f0631 2048,
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x200155ac*x**0*y**0*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x200155d0*x**0*y**0*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x200155f4*x**0*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x2001632c*x**0*y**0*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x20016350*x**0*y**0*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x20016374*x**0*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(**************** CUT -,   0 *****************)

rcut and [
(-3367617)@32<=sL0x20014898,L0x20014898<=s3367617@32,
(-3367617)@32<=sL0x200148bc,L0x200148bc<=s3367617@32,
(-3367617)@32<=sL0x200148e0,L0x200148e0<=s3367617@32,
(-3367617)@32<=sL0x20015618,L0x20015618<=s3367617@32,
(-3367617)@32<=sL0x2001563c,L0x2001563c<=s3367617@32,
(-3367617)@32<=sL0x20015660,L0x20015660<=s3367617@32
,
(-3367617)@32<=sL0x20014904,L0x20014904<=s3367617@32,
(-3367617)@32<=sL0x20014928,L0x20014928<=s3367617@32,
(-3367617)@32<=sL0x2001494c,L0x2001494c<=s3367617@32,
(-3367617)@32<=sL0x20015684,L0x20015684<=s3367617@32,
(-3367617)@32<=sL0x200156a8,L0x200156a8<=s3367617@32,
(-3367617)@32<=sL0x200156cc,L0x200156cc<=s3367617@32
,
(-3367617)@32<=sL0x20014970,L0x20014970<=s3367617@32,
(-3367617)@32<=sL0x20014994,L0x20014994<=s3367617@32,
(-3367617)@32<=sL0x200149b8,L0x200149b8<=s3367617@32,
(-3367617)@32<=sL0x200156f0,L0x200156f0<=s3367617@32,
(-3367617)@32<=sL0x20015714,L0x20015714<=s3367617@32,
(-3367617)@32<=sL0x20015738,L0x20015738<=s3367617@32
,
(-3367617)@32<=sL0x200149dc,L0x200149dc<=s3367617@32,
(-3367617)@32<=sL0x20014a00,L0x20014a00<=s3367617@32,
(-3367617)@32<=sL0x20014a24,L0x20014a24<=s3367617@32,
(-3367617)@32<=sL0x2001575c,L0x2001575c<=s3367617@32,
(-3367617)@32<=sL0x20015780,L0x20015780<=s3367617@32,
(-3367617)@32<=sL0x200157a4,L0x200157a4<=s3367617@32
,
(-3367617)@32<=sL0x20014a48,L0x20014a48<=s3367617@32,
(-3367617)@32<=sL0x20014a6c,L0x20014a6c<=s3367617@32,
(-3367617)@32<=sL0x20014a90,L0x20014a90<=s3367617@32,
(-3367617)@32<=sL0x200157c8,L0x200157c8<=s3367617@32,
(-3367617)@32<=sL0x200157ec,L0x200157ec<=s3367617@32,
(-3367617)@32<=sL0x20015810,L0x20015810<=s3367617@32
,
(-3367617)@32<=sL0x20014ab4,L0x20014ab4<=s3367617@32,
(-3367617)@32<=sL0x20014ad8,L0x20014ad8<=s3367617@32,
(-3367617)@32<=sL0x20014afc,L0x20014afc<=s3367617@32,
(-3367617)@32<=sL0x20015834,L0x20015834<=s3367617@32,
(-3367617)@32<=sL0x20015858,L0x20015858<=s3367617@32,
(-3367617)@32<=sL0x2001587c,L0x2001587c<=s3367617@32
,
(-3367617)@32<=sL0x20014b20,L0x20014b20<=s3367617@32,
(-3367617)@32<=sL0x20014b44,L0x20014b44<=s3367617@32,
(-3367617)@32<=sL0x20014b68,L0x20014b68<=s3367617@32,
(-3367617)@32<=sL0x200158a0,L0x200158a0<=s3367617@32,
(-3367617)@32<=sL0x200158c4,L0x200158c4<=s3367617@32,
(-3367617)@32<=sL0x200158e8,L0x200158e8<=s3367617@32
,
(-3367617)@32<=sL0x20014b8c,L0x20014b8c<=s3367617@32,
(-3367617)@32<=sL0x20014bb0,L0x20014bb0<=s3367617@32,
(-3367617)@32<=sL0x20014bd4,L0x20014bd4<=s3367617@32,
(-3367617)@32<=sL0x2001590c,L0x2001590c<=s3367617@32,
(-3367617)@32<=sL0x20015930,L0x20015930<=s3367617@32,
(-3367617)@32<=sL0x20015954,L0x20015954<=s3367617@32
,
(-3367617)@32<=sL0x20014bf8,L0x20014bf8<=s3367617@32,
(-3367617)@32<=sL0x20014c1c,L0x20014c1c<=s3367617@32,
(-3367617)@32<=sL0x20014c40,L0x20014c40<=s3367617@32,
(-3367617)@32<=sL0x20015978,L0x20015978<=s3367617@32,
(-3367617)@32<=sL0x2001599c,L0x2001599c<=s3367617@32,
(-3367617)@32<=sL0x200159c0,L0x200159c0<=s3367617@32
,
(-3367617)@32<=sL0x20014c64,L0x20014c64<=s3367617@32,
(-3367617)@32<=sL0x20014c88,L0x20014c88<=s3367617@32,
(-3367617)@32<=sL0x20014cac,L0x20014cac<=s3367617@32,
(-3367617)@32<=sL0x200159e4,L0x200159e4<=s3367617@32,
(-3367617)@32<=sL0x20015a08,L0x20015a08<=s3367617@32,
(-3367617)@32<=sL0x20015a2c,L0x20015a2c<=s3367617@32
,
(-3367617)@32<=sL0x20014cd0,L0x20014cd0<=s3367617@32,
(-3367617)@32<=sL0x20014cf4,L0x20014cf4<=s3367617@32,
(-3367617)@32<=sL0x20014d18,L0x20014d18<=s3367617@32,
(-3367617)@32<=sL0x20015a50,L0x20015a50<=s3367617@32,
(-3367617)@32<=sL0x20015a74,L0x20015a74<=s3367617@32,
(-3367617)@32<=sL0x20015a98,L0x20015a98<=s3367617@32
,
(-3367617)@32<=sL0x20014d3c,L0x20014d3c<=s3367617@32,
(-3367617)@32<=sL0x20014d60,L0x20014d60<=s3367617@32,
(-3367617)@32<=sL0x20014d84,L0x20014d84<=s3367617@32,
(-3367617)@32<=sL0x20015abc,L0x20015abc<=s3367617@32,
(-3367617)@32<=sL0x20015ae0,L0x20015ae0<=s3367617@32,
(-3367617)@32<=sL0x20015b04,L0x20015b04<=s3367617@32
,
(-3367617)@32<=sL0x20014da8,L0x20014da8<=s3367617@32,
(-3367617)@32<=sL0x20014dcc,L0x20014dcc<=s3367617@32,
(-3367617)@32<=sL0x20014df0,L0x20014df0<=s3367617@32,
(-3367617)@32<=sL0x20015b28,L0x20015b28<=s3367617@32,
(-3367617)@32<=sL0x20015b4c,L0x20015b4c<=s3367617@32,
(-3367617)@32<=sL0x20015b70,L0x20015b70<=s3367617@32
,
(-3367617)@32<=sL0x20014e14,L0x20014e14<=s3367617@32,
(-3367617)@32<=sL0x20014e38,L0x20014e38<=s3367617@32,
(-3367617)@32<=sL0x20014e5c,L0x20014e5c<=s3367617@32,
(-3367617)@32<=sL0x20015b94,L0x20015b94<=s3367617@32,
(-3367617)@32<=sL0x20015bb8,L0x20015bb8<=s3367617@32,
(-3367617)@32<=sL0x20015bdc,L0x20015bdc<=s3367617@32
,
(-3367617)@32<=sL0x20014e80,L0x20014e80<=s3367617@32,
(-3367617)@32<=sL0x20014ea4,L0x20014ea4<=s3367617@32,
(-3367617)@32<=sL0x20014ec8,L0x20014ec8<=s3367617@32,
(-3367617)@32<=sL0x20015c00,L0x20015c00<=s3367617@32,
(-3367617)@32<=sL0x20015c24,L0x20015c24<=s3367617@32,
(-3367617)@32<=sL0x20015c48,L0x20015c48<=s3367617@32
,
(-3367617)@32<=sL0x20014eec,L0x20014eec<=s3367617@32,
(-3367617)@32<=sL0x20014f10,L0x20014f10<=s3367617@32,
(-3367617)@32<=sL0x20014f34,L0x20014f34<=s3367617@32,
(-3367617)@32<=sL0x20015c6c,L0x20015c6c<=s3367617@32,
(-3367617)@32<=sL0x20015c90,L0x20015c90<=s3367617@32,
(-3367617)@32<=sL0x20015cb4,L0x20015cb4<=s3367617@32
,
(-3367617)@32<=sL0x20014f58,L0x20014f58<=s3367617@32,
(-3367617)@32<=sL0x20014f7c,L0x20014f7c<=s3367617@32,
(-3367617)@32<=sL0x20014fa0,L0x20014fa0<=s3367617@32,
(-3367617)@32<=sL0x20015cd8,L0x20015cd8<=s3367617@32,
(-3367617)@32<=sL0x20015cfc,L0x20015cfc<=s3367617@32,
(-3367617)@32<=sL0x20015d20,L0x20015d20<=s3367617@32
,
(-3367617)@32<=sL0x20014fc4,L0x20014fc4<=s3367617@32,
(-3367617)@32<=sL0x20014fe8,L0x20014fe8<=s3367617@32,
(-3367617)@32<=sL0x2001500c,L0x2001500c<=s3367617@32,
(-3367617)@32<=sL0x20015d44,L0x20015d44<=s3367617@32,
(-3367617)@32<=sL0x20015d68,L0x20015d68<=s3367617@32,
(-3367617)@32<=sL0x20015d8c,L0x20015d8c<=s3367617@32
,
(-3367617)@32<=sL0x20015030,L0x20015030<=s3367617@32,
(-3367617)@32<=sL0x20015054,L0x20015054<=s3367617@32,
(-3367617)@32<=sL0x20015078,L0x20015078<=s3367617@32,
(-3367617)@32<=sL0x20015db0,L0x20015db0<=s3367617@32,
(-3367617)@32<=sL0x20015dd4,L0x20015dd4<=s3367617@32,
(-3367617)@32<=sL0x20015df8,L0x20015df8<=s3367617@32
,
(-3367617)@32<=sL0x2001509c,L0x2001509c<=s3367617@32,
(-3367617)@32<=sL0x200150c0,L0x200150c0<=s3367617@32,
(-3367617)@32<=sL0x200150e4,L0x200150e4<=s3367617@32,
(-3367617)@32<=sL0x20015e1c,L0x20015e1c<=s3367617@32,
(-3367617)@32<=sL0x20015e40,L0x20015e40<=s3367617@32,
(-3367617)@32<=sL0x20015e64,L0x20015e64<=s3367617@32
,
(-3367617)@32<=sL0x20015108,L0x20015108<=s3367617@32,
(-3367617)@32<=sL0x2001512c,L0x2001512c<=s3367617@32,
(-3367617)@32<=sL0x20015150,L0x20015150<=s3367617@32,
(-3367617)@32<=sL0x20015e88,L0x20015e88<=s3367617@32,
(-3367617)@32<=sL0x20015eac,L0x20015eac<=s3367617@32,
(-3367617)@32<=sL0x20015ed0,L0x20015ed0<=s3367617@32
,
(-3367617)@32<=sL0x20015174,L0x20015174<=s3367617@32,
(-3367617)@32<=sL0x20015198,L0x20015198<=s3367617@32,
(-3367617)@32<=sL0x200151bc,L0x200151bc<=s3367617@32,
(-3367617)@32<=sL0x20015ef4,L0x20015ef4<=s3367617@32,
(-3367617)@32<=sL0x20015f18,L0x20015f18<=s3367617@32,
(-3367617)@32<=sL0x20015f3c,L0x20015f3c<=s3367617@32
,
(-3367617)@32<=sL0x200151e0,L0x200151e0<=s3367617@32,
(-3367617)@32<=sL0x20015204,L0x20015204<=s3367617@32,
(-3367617)@32<=sL0x20015228,L0x20015228<=s3367617@32,
(-3367617)@32<=sL0x20015f60,L0x20015f60<=s3367617@32,
(-3367617)@32<=sL0x20015f84,L0x20015f84<=s3367617@32,
(-3367617)@32<=sL0x20015fa8,L0x20015fa8<=s3367617@32
,
(-3367617)@32<=sL0x2001524c,L0x2001524c<=s3367617@32,
(-3367617)@32<=sL0x20015270,L0x20015270<=s3367617@32,
(-3367617)@32<=sL0x20015294,L0x20015294<=s3367617@32,
(-3367617)@32<=sL0x20015fcc,L0x20015fcc<=s3367617@32,
(-3367617)@32<=sL0x20015ff0,L0x20015ff0<=s3367617@32,
(-3367617)@32<=sL0x20016014,L0x20016014<=s3367617@32
,
(-3367617)@32<=sL0x200152b8,L0x200152b8<=s3367617@32,
(-3367617)@32<=sL0x200152dc,L0x200152dc<=s3367617@32,
(-3367617)@32<=sL0x20015300,L0x20015300<=s3367617@32,
(-3367617)@32<=sL0x20016038,L0x20016038<=s3367617@32,
(-3367617)@32<=sL0x2001605c,L0x2001605c<=s3367617@32,
(-3367617)@32<=sL0x20016080,L0x20016080<=s3367617@32
,
(-3367617)@32<=sL0x20015324,L0x20015324<=s3367617@32,
(-3367617)@32<=sL0x20015348,L0x20015348<=s3367617@32,
(-3367617)@32<=sL0x2001536c,L0x2001536c<=s3367617@32,
(-3367617)@32<=sL0x200160a4,L0x200160a4<=s3367617@32,
(-3367617)@32<=sL0x200160c8,L0x200160c8<=s3367617@32,
(-3367617)@32<=sL0x200160ec,L0x200160ec<=s3367617@32
,
(-3367617)@32<=sL0x20015390,L0x20015390<=s3367617@32,
(-3367617)@32<=sL0x200153b4,L0x200153b4<=s3367617@32,
(-3367617)@32<=sL0x200153d8,L0x200153d8<=s3367617@32,
(-3367617)@32<=sL0x20016110,L0x20016110<=s3367617@32,
(-3367617)@32<=sL0x20016134,L0x20016134<=s3367617@32,
(-3367617)@32<=sL0x20016158,L0x20016158<=s3367617@32
,
(-3367617)@32<=sL0x200153fc,L0x200153fc<=s3367617@32,
(-3367617)@32<=sL0x20015420,L0x20015420<=s3367617@32,
(-3367617)@32<=sL0x20015444,L0x20015444<=s3367617@32,
(-3367617)@32<=sL0x2001617c,L0x2001617c<=s3367617@32,
(-3367617)@32<=sL0x200161a0,L0x200161a0<=s3367617@32,
(-3367617)@32<=sL0x200161c4,L0x200161c4<=s3367617@32
,
(-3367617)@32<=sL0x20015468,L0x20015468<=s3367617@32,
(-3367617)@32<=sL0x2001548c,L0x2001548c<=s3367617@32,
(-3367617)@32<=sL0x200154b0,L0x200154b0<=s3367617@32,
(-3367617)@32<=sL0x200161e8,L0x200161e8<=s3367617@32,
(-3367617)@32<=sL0x2001620c,L0x2001620c<=s3367617@32,
(-3367617)@32<=sL0x20016230,L0x20016230<=s3367617@32
,
(-3367617)@32<=sL0x200154d4,L0x200154d4<=s3367617@32,
(-3367617)@32<=sL0x200154f8,L0x200154f8<=s3367617@32,
(-3367617)@32<=sL0x2001551c,L0x2001551c<=s3367617@32,
(-3367617)@32<=sL0x20016254,L0x20016254<=s3367617@32,
(-3367617)@32<=sL0x20016278,L0x20016278<=s3367617@32,
(-3367617)@32<=sL0x2001629c,L0x2001629c<=s3367617@32
,
(-3367617)@32<=sL0x20015540,L0x20015540<=s3367617@32,
(-3367617)@32<=sL0x20015564,L0x20015564<=s3367617@32,
(-3367617)@32<=sL0x20015588,L0x20015588<=s3367617@32,
(-3367617)@32<=sL0x200162c0,L0x200162c0<=s3367617@32,
(-3367617)@32<=sL0x200162e4,L0x200162e4<=s3367617@32,
(-3367617)@32<=sL0x20016308,L0x20016308<=s3367617@32
,
(-3367617)@32<=sL0x200155ac,L0x200155ac<=s3367617@32,
(-3367617)@32<=sL0x200155d0,L0x200155d0<=s3367617@32,
(-3367617)@32<=sL0x200155f4,L0x200155f4<=s3367617@32,
(-3367617)@32<=sL0x2001632c,L0x2001632c<=s3367617@32,
(-3367617)@32<=sL0x20016350,L0x20016350<=s3367617@32,
(-3367617)@32<=sL0x20016374,L0x20016374<=s3367617@32
] prove with [ precondition ];





(* ldrsh.w	r4, [lr]                                #! EA = L0x20018572; Value = 0x0ffd0003; PC = 0x8055e3c *)
mov r4 L0x20018572;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x200189f2; Value = 0x00000ffd; PC = 0x8055e40 *)
mov r6 L0x200189f2;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x200187b2; Value = 0x00000ffd; PC = 0x8055e44 *)
mov r9 L0x200187b2;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1000@sint32 : and [cf1000 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1300@sint32 : and [cf1300 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1600@sint32 : and [cf1600 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x2001489c; PC = 0x8055e90 *)
mov L0x2001489c r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200148c0; PC = 0x8055e94 *)
mov L0x200148c0 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x200148e4; PC = 0x8055e98 *)
mov L0x200148e4 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x2001561c; PC = 0x8055e9c *)
mov L0x2001561c r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015640; PC = 0x8055ea0 *)
mov L0x20015640 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015664; PC = 0x8055ea4 *)
mov L0x20015664 r9;



(******************** offset 1, 0,  0 ********************)


(**************** CUT  32, - *****************)

ecut and [
eqmod cf1000 f1000 2048, eqmod cf1300 f1300 2048, eqmod cf1600 f1600 2048,
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x2001489c*x**1*y**0*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x200148c0*x**1*y**0*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x200148e4*x**1*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x2001561c*x**1*y**0*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x20015640*x**1*y**0*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x20015664*x**1*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018ab8; Value = 0x00030000; PC = 0x8055ea8 *)
mov r5 L0x20018ab8;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x20018878; Value = 0x00000ffd; PC = 0x8055eac *)
mov r6 L0x20018878;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x20018638; Value = 0x00000000; PC = 0x8055eb0 *)
mov r9 L0x20018638;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1001@sint32 : and [cf1001 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1301@sint32 : and [cf1301 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1601@sint32 : and [cf1601 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014908; PC = 0x8055f04 *)
mov L0x20014908 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x2001492c; PC = 0x8055f08 *)
mov L0x2001492c r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014950; PC = 0x8055f0c *)
mov L0x20014950 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015688; PC = 0x8055f10 *)
mov L0x20015688 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200156ac; PC = 0x8055f14 *)
mov L0x200156ac r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200156d0; PC = 0x8055f18 *)
mov L0x200156d0 r9;



(******************** offset 1, 0,  1 ********************)


(**************** CUT  33, - *****************)

ecut and [
eqmod cf1001 f1001 2048, eqmod cf1301 f1301 2048, eqmod cf1601 f1601 2048,
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x20014908*x**1*y**0*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x2001492c*x**1*y**0*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x20014950*x**1*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x20015688*x**1*y**0*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x200156ac*x**1*y**0*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x200156d0*x**1*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x2001893e; Value = 0x00030000; PC = 0x8055f1c *)
mov r5 L0x2001893e;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x200186fe; Value = 0x00030000; PC = 0x8055f20 *)
mov r6 L0x200186fe;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018b7e; Value = 0x00030000; PC = 0x8055f24 *)
mov r8 L0x20018b7e;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1002@sint32 : and [cf1002 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1302@sint32 : and [cf1302 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1602@sint32 : and [cf1602 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014974; PC = 0x8055f64 *)
mov L0x20014974 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014998; PC = 0x8055f68 *)
mov L0x20014998 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200149bc; PC = 0x8055f6c *)
mov L0x200149bc r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x200156f4; PC = 0x8055f70 *)
mov L0x200156f4 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015718; PC = 0x8055f74 *)
mov L0x20015718 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x2001573c; PC = 0x8055f78 *)
mov L0x2001573c r9;



(******************** offset 1, 0,  2 ********************)


(**************** CUT  34, - *****************)

ecut and [
eqmod cf1002 f1002 2048, eqmod cf1302 f1302 2048, eqmod cf1602 f1602 2048,
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x20014974*x**1*y**0*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x20014998*x**1*y**0*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x200149bc*x**1*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x200156f4*x**1*y**0*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x20015718*x**1*y**0*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x2001573c*x**1*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x200187c4; Value = 0x0ffd0003; PC = 0x8055f7c *)
mov r5 L0x200187c4;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x20018584; Value = 0x0ffd0000; PC = 0x8055f80 *)
mov r6 L0x20018584;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018a04; Value = 0x0ffd0000; PC = 0x8055f84 *)
mov r8 L0x20018a04;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1003@sint32 : and [cf1003 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1303@sint32 : and [cf1303 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1603@sint32 : and [cf1603 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x200149e0; PC = 0x8055fc4 *)
mov L0x200149e0 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014a04; PC = 0x8055fc8 *)
mov L0x20014a04 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014a28; PC = 0x8055fcc *)
mov L0x20014a28 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015760; PC = 0x8055fd0 *)
mov L0x20015760 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015784; PC = 0x8055fd4 *)
mov L0x20015784 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x200157a8; PC = 0x8055fd8 *)
mov L0x200157a8 r9;



(******************** offset 1, 0,  3 ********************)


(**************** CUT  35, - *****************)

ecut and [
eqmod cf1003 f1003 2048, eqmod cf1303 f1303 2048, eqmod cf1603 f1603 2048,
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x200149e0*x**1*y**0*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x20014a04*x**1*y**0*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x20014a28*x**1*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x20015760*x**1*y**0*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x20015784*x**1*y**0*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x200157a8*x**1*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x2001864a; Value = 0x00030000; PC = 0x8055fdc *)
mov r5 L0x2001864a;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018aca; Value = 0x0ffd0ffd; PC = 0x8055fe0 *)
mov r7 L0x20018aca;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x2001888a; Value = 0x00000000; PC = 0x8055fe4 *)
mov r8 L0x2001888a;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1004@sint32 : and [cf1004 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1304@sint32 : and [cf1304 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1604@sint32 : and [cf1604 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014a4c; PC = 0x8056038 *)
mov L0x20014a4c r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014a70; PC = 0x805603c *)
mov L0x20014a70 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014a94; PC = 0x8056040 *)
mov L0x20014a94 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x200157cc; PC = 0x8056044 *)
mov L0x200157cc r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x200157f0; PC = 0x8056048 *)
mov L0x200157f0 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015814; PC = 0x805604c *)
mov L0x20015814 r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;



(******************** offset 1, 0,  4 ********************)


(**************** CUT  36, - *****************)

ecut and [
eqmod cf1004 f1004 2048, eqmod cf1304 f1304 2048, eqmod cf1604 f1604 2048,
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x20014a4c*x**1*y**0*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x20014a70*x**1*y**0*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x20014a94*x**1*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x200157cc*x**1*y**0*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x200157f0*x**1*y**0*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x20015814*x**1*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1566]	; 0x61e                 #! EA = L0x20018b90; Value = 0x00000ffd; PC = 0x805605c *)
mov r4 L0x20018b90;
(* ldrsh.w	r7, [lr, #990]	; 0x3de                  #! EA = L0x20018950; Value = 0x0ffd0003; PC = 0x8056060 *)
mov r7 L0x20018950;
(* ldrsh.w	r8, [lr, #414]	; 0x19e                  #! EA = L0x20018710; Value = 0x0ffd0ffd; PC = 0x8056064 *)
mov r8 L0x20018710;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056068 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1005@sint32 : and [cf1005 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805606c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1305@sint32 : and [cf1305 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056070 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1605@sint32 : and [cf1605 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056074 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056078 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x805607c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x8056080 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056084 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056088 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x805608a *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x805608c *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056090 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056094 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056098 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805609c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805609e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80560a0 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80560a4 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80560a8 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80560ac *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014ab8; PC = 0x80560b0 *)
mov L0x20014ab8 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014adc; PC = 0x80560b4 *)
mov L0x20014adc r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014b00; PC = 0x80560b8 *)
mov L0x20014b00 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015838; PC = 0x80560bc *)
mov L0x20015838 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x2001585c; PC = 0x80560c0 *)
mov L0x2001585c r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015880; PC = 0x80560c4 *)
mov L0x20015880 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80560c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 1, 0,  5 ********************)


(**************** CUT  37, - *****************)

ecut and [
eqmod cf1005 f1005 2048, eqmod cf1305 f1305 2048, eqmod cf1605 f1605 2048,
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x20014ab8*x**1*y**0*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x20014adc*x**1*y**0*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x20014b00*x**1*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x20015838*x**1*y**0*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x2001585c*x**1*y**0*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x20015880*x**1*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1188]	; 0x4a4                 #! EA = L0x20018a16; Value = 0x00000003; PC = 0x80560cc *)
mov r4 L0x20018a16;
(* ldrsh.w	r7, [lr, #612]	; 0x264                  #! EA = L0x200187d6; Value = 0x00030003; PC = 0x80560d0 *)
mov r7 L0x200187d6;
(* ldrsh.w	r8, [lr, #36]	; 0x24                    #! EA = L0x20018596; Value = 0x00000000; PC = 0x80560d4 *)
mov r8 L0x20018596;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80560d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1006@sint32 : and [cf1006 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80560dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1306@sint32 : and [cf1306 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80560e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1606@sint32 : and [cf1606 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80560e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80560e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80560ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80560f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80560f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80560f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80560fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80560fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056100 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056104 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056108 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805610c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805610e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056110 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056114 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056118 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805611c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014b24; PC = 0x8056120 *)
mov L0x20014b24 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014b48; PC = 0x8056124 *)
mov L0x20014b48 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014b6c; PC = 0x8056128 *)
mov L0x20014b6c r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200158a4; PC = 0x805612c *)
mov L0x200158a4 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200158c8; PC = 0x8056130 *)
mov L0x200158c8 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200158ec; PC = 0x8056134 *)
mov L0x200158ec r9;



(******************** offset 1, 0,  6 ********************)


(**************** CUT  38, - *****************)

ecut and [
eqmod cf1006 f1006 2048, eqmod cf1306 f1306 2048, eqmod cf1606 f1606 2048,
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x20014b24*x**1*y**0*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x20014b48*x**1*y**0*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x20014b6c*x**1*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x200158a4*x**1*y**0*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x200158c8*x**1*y**0*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x200158ec*x**1*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #810]	; 0x32a                  #! EA = L0x2001889c; Value = 0x00030ffd; PC = 0x8056138 *)
mov r4 L0x2001889c;
(* ldrsh.w	r7, [lr, #234]	; 0xea                   #! EA = L0x2001865c; Value = 0x00000003; PC = 0x805613c *)
mov r7 L0x2001865c;
(* ldrsh.w	r9, [lr, #1386]	; 0x56a                 #! EA = L0x20018adc; Value = 0x00030003; PC = 0x8056140 *)
mov r9 L0x20018adc;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056144 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1007@sint32 : and [cf1007 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056148 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1307@sint32 : and [cf1307 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x805614c *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1607@sint32 : and [cf1607 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056150 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056154 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x8056158 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x805615c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056160 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056164 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x8056168 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x805616c *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056170 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056174 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x8056176 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056178 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014b90; PC = 0x805617c *)
mov L0x20014b90 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014bb4; PC = 0x8056180 *)
mov L0x20014bb4 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014bd8; PC = 0x8056184 *)
mov L0x20014bd8 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015910; PC = 0x8056188 *)
mov L0x20015910 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015934; PC = 0x805618c *)
mov L0x20015934 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015958; PC = 0x8056190 *)
mov L0x20015958 r9;



(******************** offset 1, 0,  7 ********************)


(**************** CUT  39, - *****************)

ecut and [
eqmod cf1007 f1007 2048, eqmod cf1307 f1307 2048, eqmod cf1607 f1607 2048,
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20014b90*x**1*y**0*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20014bb4*x**1*y**0*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20014bd8*x**1*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20015910*x**1*y**0*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20015934*x**1*y**0*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20015958*x**1*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #432]	; 0x1b0                  #! EA = L0x20018722; Value = 0x0ffd0ffd; PC = 0x8056194 *)
mov r4 L0x20018722;
(* ldrsh.w	r6, [lr, #1584]	; 0x630                 #! EA = L0x20018ba2; Value = 0x0ffd0003; PC = 0x8056198 *)
mov r6 L0x20018ba2;
(* ldrsh.w	r9, [lr, #1008]	; 0x3f0                 #! EA = L0x20018962; Value = 0x00030ffd; PC = 0x805619c *)
mov r9 L0x20018962;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80561a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1008@sint32 : and [cf1008 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80561a4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1308@sint32 : and [cf1308 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80561a8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1608@sint32 : and [cf1608 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80561ac *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80561b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80561b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80561b8 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80561bc *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80561c0 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80561c2 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80561c4 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80561c8 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80561cc *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80561d0 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80561d4 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80561d6 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80561d8 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80561dc *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80561e0 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80561e4 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014bfc; PC = 0x80561e8 *)
mov L0x20014bfc r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014c20; PC = 0x80561ec *)
mov L0x20014c20 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014c44; PC = 0x80561f0 *)
mov L0x20014c44 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x2001597c; PC = 0x80561f4 *)
mov L0x2001597c r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x200159a0; PC = 0x80561f8 *)
mov L0x200159a0 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200159c4; PC = 0x80561fc *)
mov L0x200159c4 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056200 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056204 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x8055e3c <_Good_loop0_inner>              #! PC = 0x8056208 *)
#b.w	0x8055e3c <_Good_loop0_inner>              #! 0x8056208 = 0x8056208;



(******************** offset 1, 0,  8 ********************)


(**************** CUT  40, - *****************)

ecut and [
eqmod cf1008 f1008 2048, eqmod cf1308 f1308 2048, eqmod cf1608 f1608 2048,
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x20014bfc*x**1*y**0*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x20014c20*x**1*y**0*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x20014c44*x**1*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x2001597c*x**1*y**0*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x200159a0*x**1*y**0*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x200159c4*x**1*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr]                                #! EA = L0x200185a8; Value = 0x0ffd0003; PC = 0x8055e3c *)
mov r4 L0x200185a8;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x20018a28; Value = 0x00030ffd; PC = 0x8055e40 *)
mov r6 L0x20018a28;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x200187e8; Value = 0x00000003; PC = 0x8055e44 *)
mov r9 L0x200187e8;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1009@sint32 : and [cf1009 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1309@sint32 : and [cf1309 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1609@sint32 : and [cf1609 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014c68; PC = 0x8055e90 *)
mov L0x20014c68 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014c8c; PC = 0x8055e94 *)
mov L0x20014c8c r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014cb0; PC = 0x8055e98 *)
mov L0x20014cb0 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200159e8; PC = 0x8055e9c *)
mov L0x200159e8 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015a0c; PC = 0x8055ea0 *)
mov L0x20015a0c r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015a30; PC = 0x8055ea4 *)
mov L0x20015a30 r9;



(******************** offset 1, 0,  9 ********************)


(**************** CUT  41, - *****************)

ecut and [
eqmod cf1009 f1009 2048, eqmod cf1309 f1309 2048, eqmod cf1609 f1609 2048,
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x20014c68*x**1*y**0*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x20014c8c*x**1*y**0*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x20014cb0*x**1*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x200159e8*x**1*y**0*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x20015a0c*x**1*y**0*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x20015a30*x**1*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018aee; Value = 0x0ffd0000; PC = 0x8055ea8 *)
mov r5 L0x20018aee;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x200188ae; Value = 0x0ffd0000; PC = 0x8055eac *)
mov r6 L0x200188ae;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x2001866e; Value = 0x00000ffd; PC = 0x8055eb0 *)
mov r9 L0x2001866e;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1010@sint32 : and [cf1010 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1310@sint32 : and [cf1310 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1610@sint32 : and [cf1610 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014cd4; PC = 0x8055f04 *)
mov L0x20014cd4 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014cf8; PC = 0x8055f08 *)
mov L0x20014cf8 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014d1c; PC = 0x8055f0c *)
mov L0x20014d1c r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015a54; PC = 0x8055f10 *)
mov L0x20015a54 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015a78; PC = 0x8055f14 *)
mov L0x20015a78 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015a9c; PC = 0x8055f18 *)
mov L0x20015a9c r9;



(******************** offset 1, 0, 10 ********************)


(**************** CUT  42, - *****************)

ecut and [
eqmod cf1010 f1010 2048, eqmod cf1310 f1310 2048, eqmod cf1610 f1610 2048,
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20014cd4*x**1*y**0*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20014cf8*x**1*y**0*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20014d1c*x**1*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20015a54*x**1*y**0*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20015a78*x**1*y**0*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20015a9c*x**1*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x20018974; Value = 0x0ffd0000; PC = 0x8055f1c *)
mov r5 L0x20018974;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x20018734; Value = 0x0ffd0000; PC = 0x8055f20 *)
mov r6 L0x20018734;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018bb4; Value = 0x0ffd0ffd; PC = 0x8055f24 *)
mov r8 L0x20018bb4;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1011@sint32 : and [cf1011 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1311@sint32 : and [cf1311 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1611@sint32 : and [cf1611 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014d40; PC = 0x8055f64 *)
mov L0x20014d40 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014d64; PC = 0x8055f68 *)
mov L0x20014d64 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014d88; PC = 0x8055f6c *)
mov L0x20014d88 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015ac0; PC = 0x8055f70 *)
mov L0x20015ac0 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015ae4; PC = 0x8055f74 *)
mov L0x20015ae4 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015b08; PC = 0x8055f78 *)
mov L0x20015b08 r9;



(******************** offset 1, 0, 11 ********************)


(**************** CUT  43, - *****************)

ecut and [
eqmod cf1011 f1011 2048, eqmod cf1311 f1311 2048, eqmod cf1611 f1611 2048,
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20014d40*x**1*y**0*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20014d64*x**1*y**0*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20014d88*x**1*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20015ac0*x**1*y**0*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20015ae4*x**1*y**0*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20015b08*x**1*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x200187fa; Value = 0x00030003; PC = 0x8055f7c *)
mov r5 L0x200187fa;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x200185ba; Value = 0x00000ffd; PC = 0x8055f80 *)
mov r6 L0x200185ba;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018a3a; Value = 0x00000000; PC = 0x8055f84 *)
mov r8 L0x20018a3a;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1012@sint32 : and [cf1012 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1312@sint32 : and [cf1312 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1612@sint32 : and [cf1612 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20014dac; PC = 0x8055fc4 *)
mov L0x20014dac r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014dd0; PC = 0x8055fc8 *)
mov L0x20014dd0 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014df4; PC = 0x8055fcc *)
mov L0x20014df4 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015b2c; PC = 0x8055fd0 *)
mov L0x20015b2c r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015b50; PC = 0x8055fd4 *)
mov L0x20015b50 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015b74; PC = 0x8055fd8 *)
mov L0x20015b74 r9;



(******************** offset 1, 0, 12 ********************)


(**************** CUT  44, - *****************)

ecut and [
eqmod cf1012 f1012 2048, eqmod cf1312 f1312 2048, eqmod cf1612 f1612 2048,
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20014dac*x**1*y**0*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20014dd0*x**1*y**0*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20014df4*x**1*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20015b2c*x**1*y**0*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20015b50*x**1*y**0*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20015b74*x**1*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x20018680; Value = 0x0ffd0000; PC = 0x8055fdc *)
mov r5 L0x20018680;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018b00; Value = 0x0ffd0003; PC = 0x8055fe0 *)
mov r7 L0x20018b00;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x200188c0; Value = 0x0ffd0003; PC = 0x8055fe4 *)
mov r8 L0x200188c0;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1013@sint32 : and [cf1013 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1313@sint32 : and [cf1313 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1613@sint32 : and [cf1613 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014e18; PC = 0x8056038 *)
mov L0x20014e18 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014e3c; PC = 0x805603c *)
mov L0x20014e3c r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014e60; PC = 0x8056040 *)
mov L0x20014e60 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015b98; PC = 0x8056044 *)
mov L0x20015b98 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015bbc; PC = 0x8056048 *)
mov L0x20015bbc r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015be0; PC = 0x805604c *)
mov L0x20015be0 r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;



(******************** offset 1, 0, 13 ********************)


(**************** CUT  45, - *****************)

ecut and [
eqmod cf1013 f1013 2048, eqmod cf1313 f1313 2048, eqmod cf1613 f1613 2048,
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20014e18*x**1*y**0*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20014e3c*x**1*y**0*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20014e60*x**1*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20015b98*x**1*y**0*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20015bbc*x**1*y**0*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20015be0*x**1*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1566]	; 0x61e                 #! EA = L0x20018bc6; Value = 0x00000000; PC = 0x805605c *)
mov r4 L0x20018bc6;
(* ldrsh.w	r7, [lr, #990]	; 0x3de                  #! EA = L0x20018986; Value = 0x00000003; PC = 0x8056060 *)
mov r7 L0x20018986;
(* ldrsh.w	r8, [lr, #414]	; 0x19e                  #! EA = L0x20018746; Value = 0x0ffd0003; PC = 0x8056064 *)
mov r8 L0x20018746;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056068 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1014@sint32 : and [cf1014 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805606c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1314@sint32 : and [cf1314 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056070 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1614@sint32 : and [cf1614 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056074 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056078 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x805607c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x8056080 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056084 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056088 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x805608a *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x805608c *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056090 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056094 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056098 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805609c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805609e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80560a0 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80560a4 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80560a8 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80560ac *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014e84; PC = 0x80560b0 *)
mov L0x20014e84 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014ea8; PC = 0x80560b4 *)
mov L0x20014ea8 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014ecc; PC = 0x80560b8 *)
mov L0x20014ecc r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015c04; PC = 0x80560bc *)
mov L0x20015c04 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015c28; PC = 0x80560c0 *)
mov L0x20015c28 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015c4c; PC = 0x80560c4 *)
mov L0x20015c4c r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80560c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 1, 0, 14 ********************)


(**************** CUT  46, - *****************)

ecut and [
eqmod cf1014 f1014 2048, eqmod cf1314 f1314 2048, eqmod cf1614 f1614 2048,
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20014e84*x**1*y**0*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20014ea8*x**1*y**0*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20014ecc*x**1*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20015c04*x**1*y**0*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20015c28*x**1*y**0*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20015c4c*x**1*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1188]	; 0x4a4                 #! EA = L0x20018a4c; Value = 0x00030003; PC = 0x80560cc *)
mov r4 L0x20018a4c;
(* ldrsh.w	r7, [lr, #612]	; 0x264                  #! EA = L0x2001880c; Value = 0x00000000; PC = 0x80560d0 *)
mov r7 L0x2001880c;
(* ldrsh.w	r8, [lr, #36]	; 0x24                    #! EA = L0x200185cc; Value = 0x00030000; PC = 0x80560d4 *)
mov r8 L0x200185cc;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80560d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1015@sint32 : and [cf1015 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80560dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1315@sint32 : and [cf1315 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80560e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1615@sint32 : and [cf1615 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80560e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80560e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80560ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80560f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80560f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80560f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80560fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80560fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056100 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056104 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056108 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805610c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805610e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056110 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056114 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056118 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805611c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014ef0; PC = 0x8056120 *)
mov L0x20014ef0 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014f14; PC = 0x8056124 *)
mov L0x20014f14 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014f38; PC = 0x8056128 *)
mov L0x20014f38 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015c70; PC = 0x805612c *)
mov L0x20015c70 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015c94; PC = 0x8056130 *)
mov L0x20015c94 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015cb8; PC = 0x8056134 *)
mov L0x20015cb8 r9;



(******************** offset 1, 0, 15 ********************)


(**************** CUT  47, - *****************)

ecut and [
eqmod cf1015 f1015 2048, eqmod cf1315 f1315 2048, eqmod cf1615 f1615 2048,
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20014ef0*x**1*y**0*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20014f14*x**1*y**0*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20014f38*x**1*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20015c70*x**1*y**0*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20015c94*x**1*y**0*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20015cb8*x**1*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #810]	; 0x32a                  #! EA = L0x200188d2; Value = 0x0ffd0ffd; PC = 0x8056138 *)
mov r4 L0x200188d2;
(* ldrsh.w	r7, [lr, #234]	; 0xea                   #! EA = L0x20018692; Value = 0x0ffd0003; PC = 0x805613c *)
mov r7 L0x20018692;
(* ldrsh.w	r9, [lr, #1386]	; 0x56a                 #! EA = L0x20018b12; Value = 0x0ffd0003; PC = 0x8056140 *)
mov r9 L0x20018b12;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056144 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1016@sint32 : and [cf1016 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056148 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1316@sint32 : and [cf1316 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x805614c *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1616@sint32 : and [cf1616 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056150 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056154 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x8056158 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x805615c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056160 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056164 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x8056168 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x805616c *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056170 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056174 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x8056176 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056178 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014f5c; PC = 0x805617c *)
mov L0x20014f5c r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014f80; PC = 0x8056180 *)
mov L0x20014f80 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014fa4; PC = 0x8056184 *)
mov L0x20014fa4 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015cdc; PC = 0x8056188 *)
mov L0x20015cdc r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015d00; PC = 0x805618c *)
mov L0x20015d00 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015d24; PC = 0x8056190 *)
mov L0x20015d24 r9;



(******************** offset 1, 0, 16 ********************)


(**************** CUT  48, - *****************)

ecut and [
eqmod cf1016 f1016 2048, eqmod cf1316 f1316 2048, eqmod cf1616 f1616 2048,
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20014f5c*x**1*y**0*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20014f80*x**1*y**0*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20014fa4*x**1*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20015cdc*x**1*y**0*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20015d00*x**1*y**0*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20015d24*x**1*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #432]	; 0x1b0                  #! EA = L0x20018758; Value = 0x00030ffd; PC = 0x8056194 *)
mov r4 L0x20018758;
(* ldrsh.w	r6, [lr, #1584]	; 0x630                 #! EA = L0x20018bd8; Value = 0x00000000; PC = 0x8056198 *)
mov r6 L0x20018bd8;
(* ldrsh.w	r9, [lr, #1008]	; 0x3f0                 #! EA = L0x20018998; Value = 0x0ffd0003; PC = 0x805619c *)
mov r9 L0x20018998;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80561a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1017@sint32 : and [cf1017 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80561a4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1317@sint32 : and [cf1317 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80561a8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1617@sint32 : and [cf1617 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80561ac *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80561b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80561b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80561b8 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80561bc *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80561c0 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80561c2 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80561c4 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80561c8 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80561cc *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80561d0 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80561d4 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80561d6 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80561d8 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80561dc *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80561e0 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80561e4 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014fc8; PC = 0x80561e8 *)
mov L0x20014fc8 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014fec; PC = 0x80561ec *)
mov L0x20014fec r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015010; PC = 0x80561f0 *)
mov L0x20015010 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015d48; PC = 0x80561f4 *)
mov L0x20015d48 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015d6c; PC = 0x80561f8 *)
mov L0x20015d6c r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015d90; PC = 0x80561fc *)
mov L0x20015d90 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056200 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056204 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x8055e3c <_Good_loop0_inner>              #! PC = 0x8056208 *)
#b.w	0x8055e3c <_Good_loop0_inner>              #! 0x8056208 = 0x8056208;



(******************** offset 1, 0, 17 ********************)


(**************** CUT  49, - *****************)

ecut and [
eqmod cf1017 f1017 2048, eqmod cf1317 f1317 2048, eqmod cf1617 f1617 2048,
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20014fc8*x**1*y**0*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20014fec*x**1*y**0*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20015010*x**1*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20015d48*x**1*y**0*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20015d6c*x**1*y**0*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20015d90*x**1*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr]                                #! EA = L0x200185de; Value = 0x00030003; PC = 0x8055e3c *)
mov r4 L0x200185de;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x20018a5e; Value = 0x00000003; PC = 0x8055e40 *)
mov r6 L0x20018a5e;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x2001881e; Value = 0x0ffd0000; PC = 0x8055e44 *)
mov r9 L0x2001881e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1018@sint32 : and [cf1018 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1318@sint32 : and [cf1318 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1618@sint32 : and [cf1618 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015034; PC = 0x8055e90 *)
mov L0x20015034 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015058; PC = 0x8055e94 *)
mov L0x20015058 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x2001507c; PC = 0x8055e98 *)
mov L0x2001507c r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015db4; PC = 0x8055e9c *)
mov L0x20015db4 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015dd8; PC = 0x8055ea0 *)
mov L0x20015dd8 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015dfc; PC = 0x8055ea4 *)
mov L0x20015dfc r9;



(******************** offset 1, 0, 18 ********************)


(**************** CUT  50, - *****************)

ecut and [
eqmod cf1018 f1018 2048, eqmod cf1318 f1318 2048, eqmod cf1618 f1618 2048,
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x20015034*x**1*y**0*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x20015058*x**1*y**0*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x2001507c*x**1*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x20015db4*x**1*y**0*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x20015dd8*x**1*y**0*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x20015dfc*x**1*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018b24; Value = 0x0ffd0ffd; PC = 0x8055ea8 *)
mov r5 L0x20018b24;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x200188e4; Value = 0x0ffd0003; PC = 0x8055eac *)
mov r6 L0x200188e4;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x200186a4; Value = 0x0ffd0000; PC = 0x8055eb0 *)
mov r9 L0x200186a4;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1019@sint32 : and [cf1019 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1319@sint32 : and [cf1319 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1619@sint32 : and [cf1619 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x200150a0; PC = 0x8055f04 *)
mov L0x200150a0 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200150c4; PC = 0x8055f08 *)
mov L0x200150c4 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200150e8; PC = 0x8055f0c *)
mov L0x200150e8 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015e20; PC = 0x8055f10 *)
mov L0x20015e20 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015e44; PC = 0x8055f14 *)
mov L0x20015e44 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015e68; PC = 0x8055f18 *)
mov L0x20015e68 r9;



(******************** offset 1, 0, 19 ********************)


(**************** CUT  51, - *****************)

ecut and [
eqmod cf1019 f1019 2048, eqmod cf1319 f1319 2048, eqmod cf1619 f1619 2048,
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x200150a0*x**1*y**0*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x200150c4*x**1*y**0*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x200150e8*x**1*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x20015e20*x**1*y**0*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x20015e44*x**1*y**0*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x20015e68*x**1*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x200189aa; Value = 0x00000003; PC = 0x8055f1c *)
mov r5 L0x200189aa;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x2001876a; Value = 0x00030003; PC = 0x8055f20 *)
mov r6 L0x2001876a;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018bea; Value = 0x00000000; PC = 0x8055f24 *)
mov r8 L0x20018bea;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1020@sint32 : and [cf1020 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1320@sint32 : and [cf1320 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1620@sint32 : and [cf1620 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x2001510c; PC = 0x8055f64 *)
mov L0x2001510c r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015130; PC = 0x8055f68 *)
mov L0x20015130 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015154; PC = 0x8055f6c *)
mov L0x20015154 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015e8c; PC = 0x8055f70 *)
mov L0x20015e8c r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015eb0; PC = 0x8055f74 *)
mov L0x20015eb0 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015ed4; PC = 0x8055f78 *)
mov L0x20015ed4 r9;



(******************** offset 1, 0, 20 ********************)


(**************** CUT  52, - *****************)

ecut and [
eqmod cf1020 f1020 2048, eqmod cf1320 f1320 2048, eqmod cf1620 f1620 2048,
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x2001510c*x**1*y**0*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x20015130*x**1*y**0*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x20015154*x**1*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x20015e8c*x**1*y**0*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x20015eb0*x**1*y**0*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x20015ed4*x**1*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x20018830; Value = 0x00000003; PC = 0x8055f7c *)
mov r5 L0x20018830;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x200185f0; Value = 0x00000ffd; PC = 0x8055f80 *)
mov r6 L0x200185f0;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018a70; Value = 0x00030ffd; PC = 0x8055f84 *)
mov r8 L0x20018a70;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1021@sint32 : and [cf1021 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1321@sint32 : and [cf1321 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1621@sint32 : and [cf1621 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015178; PC = 0x8055fc4 *)
mov L0x20015178 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x2001519c; PC = 0x8055fc8 *)
mov L0x2001519c r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200151c0; PC = 0x8055fcc *)
mov L0x200151c0 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015ef8; PC = 0x8055fd0 *)
mov L0x20015ef8 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015f1c; PC = 0x8055fd4 *)
mov L0x20015f1c r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015f40; PC = 0x8055fd8 *)
mov L0x20015f40 r9;



(******************** offset 1, 0, 21 ********************)


(**************** CUT  53, - *****************)

ecut and [
eqmod cf1021 f1021 2048, eqmod cf1321 f1321 2048, eqmod cf1621 f1621 2048,
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x20015178*x**1*y**0*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x2001519c*x**1*y**0*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x200151c0*x**1*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x20015ef8*x**1*y**0*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x20015f1c*x**1*y**0*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x20015f40*x**1*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x200186b6; Value = 0x00000ffd; PC = 0x8055fdc *)
mov r5 L0x200186b6;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018b36; Value = 0x0ffd0000; PC = 0x8055fe0 *)
mov r7 L0x20018b36;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x200188f6; Value = 0x00030003; PC = 0x8055fe4 *)
mov r8 L0x200188f6;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1022@sint32 : and [cf1022 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1322@sint32 : and [cf1322 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1622@sint32 : and [cf1622 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200151e4; PC = 0x8056038 *)
mov L0x200151e4 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20015208; PC = 0x805603c *)
mov L0x20015208 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x2001522c; PC = 0x8056040 *)
mov L0x2001522c r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015f64; PC = 0x8056044 *)
mov L0x20015f64 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015f88; PC = 0x8056048 *)
mov L0x20015f88 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015fac; PC = 0x805604c *)
mov L0x20015fac r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;



(******************** offset 1, 0, 22 ********************)


(**************** CUT  54, - *****************)

ecut and [
eqmod cf1022 f1022 2048, eqmod cf1322 f1322 2048, eqmod cf1622 f1622 2048,
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x200151e4*x**1*y**0*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x20015208*x**1*y**0*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x2001522c*x**1*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x20015f64*x**1*y**0*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x20015f88*x**1*y**0*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x20015fac*x**1*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1566]	; 0x61e                 #! EA = L0x20018bfc; Value = 0x00000000; PC = 0x805605c *)
mov r4 L0x20018bfc;
(* ldrsh.w	r7, [lr, #990]	; 0x3de                  #! EA = L0x200189bc; Value = 0x0ffd0003; PC = 0x8056060 *)
mov r7 L0x200189bc;
(* ldrsh.w	r8, [lr, #414]	; 0x19e                  #! EA = L0x2001877c; Value = 0x00000000; PC = 0x8056064 *)
mov r8 L0x2001877c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056068 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1023@sint32 : and [cf1023 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805606c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1323@sint32 : and [cf1323 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056070 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1623@sint32 : and [cf1623 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056074 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056078 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x805607c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x8056080 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056084 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056088 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x805608a *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x805608c *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056090 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056094 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056098 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805609c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805609e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80560a0 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80560a4 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80560a8 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80560ac *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20015250; PC = 0x80560b0 *)
mov L0x20015250 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20015274; PC = 0x80560b4 *)
mov L0x20015274 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20015298; PC = 0x80560b8 *)
mov L0x20015298 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015fd0; PC = 0x80560bc *)
mov L0x20015fd0 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015ff4; PC = 0x80560c0 *)
mov L0x20015ff4 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20016018; PC = 0x80560c4 *)
mov L0x20016018 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80560c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 1, 0, 23 ********************)


(**************** CUT  55, - *****************)

ecut and [
eqmod cf1023 f1023 2048, eqmod cf1323 f1323 2048, eqmod cf1623 f1623 2048,
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20015250*x**1*y**0*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20015274*x**1*y**0*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20015298*x**1*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20015fd0*x**1*y**0*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20015ff4*x**1*y**0*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20016018*x**1*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1188]	; 0x4a4                 #! EA = L0x20018a82; Value = 0x00000000; PC = 0x80560cc *)
mov r4 L0x20018a82;
(* ldrsh.w	r7, [lr, #612]	; 0x264                  #! EA = L0x20018842; Value = 0x00000000; PC = 0x80560d0 *)
mov r7 L0x20018842;
(* ldrsh.w	r8, [lr, #36]	; 0x24                    #! EA = L0x20018602; Value = 0x00000003; PC = 0x80560d4 *)
mov r8 L0x20018602;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80560d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1024@sint32 : and [cf1024 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80560dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1324@sint32 : and [cf1324 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80560e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1624@sint32 : and [cf1624 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80560e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80560e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80560ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80560f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80560f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80560f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80560fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80560fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056100 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056104 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056108 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805610c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805610e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056110 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056114 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056118 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805611c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200152bc; PC = 0x8056120 *)
mov L0x200152bc r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200152e0; PC = 0x8056124 *)
mov L0x200152e0 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015304; PC = 0x8056128 *)
mov L0x20015304 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x2001603c; PC = 0x805612c *)
mov L0x2001603c r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20016060; PC = 0x8056130 *)
mov L0x20016060 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20016084; PC = 0x8056134 *)
mov L0x20016084 r9;



(******************** offset 1, 0, 24 ********************)


(**************** CUT  56, - *****************)

ecut and [
eqmod cf1024 f1024 2048, eqmod cf1324 f1324 2048, eqmod cf1624 f1624 2048,
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x200152bc*x**1*y**0*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x200152e0*x**1*y**0*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x20015304*x**1*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x2001603c*x**1*y**0*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x20016060*x**1*y**0*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x20016084*x**1*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #810]	; 0x32a                  #! EA = L0x20018908; Value = 0x00030003; PC = 0x8056138 *)
mov r4 L0x20018908;
(* ldrsh.w	r7, [lr, #234]	; 0xea                   #! EA = L0x200186c8; Value = 0x00000000; PC = 0x805613c *)
mov r7 L0x200186c8;
(* ldrsh.w	r9, [lr, #1386]	; 0x56a                 #! EA = L0x20018b48; Value = 0x0ffd0ffd; PC = 0x8056140 *)
mov r9 L0x20018b48;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056144 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1025@sint32 : and [cf1025 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056148 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1325@sint32 : and [cf1325 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x805614c *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1625@sint32 : and [cf1625 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056150 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056154 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x8056158 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x805615c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056160 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056164 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x8056168 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x805616c *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056170 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056174 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x8056176 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056178 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015328; PC = 0x805617c *)
mov L0x20015328 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x2001534c; PC = 0x8056180 *)
mov L0x2001534c r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20015370; PC = 0x8056184 *)
mov L0x20015370 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200160a8; PC = 0x8056188 *)
mov L0x200160a8 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200160cc; PC = 0x805618c *)
mov L0x200160cc r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200160f0; PC = 0x8056190 *)
mov L0x200160f0 r9;



(******************** offset 1, 0, 25 ********************)


(**************** CUT  57, - *****************)

ecut and [
eqmod cf1025 f1025 2048, eqmod cf1325 f1325 2048, eqmod cf1625 f1625 2048,
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x20015328*x**1*y**0*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x2001534c*x**1*y**0*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x20015370*x**1*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x200160a8*x**1*y**0*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x200160cc*x**1*y**0*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x200160f0*x**1*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #432]	; 0x1b0                  #! EA = L0x2001878e; Value = 0x00000000; PC = 0x8056194 *)
mov r4 L0x2001878e;
(* ldrsh.w	r6, [lr, #1584]	; 0x630                 #! EA = L0x20018c0e; Value = 0x00000000; PC = 0x8056198 *)
mov r6 L0x20018c0e;
(* ldrsh.w	r9, [lr, #1008]	; 0x3f0                 #! EA = L0x200189ce; Value = 0x00030ffd; PC = 0x805619c *)
mov r9 L0x200189ce;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80561a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1026@sint32 : and [cf1026 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80561a4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1326@sint32 : and [cf1326 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80561a8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1626@sint32 : and [cf1626 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80561ac *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80561b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80561b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80561b8 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80561bc *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80561c0 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80561c2 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80561c4 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80561c8 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80561cc *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80561d0 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80561d4 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80561d6 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80561d8 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80561dc *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80561e0 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80561e4 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20015394; PC = 0x80561e8 *)
mov L0x20015394 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200153b8; PC = 0x80561ec *)
mov L0x200153b8 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200153dc; PC = 0x80561f0 *)
mov L0x200153dc r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016114; PC = 0x80561f4 *)
mov L0x20016114 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016138; PC = 0x80561f8 *)
mov L0x20016138 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x2001615c; PC = 0x80561fc *)
mov L0x2001615c r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056200 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056204 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x8055e3c <_Good_loop0_inner>              #! PC = 0x8056208 *)
#b.w	0x8055e3c <_Good_loop0_inner>              #! 0x8056208 = 0x8056208;



(******************** offset 1, 0, 26 ********************)


(**************** CUT  58, - *****************)

ecut and [
eqmod cf1026 f1026 2048, eqmod cf1326 f1326 2048, eqmod cf1626 f1626 2048,
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x20015394*x**1*y**0*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x200153b8*x**1*y**0*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x200153dc*x**1*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x20016114*x**1*y**0*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x20016138*x**1*y**0*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x2001615c*x**1*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr]                                #! EA = L0x20018614; Value = 0x00030003; PC = 0x8055e3c *)
mov r4 L0x20018614;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x20018a94; Value = 0x00030003; PC = 0x8055e40 *)
mov r6 L0x20018a94;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x20018854; Value = 0x0ffd0003; PC = 0x8055e44 *)
mov r9 L0x20018854;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1027@sint32 : and [cf1027 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1327@sint32 : and [cf1327 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1627@sint32 : and [cf1627 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015400; PC = 0x8055e90 *)
mov L0x20015400 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015424; PC = 0x8055e94 *)
mov L0x20015424 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015448; PC = 0x8055e98 *)
mov L0x20015448 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016180; PC = 0x8055e9c *)
mov L0x20016180 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200161a4; PC = 0x8055ea0 *)
mov L0x200161a4 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200161c8; PC = 0x8055ea4 *)
mov L0x200161c8 r9;



(******************** offset 1, 0, 27 ********************)


(**************** CUT  59, - *****************)

ecut and [
eqmod cf1027 f1027 2048, eqmod cf1327 f1327 2048, eqmod cf1627 f1627 2048,
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x20015400*x**1*y**0*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x20015424*x**1*y**0*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x20015448*x**1*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x20016180*x**1*y**0*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x200161a4*x**1*y**0*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x200161c8*x**1*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018b5a; Value = 0x0ffd0ffd; PC = 0x8055ea8 *)
mov r5 L0x20018b5a;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x2001891a; Value = 0x00030000; PC = 0x8055eac *)
mov r6 L0x2001891a;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x200186da; Value = 0x00030003; PC = 0x8055eb0 *)
mov r9 L0x200186da;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1028@sint32 : and [cf1028 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1328@sint32 : and [cf1328 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1628@sint32 : and [cf1628 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x2001546c; PC = 0x8055f04 *)
mov L0x2001546c r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20015490; PC = 0x8055f08 *)
mov L0x20015490 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200154b4; PC = 0x8055f0c *)
mov L0x200154b4 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200161ec; PC = 0x8055f10 *)
mov L0x200161ec r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20016210; PC = 0x8055f14 *)
mov L0x20016210 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20016234; PC = 0x8055f18 *)
mov L0x20016234 r9;



(******************** offset 1, 0, 28 ********************)


(**************** CUT  60, - *****************)

ecut and [
eqmod cf1028 f1028 2048, eqmod cf1328 f1328 2048, eqmod cf1628 f1628 2048,
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x2001546c*x**1*y**0*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x20015490*x**1*y**0*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x200154b4*x**1*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x200161ec*x**1*y**0*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x20016210*x**1*y**0*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x20016234*x**1*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x200189e0; Value = 0x00030003; PC = 0x8055f1c *)
mov r5 L0x200189e0;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x200187a0; Value = 0x00000ffd; PC = 0x8055f20 *)
mov r6 L0x200187a0;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018c20; Value = 0x00000000; PC = 0x8055f24 *)
mov r8 L0x20018c20;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1029@sint32 : and [cf1029 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1329@sint32 : and [cf1329 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1629@sint32 : and [cf1629 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200154d8; PC = 0x8055f64 *)
mov L0x200154d8 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200154fc; PC = 0x8055f68 *)
mov L0x200154fc r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015520; PC = 0x8055f6c *)
mov L0x20015520 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016258; PC = 0x8055f70 *)
mov L0x20016258 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x2001627c; PC = 0x8055f74 *)
mov L0x2001627c r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200162a0; PC = 0x8055f78 *)
mov L0x200162a0 r9;



(******************** offset 1, 0, 29 ********************)


(**************** CUT  61, - *****************)

ecut and [
eqmod cf1029 f1029 2048, eqmod cf1329 f1329 2048, eqmod cf1629 f1629 2048,
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x200154d8*x**1*y**0*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x200154fc*x**1*y**0*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x20015520*x**1*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x20016258*x**1*y**0*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x2001627c*x**1*y**0*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x200162a0*x**1*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x20018866; Value = 0x00000ffd; PC = 0x8055f7c *)
mov r5 L0x20018866;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x20018626; Value = 0x0ffd0003; PC = 0x8055f80 *)
mov r6 L0x20018626;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018aa6; Value = 0x00000ffd; PC = 0x8055f84 *)
mov r8 L0x20018aa6;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1030@sint32 : and [cf1030 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1330@sint32 : and [cf1330 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1630@sint32 : and [cf1630 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015544; PC = 0x8055fc4 *)
mov L0x20015544 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20015568; PC = 0x8055fc8 *)
mov L0x20015568 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x2001558c; PC = 0x8055fcc *)
mov L0x2001558c r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x200162c4; PC = 0x8055fd0 *)
mov L0x200162c4 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x200162e8; PC = 0x8055fd4 *)
mov L0x200162e8 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x2001630c; PC = 0x8055fd8 *)
mov L0x2001630c r9;



(******************** offset 1, 0, 30 ********************)


(**************** CUT  62, - *****************)

ecut and [
eqmod cf1030 f1030 2048, eqmod cf1330 f1330 2048, eqmod cf1630 f1630 2048,
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x20015544*x**1*y**0*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x20015568*x**1*y**0*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x2001558c*x**1*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x200162c4*x**1*y**0*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x200162e8*x**1*y**0*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x2001630c*x**1*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x200186ec; Value = 0x00000000; PC = 0x8055fdc *)
mov r5 L0x200186ec;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018b6c; Value = 0x0ffd0000; PC = 0x8055fe0 *)
mov r7 L0x20018b6c;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x2001892c; Value = 0x00030003; PC = 0x8055fe4 *)
mov r8 L0x2001892c;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1031@sint32 : and [cf1031 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1331@sint32 : and [cf1331 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1631@sint32 : and [cf1631 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200155b0; PC = 0x8056038 *)
mov L0x200155b0 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x200155d4; PC = 0x805603c *)
mov L0x200155d4 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x200155f8; PC = 0x8056040 *)
mov L0x200155f8 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20016330; PC = 0x8056044 *)
mov L0x20016330 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20016354; PC = 0x8056048 *)
mov L0x20016354 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20016378; PC = 0x805604c *)
mov L0x20016378 r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;
(* sub.w	lr, lr, #162	; 0xa2                       #! PC = 0x805620c *)
subs dc lr lr 162@uint32;
(* subw	r0, r0, #2916	; 0xb64                      #! PC = 0x8056210 *)
subc dc r0 r0 2916@uint32;
(* add.w	lr, lr, #2                                #! PC = 0x8056214 *)
adds dc lr lr 2@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8056218 *)
adds dc r0 r0 4@uint32;
(* cmp.w	r0, r12                                   #! PC = 0x805621c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8055e34 <_Good_loop0>                  #! PC = 0x8056220 *)
#bne.w	0x8055e34 <_Good_loop0>                  #! 0x8056220 = 0x8056220;
(* addw	r1, r0, #2916	; 0xb64                      #! PC = 0x8055e34 *)
adds dc r1 r0 2916@uint32;
(* vmov	s3, r1                                     #! PC = 0x8055e38 *)
mov s3 r1;



(******************** offset 1, 0, 31 ********************)


(**************** CUT  63, - *****************)

ecut and [
eqmod cf1031 f1031 2048, eqmod cf1331 f1331 2048, eqmod cf1631 f1631 2048,
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x200155b0*x**1*y**0*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x200155d4*x**1*y**0*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x200155f8*x**1*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x20016330*x**1*y**0*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x20016354*x**1*y**0*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x20016378*x**1*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(**************** CUT -,   1 *****************)

rcut and [
(-3367617)@32<=sL0x2001489c,L0x2001489c<=s3367617@32,
(-3367617)@32<=sL0x200148c0,L0x200148c0<=s3367617@32,
(-3367617)@32<=sL0x200148e4,L0x200148e4<=s3367617@32,
(-3367617)@32<=sL0x2001561c,L0x2001561c<=s3367617@32,
(-3367617)@32<=sL0x20015640,L0x20015640<=s3367617@32,
(-3367617)@32<=sL0x20015664,L0x20015664<=s3367617@32
,
(-3367617)@32<=sL0x20014908,L0x20014908<=s3367617@32,
(-3367617)@32<=sL0x2001492c,L0x2001492c<=s3367617@32,
(-3367617)@32<=sL0x20014950,L0x20014950<=s3367617@32,
(-3367617)@32<=sL0x20015688,L0x20015688<=s3367617@32,
(-3367617)@32<=sL0x200156ac,L0x200156ac<=s3367617@32,
(-3367617)@32<=sL0x200156d0,L0x200156d0<=s3367617@32
,
(-3367617)@32<=sL0x20014974,L0x20014974<=s3367617@32,
(-3367617)@32<=sL0x20014998,L0x20014998<=s3367617@32,
(-3367617)@32<=sL0x200149bc,L0x200149bc<=s3367617@32,
(-3367617)@32<=sL0x200156f4,L0x200156f4<=s3367617@32,
(-3367617)@32<=sL0x20015718,L0x20015718<=s3367617@32,
(-3367617)@32<=sL0x2001573c,L0x2001573c<=s3367617@32
,
(-3367617)@32<=sL0x200149e0,L0x200149e0<=s3367617@32,
(-3367617)@32<=sL0x20014a04,L0x20014a04<=s3367617@32,
(-3367617)@32<=sL0x20014a28,L0x20014a28<=s3367617@32,
(-3367617)@32<=sL0x20015760,L0x20015760<=s3367617@32,
(-3367617)@32<=sL0x20015784,L0x20015784<=s3367617@32,
(-3367617)@32<=sL0x200157a8,L0x200157a8<=s3367617@32
,
(-3367617)@32<=sL0x20014a4c,L0x20014a4c<=s3367617@32,
(-3367617)@32<=sL0x20014a70,L0x20014a70<=s3367617@32,
(-3367617)@32<=sL0x20014a94,L0x20014a94<=s3367617@32,
(-3367617)@32<=sL0x200157cc,L0x200157cc<=s3367617@32,
(-3367617)@32<=sL0x200157f0,L0x200157f0<=s3367617@32,
(-3367617)@32<=sL0x20015814,L0x20015814<=s3367617@32
,
(-3367617)@32<=sL0x20014ab8,L0x20014ab8<=s3367617@32,
(-3367617)@32<=sL0x20014adc,L0x20014adc<=s3367617@32,
(-3367617)@32<=sL0x20014b00,L0x20014b00<=s3367617@32,
(-3367617)@32<=sL0x20015838,L0x20015838<=s3367617@32,
(-3367617)@32<=sL0x2001585c,L0x2001585c<=s3367617@32,
(-3367617)@32<=sL0x20015880,L0x20015880<=s3367617@32
,
(-3367617)@32<=sL0x20014b24,L0x20014b24<=s3367617@32,
(-3367617)@32<=sL0x20014b48,L0x20014b48<=s3367617@32,
(-3367617)@32<=sL0x20014b6c,L0x20014b6c<=s3367617@32,
(-3367617)@32<=sL0x200158a4,L0x200158a4<=s3367617@32,
(-3367617)@32<=sL0x200158c8,L0x200158c8<=s3367617@32,
(-3367617)@32<=sL0x200158ec,L0x200158ec<=s3367617@32
,
(-3367617)@32<=sL0x20014b90,L0x20014b90<=s3367617@32,
(-3367617)@32<=sL0x20014bb4,L0x20014bb4<=s3367617@32,
(-3367617)@32<=sL0x20014bd8,L0x20014bd8<=s3367617@32,
(-3367617)@32<=sL0x20015910,L0x20015910<=s3367617@32,
(-3367617)@32<=sL0x20015934,L0x20015934<=s3367617@32,
(-3367617)@32<=sL0x20015958,L0x20015958<=s3367617@32
,
(-3367617)@32<=sL0x20014bfc,L0x20014bfc<=s3367617@32,
(-3367617)@32<=sL0x20014c20,L0x20014c20<=s3367617@32,
(-3367617)@32<=sL0x20014c44,L0x20014c44<=s3367617@32,
(-3367617)@32<=sL0x2001597c,L0x2001597c<=s3367617@32,
(-3367617)@32<=sL0x200159a0,L0x200159a0<=s3367617@32,
(-3367617)@32<=sL0x200159c4,L0x200159c4<=s3367617@32
,
(-3367617)@32<=sL0x20014c68,L0x20014c68<=s3367617@32,
(-3367617)@32<=sL0x20014c8c,L0x20014c8c<=s3367617@32,
(-3367617)@32<=sL0x20014cb0,L0x20014cb0<=s3367617@32,
(-3367617)@32<=sL0x200159e8,L0x200159e8<=s3367617@32,
(-3367617)@32<=sL0x20015a0c,L0x20015a0c<=s3367617@32,
(-3367617)@32<=sL0x20015a30,L0x20015a30<=s3367617@32
,
(-3367617)@32<=sL0x20014cd4,L0x20014cd4<=s3367617@32,
(-3367617)@32<=sL0x20014cf8,L0x20014cf8<=s3367617@32,
(-3367617)@32<=sL0x20014d1c,L0x20014d1c<=s3367617@32,
(-3367617)@32<=sL0x20015a54,L0x20015a54<=s3367617@32,
(-3367617)@32<=sL0x20015a78,L0x20015a78<=s3367617@32,
(-3367617)@32<=sL0x20015a9c,L0x20015a9c<=s3367617@32
,
(-3367617)@32<=sL0x20014d40,L0x20014d40<=s3367617@32,
(-3367617)@32<=sL0x20014d64,L0x20014d64<=s3367617@32,
(-3367617)@32<=sL0x20014d88,L0x20014d88<=s3367617@32,
(-3367617)@32<=sL0x20015ac0,L0x20015ac0<=s3367617@32,
(-3367617)@32<=sL0x20015ae4,L0x20015ae4<=s3367617@32,
(-3367617)@32<=sL0x20015b08,L0x20015b08<=s3367617@32
,
(-3367617)@32<=sL0x20014dac,L0x20014dac<=s3367617@32,
(-3367617)@32<=sL0x20014dd0,L0x20014dd0<=s3367617@32,
(-3367617)@32<=sL0x20014df4,L0x20014df4<=s3367617@32,
(-3367617)@32<=sL0x20015b2c,L0x20015b2c<=s3367617@32,
(-3367617)@32<=sL0x20015b50,L0x20015b50<=s3367617@32,
(-3367617)@32<=sL0x20015b74,L0x20015b74<=s3367617@32
,
(-3367617)@32<=sL0x20014e18,L0x20014e18<=s3367617@32,
(-3367617)@32<=sL0x20014e3c,L0x20014e3c<=s3367617@32,
(-3367617)@32<=sL0x20014e60,L0x20014e60<=s3367617@32,
(-3367617)@32<=sL0x20015b98,L0x20015b98<=s3367617@32,
(-3367617)@32<=sL0x20015bbc,L0x20015bbc<=s3367617@32,
(-3367617)@32<=sL0x20015be0,L0x20015be0<=s3367617@32
,
(-3367617)@32<=sL0x20014e84,L0x20014e84<=s3367617@32,
(-3367617)@32<=sL0x20014ea8,L0x20014ea8<=s3367617@32,
(-3367617)@32<=sL0x20014ecc,L0x20014ecc<=s3367617@32,
(-3367617)@32<=sL0x20015c04,L0x20015c04<=s3367617@32,
(-3367617)@32<=sL0x20015c28,L0x20015c28<=s3367617@32,
(-3367617)@32<=sL0x20015c4c,L0x20015c4c<=s3367617@32
,
(-3367617)@32<=sL0x20014ef0,L0x20014ef0<=s3367617@32,
(-3367617)@32<=sL0x20014f14,L0x20014f14<=s3367617@32,
(-3367617)@32<=sL0x20014f38,L0x20014f38<=s3367617@32,
(-3367617)@32<=sL0x20015c70,L0x20015c70<=s3367617@32,
(-3367617)@32<=sL0x20015c94,L0x20015c94<=s3367617@32,
(-3367617)@32<=sL0x20015cb8,L0x20015cb8<=s3367617@32
,
(-3367617)@32<=sL0x20014f5c,L0x20014f5c<=s3367617@32,
(-3367617)@32<=sL0x20014f80,L0x20014f80<=s3367617@32,
(-3367617)@32<=sL0x20014fa4,L0x20014fa4<=s3367617@32,
(-3367617)@32<=sL0x20015cdc,L0x20015cdc<=s3367617@32,
(-3367617)@32<=sL0x20015d00,L0x20015d00<=s3367617@32,
(-3367617)@32<=sL0x20015d24,L0x20015d24<=s3367617@32
,
(-3367617)@32<=sL0x20014fc8,L0x20014fc8<=s3367617@32,
(-3367617)@32<=sL0x20014fec,L0x20014fec<=s3367617@32,
(-3367617)@32<=sL0x20015010,L0x20015010<=s3367617@32,
(-3367617)@32<=sL0x20015d48,L0x20015d48<=s3367617@32,
(-3367617)@32<=sL0x20015d6c,L0x20015d6c<=s3367617@32,
(-3367617)@32<=sL0x20015d90,L0x20015d90<=s3367617@32
,
(-3367617)@32<=sL0x20015034,L0x20015034<=s3367617@32,
(-3367617)@32<=sL0x20015058,L0x20015058<=s3367617@32,
(-3367617)@32<=sL0x2001507c,L0x2001507c<=s3367617@32,
(-3367617)@32<=sL0x20015db4,L0x20015db4<=s3367617@32,
(-3367617)@32<=sL0x20015dd8,L0x20015dd8<=s3367617@32,
(-3367617)@32<=sL0x20015dfc,L0x20015dfc<=s3367617@32
,
(-3367617)@32<=sL0x200150a0,L0x200150a0<=s3367617@32,
(-3367617)@32<=sL0x200150c4,L0x200150c4<=s3367617@32,
(-3367617)@32<=sL0x200150e8,L0x200150e8<=s3367617@32,
(-3367617)@32<=sL0x20015e20,L0x20015e20<=s3367617@32,
(-3367617)@32<=sL0x20015e44,L0x20015e44<=s3367617@32,
(-3367617)@32<=sL0x20015e68,L0x20015e68<=s3367617@32
,
(-3367617)@32<=sL0x2001510c,L0x2001510c<=s3367617@32,
(-3367617)@32<=sL0x20015130,L0x20015130<=s3367617@32,
(-3367617)@32<=sL0x20015154,L0x20015154<=s3367617@32,
(-3367617)@32<=sL0x20015e8c,L0x20015e8c<=s3367617@32,
(-3367617)@32<=sL0x20015eb0,L0x20015eb0<=s3367617@32,
(-3367617)@32<=sL0x20015ed4,L0x20015ed4<=s3367617@32
,
(-3367617)@32<=sL0x20015178,L0x20015178<=s3367617@32,
(-3367617)@32<=sL0x2001519c,L0x2001519c<=s3367617@32,
(-3367617)@32<=sL0x200151c0,L0x200151c0<=s3367617@32,
(-3367617)@32<=sL0x20015ef8,L0x20015ef8<=s3367617@32,
(-3367617)@32<=sL0x20015f1c,L0x20015f1c<=s3367617@32,
(-3367617)@32<=sL0x20015f40,L0x20015f40<=s3367617@32
,
(-3367617)@32<=sL0x200151e4,L0x200151e4<=s3367617@32,
(-3367617)@32<=sL0x20015208,L0x20015208<=s3367617@32,
(-3367617)@32<=sL0x2001522c,L0x2001522c<=s3367617@32,
(-3367617)@32<=sL0x20015f64,L0x20015f64<=s3367617@32,
(-3367617)@32<=sL0x20015f88,L0x20015f88<=s3367617@32,
(-3367617)@32<=sL0x20015fac,L0x20015fac<=s3367617@32
,
(-3367617)@32<=sL0x20015250,L0x20015250<=s3367617@32,
(-3367617)@32<=sL0x20015274,L0x20015274<=s3367617@32,
(-3367617)@32<=sL0x20015298,L0x20015298<=s3367617@32,
(-3367617)@32<=sL0x20015fd0,L0x20015fd0<=s3367617@32,
(-3367617)@32<=sL0x20015ff4,L0x20015ff4<=s3367617@32,
(-3367617)@32<=sL0x20016018,L0x20016018<=s3367617@32
,
(-3367617)@32<=sL0x200152bc,L0x200152bc<=s3367617@32,
(-3367617)@32<=sL0x200152e0,L0x200152e0<=s3367617@32,
(-3367617)@32<=sL0x20015304,L0x20015304<=s3367617@32,
(-3367617)@32<=sL0x2001603c,L0x2001603c<=s3367617@32,
(-3367617)@32<=sL0x20016060,L0x20016060<=s3367617@32,
(-3367617)@32<=sL0x20016084,L0x20016084<=s3367617@32
,
(-3367617)@32<=sL0x20015328,L0x20015328<=s3367617@32,
(-3367617)@32<=sL0x2001534c,L0x2001534c<=s3367617@32,
(-3367617)@32<=sL0x20015370,L0x20015370<=s3367617@32,
(-3367617)@32<=sL0x200160a8,L0x200160a8<=s3367617@32,
(-3367617)@32<=sL0x200160cc,L0x200160cc<=s3367617@32,
(-3367617)@32<=sL0x200160f0,L0x200160f0<=s3367617@32
,
(-3367617)@32<=sL0x20015394,L0x20015394<=s3367617@32,
(-3367617)@32<=sL0x200153b8,L0x200153b8<=s3367617@32,
(-3367617)@32<=sL0x200153dc,L0x200153dc<=s3367617@32,
(-3367617)@32<=sL0x20016114,L0x20016114<=s3367617@32,
(-3367617)@32<=sL0x20016138,L0x20016138<=s3367617@32,
(-3367617)@32<=sL0x2001615c,L0x2001615c<=s3367617@32
,
(-3367617)@32<=sL0x20015400,L0x20015400<=s3367617@32,
(-3367617)@32<=sL0x20015424,L0x20015424<=s3367617@32,
(-3367617)@32<=sL0x20015448,L0x20015448<=s3367617@32,
(-3367617)@32<=sL0x20016180,L0x20016180<=s3367617@32,
(-3367617)@32<=sL0x200161a4,L0x200161a4<=s3367617@32,
(-3367617)@32<=sL0x200161c8,L0x200161c8<=s3367617@32
,
(-3367617)@32<=sL0x2001546c,L0x2001546c<=s3367617@32,
(-3367617)@32<=sL0x20015490,L0x20015490<=s3367617@32,
(-3367617)@32<=sL0x200154b4,L0x200154b4<=s3367617@32,
(-3367617)@32<=sL0x200161ec,L0x200161ec<=s3367617@32,
(-3367617)@32<=sL0x20016210,L0x20016210<=s3367617@32,
(-3367617)@32<=sL0x20016234,L0x20016234<=s3367617@32
,
(-3367617)@32<=sL0x200154d8,L0x200154d8<=s3367617@32,
(-3367617)@32<=sL0x200154fc,L0x200154fc<=s3367617@32,
(-3367617)@32<=sL0x20015520,L0x20015520<=s3367617@32,
(-3367617)@32<=sL0x20016258,L0x20016258<=s3367617@32,
(-3367617)@32<=sL0x2001627c,L0x2001627c<=s3367617@32,
(-3367617)@32<=sL0x200162a0,L0x200162a0<=s3367617@32
,
(-3367617)@32<=sL0x20015544,L0x20015544<=s3367617@32,
(-3367617)@32<=sL0x20015568,L0x20015568<=s3367617@32,
(-3367617)@32<=sL0x2001558c,L0x2001558c<=s3367617@32,
(-3367617)@32<=sL0x200162c4,L0x200162c4<=s3367617@32,
(-3367617)@32<=sL0x200162e8,L0x200162e8<=s3367617@32,
(-3367617)@32<=sL0x2001630c,L0x2001630c<=s3367617@32
,
(-3367617)@32<=sL0x200155b0,L0x200155b0<=s3367617@32,
(-3367617)@32<=sL0x200155d4,L0x200155d4<=s3367617@32,
(-3367617)@32<=sL0x200155f8,L0x200155f8<=s3367617@32,
(-3367617)@32<=sL0x20016330,L0x20016330<=s3367617@32,
(-3367617)@32<=sL0x20016354,L0x20016354<=s3367617@32,
(-3367617)@32<=sL0x20016378,L0x20016378<=s3367617@32
] prove with [ precondition ];





(* ldrsh.w	r4, [lr]                                #! EA = L0x20018574; Value = 0x0ffd0ffd; PC = 0x8055e3c *)
mov r4 L0x20018574;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x200189f4; Value = 0x0ffd0000; PC = 0x8055e40 *)
mov r6 L0x200189f4;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x200187b4; Value = 0x00030000; PC = 0x8055e44 *)
mov r9 L0x200187b4;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2000@sint32 : and [cf2000 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2300@sint32 : and [cf2300 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2600@sint32 : and [cf2600 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200148a0; PC = 0x8055e90 *)
mov L0x200148a0 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200148c4; PC = 0x8055e94 *)
mov L0x200148c4 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x200148e8; PC = 0x8055e98 *)
mov L0x200148e8 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015620; PC = 0x8055e9c *)
mov L0x20015620 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015644; PC = 0x8055ea0 *)
mov L0x20015644 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015668; PC = 0x8055ea4 *)
mov L0x20015668 r9;



(******************** offset 2, 0,  0 ********************)


(**************** CUT  64, - *****************)

ecut and [
eqmod cf2000 f2000 2048, eqmod cf2300 f2300 2048, eqmod cf2600 f2600 2048,
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x200148a0*x**2*y**0*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x200148c4*x**2*y**0*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x200148e8*x**2*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x20015620*x**2*y**0*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x20015644*x**2*y**0*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x20015668*x**2*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018aba; Value = 0x0ffd0003; PC = 0x8055ea8 *)
mov r5 L0x20018aba;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x2001887a; Value = 0x0ffd0000; PC = 0x8055eac *)
mov r6 L0x2001887a;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x2001863a; Value = 0x0ffd0000; PC = 0x8055eb0 *)
mov r9 L0x2001863a;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2001@sint32 : and [cf2001 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2301@sint32 : and [cf2301 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2601@sint32 : and [cf2601 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x2001490c; PC = 0x8055f04 *)
mov L0x2001490c r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014930; PC = 0x8055f08 *)
mov L0x20014930 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014954; PC = 0x8055f0c *)
mov L0x20014954 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x2001568c; PC = 0x8055f10 *)
mov L0x2001568c r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200156b0; PC = 0x8055f14 *)
mov L0x200156b0 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200156d4; PC = 0x8055f18 *)
mov L0x200156d4 r9;



(******************** offset 2, 0,  1 ********************)


(**************** CUT  65, - *****************)

ecut and [
eqmod cf2001 f2001 2048, eqmod cf2301 f2301 2048, eqmod cf2601 f2601 2048,
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x2001490c*x**2*y**0*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x20014930*x**2*y**0*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x20014954*x**2*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x2001568c*x**2*y**0*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x200156b0*x**2*y**0*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x200156d4*x**2*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x20018940; Value = 0x00030003; PC = 0x8055f1c *)
mov r5 L0x20018940;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x20018700; Value = 0x00000003; PC = 0x8055f20 *)
mov r6 L0x20018700;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018b80; Value = 0x00000003; PC = 0x8055f24 *)
mov r8 L0x20018b80;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2002@sint32 : and [cf2002 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2302@sint32 : and [cf2302 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2602@sint32 : and [cf2602 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014978; PC = 0x8055f64 *)
mov L0x20014978 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x2001499c; PC = 0x8055f68 *)
mov L0x2001499c r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200149c0; PC = 0x8055f6c *)
mov L0x200149c0 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x200156f8; PC = 0x8055f70 *)
mov L0x200156f8 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x2001571c; PC = 0x8055f74 *)
mov L0x2001571c r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015740; PC = 0x8055f78 *)
mov L0x20015740 r9;



(******************** offset 2, 0,  2 ********************)


(**************** CUT  66, - *****************)

ecut and [
eqmod cf2002 f2002 2048, eqmod cf2302 f2302 2048, eqmod cf2602 f2602 2048,
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x20014978*x**2*y**0*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x2001499c*x**2*y**0*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x200149c0*x**2*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x200156f8*x**2*y**0*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x2001571c*x**2*y**0*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x20015740*x**2*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x200187c6; Value = 0x00030ffd; PC = 0x8055f7c *)
mov r5 L0x200187c6;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x20018586; Value = 0x00000ffd; PC = 0x8055f80 *)
mov r6 L0x20018586;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018a06; Value = 0x0ffd0ffd; PC = 0x8055f84 *)
mov r8 L0x20018a06;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2003@sint32 : and [cf2003 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2303@sint32 : and [cf2303 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2603@sint32 : and [cf2603 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x200149e4; PC = 0x8055fc4 *)
mov L0x200149e4 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014a08; PC = 0x8055fc8 *)
mov L0x20014a08 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014a2c; PC = 0x8055fcc *)
mov L0x20014a2c r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015764; PC = 0x8055fd0 *)
mov L0x20015764 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015788; PC = 0x8055fd4 *)
mov L0x20015788 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x200157ac; PC = 0x8055fd8 *)
mov L0x200157ac r9;



(******************** offset 2, 0,  3 ********************)


(**************** CUT  67, - *****************)

ecut and [
eqmod cf2003 f2003 2048, eqmod cf2303 f2303 2048, eqmod cf2603 f2603 2048,
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x200149e4*x**2*y**0*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x20014a08*x**2*y**0*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x20014a2c*x**2*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x20015764*x**2*y**0*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x20015788*x**2*y**0*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x200157ac*x**2*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x2001864c; Value = 0x0ffd0003; PC = 0x8055fdc *)
mov r5 L0x2001864c;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018acc; Value = 0x0ffd0ffd; PC = 0x8055fe0 *)
mov r7 L0x20018acc;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x2001888c; Value = 0x0ffd0000; PC = 0x8055fe4 *)
mov r8 L0x2001888c;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2004@sint32 : and [cf2004 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2304@sint32 : and [cf2304 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2604@sint32 : and [cf2604 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014a50; PC = 0x8056038 *)
mov L0x20014a50 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014a74; PC = 0x805603c *)
mov L0x20014a74 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014a98; PC = 0x8056040 *)
mov L0x20014a98 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x200157d0; PC = 0x8056044 *)
mov L0x200157d0 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x200157f4; PC = 0x8056048 *)
mov L0x200157f4 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015818; PC = 0x805604c *)
mov L0x20015818 r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;



(******************** offset 2, 0,  4 ********************)


(**************** CUT  68, - *****************)

ecut and [
eqmod cf2004 f2004 2048, eqmod cf2304 f2304 2048, eqmod cf2604 f2604 2048,
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x20014a50*x**2*y**0*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x20014a74*x**2*y**0*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x20014a98*x**2*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x200157d0*x**2*y**0*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x200157f4*x**2*y**0*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x20015818*x**2*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1566]	; 0x61e                 #! EA = L0x20018b92; Value = 0x00030000; PC = 0x805605c *)
mov r4 L0x20018b92;
(* ldrsh.w	r7, [lr, #990]	; 0x3de                  #! EA = L0x20018952; Value = 0x00000ffd; PC = 0x8056060 *)
mov r7 L0x20018952;
(* ldrsh.w	r8, [lr, #414]	; 0x19e                  #! EA = L0x20018712; Value = 0x00000ffd; PC = 0x8056064 *)
mov r8 L0x20018712;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056068 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2005@sint32 : and [cf2005 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805606c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2305@sint32 : and [cf2305 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056070 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2605@sint32 : and [cf2605 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056074 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056078 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x805607c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x8056080 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056084 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056088 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x805608a *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x805608c *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056090 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056094 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056098 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805609c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805609e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80560a0 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80560a4 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80560a8 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80560ac *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014abc; PC = 0x80560b0 *)
mov L0x20014abc r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014ae0; PC = 0x80560b4 *)
mov L0x20014ae0 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014b04; PC = 0x80560b8 *)
mov L0x20014b04 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x2001583c; PC = 0x80560bc *)
mov L0x2001583c r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015860; PC = 0x80560c0 *)
mov L0x20015860 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015884; PC = 0x80560c4 *)
mov L0x20015884 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80560c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 2, 0,  5 ********************)


(**************** CUT  69, - *****************)

ecut and [
eqmod cf2005 f2005 2048, eqmod cf2305 f2305 2048, eqmod cf2605 f2605 2048,
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x20014abc*x**2*y**0*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x20014ae0*x**2*y**0*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x20014b04*x**2*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x2001583c*x**2*y**0*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x20015860*x**2*y**0*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x20015884*x**2*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1188]	; 0x4a4                 #! EA = L0x20018a18; Value = 0x0ffd0000; PC = 0x80560cc *)
mov r4 L0x20018a18;
(* ldrsh.w	r7, [lr, #612]	; 0x264                  #! EA = L0x200187d8; Value = 0x00000003; PC = 0x80560d0 *)
mov r7 L0x200187d8;
(* ldrsh.w	r8, [lr, #36]	; 0x24                    #! EA = L0x20018598; Value = 0x0ffd0000; PC = 0x80560d4 *)
mov r8 L0x20018598;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80560d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2006@sint32 : and [cf2006 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80560dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2306@sint32 : and [cf2306 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80560e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2606@sint32 : and [cf2606 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80560e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80560e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80560ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80560f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80560f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80560f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80560fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80560fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056100 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056104 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056108 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805610c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805610e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056110 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056114 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056118 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805611c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014b28; PC = 0x8056120 *)
mov L0x20014b28 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014b4c; PC = 0x8056124 *)
mov L0x20014b4c r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014b70; PC = 0x8056128 *)
mov L0x20014b70 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200158a8; PC = 0x805612c *)
mov L0x200158a8 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200158cc; PC = 0x8056130 *)
mov L0x200158cc r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200158f0; PC = 0x8056134 *)
mov L0x200158f0 r9;



(******************** offset 2, 0,  6 ********************)


(**************** CUT  70, - *****************)

ecut and [
eqmod cf2006 f2006 2048, eqmod cf2306 f2306 2048, eqmod cf2606 f2606 2048,
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x20014b28*x**2*y**0*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x20014b4c*x**2*y**0*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x20014b70*x**2*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x200158a8*x**2*y**0*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x200158cc*x**2*y**0*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x200158f0*x**2*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #810]	; 0x32a                  #! EA = L0x2001889e; Value = 0x00000003; PC = 0x8056138 *)
mov r4 L0x2001889e;
(* ldrsh.w	r7, [lr, #234]	; 0xea                   #! EA = L0x2001865e; Value = 0x00000000; PC = 0x805613c *)
mov r7 L0x2001865e;
(* ldrsh.w	r9, [lr, #1386]	; 0x56a                 #! EA = L0x20018ade; Value = 0x00030003; PC = 0x8056140 *)
mov r9 L0x20018ade;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056144 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2007@sint32 : and [cf2007 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056148 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2307@sint32 : and [cf2307 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x805614c *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2607@sint32 : and [cf2607 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056150 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056154 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x8056158 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x805615c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056160 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056164 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x8056168 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x805616c *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056170 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056174 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x8056176 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056178 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014b94; PC = 0x805617c *)
mov L0x20014b94 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014bb8; PC = 0x8056180 *)
mov L0x20014bb8 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014bdc; PC = 0x8056184 *)
mov L0x20014bdc r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015914; PC = 0x8056188 *)
mov L0x20015914 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015938; PC = 0x805618c *)
mov L0x20015938 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x2001595c; PC = 0x8056190 *)
mov L0x2001595c r9;



(******************** offset 2, 0,  7 ********************)


(**************** CUT  71, - *****************)

ecut and [
eqmod cf2007 f2007 2048, eqmod cf2307 f2307 2048, eqmod cf2607 f2607 2048,
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x20014b94*x**2*y**0*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x20014bb8*x**2*y**0*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x20014bdc*x**2*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x20015914*x**2*y**0*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x20015938*x**2*y**0*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x2001595c*x**2*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #432]	; 0x1b0                  #! EA = L0x20018724; Value = 0x00000ffd; PC = 0x8056194 *)
mov r4 L0x20018724;
(* ldrsh.w	r6, [lr, #1584]	; 0x630                 #! EA = L0x20018ba4; Value = 0x00000ffd; PC = 0x8056198 *)
mov r6 L0x20018ba4;
(* ldrsh.w	r9, [lr, #1008]	; 0x3f0                 #! EA = L0x20018964; Value = 0x0ffd0003; PC = 0x805619c *)
mov r9 L0x20018964;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80561a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2008@sint32 : and [cf2008 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80561a4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2308@sint32 : and [cf2308 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80561a8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2608@sint32 : and [cf2608 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80561ac *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80561b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80561b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80561b8 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80561bc *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80561c0 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80561c2 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80561c4 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80561c8 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80561cc *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80561d0 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80561d4 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80561d6 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80561d8 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80561dc *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80561e0 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80561e4 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014c00; PC = 0x80561e8 *)
mov L0x20014c00 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014c24; PC = 0x80561ec *)
mov L0x20014c24 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014c48; PC = 0x80561f0 *)
mov L0x20014c48 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015980; PC = 0x80561f4 *)
mov L0x20015980 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x200159a4; PC = 0x80561f8 *)
mov L0x200159a4 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200159c8; PC = 0x80561fc *)
mov L0x200159c8 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056200 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056204 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x8055e3c <_Good_loop0_inner>              #! PC = 0x8056208 *)
#b.w	0x8055e3c <_Good_loop0_inner>              #! 0x8056208 = 0x8056208;



(******************** offset 2, 0,  8 ********************)


(**************** CUT  72, - *****************)

ecut and [
eqmod cf2008 f2008 2048, eqmod cf2308 f2308 2048, eqmod cf2608 f2608 2048,
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x20014c00*x**2*y**0*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x20014c24*x**2*y**0*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x20014c48*x**2*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x20015980*x**2*y**0*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x200159a4*x**2*y**0*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x200159c8*x**2*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr]                                #! EA = L0x200185aa; Value = 0x00000ffd; PC = 0x8055e3c *)
mov r4 L0x200185aa;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x20018a2a; Value = 0x0ffd0003; PC = 0x8055e40 *)
mov r6 L0x20018a2a;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x200187ea; Value = 0x00030000; PC = 0x8055e44 *)
mov r9 L0x200187ea;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2009@sint32 : and [cf2009 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2309@sint32 : and [cf2309 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2609@sint32 : and [cf2609 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014c6c; PC = 0x8055e90 *)
mov L0x20014c6c r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014c90; PC = 0x8055e94 *)
mov L0x20014c90 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014cb4; PC = 0x8055e98 *)
mov L0x20014cb4 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200159ec; PC = 0x8055e9c *)
mov L0x200159ec r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015a10; PC = 0x8055ea0 *)
mov L0x20015a10 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015a34; PC = 0x8055ea4 *)
mov L0x20015a34 r9;



(******************** offset 2, 0,  9 ********************)


(**************** CUT  73, - *****************)

ecut and [
eqmod cf2009 f2009 2048, eqmod cf2309 f2309 2048, eqmod cf2609 f2609 2048,
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x20014c6c*x**2*y**0*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x20014c90*x**2*y**0*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x20014cb4*x**2*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x200159ec*x**2*y**0*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x20015a10*x**2*y**0*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x20015a34*x**2*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018af0; Value = 0x00030ffd; PC = 0x8055ea8 *)
mov r5 L0x20018af0;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x200188b0; Value = 0x00030ffd; PC = 0x8055eac *)
mov r6 L0x200188b0;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x20018670; Value = 0x0ffd0000; PC = 0x8055eb0 *)
mov r9 L0x20018670;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2010@sint32 : and [cf2010 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2310@sint32 : and [cf2310 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2610@sint32 : and [cf2610 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014cd8; PC = 0x8055f04 *)
mov L0x20014cd8 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014cfc; PC = 0x8055f08 *)
mov L0x20014cfc r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014d20; PC = 0x8055f0c *)
mov L0x20014d20 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015a58; PC = 0x8055f10 *)
mov L0x20015a58 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015a7c; PC = 0x8055f14 *)
mov L0x20015a7c r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015aa0; PC = 0x8055f18 *)
mov L0x20015aa0 r9;



(******************** offset 2, 0, 10 ********************)


(**************** CUT  74, - *****************)

ecut and [
eqmod cf2010 f2010 2048, eqmod cf2310 f2310 2048, eqmod cf2610 f2610 2048,
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20014cd8*x**2*y**0*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20014cfc*x**2*y**0*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20014d20*x**2*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20015a58*x**2*y**0*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20015a7c*x**2*y**0*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20015aa0*x**2*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x20018976; Value = 0x00000ffd; PC = 0x8055f1c *)
mov r5 L0x20018976;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x20018736; Value = 0x00000ffd; PC = 0x8055f20 *)
mov r6 L0x20018736;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018bb6; Value = 0x00000ffd; PC = 0x8055f24 *)
mov r8 L0x20018bb6;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2011@sint32 : and [cf2011 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2311@sint32 : and [cf2311 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2611@sint32 : and [cf2611 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014d44; PC = 0x8055f64 *)
mov L0x20014d44 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014d68; PC = 0x8055f68 *)
mov L0x20014d68 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014d8c; PC = 0x8055f6c *)
mov L0x20014d8c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015ac4; PC = 0x8055f70 *)
mov L0x20015ac4 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015ae8; PC = 0x8055f74 *)
mov L0x20015ae8 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015b0c; PC = 0x8055f78 *)
mov L0x20015b0c r9;



(******************** offset 2, 0, 11 ********************)


(**************** CUT  75, - *****************)

ecut and [
eqmod cf2011 f2011 2048, eqmod cf2311 f2311 2048, eqmod cf2611 f2611 2048,
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20014d44*x**2*y**0*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20014d68*x**2*y**0*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20014d8c*x**2*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20015ac4*x**2*y**0*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20015ae8*x**2*y**0*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20015b0c*x**2*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x200187fc; Value = 0x00000003; PC = 0x8055f7c *)
mov r5 L0x200187fc;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x200185bc; Value = 0x00000000; PC = 0x8055f80 *)
mov r6 L0x200185bc;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018a3c; Value = 0x00030000; PC = 0x8055f84 *)
mov r8 L0x20018a3c;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2012@sint32 : and [cf2012 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2312@sint32 : and [cf2312 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2612@sint32 : and [cf2612 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20014db0; PC = 0x8055fc4 *)
mov L0x20014db0 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014dd4; PC = 0x8055fc8 *)
mov L0x20014dd4 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014df8; PC = 0x8055fcc *)
mov L0x20014df8 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015b30; PC = 0x8055fd0 *)
mov L0x20015b30 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015b54; PC = 0x8055fd4 *)
mov L0x20015b54 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015b78; PC = 0x8055fd8 *)
mov L0x20015b78 r9;



(******************** offset 2, 0, 12 ********************)


(**************** CUT  76, - *****************)

ecut and [
eqmod cf2012 f2012 2048, eqmod cf2312 f2312 2048, eqmod cf2612 f2612 2048,
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20014db0*x**2*y**0*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20014dd4*x**2*y**0*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20014df8*x**2*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20015b30*x**2*y**0*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20015b54*x**2*y**0*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20015b78*x**2*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x20018682; Value = 0x00000ffd; PC = 0x8055fdc *)
mov r5 L0x20018682;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018b02; Value = 0x0ffd0ffd; PC = 0x8055fe0 *)
mov r7 L0x20018b02;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x200188c2; Value = 0x00030ffd; PC = 0x8055fe4 *)
mov r8 L0x200188c2;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2013@sint32 : and [cf2013 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2313@sint32 : and [cf2313 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2613@sint32 : and [cf2613 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014e1c; PC = 0x8056038 *)
mov L0x20014e1c r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014e40; PC = 0x805603c *)
mov L0x20014e40 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014e64; PC = 0x8056040 *)
mov L0x20014e64 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015b9c; PC = 0x8056044 *)
mov L0x20015b9c r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015bc0; PC = 0x8056048 *)
mov L0x20015bc0 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015be4; PC = 0x805604c *)
mov L0x20015be4 r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;



(******************** offset 2, 0, 13 ********************)


(**************** CUT  77, - *****************)

ecut and [
eqmod cf2013 f2013 2048, eqmod cf2313 f2313 2048, eqmod cf2613 f2613 2048,
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20014e1c*x**2*y**0*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20014e40*x**2*y**0*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20014e64*x**2*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20015b9c*x**2*y**0*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20015bc0*x**2*y**0*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20015be4*x**2*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1566]	; 0x61e                 #! EA = L0x20018bc8; Value = 0x00030000; PC = 0x805605c *)
mov r4 L0x20018bc8;
(* ldrsh.w	r7, [lr, #990]	; 0x3de                  #! EA = L0x20018988; Value = 0x00030000; PC = 0x8056060 *)
mov r7 L0x20018988;
(* ldrsh.w	r8, [lr, #414]	; 0x19e                  #! EA = L0x20018748; Value = 0x00030ffd; PC = 0x8056064 *)
mov r8 L0x20018748;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056068 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2014@sint32 : and [cf2014 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805606c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2314@sint32 : and [cf2314 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056070 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2614@sint32 : and [cf2614 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056074 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056078 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x805607c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x8056080 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056084 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056088 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x805608a *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x805608c *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056090 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056094 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056098 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805609c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805609e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80560a0 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80560a4 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80560a8 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80560ac *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014e88; PC = 0x80560b0 *)
mov L0x20014e88 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014eac; PC = 0x80560b4 *)
mov L0x20014eac r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014ed0; PC = 0x80560b8 *)
mov L0x20014ed0 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015c08; PC = 0x80560bc *)
mov L0x20015c08 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015c2c; PC = 0x80560c0 *)
mov L0x20015c2c r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015c50; PC = 0x80560c4 *)
mov L0x20015c50 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80560c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 2, 0, 14 ********************)


(**************** CUT  78, - *****************)

ecut and [
eqmod cf2014 f2014 2048, eqmod cf2314 f2314 2048, eqmod cf2614 f2614 2048,
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20014e88*x**2*y**0*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20014eac*x**2*y**0*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20014ed0*x**2*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20015c08*x**2*y**0*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20015c2c*x**2*y**0*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20015c50*x**2*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1188]	; 0x4a4                 #! EA = L0x20018a4e; Value = 0x00030003; PC = 0x80560cc *)
mov r4 L0x20018a4e;
(* ldrsh.w	r7, [lr, #612]	; 0x264                  #! EA = L0x2001880e; Value = 0x00000000; PC = 0x80560d0 *)
mov r7 L0x2001880e;
(* ldrsh.w	r8, [lr, #36]	; 0x24                    #! EA = L0x200185ce; Value = 0x00000003; PC = 0x80560d4 *)
mov r8 L0x200185ce;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80560d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2015@sint32 : and [cf2015 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80560dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2315@sint32 : and [cf2315 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80560e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2615@sint32 : and [cf2615 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80560e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80560e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80560ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80560f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80560f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80560f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80560fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80560fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056100 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056104 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056108 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805610c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805610e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056110 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056114 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056118 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805611c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014ef4; PC = 0x8056120 *)
mov L0x20014ef4 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014f18; PC = 0x8056124 *)
mov L0x20014f18 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014f3c; PC = 0x8056128 *)
mov L0x20014f3c r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015c74; PC = 0x805612c *)
mov L0x20015c74 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015c98; PC = 0x8056130 *)
mov L0x20015c98 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015cbc; PC = 0x8056134 *)
mov L0x20015cbc r9;



(******************** offset 2, 0, 15 ********************)


(**************** CUT  79, - *****************)

ecut and [
eqmod cf2015 f2015 2048, eqmod cf2315 f2315 2048, eqmod cf2615 f2615 2048,
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20014ef4*x**2*y**0*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20014f18*x**2*y**0*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20014f3c*x**2*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20015c74*x**2*y**0*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20015c98*x**2*y**0*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20015cbc*x**2*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #810]	; 0x32a                  #! EA = L0x200188d4; Value = 0x00030ffd; PC = 0x8056138 *)
mov r4 L0x200188d4;
(* ldrsh.w	r7, [lr, #234]	; 0xea                   #! EA = L0x20018694; Value = 0x00030ffd; PC = 0x805613c *)
mov r7 L0x20018694;
(* ldrsh.w	r9, [lr, #1386]	; 0x56a                 #! EA = L0x20018b14; Value = 0x00030ffd; PC = 0x8056140 *)
mov r9 L0x20018b14;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056144 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2016@sint32 : and [cf2016 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056148 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2316@sint32 : and [cf2316 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x805614c *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2616@sint32 : and [cf2616 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056150 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056154 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x8056158 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x805615c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056160 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056164 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x8056168 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x805616c *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056170 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056174 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x8056176 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056178 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014f60; PC = 0x805617c *)
mov L0x20014f60 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014f84; PC = 0x8056180 *)
mov L0x20014f84 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014fa8; PC = 0x8056184 *)
mov L0x20014fa8 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015ce0; PC = 0x8056188 *)
mov L0x20015ce0 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015d04; PC = 0x805618c *)
mov L0x20015d04 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015d28; PC = 0x8056190 *)
mov L0x20015d28 r9;



(******************** offset 2, 0, 16 ********************)


(**************** CUT  80, - *****************)

ecut and [
eqmod cf2016 f2016 2048, eqmod cf2316 f2316 2048, eqmod cf2616 f2616 2048,
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20014f60*x**2*y**0*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20014f84*x**2*y**0*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20014fa8*x**2*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20015ce0*x**2*y**0*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20015d04*x**2*y**0*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20015d28*x**2*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #432]	; 0x1b0                  #! EA = L0x2001875a; Value = 0x00000003; PC = 0x8056194 *)
mov r4 L0x2001875a;
(* ldrsh.w	r6, [lr, #1584]	; 0x630                 #! EA = L0x20018bda; Value = 0x00000000; PC = 0x8056198 *)
mov r6 L0x20018bda;
(* ldrsh.w	r9, [lr, #1008]	; 0x3f0                 #! EA = L0x2001899a; Value = 0x00030ffd; PC = 0x805619c *)
mov r9 L0x2001899a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80561a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2017@sint32 : and [cf2017 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80561a4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2317@sint32 : and [cf2317 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80561a8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2617@sint32 : and [cf2617 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80561ac *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80561b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80561b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80561b8 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80561bc *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80561c0 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80561c2 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80561c4 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80561c8 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80561cc *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80561d0 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80561d4 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80561d6 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80561d8 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80561dc *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80561e0 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80561e4 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014fcc; PC = 0x80561e8 *)
mov L0x20014fcc r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014ff0; PC = 0x80561ec *)
mov L0x20014ff0 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015014; PC = 0x80561f0 *)
mov L0x20015014 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015d4c; PC = 0x80561f4 *)
mov L0x20015d4c r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015d70; PC = 0x80561f8 *)
mov L0x20015d70 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015d94; PC = 0x80561fc *)
mov L0x20015d94 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056200 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056204 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x8055e3c <_Good_loop0_inner>              #! PC = 0x8056208 *)
#b.w	0x8055e3c <_Good_loop0_inner>              #! 0x8056208 = 0x8056208;



(******************** offset 2, 0, 17 ********************)


(**************** CUT  81, - *****************)

ecut and [
eqmod cf2017 f2017 2048, eqmod cf2317 f2317 2048, eqmod cf2617 f2617 2048,
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20014fcc*x**2*y**0*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20014ff0*x**2*y**0*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20015014*x**2*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20015d4c*x**2*y**0*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20015d70*x**2*y**0*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20015d94*x**2*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr]                                #! EA = L0x200185e0; Value = 0x00000003; PC = 0x8055e3c *)
mov r4 L0x200185e0;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x20018a60; Value = 0x00000000; PC = 0x8055e40 *)
mov r6 L0x20018a60;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x20018820; Value = 0x00000ffd; PC = 0x8055e44 *)
mov r9 L0x20018820;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2018@sint32 : and [cf2018 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2318@sint32 : and [cf2318 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2618@sint32 : and [cf2618 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015038; PC = 0x8055e90 *)
mov L0x20015038 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x2001505c; PC = 0x8055e94 *)
mov L0x2001505c r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015080; PC = 0x8055e98 *)
mov L0x20015080 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015db8; PC = 0x8055e9c *)
mov L0x20015db8 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015ddc; PC = 0x8055ea0 *)
mov L0x20015ddc r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015e00; PC = 0x8055ea4 *)
mov L0x20015e00 r9;



(******************** offset 2, 0, 18 ********************)


(**************** CUT  82, - *****************)

ecut and [
eqmod cf2018 f2018 2048, eqmod cf2318 f2318 2048, eqmod cf2618 f2618 2048,
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x20015038*x**2*y**0*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x2001505c*x**2*y**0*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x20015080*x**2*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x20015db8*x**2*y**0*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x20015ddc*x**2*y**0*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x20015e00*x**2*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018b26; Value = 0x00000ffd; PC = 0x8055ea8 *)
mov r5 L0x20018b26;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x200188e6; Value = 0x0ffd0ffd; PC = 0x8055eac *)
mov r6 L0x200188e6;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x200186a6; Value = 0x00000ffd; PC = 0x8055eb0 *)
mov r9 L0x200186a6;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2019@sint32 : and [cf2019 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2319@sint32 : and [cf2319 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2619@sint32 : and [cf2619 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x200150a4; PC = 0x8055f04 *)
mov L0x200150a4 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200150c8; PC = 0x8055f08 *)
mov L0x200150c8 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200150ec; PC = 0x8055f0c *)
mov L0x200150ec r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015e24; PC = 0x8055f10 *)
mov L0x20015e24 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015e48; PC = 0x8055f14 *)
mov L0x20015e48 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015e6c; PC = 0x8055f18 *)
mov L0x20015e6c r9;



(******************** offset 2, 0, 19 ********************)


(**************** CUT  83, - *****************)

ecut and [
eqmod cf2019 f2019 2048, eqmod cf2319 f2319 2048, eqmod cf2619 f2619 2048,
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x200150a4*x**2*y**0*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x200150c8*x**2*y**0*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x200150ec*x**2*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x20015e24*x**2*y**0*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x20015e48*x**2*y**0*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x20015e6c*x**2*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x200189ac; Value = 0x00030000; PC = 0x8055f1c *)
mov r5 L0x200189ac;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x2001876c; Value = 0x0ffd0003; PC = 0x8055f20 *)
mov r6 L0x2001876c;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018bec; Value = 0x00000000; PC = 0x8055f24 *)
mov r8 L0x20018bec;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2020@sint32 : and [cf2020 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2320@sint32 : and [cf2320 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2620@sint32 : and [cf2620 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20015110; PC = 0x8055f64 *)
mov L0x20015110 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015134; PC = 0x8055f68 *)
mov L0x20015134 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015158; PC = 0x8055f6c *)
mov L0x20015158 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015e90; PC = 0x8055f70 *)
mov L0x20015e90 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015eb4; PC = 0x8055f74 *)
mov L0x20015eb4 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015ed8; PC = 0x8055f78 *)
mov L0x20015ed8 r9;



(******************** offset 2, 0, 20 ********************)


(**************** CUT  84, - *****************)

ecut and [
eqmod cf2020 f2020 2048, eqmod cf2320 f2320 2048, eqmod cf2620 f2620 2048,
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015110*x**2*y**0*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015134*x**2*y**0*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015158*x**2*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015e90*x**2*y**0*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015eb4*x**2*y**0*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015ed8*x**2*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x20018832; Value = 0x00000000; PC = 0x8055f7c *)
mov r5 L0x20018832;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x200185f2; Value = 0x00000000; PC = 0x8055f80 *)
mov r6 L0x200185f2;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018a72; Value = 0x00000003; PC = 0x8055f84 *)
mov r8 L0x20018a72;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2021@sint32 : and [cf2021 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2321@sint32 : and [cf2321 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2621@sint32 : and [cf2621 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x2001517c; PC = 0x8055fc4 *)
mov L0x2001517c r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x200151a0; PC = 0x8055fc8 *)
mov L0x200151a0 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200151c4; PC = 0x8055fcc *)
mov L0x200151c4 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015efc; PC = 0x8055fd0 *)
mov L0x20015efc r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015f20; PC = 0x8055fd4 *)
mov L0x20015f20 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015f44; PC = 0x8055fd8 *)
mov L0x20015f44 r9;



(******************** offset 2, 0, 21 ********************)


(**************** CUT  85, - *****************)

ecut and [
eqmod cf2021 f2021 2048, eqmod cf2321 f2321 2048, eqmod cf2621 f2621 2048,
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x2001517c*x**2*y**0*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x200151a0*x**2*y**0*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x200151c4*x**2*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x20015efc*x**2*y**0*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x20015f20*x**2*y**0*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x20015f44*x**2*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x200186b8; Value = 0x00030000; PC = 0x8055fdc *)
mov r5 L0x200186b8;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018b38; Value = 0x00030ffd; PC = 0x8055fe0 *)
mov r7 L0x20018b38;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x200188f8; Value = 0x00030003; PC = 0x8055fe4 *)
mov r8 L0x200188f8;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2022@sint32 : and [cf2022 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2322@sint32 : and [cf2322 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2622@sint32 : and [cf2622 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200151e8; PC = 0x8056038 *)
mov L0x200151e8 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x2001520c; PC = 0x805603c *)
mov L0x2001520c r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015230; PC = 0x8056040 *)
mov L0x20015230 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015f68; PC = 0x8056044 *)
mov L0x20015f68 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015f8c; PC = 0x8056048 *)
mov L0x20015f8c r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015fb0; PC = 0x805604c *)
mov L0x20015fb0 r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;



(******************** offset 2, 0, 22 ********************)


(**************** CUT  86, - *****************)

ecut and [
eqmod cf2022 f2022 2048, eqmod cf2322 f2322 2048, eqmod cf2622 f2622 2048,
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x200151e8*x**2*y**0*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x2001520c*x**2*y**0*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x20015230*x**2*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x20015f68*x**2*y**0*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x20015f8c*x**2*y**0*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x20015fb0*x**2*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1566]	; 0x61e                 #! EA = L0x20018bfe; Value = 0x00000000; PC = 0x805605c *)
mov r4 L0x20018bfe;
(* ldrsh.w	r7, [lr, #990]	; 0x3de                  #! EA = L0x200189be; Value = 0x00000ffd; PC = 0x8056060 *)
mov r7 L0x200189be;
(* ldrsh.w	r8, [lr, #414]	; 0x19e                  #! EA = L0x2001877e; Value = 0x0ffd0000; PC = 0x8056064 *)
mov r8 L0x2001877e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056068 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2023@sint32 : and [cf2023 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805606c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2323@sint32 : and [cf2323 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056070 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2623@sint32 : and [cf2623 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056074 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056078 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x805607c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x8056080 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056084 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056088 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x805608a *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x805608c *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056090 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056094 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056098 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805609c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805609e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80560a0 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80560a4 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80560a8 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80560ac *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20015254; PC = 0x80560b0 *)
mov L0x20015254 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20015278; PC = 0x80560b4 *)
mov L0x20015278 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x2001529c; PC = 0x80560b8 *)
mov L0x2001529c r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015fd4; PC = 0x80560bc *)
mov L0x20015fd4 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015ff8; PC = 0x80560c0 *)
mov L0x20015ff8 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x2001601c; PC = 0x80560c4 *)
mov L0x2001601c r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80560c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 2, 0, 23 ********************)


(**************** CUT  87, - *****************)

ecut and [
eqmod cf2023 f2023 2048, eqmod cf2323 f2323 2048, eqmod cf2623 f2623 2048,
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x20015254*x**2*y**0*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x20015278*x**2*y**0*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x2001529c*x**2*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x20015fd4*x**2*y**0*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x20015ff8*x**2*y**0*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x2001601c*x**2*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1188]	; 0x4a4                 #! EA = L0x20018a84; Value = 0x00030000; PC = 0x80560cc *)
mov r4 L0x20018a84;
(* ldrsh.w	r7, [lr, #612]	; 0x264                  #! EA = L0x20018844; Value = 0x0ffd0000; PC = 0x80560d0 *)
mov r7 L0x20018844;
(* ldrsh.w	r8, [lr, #36]	; 0x24                    #! EA = L0x20018604; Value = 0x00000000; PC = 0x80560d4 *)
mov r8 L0x20018604;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80560d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2024@sint32 : and [cf2024 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80560dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2324@sint32 : and [cf2324 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80560e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2624@sint32 : and [cf2624 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80560e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80560e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80560ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80560f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80560f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80560f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80560fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80560fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056100 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056104 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056108 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805610c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805610e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056110 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056114 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056118 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805611c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200152c0; PC = 0x8056120 *)
mov L0x200152c0 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200152e4; PC = 0x8056124 *)
mov L0x200152e4 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015308; PC = 0x8056128 *)
mov L0x20015308 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016040; PC = 0x805612c *)
mov L0x20016040 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20016064; PC = 0x8056130 *)
mov L0x20016064 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20016088; PC = 0x8056134 *)
mov L0x20016088 r9;



(******************** offset 2, 0, 24 ********************)


(**************** CUT  88, - *****************)

ecut and [
eqmod cf2024 f2024 2048, eqmod cf2324 f2324 2048, eqmod cf2624 f2624 2048,
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x200152c0*x**2*y**0*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x200152e4*x**2*y**0*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x20015308*x**2*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x20016040*x**2*y**0*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x20016064*x**2*y**0*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x20016088*x**2*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #810]	; 0x32a                  #! EA = L0x2001890a; Value = 0x0ffd0003; PC = 0x8056138 *)
mov r4 L0x2001890a;
(* ldrsh.w	r7, [lr, #234]	; 0xea                   #! EA = L0x200186ca; Value = 0x0ffd0000; PC = 0x805613c *)
mov r7 L0x200186ca;
(* ldrsh.w	r9, [lr, #1386]	; 0x56a                 #! EA = L0x20018b4a; Value = 0x00000ffd; PC = 0x8056140 *)
mov r9 L0x20018b4a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056144 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2025@sint32 : and [cf2025 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056148 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2325@sint32 : and [cf2325 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x805614c *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2625@sint32 : and [cf2625 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056150 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056154 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x8056158 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x805615c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056160 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056164 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x8056168 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x805616c *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056170 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056174 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x8056176 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056178 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x2001532c; PC = 0x805617c *)
mov L0x2001532c r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20015350; PC = 0x8056180 *)
mov L0x20015350 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20015374; PC = 0x8056184 *)
mov L0x20015374 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200160ac; PC = 0x8056188 *)
mov L0x200160ac r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200160d0; PC = 0x805618c *)
mov L0x200160d0 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200160f4; PC = 0x8056190 *)
mov L0x200160f4 r9;



(******************** offset 2, 0, 25 ********************)


(**************** CUT  89, - *****************)

ecut and [
eqmod cf2025 f2025 2048, eqmod cf2325 f2325 2048, eqmod cf2625 f2625 2048,
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x2001532c*x**2*y**0*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x20015350*x**2*y**0*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x20015374*x**2*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x200160ac*x**2*y**0*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x200160d0*x**2*y**0*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x200160f4*x**2*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #432]	; 0x1b0                  #! EA = L0x20018790; Value = 0x00030000; PC = 0x8056194 *)
mov r4 L0x20018790;
(* ldrsh.w	r6, [lr, #1584]	; 0x630                 #! EA = L0x20018c10; Value = 0x00000000; PC = 0x8056198 *)
mov r6 L0x20018c10;
(* ldrsh.w	r9, [lr, #1008]	; 0x3f0                 #! EA = L0x200189d0; Value = 0x0ffd0003; PC = 0x805619c *)
mov r9 L0x200189d0;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80561a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2026@sint32 : and [cf2026 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80561a4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2326@sint32 : and [cf2326 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80561a8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2626@sint32 : and [cf2626 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80561ac *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80561b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80561b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80561b8 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80561bc *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80561c0 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80561c2 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80561c4 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80561c8 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80561cc *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80561d0 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80561d4 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80561d6 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80561d8 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80561dc *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80561e0 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80561e4 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20015398; PC = 0x80561e8 *)
mov L0x20015398 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200153bc; PC = 0x80561ec *)
mov L0x200153bc r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200153e0; PC = 0x80561f0 *)
mov L0x200153e0 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016118; PC = 0x80561f4 *)
mov L0x20016118 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x2001613c; PC = 0x80561f8 *)
mov L0x2001613c r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20016160; PC = 0x80561fc *)
mov L0x20016160 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056200 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056204 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x8055e3c <_Good_loop0_inner>              #! PC = 0x8056208 *)
#b.w	0x8055e3c <_Good_loop0_inner>              #! 0x8056208 = 0x8056208;



(******************** offset 2, 0, 26 ********************)


(**************** CUT  90, - *****************)

ecut and [
eqmod cf2026 f2026 2048, eqmod cf2326 f2326 2048, eqmod cf2626 f2626 2048,
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x20015398*x**2*y**0*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x200153bc*x**2*y**0*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x200153e0*x**2*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x20016118*x**2*y**0*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x2001613c*x**2*y**0*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x20016160*x**2*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr]                                #! EA = L0x20018616; Value = 0x0ffd0003; PC = 0x8055e3c *)
mov r4 L0x20018616;
(* ldrsh.w	r6, [lr, #1152]	; 0x480                 #! EA = L0x20018a96; Value = 0x00000003; PC = 0x8055e40 *)
mov r6 L0x20018a96;
(* ldrsh.w	r9, [lr, #576]	; 0x240                  #! EA = L0x20018856; Value = 0x0ffd0ffd; PC = 0x8055e44 *)
mov r9 L0x20018856;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8055e48 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2027@sint32 : and [cf2027 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055e4c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2327@sint32 : and [cf2327 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055e50 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2627@sint32 : and [cf2627 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8055e54 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8055e58 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8055e5c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8055e60 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8055e64 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8055e68 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8055e6a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8055e6c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8055e70 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8055e74 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8055e78 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8055e7c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8055e7e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8055e80 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8055e84 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8055e88 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8055e8c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015404; PC = 0x8055e90 *)
mov L0x20015404 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015428; PC = 0x8055e94 *)
mov L0x20015428 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x2001544c; PC = 0x8055e98 *)
mov L0x2001544c r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016184; PC = 0x8055e9c *)
mov L0x20016184 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200161a8; PC = 0x8055ea0 *)
mov L0x200161a8 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200161cc; PC = 0x8055ea4 *)
mov L0x200161cc r9;



(******************** offset 2, 0, 27 ********************)


(**************** CUT  91, - *****************)

ecut and [
eqmod cf2027 f2027 2048, eqmod cf2327 f2327 2048, eqmod cf2627 f2627 2048,
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x20015404*x**2*y**0*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x20015428*x**2*y**0*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x2001544c*x**2*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x20016184*x**2*y**0*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x200161a8*x**2*y**0*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x200161cc*x**2*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1350]	; 0x546                 #! EA = L0x20018b5c; Value = 0x00000ffd; PC = 0x8055ea8 *)
mov r5 L0x20018b5c;
(* ldrsh.w	r6, [lr, #774]	; 0x306                  #! EA = L0x2001891c; Value = 0x00000003; PC = 0x8055eac *)
mov r6 L0x2001891c;
(* ldrsh.w	r9, [lr, #198]	; 0xc6                   #! EA = L0x200186dc; Value = 0x00000003; PC = 0x8055eb0 *)
mov r9 L0x200186dc;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055eb4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2028@sint32 : and [cf2028 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055eb8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2328@sint32 : and [cf2328 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8055ebc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2628@sint32 : and [cf2628 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8055ec0 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ec4 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8055ec8 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8055ecc *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8055ed0 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8055ed4 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8055ed6 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8055ed8 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8055edc *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x8055ee0 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8055ee4 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8055ee8 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8055eec *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x8055ef0 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8055ef4 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8055ef8 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8055efc *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f00 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015470; PC = 0x8055f04 *)
mov L0x20015470 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20015494; PC = 0x8055f08 *)
mov L0x20015494 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200154b8; PC = 0x8055f0c *)
mov L0x200154b8 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200161f0; PC = 0x8055f10 *)
mov L0x200161f0 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20016214; PC = 0x8055f14 *)
mov L0x20016214 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20016238; PC = 0x8055f18 *)
mov L0x20016238 r9;



(******************** offset 2, 0, 28 ********************)


(**************** CUT  92, - *****************)

ecut and [
eqmod cf2028 f2028 2048, eqmod cf2328 f2328 2048, eqmod cf2628 f2628 2048,
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x20015470*x**2*y**0*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x20015494*x**2*y**0*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x200154b8*x**2*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x200161f0*x**2*y**0*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x20016214*x**2*y**0*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x20016238*x**2*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #972]	; 0x3cc                  #! EA = L0x200189e2; Value = 0x00030003; PC = 0x8055f1c *)
mov r5 L0x200189e2;
(* ldrsh.w	r6, [lr, #396]	; 0x18c                  #! EA = L0x200187a2; Value = 0x00030000; PC = 0x8055f20 *)
mov r6 L0x200187a2;
(* ldrsh.w	r8, [lr, #1548]	; 0x60c                 #! EA = L0x20018c22; Value = 0x00000000; PC = 0x8055f24 *)
mov r8 L0x20018c22;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f28 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2029@sint32 : and [cf2029 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f2c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2329@sint32 : and [cf2329 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f30 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2629@sint32 : and [cf2629 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f34 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f38 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f3c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055f40 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055f44 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055f48 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055f4c *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055f50 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055f54 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055f58 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055f5c *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055f5e *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055f60 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200154dc; PC = 0x8055f64 *)
mov L0x200154dc r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015500; PC = 0x8055f68 *)
mov L0x20015500 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015524; PC = 0x8055f6c *)
mov L0x20015524 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x2001625c; PC = 0x8055f70 *)
mov L0x2001625c r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016280; PC = 0x8055f74 *)
mov L0x20016280 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200162a4; PC = 0x8055f78 *)
mov L0x200162a4 r9;



(******************** offset 2, 0, 29 ********************)


(**************** CUT  93, - *****************)

ecut and [
eqmod cf2029 f2029 2048, eqmod cf2329 f2329 2048, eqmod cf2629 f2629 2048,
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x200154dc*x**2*y**0*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x20015500*x**2*y**0*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x20015524*x**2*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x2001625c*x**2*y**0*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x20016280*x**2*y**0*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x200162a4*x**2*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #594]	; 0x252                  #! EA = L0x20018868; Value = 0x0ffd0000; PC = 0x8055f7c *)
mov r5 L0x20018868;
(* ldrsh.w	r6, [lr, #18]                           #! EA = L0x20018628; Value = 0x00000ffd; PC = 0x8055f80 *)
mov r6 L0x20018628;
(* ldrsh.w	r8, [lr, #1170]	; 0x492                 #! EA = L0x20018aa8; Value = 0x00000000; PC = 0x8055f84 *)
mov r8 L0x20018aa8;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055f88 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2030@sint32 : and [cf2030 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8055f8c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2330@sint32 : and [cf2330 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055f90 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2630@sint32 : and [cf2630 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8055f94 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8055f98 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8055f9c *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8055fa0 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8055fa4 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8055fa8 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8055fac *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8055fb0 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8055fb4 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8055fb8 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8055fbc *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8055fbe *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8055fc0 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015548; PC = 0x8055fc4 *)
mov L0x20015548 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x2001556c; PC = 0x8055fc8 *)
mov L0x2001556c r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20015590; PC = 0x8055fcc *)
mov L0x20015590 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x200162c8; PC = 0x8055fd0 *)
mov L0x200162c8 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x200162ec; PC = 0x8055fd4 *)
mov L0x200162ec r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20016310; PC = 0x8055fd8 *)
mov L0x20016310 r9;



(******************** offset 2, 0, 30 ********************)


(**************** CUT  94, - *****************)

ecut and [
eqmod cf2030 f2030 2048, eqmod cf2330 f2330 2048, eqmod cf2630 f2630 2048,
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x20015548*x**2*y**0*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x2001556c*x**2*y**0*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x20015590*x**2*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x200162c8*x**2*y**0*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x200162ec*x**2*y**0*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x20016310*x**2*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #216]	; 0xd8                   #! EA = L0x200186ee; Value = 0x00000000; PC = 0x8055fdc *)
mov r5 L0x200186ee;
(* ldrsh.w	r7, [lr, #1368]	; 0x558                 #! EA = L0x20018b6e; Value = 0x00000ffd; PC = 0x8055fe0 *)
mov r7 L0x20018b6e;
(* ldrsh.w	r8, [lr, #792]	; 0x318                  #! EA = L0x2001892e; Value = 0x00000003; PC = 0x8055fe4 *)
mov r8 L0x2001892e;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8055fe8 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2031@sint32 : and [cf2031 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8055fec *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2331@sint32 : and [cf2331 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8055ff0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2631@sint32 : and [cf2631 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x8055ff4 *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8055ff8 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8055ffc *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056000 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x8056004 *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056008 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x805600a *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x805600c *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056010 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x8056014 *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056018 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x805601c *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056020 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x8056024 *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056028 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x805602c *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056030 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x8056034 *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200155b4; PC = 0x8056038 *)
mov L0x200155b4 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x200155d8; PC = 0x805603c *)
mov L0x200155d8 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x200155fc; PC = 0x8056040 *)
mov L0x200155fc r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20016334; PC = 0x8056044 *)
mov L0x20016334 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20016358; PC = 0x8056048 *)
mov L0x20016358 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x2001637c; PC = 0x805604c *)
mov L0x2001637c r9;
(* vmov	r1, s3                                     #! PC = 0x8056050 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056054 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805620c <_Good_loop0_inner_end>        #! PC = 0x8056058 *)
#beq.w	0x805620c <_Good_loop0_inner_end>        #! 0x8056058 = 0x8056058;
(* sub.w	lr, lr, #162	; 0xa2                       #! PC = 0x805620c *)
subs dc lr lr 162@uint32;
(* subw	r0, r0, #2916	; 0xb64                      #! PC = 0x8056210 *)
subc dc r0 r0 2916@uint32;
(* add.w	lr, lr, #2                                #! PC = 0x8056214 *)
adds dc lr lr 2@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8056218 *)
adds dc r0 r0 4@uint32;
(* cmp.w	r0, r12                                   #! PC = 0x805621c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8055e34 <_Good_loop0>                  #! PC = 0x8056220 *)
#bne.w	0x8055e34 <_Good_loop0>                  #! 0x8056220 = 0x8056220;
(* sub.w	lr, lr, #6                                #! PC = 0x8056224 *)
subs dc lr lr 6@uint32;
(* sub.w	r0, r0, #12                               #! PC = 0x8056228 *)
subs dc r0 r0 12@uint32;
(* add.w	r0, r0, #12                               #! PC = 0x805622c *)
adds dc r0 r0 12@uint32;
(* add.w	r12, r0, #12                              #! PC = 0x8056230 *)
adds dc r12 r0 12@uint32;
(* addw	r1, r0, #2916	; 0xb64                      #! PC = 0x8056234 *)
adds dc r1 r0 2916@uint32;
(* vmov	s3, r1                                     #! PC = 0x8056238 *)
mov s3 r1;



(******************** offset 2, 0, 31 ********************)


(**************** CUT  95, - *****************)

ecut and [
eqmod cf2031 f2031 2048, eqmod cf2331 f2331 2048, eqmod cf2631 f2631 2048,
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x200155b4*x**2*y**0*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x200155d8*x**2*y**0*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x200155fc*x**2*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x20016334*x**2*y**0*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x20016358*x**2*y**0*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x2001637c*x**2*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(**************** CUT -,   2 *****************)

rcut and [
(-3367617)@32<=sL0x200148a0,L0x200148a0<=s3367617@32,
(-3367617)@32<=sL0x200148c4,L0x200148c4<=s3367617@32,
(-3367617)@32<=sL0x200148e8,L0x200148e8<=s3367617@32,
(-3367617)@32<=sL0x20015620,L0x20015620<=s3367617@32,
(-3367617)@32<=sL0x20015644,L0x20015644<=s3367617@32,
(-3367617)@32<=sL0x20015668,L0x20015668<=s3367617@32
,
(-3367617)@32<=sL0x2001490c,L0x2001490c<=s3367617@32,
(-3367617)@32<=sL0x20014930,L0x20014930<=s3367617@32,
(-3367617)@32<=sL0x20014954,L0x20014954<=s3367617@32,
(-3367617)@32<=sL0x2001568c,L0x2001568c<=s3367617@32,
(-3367617)@32<=sL0x200156b0,L0x200156b0<=s3367617@32,
(-3367617)@32<=sL0x200156d4,L0x200156d4<=s3367617@32
,
(-3367617)@32<=sL0x20014978,L0x20014978<=s3367617@32,
(-3367617)@32<=sL0x2001499c,L0x2001499c<=s3367617@32,
(-3367617)@32<=sL0x200149c0,L0x200149c0<=s3367617@32,
(-3367617)@32<=sL0x200156f8,L0x200156f8<=s3367617@32,
(-3367617)@32<=sL0x2001571c,L0x2001571c<=s3367617@32,
(-3367617)@32<=sL0x20015740,L0x20015740<=s3367617@32
,
(-3367617)@32<=sL0x200149e4,L0x200149e4<=s3367617@32,
(-3367617)@32<=sL0x20014a08,L0x20014a08<=s3367617@32,
(-3367617)@32<=sL0x20014a2c,L0x20014a2c<=s3367617@32,
(-3367617)@32<=sL0x20015764,L0x20015764<=s3367617@32,
(-3367617)@32<=sL0x20015788,L0x20015788<=s3367617@32,
(-3367617)@32<=sL0x200157ac,L0x200157ac<=s3367617@32
,
(-3367617)@32<=sL0x20014a50,L0x20014a50<=s3367617@32,
(-3367617)@32<=sL0x20014a74,L0x20014a74<=s3367617@32,
(-3367617)@32<=sL0x20014a98,L0x20014a98<=s3367617@32,
(-3367617)@32<=sL0x200157d0,L0x200157d0<=s3367617@32,
(-3367617)@32<=sL0x200157f4,L0x200157f4<=s3367617@32,
(-3367617)@32<=sL0x20015818,L0x20015818<=s3367617@32
,
(-3367617)@32<=sL0x20014abc,L0x20014abc<=s3367617@32,
(-3367617)@32<=sL0x20014ae0,L0x20014ae0<=s3367617@32,
(-3367617)@32<=sL0x20014b04,L0x20014b04<=s3367617@32,
(-3367617)@32<=sL0x2001583c,L0x2001583c<=s3367617@32,
(-3367617)@32<=sL0x20015860,L0x20015860<=s3367617@32,
(-3367617)@32<=sL0x20015884,L0x20015884<=s3367617@32
,
(-3367617)@32<=sL0x20014b28,L0x20014b28<=s3367617@32,
(-3367617)@32<=sL0x20014b4c,L0x20014b4c<=s3367617@32,
(-3367617)@32<=sL0x20014b70,L0x20014b70<=s3367617@32,
(-3367617)@32<=sL0x200158a8,L0x200158a8<=s3367617@32,
(-3367617)@32<=sL0x200158cc,L0x200158cc<=s3367617@32,
(-3367617)@32<=sL0x200158f0,L0x200158f0<=s3367617@32
,
(-3367617)@32<=sL0x20014b94,L0x20014b94<=s3367617@32,
(-3367617)@32<=sL0x20014bb8,L0x20014bb8<=s3367617@32,
(-3367617)@32<=sL0x20014bdc,L0x20014bdc<=s3367617@32,
(-3367617)@32<=sL0x20015914,L0x20015914<=s3367617@32,
(-3367617)@32<=sL0x20015938,L0x20015938<=s3367617@32,
(-3367617)@32<=sL0x2001595c,L0x2001595c<=s3367617@32
,
(-3367617)@32<=sL0x20014c00,L0x20014c00<=s3367617@32,
(-3367617)@32<=sL0x20014c24,L0x20014c24<=s3367617@32,
(-3367617)@32<=sL0x20014c48,L0x20014c48<=s3367617@32,
(-3367617)@32<=sL0x20015980,L0x20015980<=s3367617@32,
(-3367617)@32<=sL0x200159a4,L0x200159a4<=s3367617@32,
(-3367617)@32<=sL0x200159c8,L0x200159c8<=s3367617@32
,
(-3367617)@32<=sL0x20014c6c,L0x20014c6c<=s3367617@32,
(-3367617)@32<=sL0x20014c90,L0x20014c90<=s3367617@32,
(-3367617)@32<=sL0x20014cb4,L0x20014cb4<=s3367617@32,
(-3367617)@32<=sL0x200159ec,L0x200159ec<=s3367617@32,
(-3367617)@32<=sL0x20015a10,L0x20015a10<=s3367617@32,
(-3367617)@32<=sL0x20015a34,L0x20015a34<=s3367617@32
,
(-3367617)@32<=sL0x20014cd8,L0x20014cd8<=s3367617@32,
(-3367617)@32<=sL0x20014cfc,L0x20014cfc<=s3367617@32,
(-3367617)@32<=sL0x20014d20,L0x20014d20<=s3367617@32,
(-3367617)@32<=sL0x20015a58,L0x20015a58<=s3367617@32,
(-3367617)@32<=sL0x20015a7c,L0x20015a7c<=s3367617@32,
(-3367617)@32<=sL0x20015aa0,L0x20015aa0<=s3367617@32
,
(-3367617)@32<=sL0x20014d44,L0x20014d44<=s3367617@32,
(-3367617)@32<=sL0x20014d68,L0x20014d68<=s3367617@32,
(-3367617)@32<=sL0x20014d8c,L0x20014d8c<=s3367617@32,
(-3367617)@32<=sL0x20015ac4,L0x20015ac4<=s3367617@32,
(-3367617)@32<=sL0x20015ae8,L0x20015ae8<=s3367617@32,
(-3367617)@32<=sL0x20015b0c,L0x20015b0c<=s3367617@32
,
(-3367617)@32<=sL0x20014db0,L0x20014db0<=s3367617@32,
(-3367617)@32<=sL0x20014dd4,L0x20014dd4<=s3367617@32,
(-3367617)@32<=sL0x20014df8,L0x20014df8<=s3367617@32,
(-3367617)@32<=sL0x20015b30,L0x20015b30<=s3367617@32,
(-3367617)@32<=sL0x20015b54,L0x20015b54<=s3367617@32,
(-3367617)@32<=sL0x20015b78,L0x20015b78<=s3367617@32
,
(-3367617)@32<=sL0x20014e1c,L0x20014e1c<=s3367617@32,
(-3367617)@32<=sL0x20014e40,L0x20014e40<=s3367617@32,
(-3367617)@32<=sL0x20014e64,L0x20014e64<=s3367617@32,
(-3367617)@32<=sL0x20015b9c,L0x20015b9c<=s3367617@32,
(-3367617)@32<=sL0x20015bc0,L0x20015bc0<=s3367617@32,
(-3367617)@32<=sL0x20015be4,L0x20015be4<=s3367617@32
,
(-3367617)@32<=sL0x20014e88,L0x20014e88<=s3367617@32,
(-3367617)@32<=sL0x20014eac,L0x20014eac<=s3367617@32,
(-3367617)@32<=sL0x20014ed0,L0x20014ed0<=s3367617@32,
(-3367617)@32<=sL0x20015c08,L0x20015c08<=s3367617@32,
(-3367617)@32<=sL0x20015c2c,L0x20015c2c<=s3367617@32,
(-3367617)@32<=sL0x20015c50,L0x20015c50<=s3367617@32
,
(-3367617)@32<=sL0x20014ef4,L0x20014ef4<=s3367617@32,
(-3367617)@32<=sL0x20014f18,L0x20014f18<=s3367617@32,
(-3367617)@32<=sL0x20014f3c,L0x20014f3c<=s3367617@32,
(-3367617)@32<=sL0x20015c74,L0x20015c74<=s3367617@32,
(-3367617)@32<=sL0x20015c98,L0x20015c98<=s3367617@32,
(-3367617)@32<=sL0x20015cbc,L0x20015cbc<=s3367617@32
,
(-3367617)@32<=sL0x20014f60,L0x20014f60<=s3367617@32,
(-3367617)@32<=sL0x20014f84,L0x20014f84<=s3367617@32,
(-3367617)@32<=sL0x20014fa8,L0x20014fa8<=s3367617@32,
(-3367617)@32<=sL0x20015ce0,L0x20015ce0<=s3367617@32,
(-3367617)@32<=sL0x20015d04,L0x20015d04<=s3367617@32,
(-3367617)@32<=sL0x20015d28,L0x20015d28<=s3367617@32
,
(-3367617)@32<=sL0x20014fcc,L0x20014fcc<=s3367617@32,
(-3367617)@32<=sL0x20014ff0,L0x20014ff0<=s3367617@32,
(-3367617)@32<=sL0x20015014,L0x20015014<=s3367617@32,
(-3367617)@32<=sL0x20015d4c,L0x20015d4c<=s3367617@32,
(-3367617)@32<=sL0x20015d70,L0x20015d70<=s3367617@32,
(-3367617)@32<=sL0x20015d94,L0x20015d94<=s3367617@32
,
(-3367617)@32<=sL0x20015038,L0x20015038<=s3367617@32,
(-3367617)@32<=sL0x2001505c,L0x2001505c<=s3367617@32,
(-3367617)@32<=sL0x20015080,L0x20015080<=s3367617@32,
(-3367617)@32<=sL0x20015db8,L0x20015db8<=s3367617@32,
(-3367617)@32<=sL0x20015ddc,L0x20015ddc<=s3367617@32,
(-3367617)@32<=sL0x20015e00,L0x20015e00<=s3367617@32
,
(-3367617)@32<=sL0x200150a4,L0x200150a4<=s3367617@32,
(-3367617)@32<=sL0x200150c8,L0x200150c8<=s3367617@32,
(-3367617)@32<=sL0x200150ec,L0x200150ec<=s3367617@32,
(-3367617)@32<=sL0x20015e24,L0x20015e24<=s3367617@32,
(-3367617)@32<=sL0x20015e48,L0x20015e48<=s3367617@32,
(-3367617)@32<=sL0x20015e6c,L0x20015e6c<=s3367617@32
,
(-3367617)@32<=sL0x20015110,L0x20015110<=s3367617@32,
(-3367617)@32<=sL0x20015134,L0x20015134<=s3367617@32,
(-3367617)@32<=sL0x20015158,L0x20015158<=s3367617@32,
(-3367617)@32<=sL0x20015e90,L0x20015e90<=s3367617@32,
(-3367617)@32<=sL0x20015eb4,L0x20015eb4<=s3367617@32,
(-3367617)@32<=sL0x20015ed8,L0x20015ed8<=s3367617@32
,
(-3367617)@32<=sL0x2001517c,L0x2001517c<=s3367617@32,
(-3367617)@32<=sL0x200151a0,L0x200151a0<=s3367617@32,
(-3367617)@32<=sL0x200151c4,L0x200151c4<=s3367617@32,
(-3367617)@32<=sL0x20015efc,L0x20015efc<=s3367617@32,
(-3367617)@32<=sL0x20015f20,L0x20015f20<=s3367617@32,
(-3367617)@32<=sL0x20015f44,L0x20015f44<=s3367617@32
,
(-3367617)@32<=sL0x200151e8,L0x200151e8<=s3367617@32,
(-3367617)@32<=sL0x2001520c,L0x2001520c<=s3367617@32,
(-3367617)@32<=sL0x20015230,L0x20015230<=s3367617@32,
(-3367617)@32<=sL0x20015f68,L0x20015f68<=s3367617@32,
(-3367617)@32<=sL0x20015f8c,L0x20015f8c<=s3367617@32,
(-3367617)@32<=sL0x20015fb0,L0x20015fb0<=s3367617@32
,
(-3367617)@32<=sL0x20015254,L0x20015254<=s3367617@32,
(-3367617)@32<=sL0x20015278,L0x20015278<=s3367617@32,
(-3367617)@32<=sL0x2001529c,L0x2001529c<=s3367617@32,
(-3367617)@32<=sL0x20015fd4,L0x20015fd4<=s3367617@32,
(-3367617)@32<=sL0x20015ff8,L0x20015ff8<=s3367617@32,
(-3367617)@32<=sL0x2001601c,L0x2001601c<=s3367617@32
,
(-3367617)@32<=sL0x200152c0,L0x200152c0<=s3367617@32,
(-3367617)@32<=sL0x200152e4,L0x200152e4<=s3367617@32,
(-3367617)@32<=sL0x20015308,L0x20015308<=s3367617@32,
(-3367617)@32<=sL0x20016040,L0x20016040<=s3367617@32,
(-3367617)@32<=sL0x20016064,L0x20016064<=s3367617@32,
(-3367617)@32<=sL0x20016088,L0x20016088<=s3367617@32
,
(-3367617)@32<=sL0x2001532c,L0x2001532c<=s3367617@32,
(-3367617)@32<=sL0x20015350,L0x20015350<=s3367617@32,
(-3367617)@32<=sL0x20015374,L0x20015374<=s3367617@32,
(-3367617)@32<=sL0x200160ac,L0x200160ac<=s3367617@32,
(-3367617)@32<=sL0x200160d0,L0x200160d0<=s3367617@32,
(-3367617)@32<=sL0x200160f4,L0x200160f4<=s3367617@32
,
(-3367617)@32<=sL0x20015398,L0x20015398<=s3367617@32,
(-3367617)@32<=sL0x200153bc,L0x200153bc<=s3367617@32,
(-3367617)@32<=sL0x200153e0,L0x200153e0<=s3367617@32,
(-3367617)@32<=sL0x20016118,L0x20016118<=s3367617@32,
(-3367617)@32<=sL0x2001613c,L0x2001613c<=s3367617@32,
(-3367617)@32<=sL0x20016160,L0x20016160<=s3367617@32
,
(-3367617)@32<=sL0x20015404,L0x20015404<=s3367617@32,
(-3367617)@32<=sL0x20015428,L0x20015428<=s3367617@32,
(-3367617)@32<=sL0x2001544c,L0x2001544c<=s3367617@32,
(-3367617)@32<=sL0x20016184,L0x20016184<=s3367617@32,
(-3367617)@32<=sL0x200161a8,L0x200161a8<=s3367617@32,
(-3367617)@32<=sL0x200161cc,L0x200161cc<=s3367617@32
,
(-3367617)@32<=sL0x20015470,L0x20015470<=s3367617@32,
(-3367617)@32<=sL0x20015494,L0x20015494<=s3367617@32,
(-3367617)@32<=sL0x200154b8,L0x200154b8<=s3367617@32,
(-3367617)@32<=sL0x200161f0,L0x200161f0<=s3367617@32,
(-3367617)@32<=sL0x20016214,L0x20016214<=s3367617@32,
(-3367617)@32<=sL0x20016238,L0x20016238<=s3367617@32
,
(-3367617)@32<=sL0x200154dc,L0x200154dc<=s3367617@32,
(-3367617)@32<=sL0x20015500,L0x20015500<=s3367617@32,
(-3367617)@32<=sL0x20015524,L0x20015524<=s3367617@32,
(-3367617)@32<=sL0x2001625c,L0x2001625c<=s3367617@32,
(-3367617)@32<=sL0x20016280,L0x20016280<=s3367617@32,
(-3367617)@32<=sL0x200162a4,L0x200162a4<=s3367617@32
,
(-3367617)@32<=sL0x20015548,L0x20015548<=s3367617@32,
(-3367617)@32<=sL0x2001556c,L0x2001556c<=s3367617@32,
(-3367617)@32<=sL0x20015590,L0x20015590<=s3367617@32,
(-3367617)@32<=sL0x200162c8,L0x200162c8<=s3367617@32,
(-3367617)@32<=sL0x200162ec,L0x200162ec<=s3367617@32,
(-3367617)@32<=sL0x20016310,L0x20016310<=s3367617@32
,
(-3367617)@32<=sL0x200155b4,L0x200155b4<=s3367617@32,
(-3367617)@32<=sL0x200155d8,L0x200155d8<=s3367617@32,
(-3367617)@32<=sL0x200155fc,L0x200155fc<=s3367617@32,
(-3367617)@32<=sL0x20016334,L0x20016334<=s3367617@32,
(-3367617)@32<=sL0x20016358,L0x20016358<=s3367617@32,
(-3367617)@32<=sL0x2001637c,L0x2001637c<=s3367617@32
] prove with [ precondition ];





(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x200186f0; Value = 0x00000000; PC = 0x805623c *)
mov r4 L0x200186f0;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018b70; Value = 0x00000000; PC = 0x8056240 *)
mov r6 L0x20018b70;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x20018930; Value = 0x00000000; PC = 0x8056244 *)
mov r9 L0x20018930;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0100@sint32 : and [cf0100 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0400@sint32 : and [cf0400 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0700@sint32 : and [cf0700 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200148a4; PC = 0x8056290 *)
mov L0x200148a4 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200148c8; PC = 0x8056294 *)
mov L0x200148c8 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x200148ec; PC = 0x8056298 *)
mov L0x200148ec r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015624; PC = 0x805629c *)
mov L0x20015624 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015648; PC = 0x80562a0 *)
mov L0x20015648 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x2001566c; PC = 0x80562a4 *)
mov L0x2001566c r9;


(******************** offset 0, 1,  0 ********************)


(**************** CUT  96, - *****************)

ecut and [
eqmod cf0100 f0100 2048, eqmod cf0400 f0400 2048, eqmod cf0700 f0700 2048,
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x200148a4*x**0*y**1*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x200148c8*x**0*y**1*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x200148ec*x**0*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x20015624*x**0*y**1*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x20015648*x**0*y**1*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x2001566c*x**0*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x20018576; Value = 0x0ffd0ffd; PC = 0x80562a8 *)
mov r4 L0x20018576;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x200189f6; Value = 0x0ffd0ffd; PC = 0x80562ac *)
mov r6 L0x200189f6;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x200187b6; Value = 0x0ffd0003; PC = 0x80562b0 *)
mov r9 L0x200187b6;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0101@sint32 : and [cf0101 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0401@sint32 : and [cf0401 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0701@sint32 : and [cf0701 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014910; PC = 0x80562fc *)
mov L0x20014910 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014934; PC = 0x8056300 *)
mov L0x20014934 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014958; PC = 0x8056304 *)
mov L0x20014958 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015690; PC = 0x8056308 *)
mov L0x20015690 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200156b4; PC = 0x805630c *)
mov L0x200156b4 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200156d8; PC = 0x8056310 *)
mov L0x200156d8 r9;



(******************** offset 0, 1,  1 ********************)


(**************** CUT  97, - *****************)

ecut and [
eqmod cf0101 f0101 2048, eqmod cf0401 f0401 2048, eqmod cf0701 f0701 2048,
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x20014910*x**0*y**1*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x20014934*x**0*y**1*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x20014958*x**0*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x20015690*x**0*y**1*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x200156b4*x**0*y**1*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x200156d8*x**0*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018abc; Value = 0x0ffd0ffd; PC = 0x8056314 *)
mov r5 L0x20018abc;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x2001887c; Value = 0x00000ffd; PC = 0x8056318 *)
mov r6 L0x2001887c;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x2001863c; Value = 0x0ffd0ffd; PC = 0x805631c *)
mov r9 L0x2001863c;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0102@sint32 : and [cf0102 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0402@sint32 : and [cf0402 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0702@sint32 : and [cf0702 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x2001497c; PC = 0x8056370 *)
mov L0x2001497c r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200149a0; PC = 0x8056374 *)
mov L0x200149a0 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200149c4; PC = 0x8056378 *)
mov L0x200149c4 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x200156fc; PC = 0x805637c *)
mov L0x200156fc r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015720; PC = 0x8056380 *)
mov L0x20015720 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015744; PC = 0x8056384 *)
mov L0x20015744 r9;



(******************** offset 0, 1,  2 ********************)


(**************** CUT  98, - *****************)

ecut and [
eqmod cf0102 f0102 2048, eqmod cf0402 f0402 2048, eqmod cf0702 f0702 2048,
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x2001497c*x**0*y**1*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x200149a0*x**0*y**1*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x200149c4*x**0*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x200156fc*x**0*y**1*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x20015720*x**0*y**1*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x20015744*x**0*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x20018942; Value = 0x0ffd0003; PC = 0x8056388 *)
mov r5 L0x20018942;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x20018702; Value = 0x0ffd0000; PC = 0x805638c *)
mov r6 L0x20018702;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018b82; Value = 0x00030000; PC = 0x8056390 *)
mov r8 L0x20018b82;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0103@sint32 : and [cf0103 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0403@sint32 : and [cf0403 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0703@sint32 : and [cf0703 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x200149e8; PC = 0x80563d0 *)
mov L0x200149e8 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014a0c; PC = 0x80563d4 *)
mov L0x20014a0c r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014a30; PC = 0x80563d8 *)
mov L0x20014a30 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015768; PC = 0x80563dc *)
mov L0x20015768 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x2001578c; PC = 0x80563e0 *)
mov L0x2001578c r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x200157b0; PC = 0x80563e4 *)
mov L0x200157b0 r9;



(******************** offset 0, 1,  3 ********************)


(**************** CUT  99, - *****************)

ecut and [
eqmod cf0103 f0103 2048, eqmod cf0403 f0403 2048, eqmod cf0703 f0703 2048,
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x200149e8*x**0*y**1*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x20014a0c*x**0*y**1*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x20014a30*x**0*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x20015768*x**0*y**1*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x2001578c*x**0*y**1*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x200157b0*x**0*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x200187c8; Value = 0x00000003; PC = 0x80563e8 *)
mov r5 L0x200187c8;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x20018588; Value = 0x00030000; PC = 0x80563ec *)
mov r6 L0x20018588;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018a08; Value = 0x0ffd0ffd; PC = 0x80563f0 *)
mov r8 L0x20018a08;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0104@sint32 : and [cf0104 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0404@sint32 : and [cf0404 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0704@sint32 : and [cf0704 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014a54; PC = 0x8056430 *)
mov L0x20014a54 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014a78; PC = 0x8056434 *)
mov L0x20014a78 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014a9c; PC = 0x8056438 *)
mov L0x20014a9c r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x200157d4; PC = 0x805643c *)
mov L0x200157d4 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x200157f8; PC = 0x8056440 *)
mov L0x200157f8 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x2001581c; PC = 0x8056444 *)
mov L0x2001581c r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;



(******************** offset 0, 1,  4 ********************)


(**************** CUT 100, - *****************)

ecut and [
eqmod cf0104 f0104 2048, eqmod cf0404 f0404 2048, eqmod cf0704 f0704 2048,
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x20014a54*x**0*y**1*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x20014a78*x**0*y**1*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x20014a9c*x**0*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x200157d4*x**0*y**1*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x200157f8*x**0*y**1*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x2001581c*x**0*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #222]	; 0xde                   #! EA = L0x2001864e; Value = 0x0ffd0ffd; PC = 0x8056454 *)
mov r5 L0x2001864e;
(* ldrsh.w	r7, [lr, #1374]	; 0x55e                 #! EA = L0x20018ace; Value = 0x00030ffd; PC = 0x8056458 *)
mov r7 L0x20018ace;
(* ldrsh.w	r8, [lr, #798]	; 0x31e                  #! EA = L0x2001888e; Value = 0x00000ffd; PC = 0x805645c *)
mov r8 L0x2001888e;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056460 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0105@sint32 : and [cf0105 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056464 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0405@sint32 : and [cf0405 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056468 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0705@sint32 : and [cf0705 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x805646c *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056470 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8056474 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056478 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x805647c *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056480 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x8056482 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x8056484 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056488 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x805648c *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056490 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x8056494 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056498 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x805649c *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80564a0 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x80564a4 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x80564a8 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x80564ac *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014ac0; PC = 0x80564b0 *)
mov L0x20014ac0 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014ae4; PC = 0x80564b4 *)
mov L0x20014ae4 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014b08; PC = 0x80564b8 *)
mov L0x20014b08 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015840; PC = 0x80564bc *)
mov L0x20015840 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015864; PC = 0x80564c0 *)
mov L0x20015864 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015888; PC = 0x80564c4 *)
mov L0x20015888 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80564c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 0, 1,  5 ********************)


(**************** CUT 101, - *****************)

ecut and [
eqmod cf0105 f0105 2048, eqmod cf0405 f0405 2048, eqmod cf0705 f0705 2048,
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20014ac0*x**0*y**1*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20014ae4*x**0*y**1*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20014b08*x**0*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20015840*x**0*y**1*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20015864*x**0*y**1*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20015888*x**0*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1572]	; 0x624                 #! EA = L0x20018b94; Value = 0x00000003; PC = 0x80564cc *)
mov r4 L0x20018b94;
(* ldrsh.w	r7, [lr, #996]	; 0x3e4                  #! EA = L0x20018954; Value = 0x0ffd0000; PC = 0x80564d0 *)
mov r7 L0x20018954;
(* ldrsh.w	r8, [lr, #420]	; 0x1a4                  #! EA = L0x20018714; Value = 0x0ffd0000; PC = 0x80564d4 *)
mov r8 L0x20018714;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80564d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0106@sint32 : and [cf0106 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80564dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0406@sint32 : and [cf0406 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80564e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0706@sint32 : and [cf0706 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80564e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80564e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80564ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80564f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80564f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80564f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80564fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80564fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056500 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056504 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056508 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805650c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805650e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056510 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056514 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056518 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805651c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014b2c; PC = 0x8056520 *)
mov L0x20014b2c r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014b50; PC = 0x8056524 *)
mov L0x20014b50 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014b74; PC = 0x8056528 *)
mov L0x20014b74 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200158ac; PC = 0x805652c *)
mov L0x200158ac r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200158d0; PC = 0x8056530 *)
mov L0x200158d0 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200158f4; PC = 0x8056534 *)
mov L0x200158f4 r9;



(******************** offset 0, 1,  6 ********************)


(**************** CUT 102, - *****************)

ecut and [
eqmod cf0106 f0106 2048, eqmod cf0406 f0406 2048, eqmod cf0706 f0706 2048,
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x20014b2c*x**0*y**1*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x20014b50*x**0*y**1*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x20014b74*x**0*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x200158ac*x**0*y**1*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x200158d0*x**0*y**1*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x200158f4*x**0*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1194]	; 0x4aa                 #! EA = L0x20018a1a; Value = 0x00000ffd; PC = 0x8056538 *)
mov r4 L0x20018a1a;
(* ldrsh.w	r7, [lr, #618]	; 0x26a                  #! EA = L0x200187da; Value = 0x00030000; PC = 0x805653c *)
mov r7 L0x200187da;
(* ldrsh.w	r8, [lr, #42]	; 0x2a                    #! EA = L0x2001859a; Value = 0x00030ffd; PC = 0x8056540 *)
mov r8 L0x2001859a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056544 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0107@sint32 : and [cf0107 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056548 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0407@sint32 : and [cf0407 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805654c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0707@sint32 : and [cf0707 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056550 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056554 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056558 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805655c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056560 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056564 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056566 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056568 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805656c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056570 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056574 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056578 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805657a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805657c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056580 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056584 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056588 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014b98; PC = 0x805658c *)
mov L0x20014b98 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014bbc; PC = 0x8056590 *)
mov L0x20014bbc r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014be0; PC = 0x8056594 *)
mov L0x20014be0 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015918; PC = 0x8056598 *)
mov L0x20015918 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x2001593c; PC = 0x805659c *)
mov L0x2001593c r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015960; PC = 0x80565a0 *)
mov L0x20015960 r9;



(******************** offset 0, 1,  7 ********************)


(**************** CUT 103, - *****************)

ecut and [
eqmod cf0107 f0107 2048, eqmod cf0407 f0407 2048, eqmod cf0707 f0707 2048,
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x20014b98*x**0*y**1*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x20014bbc*x**0*y**1*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x20014be0*x**0*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x20015918*x**0*y**1*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x2001593c*x**0*y**1*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x20015960*x**0*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #816]	; 0x330                  #! EA = L0x200188a0; Value = 0x00000000; PC = 0x80565a4 *)
mov r4 L0x200188a0;
(* ldrsh.w	r7, [lr, #240]	; 0xf0                   #! EA = L0x20018660; Value = 0x00000000; PC = 0x80565a8 *)
mov r7 L0x20018660;
(* ldrsh.w	r9, [lr, #1392]	; 0x570                 #! EA = L0x20018ae0; Value = 0x00000003; PC = 0x80565ac *)
mov r9 L0x20018ae0;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80565b0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0108@sint32 : and [cf0108 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80565b4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0408@sint32 : and [cf0408 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80565b8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0708@sint32 : and [cf0708 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x80565bc *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x80565c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x80565c4 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x80565c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x80565cc *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x80565d0 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x80565d4 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80565d8 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x80565dc *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x80565e0 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x80565e2 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80565e4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014c04; PC = 0x80565e8 *)
mov L0x20014c04 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014c28; PC = 0x80565ec *)
mov L0x20014c28 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014c4c; PC = 0x80565f0 *)
mov L0x20014c4c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015984; PC = 0x80565f4 *)
mov L0x20015984 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x200159a8; PC = 0x80565f8 *)
mov L0x200159a8 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200159cc; PC = 0x80565fc *)
mov L0x200159cc r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056600 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056604 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805623c <_Good_loop1_inner>              #! PC = 0x8056608 *)
#b.w	0x805623c <_Good_loop1_inner>              #! 0x8056608 = 0x8056608;



(******************** offset 0, 1,  8 ********************)


(**************** CUT 104, - *****************)

ecut and [
eqmod cf0108 f0108 2048, eqmod cf0408 f0408 2048, eqmod cf0708 f0708 2048,
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x20014c04*x**0*y**1*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x20014c28*x**0*y**1*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x20014c4c*x**0*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x20015984*x**0*y**1*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x200159a8*x**0*y**1*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x200159cc*x**0*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x20018726; Value = 0x0ffd0000; PC = 0x805623c *)
mov r4 L0x20018726;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018ba6; Value = 0x0ffd0000; PC = 0x8056240 *)
mov r6 L0x20018ba6;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x20018966; Value = 0x00000ffd; PC = 0x8056244 *)
mov r9 L0x20018966;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0109@sint32 : and [cf0109 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0409@sint32 : and [cf0409 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0709@sint32 : and [cf0709 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014c70; PC = 0x8056290 *)
mov L0x20014c70 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014c94; PC = 0x8056294 *)
mov L0x20014c94 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014cb8; PC = 0x8056298 *)
mov L0x20014cb8 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200159f0; PC = 0x805629c *)
mov L0x200159f0 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015a14; PC = 0x80562a0 *)
mov L0x20015a14 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015a38; PC = 0x80562a4 *)
mov L0x20015a38 r9;



(******************** offset 0, 1,  9 ********************)


(**************** CUT 105, - *****************)

ecut and [
eqmod cf0109 f0109 2048, eqmod cf0409 f0409 2048, eqmod cf0709 f0709 2048,
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x20014c70*x**0*y**1*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x20014c94*x**0*y**1*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x20014cb8*x**0*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x200159f0*x**0*y**1*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x20015a14*x**0*y**1*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x20015a38*x**0*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x200185ac; Value = 0x00000000; PC = 0x80562a8 *)
mov r4 L0x200185ac;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x20018a2c; Value = 0x00000ffd; PC = 0x80562ac *)
mov r6 L0x20018a2c;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x200187ec; Value = 0x00000003; PC = 0x80562b0 *)
mov r9 L0x200187ec;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0110@sint32 : and [cf0110 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0410@sint32 : and [cf0410 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0710@sint32 : and [cf0710 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014cdc; PC = 0x80562fc *)
mov L0x20014cdc r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014d00; PC = 0x8056300 *)
mov L0x20014d00 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014d24; PC = 0x8056304 *)
mov L0x20014d24 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015a5c; PC = 0x8056308 *)
mov L0x20015a5c r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015a80; PC = 0x805630c *)
mov L0x20015a80 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015aa4; PC = 0x8056310 *)
mov L0x20015aa4 r9;



(******************** offset 0, 1, 10 ********************)


(**************** CUT 106, - *****************)

ecut and [
eqmod cf0110 f0110 2048, eqmod cf0410 f0410 2048, eqmod cf0710 f0710 2048,
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20014cdc*x**0*y**1*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20014d00*x**0*y**1*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20014d24*x**0*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20015a5c*x**0*y**1*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20015a80*x**0*y**1*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20015aa4*x**0*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018af2; Value = 0x0ffd0003; PC = 0x8056314 *)
mov r5 L0x20018af2;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x200188b2; Value = 0x00030003; PC = 0x8056318 *)
mov r6 L0x200188b2;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x20018672; Value = 0x00000ffd; PC = 0x805631c *)
mov r9 L0x20018672;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0111@sint32 : and [cf0111 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0411@sint32 : and [cf0411 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0711@sint32 : and [cf0711 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014d48; PC = 0x8056370 *)
mov L0x20014d48 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014d6c; PC = 0x8056374 *)
mov L0x20014d6c r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014d90; PC = 0x8056378 *)
mov L0x20014d90 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015ac8; PC = 0x805637c *)
mov L0x20015ac8 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015aec; PC = 0x8056380 *)
mov L0x20015aec r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015b10; PC = 0x8056384 *)
mov L0x20015b10 r9;



(******************** offset 0, 1, 11 ********************)


(**************** CUT 107, - *****************)

ecut and [
eqmod cf0111 f0111 2048, eqmod cf0411 f0411 2048, eqmod cf0711 f0711 2048,
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20014d48*x**0*y**1*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20014d6c*x**0*y**1*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20014d90*x**0*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20015ac8*x**0*y**1*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20015aec*x**0*y**1*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20015b10*x**0*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x20018978; Value = 0x00030000; PC = 0x8056388 *)
mov r5 L0x20018978;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x20018738; Value = 0x00030000; PC = 0x805638c *)
mov r6 L0x20018738;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018bb8; Value = 0x00030000; PC = 0x8056390 *)
mov r8 L0x20018bb8;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0112@sint32 : and [cf0112 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0412@sint32 : and [cf0412 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0712@sint32 : and [cf0712 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20014db4; PC = 0x80563d0 *)
mov L0x20014db4 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014dd8; PC = 0x80563d4 *)
mov L0x20014dd8 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014dfc; PC = 0x80563d8 *)
mov L0x20014dfc r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015b34; PC = 0x80563dc *)
mov L0x20015b34 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015b58; PC = 0x80563e0 *)
mov L0x20015b58 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015b7c; PC = 0x80563e4 *)
mov L0x20015b7c r9;



(******************** offset 0, 1, 12 ********************)


(**************** CUT 108, - *****************)

ecut and [
eqmod cf0112 f0112 2048, eqmod cf0412 f0412 2048, eqmod cf0712 f0712 2048,
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20014db4*x**0*y**1*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20014dd8*x**0*y**1*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20014dfc*x**0*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20015b34*x**0*y**1*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20015b58*x**0*y**1*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20015b7c*x**0*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x200187fe; Value = 0x0ffd0000; PC = 0x80563e8 *)
mov r5 L0x200187fe;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x200185be; Value = 0x0ffd0000; PC = 0x80563ec *)
mov r6 L0x200185be;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018a3e; Value = 0x0ffd0003; PC = 0x80563f0 *)
mov r8 L0x20018a3e;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0113@sint32 : and [cf0113 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0413@sint32 : and [cf0413 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0713@sint32 : and [cf0713 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014e20; PC = 0x8056430 *)
mov L0x20014e20 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014e44; PC = 0x8056434 *)
mov L0x20014e44 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014e68; PC = 0x8056438 *)
mov L0x20014e68 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015ba0; PC = 0x805643c *)
mov L0x20015ba0 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015bc4; PC = 0x8056440 *)
mov L0x20015bc4 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015be8; PC = 0x8056444 *)
mov L0x20015be8 r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;



(******************** offset 0, 1, 13 ********************)


(**************** CUT 109, - *****************)

ecut and [
eqmod cf0113 f0113 2048, eqmod cf0413 f0413 2048, eqmod cf0713 f0713 2048,
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20014e20*x**0*y**1*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20014e44*x**0*y**1*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20014e68*x**0*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20015ba0*x**0*y**1*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20015bc4*x**0*y**1*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20015be8*x**0*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #222]	; 0xde                   #! EA = L0x20018684; Value = 0x00000000; PC = 0x8056454 *)
mov r5 L0x20018684;
(* ldrsh.w	r7, [lr, #1374]	; 0x55e                 #! EA = L0x20018b04; Value = 0x00000ffd; PC = 0x8056458 *)
mov r7 L0x20018b04;
(* ldrsh.w	r8, [lr, #798]	; 0x31e                  #! EA = L0x200188c4; Value = 0x00030003; PC = 0x805645c *)
mov r8 L0x200188c4;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056460 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0114@sint32 : and [cf0114 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056464 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0414@sint32 : and [cf0414 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056468 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0714@sint32 : and [cf0714 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x805646c *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056470 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8056474 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056478 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x805647c *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056480 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x8056482 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x8056484 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056488 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x805648c *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056490 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x8056494 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056498 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x805649c *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80564a0 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x80564a4 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x80564a8 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x80564ac *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014e8c; PC = 0x80564b0 *)
mov L0x20014e8c r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014eb0; PC = 0x80564b4 *)
mov L0x20014eb0 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014ed4; PC = 0x80564b8 *)
mov L0x20014ed4 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015c0c; PC = 0x80564bc *)
mov L0x20015c0c r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015c30; PC = 0x80564c0 *)
mov L0x20015c30 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015c54; PC = 0x80564c4 *)
mov L0x20015c54 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80564c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 0, 1, 14 ********************)


(**************** CUT 110, - *****************)

ecut and [
eqmod cf0114 f0114 2048, eqmod cf0414 f0414 2048, eqmod cf0714 f0714 2048,
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20014e8c*x**0*y**1*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20014eb0*x**0*y**1*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20014ed4*x**0*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20015c0c*x**0*y**1*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20015c30*x**0*y**1*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20015c54*x**0*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1572]	; 0x624                 #! EA = L0x20018bca; Value = 0x00030003; PC = 0x80564cc *)
mov r4 L0x20018bca;
(* ldrsh.w	r7, [lr, #996]	; 0x3e4                  #! EA = L0x2001898a; Value = 0x00000003; PC = 0x80564d0 *)
mov r7 L0x2001898a;
(* ldrsh.w	r8, [lr, #420]	; 0x1a4                  #! EA = L0x2001874a; Value = 0x00030003; PC = 0x80564d4 *)
mov r8 L0x2001874a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80564d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0115@sint32 : and [cf0115 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80564dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0415@sint32 : and [cf0415 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80564e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0715@sint32 : and [cf0715 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80564e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80564e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80564ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80564f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80564f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80564f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80564fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80564fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056500 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056504 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056508 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805650c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805650e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056510 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056514 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056518 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805651c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014ef8; PC = 0x8056520 *)
mov L0x20014ef8 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014f1c; PC = 0x8056524 *)
mov L0x20014f1c r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014f40; PC = 0x8056528 *)
mov L0x20014f40 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015c78; PC = 0x805652c *)
mov L0x20015c78 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015c9c; PC = 0x8056530 *)
mov L0x20015c9c r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015cc0; PC = 0x8056534 *)
mov L0x20015cc0 r9;



(******************** offset 0, 1, 15 ********************)


(**************** CUT 111, - *****************)

ecut and [
eqmod cf0115 f0115 2048, eqmod cf0415 f0415 2048, eqmod cf0715 f0715 2048,
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20014ef8*x**0*y**1*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20014f1c*x**0*y**1*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20014f40*x**0*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20015c78*x**0*y**1*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20015c9c*x**0*y**1*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20015cc0*x**0*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1194]	; 0x4aa                 #! EA = L0x20018a50; Value = 0x0ffd0003; PC = 0x8056538 *)
mov r4 L0x20018a50;
(* ldrsh.w	r7, [lr, #618]	; 0x26a                  #! EA = L0x20018810; Value = 0x00000000; PC = 0x805653c *)
mov r7 L0x20018810;
(* ldrsh.w	r8, [lr, #42]	; 0x2a                    #! EA = L0x200185d0; Value = 0x00030000; PC = 0x8056540 *)
mov r8 L0x200185d0;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056544 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0116@sint32 : and [cf0116 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056548 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0416@sint32 : and [cf0416 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805654c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0716@sint32 : and [cf0716 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056550 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056554 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056558 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805655c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056560 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056564 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056566 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056568 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805656c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056570 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056574 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056578 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805657a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805657c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056580 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056584 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056588 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014f64; PC = 0x805658c *)
mov L0x20014f64 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014f88; PC = 0x8056590 *)
mov L0x20014f88 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014fac; PC = 0x8056594 *)
mov L0x20014fac r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015ce4; PC = 0x8056598 *)
mov L0x20015ce4 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015d08; PC = 0x805659c *)
mov L0x20015d08 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015d2c; PC = 0x80565a0 *)
mov L0x20015d2c r9;



(******************** offset 0, 1, 16 ********************)


(**************** CUT 112, - *****************)

ecut and [
eqmod cf0116 f0116 2048, eqmod cf0416 f0416 2048, eqmod cf0716 f0716 2048,
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20014f64*x**0*y**1*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20014f88*x**0*y**1*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20014fac*x**0*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20015ce4*x**0*y**1*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20015d08*x**0*y**1*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20015d2c*x**0*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #816]	; 0x330                  #! EA = L0x200188d6; Value = 0x00030003; PC = 0x80565a4 *)
mov r4 L0x200188d6;
(* ldrsh.w	r7, [lr, #240]	; 0xf0                   #! EA = L0x20018696; Value = 0x00000003; PC = 0x80565a8 *)
mov r7 L0x20018696;
(* ldrsh.w	r9, [lr, #1392]	; 0x570                 #! EA = L0x20018b16; Value = 0x00030003; PC = 0x80565ac *)
mov r9 L0x20018b16;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80565b0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0117@sint32 : and [cf0117 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80565b4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0417@sint32 : and [cf0417 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80565b8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0717@sint32 : and [cf0717 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x80565bc *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x80565c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x80565c4 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x80565c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x80565cc *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x80565d0 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x80565d4 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80565d8 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x80565dc *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x80565e0 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x80565e2 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80565e4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014fd0; PC = 0x80565e8 *)
mov L0x20014fd0 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014ff4; PC = 0x80565ec *)
mov L0x20014ff4 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015018; PC = 0x80565f0 *)
mov L0x20015018 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015d50; PC = 0x80565f4 *)
mov L0x20015d50 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015d74; PC = 0x80565f8 *)
mov L0x20015d74 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015d98; PC = 0x80565fc *)
mov L0x20015d98 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056600 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056604 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805623c <_Good_loop1_inner>              #! PC = 0x8056608 *)
#b.w	0x805623c <_Good_loop1_inner>              #! 0x8056608 = 0x8056608;



(******************** offset 0, 1, 17 ********************)


(**************** CUT 113, - *****************)

ecut and [
eqmod cf0117 f0117 2048, eqmod cf0417 f0417 2048, eqmod cf0717 f0717 2048,
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20014fd0*x**0*y**1*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20014ff4*x**0*y**1*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20015018*x**0*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20015d50*x**0*y**1*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20015d74*x**0*y**1*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20015d98*x**0*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x2001875c; Value = 0x00000000; PC = 0x805623c *)
mov r4 L0x2001875c;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018bdc; Value = 0x00000000; PC = 0x8056240 *)
mov r6 L0x20018bdc;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x2001899c; Value = 0x00030003; PC = 0x8056244 *)
mov r9 L0x2001899c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0118@sint32 : and [cf0118 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0418@sint32 : and [cf0418 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0718@sint32 : and [cf0718 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x2001503c; PC = 0x8056290 *)
mov L0x2001503c r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015060; PC = 0x8056294 *)
mov L0x20015060 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015084; PC = 0x8056298 *)
mov L0x20015084 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015dbc; PC = 0x805629c *)
mov L0x20015dbc r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015de0; PC = 0x80562a0 *)
mov L0x20015de0 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015e04; PC = 0x80562a4 *)
mov L0x20015e04 r9;



(******************** offset 0, 1, 18 ********************)


(**************** CUT 114, - *****************)

ecut and [
eqmod cf0118 f0118 2048, eqmod cf0418 f0418 2048, eqmod cf0718 f0718 2048,
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x2001503c*x**0*y**1*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x20015060*x**0*y**1*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x20015084*x**0*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x20015dbc*x**0*y**1*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x20015de0*x**0*y**1*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x20015e04*x**0*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x200185e2; Value = 0x00030000; PC = 0x80562a8 *)
mov r4 L0x200185e2;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x20018a62; Value = 0x00000000; PC = 0x80562ac *)
mov r6 L0x20018a62;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x20018822; Value = 0x0ffd0000; PC = 0x80562b0 *)
mov r9 L0x20018822;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0119@sint32 : and [cf0119 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0419@sint32 : and [cf0419 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0719@sint32 : and [cf0719 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x200150a8; PC = 0x80562fc *)
mov L0x200150a8 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200150cc; PC = 0x8056300 *)
mov L0x200150cc r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200150f0; PC = 0x8056304 *)
mov L0x200150f0 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015e28; PC = 0x8056308 *)
mov L0x20015e28 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015e4c; PC = 0x805630c *)
mov L0x20015e4c r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015e70; PC = 0x8056310 *)
mov L0x20015e70 r9;



(******************** offset 0, 1, 19 ********************)


(**************** CUT 115, - *****************)

ecut and [
eqmod cf0119 f0119 2048, eqmod cf0419 f0419 2048, eqmod cf0719 f0719 2048,
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x200150a8*x**0*y**1*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x200150cc*x**0*y**1*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x200150f0*x**0*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x20015e28*x**0*y**1*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x20015e4c*x**0*y**1*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x20015e70*x**0*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018b28; Value = 0x00000000; PC = 0x8056314 *)
mov r5 L0x20018b28;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x200188e8; Value = 0x00000ffd; PC = 0x8056318 *)
mov r6 L0x200188e8;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x200186a8; Value = 0x0ffd0000; PC = 0x805631c *)
mov r9 L0x200186a8;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0120@sint32 : and [cf0120 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0420@sint32 : and [cf0420 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0720@sint32 : and [cf0720 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20015114; PC = 0x8056370 *)
mov L0x20015114 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015138; PC = 0x8056374 *)
mov L0x20015138 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x2001515c; PC = 0x8056378 *)
mov L0x2001515c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015e94; PC = 0x805637c *)
mov L0x20015e94 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015eb8; PC = 0x8056380 *)
mov L0x20015eb8 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015edc; PC = 0x8056384 *)
mov L0x20015edc r9;



(******************** offset 0, 1, 20 ********************)


(**************** CUT 116, - *****************)

ecut and [
eqmod cf0120 f0120 2048, eqmod cf0420 f0420 2048, eqmod cf0720 f0720 2048,
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x20015114*x**0*y**1*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x20015138*x**0*y**1*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x2001515c*x**0*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x20015e94*x**0*y**1*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x20015eb8*x**0*y**1*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x20015edc*x**0*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x200189ae; Value = 0x0ffd0003; PC = 0x8056388 *)
mov r5 L0x200189ae;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x2001876e; Value = 0x00030ffd; PC = 0x805638c *)
mov r6 L0x2001876e;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018bee; Value = 0x00000000; PC = 0x8056390 *)
mov r8 L0x20018bee;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0121@sint32 : and [cf0121 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0421@sint32 : and [cf0421 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0721@sint32 : and [cf0721 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015180; PC = 0x80563d0 *)
mov L0x20015180 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x200151a4; PC = 0x80563d4 *)
mov L0x200151a4 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200151c8; PC = 0x80563d8 *)
mov L0x200151c8 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015f00; PC = 0x80563dc *)
mov L0x20015f00 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015f24; PC = 0x80563e0 *)
mov L0x20015f24 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015f48; PC = 0x80563e4 *)
mov L0x20015f48 r9;



(******************** offset 0, 1, 21 ********************)


(**************** CUT 117, - *****************)

ecut and [
eqmod cf0121 f0121 2048, eqmod cf0421 f0421 2048, eqmod cf0721 f0721 2048,
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x20015180*x**0*y**1*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x200151a4*x**0*y**1*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x200151c8*x**0*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x20015f00*x**0*y**1*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x20015f24*x**0*y**1*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x20015f48*x**0*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x20018834; Value = 0x00000000; PC = 0x80563e8 *)
mov r5 L0x20018834;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x200185f4; Value = 0x0ffd0000; PC = 0x80563ec *)
mov r6 L0x200185f4;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018a74; Value = 0x0ffd0000; PC = 0x80563f0 *)
mov r8 L0x20018a74;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0122@sint32 : and [cf0122 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0422@sint32 : and [cf0422 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0722@sint32 : and [cf0722 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200151ec; PC = 0x8056430 *)
mov L0x200151ec r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20015210; PC = 0x8056434 *)
mov L0x20015210 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015234; PC = 0x8056438 *)
mov L0x20015234 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015f6c; PC = 0x805643c *)
mov L0x20015f6c r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015f90; PC = 0x8056440 *)
mov L0x20015f90 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015fb4; PC = 0x8056444 *)
mov L0x20015fb4 r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;



(******************** offset 0, 1, 22 ********************)


(**************** CUT 118, - *****************)

ecut and [
eqmod cf0122 f0122 2048, eqmod cf0422 f0422 2048, eqmod cf0722 f0722 2048,
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x200151ec*x**0*y**1*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x20015210*x**0*y**1*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x20015234*x**0*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x20015f6c*x**0*y**1*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x20015f90*x**0*y**1*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x20015fb4*x**0*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #222]	; 0xde                   #! EA = L0x200186ba; Value = 0x00000003; PC = 0x8056454 *)
mov r5 L0x200186ba;
(* ldrsh.w	r7, [lr, #1374]	; 0x55e                 #! EA = L0x20018b3a; Value = 0x0ffd0003; PC = 0x8056458 *)
mov r7 L0x20018b3a;
(* ldrsh.w	r8, [lr, #798]	; 0x31e                  #! EA = L0x200188fa; Value = 0x00000003; PC = 0x805645c *)
mov r8 L0x200188fa;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056460 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0123@sint32 : and [cf0123 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056464 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0423@sint32 : and [cf0423 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056468 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0723@sint32 : and [cf0723 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x805646c *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056470 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8056474 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056478 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x805647c *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056480 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x8056482 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x8056484 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056488 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x805648c *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056490 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x8056494 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056498 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x805649c *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80564a0 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x80564a4 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x80564a8 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x80564ac *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20015258; PC = 0x80564b0 *)
mov L0x20015258 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x2001527c; PC = 0x80564b4 *)
mov L0x2001527c r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x200152a0; PC = 0x80564b8 *)
mov L0x200152a0 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015fd8; PC = 0x80564bc *)
mov L0x20015fd8 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015ffc; PC = 0x80564c0 *)
mov L0x20015ffc r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20016020; PC = 0x80564c4 *)
mov L0x20016020 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80564c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 0, 1, 23 ********************)


(**************** CUT 119, - *****************)

ecut and [
eqmod cf0123 f0123 2048, eqmod cf0423 f0423 2048, eqmod cf0723 f0723 2048,
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x20015258*x**0*y**1*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x2001527c*x**0*y**1*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x200152a0*x**0*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x20015fd8*x**0*y**1*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x20015ffc*x**0*y**1*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x20016020*x**0*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1572]	; 0x624                 #! EA = L0x20018c00; Value = 0x00000000; PC = 0x80564cc *)
mov r4 L0x20018c00;
(* ldrsh.w	r7, [lr, #996]	; 0x3e4                  #! EA = L0x200189c0; Value = 0x00000000; PC = 0x80564d0 *)
mov r7 L0x200189c0;
(* ldrsh.w	r8, [lr, #420]	; 0x1a4                  #! EA = L0x20018780; Value = 0x00030ffd; PC = 0x80564d4 *)
mov r8 L0x20018780;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80564d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0124@sint32 : and [cf0124 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80564dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0424@sint32 : and [cf0424 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80564e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0724@sint32 : and [cf0724 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80564e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80564e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80564ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80564f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80564f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80564f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80564fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80564fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056500 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056504 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056508 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805650c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805650e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056510 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056514 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056518 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805651c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200152c4; PC = 0x8056520 *)
mov L0x200152c4 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200152e8; PC = 0x8056524 *)
mov L0x200152e8 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x2001530c; PC = 0x8056528 *)
mov L0x2001530c r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016044; PC = 0x805652c *)
mov L0x20016044 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20016068; PC = 0x8056530 *)
mov L0x20016068 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x2001608c; PC = 0x8056534 *)
mov L0x2001608c r9;



(******************** offset 0, 1, 24 ********************)


(**************** CUT 120, - *****************)

ecut and [
eqmod cf0124 f0124 2048, eqmod cf0424 f0424 2048, eqmod cf0724 f0724 2048,
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x200152c4*x**0*y**1*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x200152e8*x**0*y**1*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x2001530c*x**0*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x20016044*x**0*y**1*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x20016068*x**0*y**1*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x2001608c*x**0*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1194]	; 0x4aa                 #! EA = L0x20018a86; Value = 0x00030003; PC = 0x8056538 *)
mov r4 L0x20018a86;
(* ldrsh.w	r7, [lr, #618]	; 0x26a                  #! EA = L0x20018846; Value = 0x0ffd0ffd; PC = 0x805653c *)
mov r7 L0x20018846;
(* ldrsh.w	r8, [lr, #42]	; 0x2a                    #! EA = L0x20018606; Value = 0x0ffd0000; PC = 0x8056540 *)
mov r8 L0x20018606;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056544 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0125@sint32 : and [cf0125 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056548 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0425@sint32 : and [cf0425 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805654c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0725@sint32 : and [cf0725 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056550 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056554 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056558 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805655c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056560 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056564 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056566 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056568 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805656c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056570 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056574 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056578 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805657a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805657c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056580 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056584 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056588 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015330; PC = 0x805658c *)
mov L0x20015330 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20015354; PC = 0x8056590 *)
mov L0x20015354 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20015378; PC = 0x8056594 *)
mov L0x20015378 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200160b0; PC = 0x8056598 *)
mov L0x200160b0 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200160d4; PC = 0x805659c *)
mov L0x200160d4 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200160f8; PC = 0x80565a0 *)
mov L0x200160f8 r9;



(******************** offset 0, 1, 25 ********************)


(**************** CUT 121, - *****************)

ecut and [
eqmod cf0125 f0125 2048, eqmod cf0425 f0425 2048, eqmod cf0725 f0725 2048,
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x20015330*x**0*y**1*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x20015354*x**0*y**1*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x20015378*x**0*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x200160b0*x**0*y**1*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x200160d4*x**0*y**1*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x200160f8*x**0*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #816]	; 0x330                  #! EA = L0x2001890c; Value = 0x00030ffd; PC = 0x80565a4 *)
mov r4 L0x2001890c;
(* ldrsh.w	r7, [lr, #240]	; 0xf0                   #! EA = L0x200186cc; Value = 0x0ffd0ffd; PC = 0x80565a8 *)
mov r7 L0x200186cc;
(* ldrsh.w	r9, [lr, #1392]	; 0x570                 #! EA = L0x20018b4c; Value = 0x00030000; PC = 0x80565ac *)
mov r9 L0x20018b4c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80565b0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0126@sint32 : and [cf0126 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80565b4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0426@sint32 : and [cf0426 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80565b8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0726@sint32 : and [cf0726 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x80565bc *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x80565c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x80565c4 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x80565c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x80565cc *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x80565d0 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x80565d4 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80565d8 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x80565dc *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x80565e0 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x80565e2 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80565e4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x2001539c; PC = 0x80565e8 *)
mov L0x2001539c r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200153c0; PC = 0x80565ec *)
mov L0x200153c0 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200153e4; PC = 0x80565f0 *)
mov L0x200153e4 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x2001611c; PC = 0x80565f4 *)
mov L0x2001611c r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016140; PC = 0x80565f8 *)
mov L0x20016140 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20016164; PC = 0x80565fc *)
mov L0x20016164 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056600 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056604 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805623c <_Good_loop1_inner>              #! PC = 0x8056608 *)
#b.w	0x805623c <_Good_loop1_inner>              #! 0x8056608 = 0x8056608;



(******************** offset 0, 1, 26 ********************)


(**************** CUT 122, - *****************)

ecut and [
eqmod cf0126 f0126 2048, eqmod cf0426 f0426 2048, eqmod cf0726 f0726 2048,
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x2001539c*x**0*y**1*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x200153c0*x**0*y**1*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x200153e4*x**0*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x2001611c*x**0*y**1*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x20016140*x**0*y**1*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x20016164*x**0*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x20018792; Value = 0x00000003; PC = 0x805623c *)
mov r4 L0x20018792;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018c12; Value = 0x00000000; PC = 0x8056240 *)
mov r6 L0x20018c12;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x200189d2; Value = 0x00030ffd; PC = 0x8056244 *)
mov r9 L0x200189d2;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0127@sint32 : and [cf0127 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0427@sint32 : and [cf0427 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0727@sint32 : and [cf0727 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015408; PC = 0x8056290 *)
mov L0x20015408 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x2001542c; PC = 0x8056294 *)
mov L0x2001542c r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015450; PC = 0x8056298 *)
mov L0x20015450 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016188; PC = 0x805629c *)
mov L0x20016188 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200161ac; PC = 0x80562a0 *)
mov L0x200161ac r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200161d0; PC = 0x80562a4 *)
mov L0x200161d0 r9;



(******************** offset 0, 1, 27 ********************)


(**************** CUT 123, - *****************)

ecut and [
eqmod cf0127 f0127 2048, eqmod cf0427 f0427 2048, eqmod cf0727 f0727 2048,
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x20015408*x**0*y**1*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x2001542c*x**0*y**1*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x20015450*x**0*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x20016188*x**0*y**1*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x200161ac*x**0*y**1*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x200161d0*x**0*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x20018618; Value = 0x00000ffd; PC = 0x80562a8 *)
mov r4 L0x20018618;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x20018a98; Value = 0x00000000; PC = 0x80562ac *)
mov r6 L0x20018a98;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x20018858; Value = 0x00000ffd; PC = 0x80562b0 *)
mov r9 L0x20018858;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0128@sint32 : and [cf0128 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0428@sint32 : and [cf0428 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0728@sint32 : and [cf0728 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015474; PC = 0x80562fc *)
mov L0x20015474 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20015498; PC = 0x8056300 *)
mov L0x20015498 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200154bc; PC = 0x8056304 *)
mov L0x200154bc r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200161f4; PC = 0x8056308 *)
mov L0x200161f4 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20016218; PC = 0x805630c *)
mov L0x20016218 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x2001623c; PC = 0x8056310 *)
mov L0x2001623c r9;



(******************** offset 0, 1, 28 ********************)


(**************** CUT 124, - *****************)

ecut and [
eqmod cf0128 f0128 2048, eqmod cf0428 f0428 2048, eqmod cf0728 f0728 2048,
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x20015474*x**0*y**1*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x20015498*x**0*y**1*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x200154bc*x**0*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x200161f4*x**0*y**1*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x20016218*x**0*y**1*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x2001623c*x**0*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018b5e; Value = 0x00030000; PC = 0x8056314 *)
mov r5 L0x20018b5e;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x2001891e; Value = 0x00000000; PC = 0x8056318 *)
mov r6 L0x2001891e;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x200186de; Value = 0x00030000; PC = 0x805631c *)
mov r9 L0x200186de;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0129@sint32 : and [cf0129 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0429@sint32 : and [cf0429 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0729@sint32 : and [cf0729 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200154e0; PC = 0x8056370 *)
mov L0x200154e0 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015504; PC = 0x8056374 *)
mov L0x20015504 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015528; PC = 0x8056378 *)
mov L0x20015528 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016260; PC = 0x805637c *)
mov L0x20016260 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016284; PC = 0x8056380 *)
mov L0x20016284 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200162a8; PC = 0x8056384 *)
mov L0x200162a8 r9;



(******************** offset 0, 1, 29 ********************)


(**************** CUT 125, - *****************)

ecut and [
eqmod cf0129 f0129 2048, eqmod cf0429 f0429 2048, eqmod cf0729 f0729 2048,
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x200154e0*x**0*y**1*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x20015504*x**0*y**1*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x20015528*x**0*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x20016260*x**0*y**1*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x20016284*x**0*y**1*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x200162a8*x**0*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x200189e4; Value = 0x00000003; PC = 0x8056388 *)
mov r5 L0x200189e4;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x200187a4; Value = 0x00030003; PC = 0x805638c *)
mov r6 L0x200187a4;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018c24; Value = 0x00000000; PC = 0x8056390 *)
mov r8 L0x20018c24;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0130@sint32 : and [cf0130 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0430@sint32 : and [cf0430 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0730@sint32 : and [cf0730 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x2001554c; PC = 0x80563d0 *)
mov L0x2001554c r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20015570; PC = 0x80563d4 *)
mov L0x20015570 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20015594; PC = 0x80563d8 *)
mov L0x20015594 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x200162cc; PC = 0x80563dc *)
mov L0x200162cc r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x200162f0; PC = 0x80563e0 *)
mov L0x200162f0 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20016314; PC = 0x80563e4 *)
mov L0x20016314 r9;



(******************** offset 0, 1, 30 ********************)


(**************** CUT 126, - *****************)

ecut and [
eqmod cf0130 f0130 2048, eqmod cf0430 f0430 2048, eqmod cf0730 f0730 2048,
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x2001554c*x**0*y**1*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x20015570*x**0*y**1*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x20015594*x**0*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x200162cc*x**0*y**1*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x200162f0*x**0*y**1*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x20016314*x**0*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x2001886a; Value = 0x00000ffd; PC = 0x80563e8 *)
mov r5 L0x2001886a;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x2001862a; Value = 0x00000000; PC = 0x80563ec *)
mov r6 L0x2001862a;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018aaa; Value = 0x00030000; PC = 0x80563f0 *)
mov r8 L0x20018aaa;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0131@sint32 : and [cf0131 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0431@sint32 : and [cf0431 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0731@sint32 : and [cf0731 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200155b8; PC = 0x8056430 *)
mov L0x200155b8 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x200155dc; PC = 0x8056434 *)
mov L0x200155dc r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015600; PC = 0x8056438 *)
mov L0x20015600 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20016338; PC = 0x805643c *)
mov L0x20016338 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x2001635c; PC = 0x8056440 *)
mov L0x2001635c r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20016380; PC = 0x8056444 *)
mov L0x20016380 r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;
(* sub.w	lr, lr, #162	; 0xa2                       #! PC = 0x805660c *)
subs dc lr lr 162@uint32;
(* subw	r0, r0, #2916	; 0xb64                      #! PC = 0x8056610 *)
subc dc r0 r0 2916@uint32;
(* add.w	lr, lr, #2                                #! PC = 0x8056614 *)
adds dc lr lr 2@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8056618 *)
adds dc r0 r0 4@uint32;
(* cmp.w	r0, r12                                   #! PC = 0x805661c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8056234 <_Good_loop1>                  #! PC = 0x8056620 *)
#bne.w	0x8056234 <_Good_loop1>                  #! 0x8056620 = 0x8056620;
(* addw	r1, r0, #2916	; 0xb64                      #! PC = 0x8056234 *)
adds dc r1 r0 2916@uint32;
(* vmov	s3, r1                                     #! PC = 0x8056238 *)
mov s3 r1;



(******************** offset 0, 1, 31 ********************)


(**************** CUT 127, - *****************)

ecut and [
eqmod cf0131 f0131 2048, eqmod cf0431 f0431 2048, eqmod cf0731 f0731 2048,
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x200155b8*x**0*y**1*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x200155dc*x**0*y**1*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x20015600*x**0*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x20016338*x**0*y**1*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x2001635c*x**0*y**1*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x20016380*x**0*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(**************** CUT -,   3 *****************)

rcut and [
(-3367617)@32<=sL0x200148a4,L0x200148a4<=s3367617@32,
(-3367617)@32<=sL0x200148c8,L0x200148c8<=s3367617@32,
(-3367617)@32<=sL0x200148ec,L0x200148ec<=s3367617@32,
(-3367617)@32<=sL0x20015624,L0x20015624<=s3367617@32,
(-3367617)@32<=sL0x20015648,L0x20015648<=s3367617@32,
(-3367617)@32<=sL0x2001566c,L0x2001566c<=s3367617@32
,
(-3367617)@32<=sL0x20014910,L0x20014910<=s3367617@32,
(-3367617)@32<=sL0x20014934,L0x20014934<=s3367617@32,
(-3367617)@32<=sL0x20014958,L0x20014958<=s3367617@32,
(-3367617)@32<=sL0x20015690,L0x20015690<=s3367617@32,
(-3367617)@32<=sL0x200156b4,L0x200156b4<=s3367617@32,
(-3367617)@32<=sL0x200156d8,L0x200156d8<=s3367617@32
,
(-3367617)@32<=sL0x2001497c,L0x2001497c<=s3367617@32,
(-3367617)@32<=sL0x200149a0,L0x200149a0<=s3367617@32,
(-3367617)@32<=sL0x200149c4,L0x200149c4<=s3367617@32,
(-3367617)@32<=sL0x200156fc,L0x200156fc<=s3367617@32,
(-3367617)@32<=sL0x20015720,L0x20015720<=s3367617@32,
(-3367617)@32<=sL0x20015744,L0x20015744<=s3367617@32
,
(-3367617)@32<=sL0x200149e8,L0x200149e8<=s3367617@32,
(-3367617)@32<=sL0x20014a0c,L0x20014a0c<=s3367617@32,
(-3367617)@32<=sL0x20014a30,L0x20014a30<=s3367617@32,
(-3367617)@32<=sL0x20015768,L0x20015768<=s3367617@32,
(-3367617)@32<=sL0x2001578c,L0x2001578c<=s3367617@32,
(-3367617)@32<=sL0x200157b0,L0x200157b0<=s3367617@32
,
(-3367617)@32<=sL0x20014a54,L0x20014a54<=s3367617@32,
(-3367617)@32<=sL0x20014a78,L0x20014a78<=s3367617@32,
(-3367617)@32<=sL0x20014a9c,L0x20014a9c<=s3367617@32,
(-3367617)@32<=sL0x200157d4,L0x200157d4<=s3367617@32,
(-3367617)@32<=sL0x200157f8,L0x200157f8<=s3367617@32,
(-3367617)@32<=sL0x2001581c,L0x2001581c<=s3367617@32
,
(-3367617)@32<=sL0x20014ac0,L0x20014ac0<=s3367617@32,
(-3367617)@32<=sL0x20014ae4,L0x20014ae4<=s3367617@32,
(-3367617)@32<=sL0x20014b08,L0x20014b08<=s3367617@32,
(-3367617)@32<=sL0x20015840,L0x20015840<=s3367617@32,
(-3367617)@32<=sL0x20015864,L0x20015864<=s3367617@32,
(-3367617)@32<=sL0x20015888,L0x20015888<=s3367617@32
,
(-3367617)@32<=sL0x20014b2c,L0x20014b2c<=s3367617@32,
(-3367617)@32<=sL0x20014b50,L0x20014b50<=s3367617@32,
(-3367617)@32<=sL0x20014b74,L0x20014b74<=s3367617@32,
(-3367617)@32<=sL0x200158ac,L0x200158ac<=s3367617@32,
(-3367617)@32<=sL0x200158d0,L0x200158d0<=s3367617@32,
(-3367617)@32<=sL0x200158f4,L0x200158f4<=s3367617@32
,
(-3367617)@32<=sL0x20014b98,L0x20014b98<=s3367617@32,
(-3367617)@32<=sL0x20014bbc,L0x20014bbc<=s3367617@32,
(-3367617)@32<=sL0x20014be0,L0x20014be0<=s3367617@32,
(-3367617)@32<=sL0x20015918,L0x20015918<=s3367617@32,
(-3367617)@32<=sL0x2001593c,L0x2001593c<=s3367617@32,
(-3367617)@32<=sL0x20015960,L0x20015960<=s3367617@32
,
(-3367617)@32<=sL0x20014c04,L0x20014c04<=s3367617@32,
(-3367617)@32<=sL0x20014c28,L0x20014c28<=s3367617@32,
(-3367617)@32<=sL0x20014c4c,L0x20014c4c<=s3367617@32,
(-3367617)@32<=sL0x20015984,L0x20015984<=s3367617@32,
(-3367617)@32<=sL0x200159a8,L0x200159a8<=s3367617@32,
(-3367617)@32<=sL0x200159cc,L0x200159cc<=s3367617@32
,
(-3367617)@32<=sL0x20014c70,L0x20014c70<=s3367617@32,
(-3367617)@32<=sL0x20014c94,L0x20014c94<=s3367617@32,
(-3367617)@32<=sL0x20014cb8,L0x20014cb8<=s3367617@32,
(-3367617)@32<=sL0x200159f0,L0x200159f0<=s3367617@32,
(-3367617)@32<=sL0x20015a14,L0x20015a14<=s3367617@32,
(-3367617)@32<=sL0x20015a38,L0x20015a38<=s3367617@32
,
(-3367617)@32<=sL0x20014cdc,L0x20014cdc<=s3367617@32,
(-3367617)@32<=sL0x20014d00,L0x20014d00<=s3367617@32,
(-3367617)@32<=sL0x20014d24,L0x20014d24<=s3367617@32,
(-3367617)@32<=sL0x20015a5c,L0x20015a5c<=s3367617@32,
(-3367617)@32<=sL0x20015a80,L0x20015a80<=s3367617@32,
(-3367617)@32<=sL0x20015aa4,L0x20015aa4<=s3367617@32
,
(-3367617)@32<=sL0x20014d48,L0x20014d48<=s3367617@32,
(-3367617)@32<=sL0x20014d6c,L0x20014d6c<=s3367617@32,
(-3367617)@32<=sL0x20014d90,L0x20014d90<=s3367617@32,
(-3367617)@32<=sL0x20015ac8,L0x20015ac8<=s3367617@32,
(-3367617)@32<=sL0x20015aec,L0x20015aec<=s3367617@32,
(-3367617)@32<=sL0x20015b10,L0x20015b10<=s3367617@32
,
(-3367617)@32<=sL0x20014db4,L0x20014db4<=s3367617@32,
(-3367617)@32<=sL0x20014dd8,L0x20014dd8<=s3367617@32,
(-3367617)@32<=sL0x20014dfc,L0x20014dfc<=s3367617@32,
(-3367617)@32<=sL0x20015b34,L0x20015b34<=s3367617@32,
(-3367617)@32<=sL0x20015b58,L0x20015b58<=s3367617@32,
(-3367617)@32<=sL0x20015b7c,L0x20015b7c<=s3367617@32
,
(-3367617)@32<=sL0x20014e20,L0x20014e20<=s3367617@32,
(-3367617)@32<=sL0x20014e44,L0x20014e44<=s3367617@32,
(-3367617)@32<=sL0x20014e68,L0x20014e68<=s3367617@32,
(-3367617)@32<=sL0x20015ba0,L0x20015ba0<=s3367617@32,
(-3367617)@32<=sL0x20015bc4,L0x20015bc4<=s3367617@32,
(-3367617)@32<=sL0x20015be8,L0x20015be8<=s3367617@32
,
(-3367617)@32<=sL0x20014e8c,L0x20014e8c<=s3367617@32,
(-3367617)@32<=sL0x20014eb0,L0x20014eb0<=s3367617@32,
(-3367617)@32<=sL0x20014ed4,L0x20014ed4<=s3367617@32,
(-3367617)@32<=sL0x20015c0c,L0x20015c0c<=s3367617@32,
(-3367617)@32<=sL0x20015c30,L0x20015c30<=s3367617@32,
(-3367617)@32<=sL0x20015c54,L0x20015c54<=s3367617@32
,
(-3367617)@32<=sL0x20014ef8,L0x20014ef8<=s3367617@32,
(-3367617)@32<=sL0x20014f1c,L0x20014f1c<=s3367617@32,
(-3367617)@32<=sL0x20014f40,L0x20014f40<=s3367617@32,
(-3367617)@32<=sL0x20015c78,L0x20015c78<=s3367617@32,
(-3367617)@32<=sL0x20015c9c,L0x20015c9c<=s3367617@32,
(-3367617)@32<=sL0x20015cc0,L0x20015cc0<=s3367617@32
,
(-3367617)@32<=sL0x20014f64,L0x20014f64<=s3367617@32,
(-3367617)@32<=sL0x20014f88,L0x20014f88<=s3367617@32,
(-3367617)@32<=sL0x20014fac,L0x20014fac<=s3367617@32,
(-3367617)@32<=sL0x20015ce4,L0x20015ce4<=s3367617@32,
(-3367617)@32<=sL0x20015d08,L0x20015d08<=s3367617@32,
(-3367617)@32<=sL0x20015d2c,L0x20015d2c<=s3367617@32
,
(-3367617)@32<=sL0x20014fd0,L0x20014fd0<=s3367617@32,
(-3367617)@32<=sL0x20014ff4,L0x20014ff4<=s3367617@32,
(-3367617)@32<=sL0x20015018,L0x20015018<=s3367617@32,
(-3367617)@32<=sL0x20015d50,L0x20015d50<=s3367617@32,
(-3367617)@32<=sL0x20015d74,L0x20015d74<=s3367617@32,
(-3367617)@32<=sL0x20015d98,L0x20015d98<=s3367617@32
,
(-3367617)@32<=sL0x2001503c,L0x2001503c<=s3367617@32,
(-3367617)@32<=sL0x20015060,L0x20015060<=s3367617@32,
(-3367617)@32<=sL0x20015084,L0x20015084<=s3367617@32,
(-3367617)@32<=sL0x20015dbc,L0x20015dbc<=s3367617@32,
(-3367617)@32<=sL0x20015de0,L0x20015de0<=s3367617@32,
(-3367617)@32<=sL0x20015e04,L0x20015e04<=s3367617@32
,
(-3367617)@32<=sL0x200150a8,L0x200150a8<=s3367617@32,
(-3367617)@32<=sL0x200150cc,L0x200150cc<=s3367617@32,
(-3367617)@32<=sL0x200150f0,L0x200150f0<=s3367617@32,
(-3367617)@32<=sL0x20015e28,L0x20015e28<=s3367617@32,
(-3367617)@32<=sL0x20015e4c,L0x20015e4c<=s3367617@32,
(-3367617)@32<=sL0x20015e70,L0x20015e70<=s3367617@32
,
(-3367617)@32<=sL0x20015114,L0x20015114<=s3367617@32,
(-3367617)@32<=sL0x20015138,L0x20015138<=s3367617@32,
(-3367617)@32<=sL0x2001515c,L0x2001515c<=s3367617@32,
(-3367617)@32<=sL0x20015e94,L0x20015e94<=s3367617@32,
(-3367617)@32<=sL0x20015eb8,L0x20015eb8<=s3367617@32,
(-3367617)@32<=sL0x20015edc,L0x20015edc<=s3367617@32
,
(-3367617)@32<=sL0x20015180,L0x20015180<=s3367617@32,
(-3367617)@32<=sL0x200151a4,L0x200151a4<=s3367617@32,
(-3367617)@32<=sL0x200151c8,L0x200151c8<=s3367617@32,
(-3367617)@32<=sL0x20015f00,L0x20015f00<=s3367617@32,
(-3367617)@32<=sL0x20015f24,L0x20015f24<=s3367617@32,
(-3367617)@32<=sL0x20015f48,L0x20015f48<=s3367617@32
,
(-3367617)@32<=sL0x200151ec,L0x200151ec<=s3367617@32,
(-3367617)@32<=sL0x20015210,L0x20015210<=s3367617@32,
(-3367617)@32<=sL0x20015234,L0x20015234<=s3367617@32,
(-3367617)@32<=sL0x20015f6c,L0x20015f6c<=s3367617@32,
(-3367617)@32<=sL0x20015f90,L0x20015f90<=s3367617@32,
(-3367617)@32<=sL0x20015fb4,L0x20015fb4<=s3367617@32
,
(-3367617)@32<=sL0x20015258,L0x20015258<=s3367617@32,
(-3367617)@32<=sL0x2001527c,L0x2001527c<=s3367617@32,
(-3367617)@32<=sL0x200152a0,L0x200152a0<=s3367617@32,
(-3367617)@32<=sL0x20015fd8,L0x20015fd8<=s3367617@32,
(-3367617)@32<=sL0x20015ffc,L0x20015ffc<=s3367617@32,
(-3367617)@32<=sL0x20016020,L0x20016020<=s3367617@32
,
(-3367617)@32<=sL0x200152c4,L0x200152c4<=s3367617@32,
(-3367617)@32<=sL0x200152e8,L0x200152e8<=s3367617@32,
(-3367617)@32<=sL0x2001530c,L0x2001530c<=s3367617@32,
(-3367617)@32<=sL0x20016044,L0x20016044<=s3367617@32,
(-3367617)@32<=sL0x20016068,L0x20016068<=s3367617@32,
(-3367617)@32<=sL0x2001608c,L0x2001608c<=s3367617@32
,
(-3367617)@32<=sL0x20015330,L0x20015330<=s3367617@32,
(-3367617)@32<=sL0x20015354,L0x20015354<=s3367617@32,
(-3367617)@32<=sL0x20015378,L0x20015378<=s3367617@32,
(-3367617)@32<=sL0x200160b0,L0x200160b0<=s3367617@32,
(-3367617)@32<=sL0x200160d4,L0x200160d4<=s3367617@32,
(-3367617)@32<=sL0x200160f8,L0x200160f8<=s3367617@32
,
(-3367617)@32<=sL0x2001539c,L0x2001539c<=s3367617@32,
(-3367617)@32<=sL0x200153c0,L0x200153c0<=s3367617@32,
(-3367617)@32<=sL0x200153e4,L0x200153e4<=s3367617@32,
(-3367617)@32<=sL0x2001611c,L0x2001611c<=s3367617@32,
(-3367617)@32<=sL0x20016140,L0x20016140<=s3367617@32,
(-3367617)@32<=sL0x20016164,L0x20016164<=s3367617@32
,
(-3367617)@32<=sL0x20015408,L0x20015408<=s3367617@32,
(-3367617)@32<=sL0x2001542c,L0x2001542c<=s3367617@32,
(-3367617)@32<=sL0x20015450,L0x20015450<=s3367617@32,
(-3367617)@32<=sL0x20016188,L0x20016188<=s3367617@32,
(-3367617)@32<=sL0x200161ac,L0x200161ac<=s3367617@32,
(-3367617)@32<=sL0x200161d0,L0x200161d0<=s3367617@32
,
(-3367617)@32<=sL0x20015474,L0x20015474<=s3367617@32,
(-3367617)@32<=sL0x20015498,L0x20015498<=s3367617@32,
(-3367617)@32<=sL0x200154bc,L0x200154bc<=s3367617@32,
(-3367617)@32<=sL0x200161f4,L0x200161f4<=s3367617@32,
(-3367617)@32<=sL0x20016218,L0x20016218<=s3367617@32,
(-3367617)@32<=sL0x2001623c,L0x2001623c<=s3367617@32
,
(-3367617)@32<=sL0x200154e0,L0x200154e0<=s3367617@32,
(-3367617)@32<=sL0x20015504,L0x20015504<=s3367617@32,
(-3367617)@32<=sL0x20015528,L0x20015528<=s3367617@32,
(-3367617)@32<=sL0x20016260,L0x20016260<=s3367617@32,
(-3367617)@32<=sL0x20016284,L0x20016284<=s3367617@32,
(-3367617)@32<=sL0x200162a8,L0x200162a8<=s3367617@32
,
(-3367617)@32<=sL0x2001554c,L0x2001554c<=s3367617@32,
(-3367617)@32<=sL0x20015570,L0x20015570<=s3367617@32,
(-3367617)@32<=sL0x20015594,L0x20015594<=s3367617@32,
(-3367617)@32<=sL0x200162cc,L0x200162cc<=s3367617@32,
(-3367617)@32<=sL0x200162f0,L0x200162f0<=s3367617@32,
(-3367617)@32<=sL0x20016314,L0x20016314<=s3367617@32
,
(-3367617)@32<=sL0x200155b8,L0x200155b8<=s3367617@32,
(-3367617)@32<=sL0x200155dc,L0x200155dc<=s3367617@32,
(-3367617)@32<=sL0x20015600,L0x20015600<=s3367617@32,
(-3367617)@32<=sL0x20016338,L0x20016338<=s3367617@32,
(-3367617)@32<=sL0x2001635c,L0x2001635c<=s3367617@32,
(-3367617)@32<=sL0x20016380,L0x20016380<=s3367617@32
] prove with [ precondition ];





(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x200186f2; Value = 0x0ffd0000; PC = 0x805623c *)
mov r4 L0x200186f2;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018b72; Value = 0x00030000; PC = 0x8056240 *)
mov r6 L0x20018b72;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x20018932; Value = 0x0ffd0000; PC = 0x8056244 *)
mov r9 L0x20018932;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1100@sint32 : and [cf1100 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1400@sint32 : and [cf1400 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1700@sint32 : and [cf1700 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200148a8; PC = 0x8056290 *)
mov L0x200148a8 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200148cc; PC = 0x8056294 *)
mov L0x200148cc r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x200148f0; PC = 0x8056298 *)
mov L0x200148f0 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015628; PC = 0x805629c *)
mov L0x20015628 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x2001564c; PC = 0x80562a0 *)
mov L0x2001564c r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015670; PC = 0x80562a4 *)
mov L0x20015670 r9;



(******************** offset 1, 1,  0 ********************)


(**************** CUT 128, - *****************)

ecut and [
eqmod cf1100 f1100 2048, eqmod cf1400 f1400 2048, eqmod cf1700 f1700 2048,
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x200148a8*x**1*y**1*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x200148cc*x**1*y**1*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x200148f0*x**1*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x20015628*x**1*y**1*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x2001564c*x**1*y**1*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x20015670*x**1*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x20018578; Value = 0x00000ffd; PC = 0x80562a8 *)
mov r4 L0x20018578;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x200189f8; Value = 0x00030ffd; PC = 0x80562ac *)
mov r6 L0x200189f8;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x200187b8; Value = 0x00030ffd; PC = 0x80562b0 *)
mov r9 L0x200187b8;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1101@sint32 : and [cf1101 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1401@sint32 : and [cf1401 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1701@sint32 : and [cf1701 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014914; PC = 0x80562fc *)
mov L0x20014914 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014938; PC = 0x8056300 *)
mov L0x20014938 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x2001495c; PC = 0x8056304 *)
mov L0x2001495c r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015694; PC = 0x8056308 *)
mov L0x20015694 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200156b8; PC = 0x805630c *)
mov L0x200156b8 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200156dc; PC = 0x8056310 *)
mov L0x200156dc r9;



(******************** offset 1, 1,  1 ********************)


(**************** CUT 129, - *****************)

ecut and [
eqmod cf1101 f1101 2048, eqmod cf1401 f1401 2048, eqmod cf1701 f1701 2048,
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x20014914*x**1*y**1*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x20014938*x**1*y**1*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x2001495c*x**1*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x20015694*x**1*y**1*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x200156b8*x**1*y**1*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x200156dc*x**1*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018abe; Value = 0x00030ffd; PC = 0x8056314 *)
mov r5 L0x20018abe;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x2001887e; Value = 0x00000000; PC = 0x8056318 *)
mov r6 L0x2001887e;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x2001863e; Value = 0x0ffd0ffd; PC = 0x805631c *)
mov r9 L0x2001863e;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1102@sint32 : and [cf1102 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1402@sint32 : and [cf1402 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1702@sint32 : and [cf1702 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014980; PC = 0x8056370 *)
mov L0x20014980 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200149a4; PC = 0x8056374 *)
mov L0x200149a4 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200149c8; PC = 0x8056378 *)
mov L0x200149c8 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015700; PC = 0x805637c *)
mov L0x20015700 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015724; PC = 0x8056380 *)
mov L0x20015724 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015748; PC = 0x8056384 *)
mov L0x20015748 r9;



(******************** offset 1, 1,  2 ********************)


(**************** CUT 130, - *****************)

ecut and [
eqmod cf1102 f1102 2048, eqmod cf1402 f1402 2048, eqmod cf1702 f1702 2048,
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x20014980*x**1*y**1*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x200149a4*x**1*y**1*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x200149c8*x**1*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x20015700*x**1*y**1*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x20015724*x**1*y**1*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x20015748*x**1*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x20018944; Value = 0x00030ffd; PC = 0x8056388 *)
mov r5 L0x20018944;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x20018704; Value = 0x0ffd0ffd; PC = 0x805638c *)
mov r6 L0x20018704;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018b84; Value = 0x00030003; PC = 0x8056390 *)
mov r8 L0x20018b84;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1103@sint32 : and [cf1103 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1403@sint32 : and [cf1403 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1703@sint32 : and [cf1703 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x200149ec; PC = 0x80563d0 *)
mov L0x200149ec r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014a10; PC = 0x80563d4 *)
mov L0x20014a10 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014a34; PC = 0x80563d8 *)
mov L0x20014a34 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x2001576c; PC = 0x80563dc *)
mov L0x2001576c r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015790; PC = 0x80563e0 *)
mov L0x20015790 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x200157b4; PC = 0x80563e4 *)
mov L0x200157b4 r9;



(******************** offset 1, 1,  3 ********************)


(**************** CUT 131, - *****************)

ecut and [
eqmod cf1103 f1103 2048, eqmod cf1403 f1403 2048, eqmod cf1703 f1703 2048,
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x200149ec*x**1*y**1*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x20014a10*x**1*y**1*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x20014a34*x**1*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x2001576c*x**1*y**1*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x20015790*x**1*y**1*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x200157b4*x**1*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x200187ca; Value = 0x00000000; PC = 0x80563e8 *)
mov r5 L0x200187ca;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x2001858a; Value = 0x0ffd0003; PC = 0x80563ec *)
mov r6 L0x2001858a;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018a0a; Value = 0x00000ffd; PC = 0x80563f0 *)
mov r8 L0x20018a0a;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1104@sint32 : and [cf1104 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1404@sint32 : and [cf1404 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1704@sint32 : and [cf1704 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014a58; PC = 0x8056430 *)
mov L0x20014a58 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014a7c; PC = 0x8056434 *)
mov L0x20014a7c r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014aa0; PC = 0x8056438 *)
mov L0x20014aa0 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x200157d8; PC = 0x805643c *)
mov L0x200157d8 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x200157fc; PC = 0x8056440 *)
mov L0x200157fc r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015820; PC = 0x8056444 *)
mov L0x20015820 r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;



(******************** offset 1, 1,  4 ********************)


(**************** CUT 132, - *****************)

ecut and [
eqmod cf1104 f1104 2048, eqmod cf1404 f1404 2048, eqmod cf1704 f1704 2048,
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x20014a58*x**1*y**1*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x20014a7c*x**1*y**1*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x20014aa0*x**1*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x200157d8*x**1*y**1*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x200157fc*x**1*y**1*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x20015820*x**1*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #222]	; 0xde                   #! EA = L0x20018650; Value = 0x00000ffd; PC = 0x8056454 *)
mov r5 L0x20018650;
(* ldrsh.w	r7, [lr, #1374]	; 0x55e                 #! EA = L0x20018ad0; Value = 0x00030003; PC = 0x8056458 *)
mov r7 L0x20018ad0;
(* ldrsh.w	r8, [lr, #798]	; 0x31e                  #! EA = L0x20018890; Value = 0x00000000; PC = 0x805645c *)
mov r8 L0x20018890;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056460 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1105@sint32 : and [cf1105 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056464 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1405@sint32 : and [cf1405 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056468 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1705@sint32 : and [cf1705 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x805646c *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056470 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8056474 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056478 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x805647c *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056480 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x8056482 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x8056484 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056488 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x805648c *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056490 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x8056494 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056498 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x805649c *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80564a0 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x80564a4 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x80564a8 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x80564ac *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014ac4; PC = 0x80564b0 *)
mov L0x20014ac4 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014ae8; PC = 0x80564b4 *)
mov L0x20014ae8 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014b0c; PC = 0x80564b8 *)
mov L0x20014b0c r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015844; PC = 0x80564bc *)
mov L0x20015844 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015868; PC = 0x80564c0 *)
mov L0x20015868 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x2001588c; PC = 0x80564c4 *)
mov L0x2001588c r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80564c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 1, 1,  5 ********************)


(**************** CUT 133, - *****************)

ecut and [
eqmod cf1105 f1105 2048, eqmod cf1405 f1405 2048, eqmod cf1705 f1705 2048,
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x20014ac4*x**1*y**1*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x20014ae8*x**1*y**1*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x20014b0c*x**1*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x20015844*x**1*y**1*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x20015868*x**1*y**1*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x2001588c*x**1*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1572]	; 0x624                 #! EA = L0x20018b96; Value = 0x0ffd0000; PC = 0x80564cc *)
mov r4 L0x20018b96;
(* ldrsh.w	r7, [lr, #996]	; 0x3e4                  #! EA = L0x20018956; Value = 0x00000ffd; PC = 0x80564d0 *)
mov r7 L0x20018956;
(* ldrsh.w	r8, [lr, #420]	; 0x1a4                  #! EA = L0x20018716; Value = 0x00030ffd; PC = 0x80564d4 *)
mov r8 L0x20018716;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80564d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1106@sint32 : and [cf1106 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80564dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1406@sint32 : and [cf1406 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80564e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1706@sint32 : and [cf1706 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80564e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80564e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80564ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80564f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80564f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80564f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80564fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80564fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056500 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056504 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056508 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805650c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805650e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056510 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056514 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056518 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805651c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014b30; PC = 0x8056520 *)
mov L0x20014b30 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014b54; PC = 0x8056524 *)
mov L0x20014b54 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014b78; PC = 0x8056528 *)
mov L0x20014b78 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200158b0; PC = 0x805652c *)
mov L0x200158b0 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200158d4; PC = 0x8056530 *)
mov L0x200158d4 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200158f8; PC = 0x8056534 *)
mov L0x200158f8 r9;



(******************** offset 1, 1,  6 ********************)


(**************** CUT 134, - *****************)

ecut and [
eqmod cf1106 f1106 2048, eqmod cf1406 f1406 2048, eqmod cf1706 f1706 2048,
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x20014b30*x**1*y**1*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x20014b54*x**1*y**1*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x20014b78*x**1*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x200158b0*x**1*y**1*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x200158d4*x**1*y**1*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x200158f8*x**1*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1194]	; 0x4aa                 #! EA = L0x20018a1c; Value = 0x00030000; PC = 0x8056538 *)
mov r4 L0x20018a1c;
(* ldrsh.w	r7, [lr, #618]	; 0x26a                  #! EA = L0x200187dc; Value = 0x0ffd0003; PC = 0x805653c *)
mov r7 L0x200187dc;
(* ldrsh.w	r8, [lr, #42]	; 0x2a                    #! EA = L0x2001859c; Value = 0x00000003; PC = 0x8056540 *)
mov r8 L0x2001859c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056544 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1107@sint32 : and [cf1107 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056548 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1407@sint32 : and [cf1407 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805654c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1707@sint32 : and [cf1707 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056550 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056554 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056558 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805655c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056560 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056564 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056566 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056568 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805656c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056570 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056574 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056578 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805657a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805657c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056580 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056584 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056588 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014b9c; PC = 0x805658c *)
mov L0x20014b9c r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014bc0; PC = 0x8056590 *)
mov L0x20014bc0 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014be4; PC = 0x8056594 *)
mov L0x20014be4 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x2001591c; PC = 0x8056598 *)
mov L0x2001591c r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015940; PC = 0x805659c *)
mov L0x20015940 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015964; PC = 0x80565a0 *)
mov L0x20015964 r9;



(******************** offset 1, 1,  7 ********************)


(**************** CUT 135, - *****************)

ecut and [
eqmod cf1107 f1107 2048, eqmod cf1407 f1407 2048, eqmod cf1707 f1707 2048,
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x20014b9c*x**1*y**1*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x20014bc0*x**1*y**1*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x20014be4*x**1*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x2001591c*x**1*y**1*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x20015940*x**1*y**1*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x20015964*x**1*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #816]	; 0x330                  #! EA = L0x200188a2; Value = 0x00000000; PC = 0x80565a4 *)
mov r4 L0x200188a2;
(* ldrsh.w	r7, [lr, #240]	; 0xf0                   #! EA = L0x20018662; Value = 0x00000000; PC = 0x80565a8 *)
mov r7 L0x20018662;
(* ldrsh.w	r9, [lr, #1392]	; 0x570                 #! EA = L0x20018ae2; Value = 0x00030000; PC = 0x80565ac *)
mov r9 L0x20018ae2;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80565b0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1108@sint32 : and [cf1108 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80565b4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1408@sint32 : and [cf1408 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80565b8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1708@sint32 : and [cf1708 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x80565bc *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x80565c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x80565c4 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x80565c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x80565cc *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x80565d0 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x80565d4 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80565d8 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x80565dc *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x80565e0 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x80565e2 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80565e4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014c08; PC = 0x80565e8 *)
mov L0x20014c08 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014c2c; PC = 0x80565ec *)
mov L0x20014c2c r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014c50; PC = 0x80565f0 *)
mov L0x20014c50 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015988; PC = 0x80565f4 *)
mov L0x20015988 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x200159ac; PC = 0x80565f8 *)
mov L0x200159ac r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200159d0; PC = 0x80565fc *)
mov L0x200159d0 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056600 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056604 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805623c <_Good_loop1_inner>              #! PC = 0x8056608 *)
#b.w	0x805623c <_Good_loop1_inner>              #! 0x8056608 = 0x8056608;



(******************** offset 1, 1,  8 ********************)


(**************** CUT 136, - *****************)

ecut and [
eqmod cf1108 f1108 2048, eqmod cf1408 f1408 2048, eqmod cf1708 f1708 2048,
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x20014c08*x**1*y**1*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x20014c2c*x**1*y**1*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x20014c50*x**1*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x20015988*x**1*y**1*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x200159ac*x**1*y**1*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x200159d0*x**1*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x20018728; Value = 0x0ffd0ffd; PC = 0x805623c *)
mov r4 L0x20018728;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018ba8; Value = 0x00000ffd; PC = 0x8056240 *)
mov r6 L0x20018ba8;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x20018968; Value = 0x00000000; PC = 0x8056244 *)
mov r9 L0x20018968;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1109@sint32 : and [cf1109 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1409@sint32 : and [cf1409 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1709@sint32 : and [cf1709 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014c74; PC = 0x8056290 *)
mov L0x20014c74 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014c98; PC = 0x8056294 *)
mov L0x20014c98 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014cbc; PC = 0x8056298 *)
mov L0x20014cbc r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200159f4; PC = 0x805629c *)
mov L0x200159f4 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015a18; PC = 0x80562a0 *)
mov L0x20015a18 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015a3c; PC = 0x80562a4 *)
mov L0x20015a3c r9;



(******************** offset 1, 1,  9 ********************)


(**************** CUT 137, - *****************)

ecut and [
eqmod cf1109 f1109 2048, eqmod cf1409 f1409 2048, eqmod cf1709 f1709 2048,
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x20014c74*x**1*y**1*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x20014c98*x**1*y**1*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x20014cbc*x**1*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x200159f4*x**1*y**1*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x20015a18*x**1*y**1*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x20015a3c*x**1*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x200185ae; Value = 0x00000000; PC = 0x80562a8 *)
mov r4 L0x200185ae;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x20018a2e; Value = 0x00000000; PC = 0x80562ac *)
mov r6 L0x20018a2e;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x200187ee; Value = 0x00030000; PC = 0x80562b0 *)
mov r9 L0x200187ee;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1110@sint32 : and [cf1110 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1410@sint32 : and [cf1410 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1710@sint32 : and [cf1710 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014ce0; PC = 0x80562fc *)
mov L0x20014ce0 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014d04; PC = 0x8056300 *)
mov L0x20014d04 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014d28; PC = 0x8056304 *)
mov L0x20014d28 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015a60; PC = 0x8056308 *)
mov L0x20015a60 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015a84; PC = 0x805630c *)
mov L0x20015a84 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015aa8; PC = 0x8056310 *)
mov L0x20015aa8 r9;



(******************** offset 1, 1, 10 ********************)


(**************** CUT 138, - *****************)

ecut and [
eqmod cf1110 f1110 2048, eqmod cf1410 f1410 2048, eqmod cf1710 f1710 2048,
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20014ce0*x**1*y**1*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20014d04*x**1*y**1*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20014d28*x**1*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20015a60*x**1*y**1*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20015a84*x**1*y**1*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20015aa8*x**1*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018af4; Value = 0x00000ffd; PC = 0x8056314 *)
mov r5 L0x20018af4;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x200188b4; Value = 0x0ffd0003; PC = 0x8056318 *)
mov r6 L0x200188b4;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x20018674; Value = 0x0ffd0000; PC = 0x805631c *)
mov r9 L0x20018674;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1111@sint32 : and [cf1111 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1411@sint32 : and [cf1411 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1711@sint32 : and [cf1711 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014d4c; PC = 0x8056370 *)
mov L0x20014d4c r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014d70; PC = 0x8056374 *)
mov L0x20014d70 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014d94; PC = 0x8056378 *)
mov L0x20014d94 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015acc; PC = 0x805637c *)
mov L0x20015acc r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015af0; PC = 0x8056380 *)
mov L0x20015af0 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015b14; PC = 0x8056384 *)
mov L0x20015b14 r9;



(******************** offset 1, 1, 11 ********************)


(**************** CUT 139, - *****************)

ecut and [
eqmod cf1111 f1111 2048, eqmod cf1411 f1411 2048, eqmod cf1711 f1711 2048,
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20014d4c*x**1*y**1*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20014d70*x**1*y**1*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20014d94*x**1*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20015acc*x**1*y**1*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20015af0*x**1*y**1*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20015b14*x**1*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x2001897a; Value = 0x0ffd0003; PC = 0x8056388 *)
mov r5 L0x2001897a;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x2001873a; Value = 0x0ffd0003; PC = 0x805638c *)
mov r6 L0x2001873a;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018bba; Value = 0x0ffd0003; PC = 0x8056390 *)
mov r8 L0x20018bba;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1112@sint32 : and [cf1112 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1412@sint32 : and [cf1412 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1712@sint32 : and [cf1712 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20014db8; PC = 0x80563d0 *)
mov L0x20014db8 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014ddc; PC = 0x80563d4 *)
mov L0x20014ddc r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014e00; PC = 0x80563d8 *)
mov L0x20014e00 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015b38; PC = 0x80563dc *)
mov L0x20015b38 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015b5c; PC = 0x80563e0 *)
mov L0x20015b5c r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015b80; PC = 0x80563e4 *)
mov L0x20015b80 r9;



(******************** offset 1, 1, 12 ********************)


(**************** CUT 140, - *****************)

ecut and [
eqmod cf1112 f1112 2048, eqmod cf1412 f1412 2048, eqmod cf1712 f1712 2048,
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20014db8*x**1*y**1*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20014ddc*x**1*y**1*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20014e00*x**1*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20015b38*x**1*y**1*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20015b5c*x**1*y**1*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20015b80*x**1*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x20018800; Value = 0x00000ffd; PC = 0x80563e8 *)
mov r5 L0x20018800;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x200185c0; Value = 0x00030ffd; PC = 0x80563ec *)
mov r6 L0x200185c0;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018a40; Value = 0x00030ffd; PC = 0x80563f0 *)
mov r8 L0x20018a40;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1113@sint32 : and [cf1113 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1413@sint32 : and [cf1413 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1713@sint32 : and [cf1713 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014e24; PC = 0x8056430 *)
mov L0x20014e24 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014e48; PC = 0x8056434 *)
mov L0x20014e48 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014e6c; PC = 0x8056438 *)
mov L0x20014e6c r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015ba4; PC = 0x805643c *)
mov L0x20015ba4 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015bc8; PC = 0x8056440 *)
mov L0x20015bc8 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015bec; PC = 0x8056444 *)
mov L0x20015bec r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;



(******************** offset 1, 1, 13 ********************)


(**************** CUT 141, - *****************)

ecut and [
eqmod cf1113 f1113 2048, eqmod cf1413 f1413 2048, eqmod cf1713 f1713 2048,
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20014e24*x**1*y**1*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20014e48*x**1*y**1*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20014e6c*x**1*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20015ba4*x**1*y**1*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20015bc8*x**1*y**1*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20015bec*x**1*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #222]	; 0xde                   #! EA = L0x20018686; Value = 0x00000000; PC = 0x8056454 *)
mov r5 L0x20018686;
(* ldrsh.w	r7, [lr, #1374]	; 0x55e                 #! EA = L0x20018b06; Value = 0x0ffd0000; PC = 0x8056458 *)
mov r7 L0x20018b06;
(* ldrsh.w	r8, [lr, #798]	; 0x31e                  #! EA = L0x200188c6; Value = 0x00000003; PC = 0x805645c *)
mov r8 L0x200188c6;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056460 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1114@sint32 : and [cf1114 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056464 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1414@sint32 : and [cf1414 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056468 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1714@sint32 : and [cf1714 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x805646c *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056470 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8056474 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056478 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x805647c *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056480 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x8056482 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x8056484 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056488 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x805648c *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056490 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x8056494 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056498 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x805649c *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80564a0 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x80564a4 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x80564a8 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x80564ac *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014e90; PC = 0x80564b0 *)
mov L0x20014e90 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014eb4; PC = 0x80564b4 *)
mov L0x20014eb4 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014ed8; PC = 0x80564b8 *)
mov L0x20014ed8 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015c10; PC = 0x80564bc *)
mov L0x20015c10 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015c34; PC = 0x80564c0 *)
mov L0x20015c34 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015c58; PC = 0x80564c4 *)
mov L0x20015c58 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80564c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 1, 1, 14 ********************)


(**************** CUT 142, - *****************)

ecut and [
eqmod cf1114 f1114 2048, eqmod cf1414 f1414 2048, eqmod cf1714 f1714 2048,
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20014e90*x**1*y**1*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20014eb4*x**1*y**1*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20014ed8*x**1*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20015c10*x**1*y**1*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20015c34*x**1*y**1*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20015c58*x**1*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1572]	; 0x624                 #! EA = L0x20018bcc; Value = 0x00000003; PC = 0x80564cc *)
mov r4 L0x20018bcc;
(* ldrsh.w	r7, [lr, #996]	; 0x3e4                  #! EA = L0x2001898c; Value = 0x0ffd0000; PC = 0x80564d0 *)
mov r7 L0x2001898c;
(* ldrsh.w	r8, [lr, #420]	; 0x1a4                  #! EA = L0x2001874c; Value = 0x00000003; PC = 0x80564d4 *)
mov r8 L0x2001874c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80564d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1115@sint32 : and [cf1115 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80564dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1415@sint32 : and [cf1415 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80564e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1715@sint32 : and [cf1715 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80564e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80564e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80564ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80564f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80564f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80564f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80564fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80564fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056500 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056504 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056508 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805650c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805650e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056510 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056514 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056518 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805651c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014efc; PC = 0x8056520 *)
mov L0x20014efc r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014f20; PC = 0x8056524 *)
mov L0x20014f20 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014f44; PC = 0x8056528 *)
mov L0x20014f44 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015c7c; PC = 0x805652c *)
mov L0x20015c7c r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015ca0; PC = 0x8056530 *)
mov L0x20015ca0 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015cc4; PC = 0x8056534 *)
mov L0x20015cc4 r9;



(******************** offset 1, 1, 15 ********************)


(**************** CUT 143, - *****************)

ecut and [
eqmod cf1115 f1115 2048, eqmod cf1415 f1415 2048, eqmod cf1715 f1715 2048,
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20014efc*x**1*y**1*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20014f20*x**1*y**1*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20014f44*x**1*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20015c7c*x**1*y**1*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20015ca0*x**1*y**1*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20015cc4*x**1*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1194]	; 0x4aa                 #! EA = L0x20018a52; Value = 0x0ffd0ffd; PC = 0x8056538 *)
mov r4 L0x20018a52;
(* ldrsh.w	r7, [lr, #618]	; 0x26a                  #! EA = L0x20018812; Value = 0x00000000; PC = 0x805653c *)
mov r7 L0x20018812;
(* ldrsh.w	r8, [lr, #42]	; 0x2a                    #! EA = L0x200185d2; Value = 0x00030003; PC = 0x8056540 *)
mov r8 L0x200185d2;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056544 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1116@sint32 : and [cf1116 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056548 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1416@sint32 : and [cf1416 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805654c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1716@sint32 : and [cf1716 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056550 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056554 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056558 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805655c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056560 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056564 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056566 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056568 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805656c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056570 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056574 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056578 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805657a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805657c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056580 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056584 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056588 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014f68; PC = 0x805658c *)
mov L0x20014f68 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014f8c; PC = 0x8056590 *)
mov L0x20014f8c r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014fb0; PC = 0x8056594 *)
mov L0x20014fb0 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015ce8; PC = 0x8056598 *)
mov L0x20015ce8 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015d0c; PC = 0x805659c *)
mov L0x20015d0c r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015d30; PC = 0x80565a0 *)
mov L0x20015d30 r9;



(******************** offset 1, 1, 16 ********************)


(**************** CUT 144, - *****************)

ecut and [
eqmod cf1116 f1116 2048, eqmod cf1416 f1416 2048, eqmod cf1716 f1716 2048,
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20014f68*x**1*y**1*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20014f8c*x**1*y**1*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20014fb0*x**1*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20015ce8*x**1*y**1*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20015d0c*x**1*y**1*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20015d30*x**1*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #816]	; 0x330                  #! EA = L0x200188d8; Value = 0x00000003; PC = 0x80565a4 *)
mov r4 L0x200188d8;
(* ldrsh.w	r7, [lr, #240]	; 0xf0                   #! EA = L0x20018698; Value = 0x0ffd0000; PC = 0x80565a8 *)
mov r7 L0x20018698;
(* ldrsh.w	r9, [lr, #1392]	; 0x570                 #! EA = L0x20018b18; Value = 0x00030003; PC = 0x80565ac *)
mov r9 L0x20018b18;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80565b0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1117@sint32 : and [cf1117 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80565b4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1417@sint32 : and [cf1417 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80565b8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1717@sint32 : and [cf1717 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x80565bc *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x80565c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x80565c4 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x80565c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x80565cc *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x80565d0 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x80565d4 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80565d8 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x80565dc *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x80565e0 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x80565e2 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80565e4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014fd4; PC = 0x80565e8 *)
mov L0x20014fd4 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014ff8; PC = 0x80565ec *)
mov L0x20014ff8 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x2001501c; PC = 0x80565f0 *)
mov L0x2001501c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015d54; PC = 0x80565f4 *)
mov L0x20015d54 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015d78; PC = 0x80565f8 *)
mov L0x20015d78 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015d9c; PC = 0x80565fc *)
mov L0x20015d9c r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056600 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056604 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805623c <_Good_loop1_inner>              #! PC = 0x8056608 *)
#b.w	0x805623c <_Good_loop1_inner>              #! 0x8056608 = 0x8056608;



(******************** offset 1, 1, 17 ********************)


(**************** CUT 145, - *****************)

ecut and [
eqmod cf1117 f1117 2048, eqmod cf1417 f1417 2048, eqmod cf1717 f1717 2048,
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x20014fd4*x**1*y**1*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x20014ff8*x**1*y**1*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x2001501c*x**1*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x20015d54*x**1*y**1*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x20015d78*x**1*y**1*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x20015d9c*x**1*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x2001875e; Value = 0x0ffd0000; PC = 0x805623c *)
mov r4 L0x2001875e;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018bde; Value = 0x00000000; PC = 0x8056240 *)
mov r6 L0x20018bde;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x2001899e; Value = 0x00030003; PC = 0x8056244 *)
mov r9 L0x2001899e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1118@sint32 : and [cf1118 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1418@sint32 : and [cf1418 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1718@sint32 : and [cf1718 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015040; PC = 0x8056290 *)
mov L0x20015040 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015064; PC = 0x8056294 *)
mov L0x20015064 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015088; PC = 0x8056298 *)
mov L0x20015088 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015dc0; PC = 0x805629c *)
mov L0x20015dc0 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015de4; PC = 0x80562a0 *)
mov L0x20015de4 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015e08; PC = 0x80562a4 *)
mov L0x20015e08 r9;



(******************** offset 1, 1, 18 ********************)


(**************** CUT 146, - *****************)

ecut and [
eqmod cf1118 f1118 2048, eqmod cf1418 f1418 2048, eqmod cf1718 f1718 2048,
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015040*x**1*y**1*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015064*x**1*y**1*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015088*x**1*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015dc0*x**1*y**1*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015de4*x**1*y**1*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015e08*x**1*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x200185e4; Value = 0x00030003; PC = 0x80562a8 *)
mov r4 L0x200185e4;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x20018a64; Value = 0x00000000; PC = 0x80562ac *)
mov r6 L0x20018a64;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x20018824; Value = 0x00030ffd; PC = 0x80562b0 *)
mov r9 L0x20018824;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1119@sint32 : and [cf1119 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1419@sint32 : and [cf1419 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1719@sint32 : and [cf1719 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x200150ac; PC = 0x80562fc *)
mov L0x200150ac r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200150d0; PC = 0x8056300 *)
mov L0x200150d0 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200150f4; PC = 0x8056304 *)
mov L0x200150f4 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015e2c; PC = 0x8056308 *)
mov L0x20015e2c r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015e50; PC = 0x805630c *)
mov L0x20015e50 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015e74; PC = 0x8056310 *)
mov L0x20015e74 r9;



(******************** offset 1, 1, 19 ********************)


(**************** CUT 147, - *****************)

ecut and [
eqmod cf1119 f1119 2048, eqmod cf1419 f1419 2048, eqmod cf1719 f1719 2048,
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x200150ac*x**1*y**1*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x200150d0*x**1*y**1*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x200150f4*x**1*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x20015e2c*x**1*y**1*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x20015e50*x**1*y**1*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x20015e74*x**1*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018b2a; Value = 0x00000000; PC = 0x8056314 *)
mov r5 L0x20018b2a;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x200188ea; Value = 0x00000000; PC = 0x8056318 *)
mov r6 L0x200188ea;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x200186aa; Value = 0x00000ffd; PC = 0x805631c *)
mov r9 L0x200186aa;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1120@sint32 : and [cf1120 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1420@sint32 : and [cf1420 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1720@sint32 : and [cf1720 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20015118; PC = 0x8056370 *)
mov L0x20015118 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x2001513c; PC = 0x8056374 *)
mov L0x2001513c r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015160; PC = 0x8056378 *)
mov L0x20015160 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015e98; PC = 0x805637c *)
mov L0x20015e98 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015ebc; PC = 0x8056380 *)
mov L0x20015ebc r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015ee0; PC = 0x8056384 *)
mov L0x20015ee0 r9;



(******************** offset 1, 1, 20 ********************)


(**************** CUT 148, - *****************)

ecut and [
eqmod cf1120 f1120 2048, eqmod cf1420 f1420 2048, eqmod cf1720 f1720 2048,
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x20015118*x**1*y**1*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x2001513c*x**1*y**1*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x20015160*x**1*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x20015e98*x**1*y**1*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x20015ebc*x**1*y**1*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x20015ee0*x**1*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x200189b0; Value = 0x00000ffd; PC = 0x8056388 *)
mov r5 L0x200189b0;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x20018770; Value = 0x0ffd0003; PC = 0x805638c *)
mov r6 L0x20018770;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018bf0; Value = 0x00000000; PC = 0x8056390 *)
mov r8 L0x20018bf0;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1121@sint32 : and [cf1121 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1421@sint32 : and [cf1421 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1721@sint32 : and [cf1721 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015184; PC = 0x80563d0 *)
mov L0x20015184 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x200151a8; PC = 0x80563d4 *)
mov L0x200151a8 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200151cc; PC = 0x80563d8 *)
mov L0x200151cc r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015f04; PC = 0x80563dc *)
mov L0x20015f04 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015f28; PC = 0x80563e0 *)
mov L0x20015f28 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015f4c; PC = 0x80563e4 *)
mov L0x20015f4c r9;



(******************** offset 1, 1, 21 ********************)


(**************** CUT 149, - *****************)

ecut and [
eqmod cf1121 f1121 2048, eqmod cf1421 f1421 2048, eqmod cf1721 f1721 2048,
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x20015184*x**1*y**1*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x200151a8*x**1*y**1*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x200151cc*x**1*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x20015f04*x**1*y**1*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x20015f28*x**1*y**1*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x20015f4c*x**1*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x20018836; Value = 0x0ffd0000; PC = 0x80563e8 *)
mov r5 L0x20018836;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x200185f6; Value = 0x00000ffd; PC = 0x80563ec *)
mov r6 L0x200185f6;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018a76; Value = 0x00000ffd; PC = 0x80563f0 *)
mov r8 L0x20018a76;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1122@sint32 : and [cf1122 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1422@sint32 : and [cf1422 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1722@sint32 : and [cf1722 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200151f0; PC = 0x8056430 *)
mov L0x200151f0 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20015214; PC = 0x8056434 *)
mov L0x20015214 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015238; PC = 0x8056438 *)
mov L0x20015238 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015f70; PC = 0x805643c *)
mov L0x20015f70 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015f94; PC = 0x8056440 *)
mov L0x20015f94 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015fb8; PC = 0x8056444 *)
mov L0x20015fb8 r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;



(******************** offset 1, 1, 22 ********************)


(**************** CUT 150, - *****************)

ecut and [
eqmod cf1122 f1122 2048, eqmod cf1422 f1422 2048, eqmod cf1722 f1722 2048,
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x200151f0*x**1*y**1*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x20015214*x**1*y**1*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x20015238*x**1*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x20015f70*x**1*y**1*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x20015f94*x**1*y**1*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x20015fb8*x**1*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #222]	; 0xde                   #! EA = L0x200186bc; Value = 0x00000000; PC = 0x8056454 *)
mov r5 L0x200186bc;
(* ldrsh.w	r7, [lr, #1374]	; 0x55e                 #! EA = L0x20018b3c; Value = 0x0ffd0ffd; PC = 0x8056458 *)
mov r7 L0x20018b3c;
(* ldrsh.w	r8, [lr, #798]	; 0x31e                  #! EA = L0x200188fc; Value = 0x0ffd0000; PC = 0x805645c *)
mov r8 L0x200188fc;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056460 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1123@sint32 : and [cf1123 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056464 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1423@sint32 : and [cf1423 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056468 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1723@sint32 : and [cf1723 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x805646c *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056470 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8056474 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056478 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x805647c *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056480 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x8056482 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x8056484 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056488 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x805648c *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056490 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x8056494 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056498 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x805649c *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80564a0 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x80564a4 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x80564a8 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x80564ac *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x2001525c; PC = 0x80564b0 *)
mov L0x2001525c r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20015280; PC = 0x80564b4 *)
mov L0x20015280 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x200152a4; PC = 0x80564b8 *)
mov L0x200152a4 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015fdc; PC = 0x80564bc *)
mov L0x20015fdc r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20016000; PC = 0x80564c0 *)
mov L0x20016000 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20016024; PC = 0x80564c4 *)
mov L0x20016024 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80564c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 1, 1, 23 ********************)


(**************** CUT 151, - *****************)

ecut and [
eqmod cf1123 f1123 2048, eqmod cf1423 f1423 2048, eqmod cf1723 f1723 2048,
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x2001525c*x**1*y**1*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x20015280*x**1*y**1*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x200152a4*x**1*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x20015fdc*x**1*y**1*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x20016000*x**1*y**1*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x20016024*x**1*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1572]	; 0x624                 #! EA = L0x20018c02; Value = 0x00000000; PC = 0x80564cc *)
mov r4 L0x20018c02;
(* ldrsh.w	r7, [lr, #996]	; 0x3e4                  #! EA = L0x200189c2; Value = 0x00030000; PC = 0x80564d0 *)
mov r7 L0x200189c2;
(* ldrsh.w	r8, [lr, #420]	; 0x1a4                  #! EA = L0x20018782; Value = 0x0ffd0003; PC = 0x80564d4 *)
mov r8 L0x20018782;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80564d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1124@sint32 : and [cf1124 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80564dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1424@sint32 : and [cf1424 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80564e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1724@sint32 : and [cf1724 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80564e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80564e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80564ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80564f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80564f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80564f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80564fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80564fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056500 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056504 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056508 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805650c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805650e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056510 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056514 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056518 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805651c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200152c8; PC = 0x8056520 *)
mov L0x200152c8 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200152ec; PC = 0x8056524 *)
mov L0x200152ec r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015310; PC = 0x8056528 *)
mov L0x20015310 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016048; PC = 0x805652c *)
mov L0x20016048 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x2001606c; PC = 0x8056530 *)
mov L0x2001606c r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20016090; PC = 0x8056534 *)
mov L0x20016090 r9;



(******************** offset 1, 1, 24 ********************)


(**************** CUT 152, - *****************)

ecut and [
eqmod cf1124 f1124 2048, eqmod cf1424 f1424 2048, eqmod cf1724 f1724 2048,
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x200152c8*x**1*y**1*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x200152ec*x**1*y**1*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x20015310*x**1*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x20016048*x**1*y**1*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x2001606c*x**1*y**1*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x20016090*x**1*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1194]	; 0x4aa                 #! EA = L0x20018a88; Value = 0x00030003; PC = 0x8056538 *)
mov r4 L0x20018a88;
(* ldrsh.w	r7, [lr, #618]	; 0x26a                  #! EA = L0x20018848; Value = 0x00000ffd; PC = 0x805653c *)
mov r7 L0x20018848;
(* ldrsh.w	r8, [lr, #42]	; 0x2a                    #! EA = L0x20018608; Value = 0x00000ffd; PC = 0x8056540 *)
mov r8 L0x20018608;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056544 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1125@sint32 : and [cf1125 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056548 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1425@sint32 : and [cf1425 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805654c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1725@sint32 : and [cf1725 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056550 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056554 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056558 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805655c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056560 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056564 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056566 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056568 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805656c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056570 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056574 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056578 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805657a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805657c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056580 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056584 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056588 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015334; PC = 0x805658c *)
mov L0x20015334 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20015358; PC = 0x8056590 *)
mov L0x20015358 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x2001537c; PC = 0x8056594 *)
mov L0x2001537c r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200160b4; PC = 0x8056598 *)
mov L0x200160b4 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200160d8; PC = 0x805659c *)
mov L0x200160d8 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200160fc; PC = 0x80565a0 *)
mov L0x200160fc r9;



(******************** offset 1, 1, 25 ********************)


(**************** CUT 153, - *****************)

ecut and [
eqmod cf1125 f1125 2048, eqmod cf1425 f1425 2048, eqmod cf1725 f1725 2048,
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x20015334*x**1*y**1*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x20015358*x**1*y**1*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x2001537c*x**1*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x200160b4*x**1*y**1*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x200160d8*x**1*y**1*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x200160fc*x**1*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #816]	; 0x330                  #! EA = L0x2001890e; Value = 0x00000003; PC = 0x80565a4 *)
mov r4 L0x2001890e;
(* ldrsh.w	r7, [lr, #240]	; 0xf0                   #! EA = L0x200186ce; Value = 0x00000ffd; PC = 0x80565a8 *)
mov r7 L0x200186ce;
(* ldrsh.w	r9, [lr, #1392]	; 0x570                 #! EA = L0x20018b4e; Value = 0x00030003; PC = 0x80565ac *)
mov r9 L0x20018b4e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80565b0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1126@sint32 : and [cf1126 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80565b4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1426@sint32 : and [cf1426 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80565b8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1726@sint32 : and [cf1726 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x80565bc *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x80565c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x80565c4 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x80565c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x80565cc *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x80565d0 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x80565d4 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80565d8 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x80565dc *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x80565e0 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x80565e2 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80565e4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200153a0; PC = 0x80565e8 *)
mov L0x200153a0 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200153c4; PC = 0x80565ec *)
mov L0x200153c4 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200153e8; PC = 0x80565f0 *)
mov L0x200153e8 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016120; PC = 0x80565f4 *)
mov L0x20016120 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016144; PC = 0x80565f8 *)
mov L0x20016144 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20016168; PC = 0x80565fc *)
mov L0x20016168 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056600 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056604 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805623c <_Good_loop1_inner>              #! PC = 0x8056608 *)
#b.w	0x805623c <_Good_loop1_inner>              #! 0x8056608 = 0x8056608;



(******************** offset 1, 1, 26 ********************)


(**************** CUT 154, - *****************)

ecut and [
eqmod cf1126 f1126 2048, eqmod cf1426 f1426 2048, eqmod cf1726 f1726 2048,
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x200153a0*x**1*y**1*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x200153c4*x**1*y**1*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x200153e8*x**1*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x20016120*x**1*y**1*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x20016144*x**1*y**1*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x20016168*x**1*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x20018794; Value = 0x0ffd0000; PC = 0x805623c *)
mov r4 L0x20018794;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018c14; Value = 0x00000000; PC = 0x8056240 *)
mov r6 L0x20018c14;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x200189d4; Value = 0x00030003; PC = 0x8056244 *)
mov r9 L0x200189d4;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1127@sint32 : and [cf1127 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1427@sint32 : and [cf1427 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1727@sint32 : and [cf1727 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x2001540c; PC = 0x8056290 *)
mov L0x2001540c r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015430; PC = 0x8056294 *)
mov L0x20015430 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015454; PC = 0x8056298 *)
mov L0x20015454 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x2001618c; PC = 0x805629c *)
mov L0x2001618c r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200161b0; PC = 0x80562a0 *)
mov L0x200161b0 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200161d4; PC = 0x80562a4 *)
mov L0x200161d4 r9;



(******************** offset 1, 1, 27 ********************)


(**************** CUT 155, - *****************)

ecut and [
eqmod cf1127 f1127 2048, eqmod cf1427 f1427 2048, eqmod cf1727 f1727 2048,
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x2001540c*x**1*y**1*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x20015430*x**1*y**1*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x20015454*x**1*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x2001618c*x**1*y**1*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x200161b0*x**1*y**1*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x200161d4*x**1*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x2001861a; Value = 0x00000000; PC = 0x80562a8 *)
mov r4 L0x2001861a;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x20018a9a; Value = 0x00030000; PC = 0x80562ac *)
mov r6 L0x20018a9a;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x2001885a; Value = 0x00030000; PC = 0x80562b0 *)
mov r9 L0x2001885a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1128@sint32 : and [cf1128 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1428@sint32 : and [cf1428 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1728@sint32 : and [cf1728 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015478; PC = 0x80562fc *)
mov L0x20015478 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x2001549c; PC = 0x8056300 *)
mov L0x2001549c r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200154c0; PC = 0x8056304 *)
mov L0x200154c0 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200161f8; PC = 0x8056308 *)
mov L0x200161f8 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x2001621c; PC = 0x805630c *)
mov L0x2001621c r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20016240; PC = 0x8056310 *)
mov L0x20016240 r9;



(******************** offset 1, 1, 28 ********************)


(**************** CUT 156, - *****************)

ecut and [
eqmod cf1128 f1128 2048, eqmod cf1428 f1428 2048, eqmod cf1728 f1728 2048,
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x20015478*x**1*y**1*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x2001549c*x**1*y**1*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x200154c0*x**1*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x200161f8*x**1*y**1*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x2001621c*x**1*y**1*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x20016240*x**1*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018b60; Value = 0x00000003; PC = 0x8056314 *)
mov r5 L0x20018b60;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x20018920; Value = 0x00000000; PC = 0x8056318 *)
mov r6 L0x20018920;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x200186e0; Value = 0x00000003; PC = 0x805631c *)
mov r9 L0x200186e0;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1129@sint32 : and [cf1129 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1429@sint32 : and [cf1429 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1729@sint32 : and [cf1729 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200154e4; PC = 0x8056370 *)
mov L0x200154e4 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015508; PC = 0x8056374 *)
mov L0x20015508 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x2001552c; PC = 0x8056378 *)
mov L0x2001552c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016264; PC = 0x805637c *)
mov L0x20016264 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016288; PC = 0x8056380 *)
mov L0x20016288 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200162ac; PC = 0x8056384 *)
mov L0x200162ac r9;



(******************** offset 1, 1, 29 ********************)


(**************** CUT 157, - *****************)

ecut and [
eqmod cf1129 f1129 2048, eqmod cf1429 f1429 2048, eqmod cf1729 f1729 2048,
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x200154e4*x**1*y**1*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x20015508*x**1*y**1*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x2001552c*x**1*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x20016264*x**1*y**1*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x20016288*x**1*y**1*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x200162ac*x**1*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x200189e6; Value = 0x00030000; PC = 0x8056388 *)
mov r5 L0x200189e6;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x200187a6; Value = 0x0ffd0003; PC = 0x805638c *)
mov r6 L0x200187a6;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018c26; Value = 0x00000000; PC = 0x8056390 *)
mov r8 L0x20018c26;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1130@sint32 : and [cf1130 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1430@sint32 : and [cf1430 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1730@sint32 : and [cf1730 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015550; PC = 0x80563d0 *)
mov L0x20015550 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20015574; PC = 0x80563d4 *)
mov L0x20015574 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20015598; PC = 0x80563d8 *)
mov L0x20015598 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x200162d0; PC = 0x80563dc *)
mov L0x200162d0 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x200162f4; PC = 0x80563e0 *)
mov L0x200162f4 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20016318; PC = 0x80563e4 *)
mov L0x20016318 r9;



(******************** offset 1, 1, 30 ********************)


(**************** CUT 158, - *****************)

ecut and [
eqmod cf1130 f1130 2048, eqmod cf1430 f1430 2048, eqmod cf1730 f1730 2048,
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x20015550*x**1*y**1*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x20015574*x**1*y**1*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x20015598*x**1*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x200162d0*x**1*y**1*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x200162f4*x**1*y**1*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x20016318*x**1*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x2001886c; Value = 0x0ffd0000; PC = 0x80563e8 *)
mov r5 L0x2001886c;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x2001862c; Value = 0x0ffd0000; PC = 0x80563ec *)
mov r6 L0x2001862c;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018aac; Value = 0x00000003; PC = 0x80563f0 *)
mov r8 L0x20018aac;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1131@sint32 : and [cf1131 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1431@sint32 : and [cf1431 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1731@sint32 : and [cf1731 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200155bc; PC = 0x8056430 *)
mov L0x200155bc r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x200155e0; PC = 0x8056434 *)
mov L0x200155e0 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015604; PC = 0x8056438 *)
mov L0x20015604 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x2001633c; PC = 0x805643c *)
mov L0x2001633c r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20016360; PC = 0x8056440 *)
mov L0x20016360 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20016384; PC = 0x8056444 *)
mov L0x20016384 r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;
(* sub.w	lr, lr, #162	; 0xa2                       #! PC = 0x805660c *)
subs dc lr lr 162@uint32;
(* subw	r0, r0, #2916	; 0xb64                      #! PC = 0x8056610 *)
subc dc r0 r0 2916@uint32;
(* add.w	lr, lr, #2                                #! PC = 0x8056614 *)
adds dc lr lr 2@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8056618 *)
adds dc r0 r0 4@uint32;
(* cmp.w	r0, r12                                   #! PC = 0x805661c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8056234 <_Good_loop1>                  #! PC = 0x8056620 *)
#bne.w	0x8056234 <_Good_loop1>                  #! 0x8056620 = 0x8056620;
(* addw	r1, r0, #2916	; 0xb64                      #! PC = 0x8056234 *)
adds dc r1 r0 2916@uint32;
(* vmov	s3, r1                                     #! PC = 0x8056238 *)
mov s3 r1;



(******************** offset 1, 1, 31 ********************)


(**************** CUT 159, - *****************)

ecut and [
eqmod cf1131 f1131 2048, eqmod cf1431 f1431 2048, eqmod cf1731 f1731 2048,
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x200155bc*x**1*y**1*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x200155e0*x**1*y**1*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x20015604*x**1*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x2001633c*x**1*y**1*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x20016360*x**1*y**1*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x20016384*x**1*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(**************** CUT -,   4 *****************)

rcut and [
(-3367617)@32<=sL0x200148a8,L0x200148a8<=s3367617@32,
(-3367617)@32<=sL0x200148cc,L0x200148cc<=s3367617@32,
(-3367617)@32<=sL0x200148f0,L0x200148f0<=s3367617@32,
(-3367617)@32<=sL0x20015628,L0x20015628<=s3367617@32,
(-3367617)@32<=sL0x2001564c,L0x2001564c<=s3367617@32,
(-3367617)@32<=sL0x20015670,L0x20015670<=s3367617@32
,
(-3367617)@32<=sL0x20014914,L0x20014914<=s3367617@32,
(-3367617)@32<=sL0x20014938,L0x20014938<=s3367617@32,
(-3367617)@32<=sL0x2001495c,L0x2001495c<=s3367617@32,
(-3367617)@32<=sL0x20015694,L0x20015694<=s3367617@32,
(-3367617)@32<=sL0x200156b8,L0x200156b8<=s3367617@32,
(-3367617)@32<=sL0x200156dc,L0x200156dc<=s3367617@32
,
(-3367617)@32<=sL0x20014980,L0x20014980<=s3367617@32,
(-3367617)@32<=sL0x200149a4,L0x200149a4<=s3367617@32,
(-3367617)@32<=sL0x200149c8,L0x200149c8<=s3367617@32,
(-3367617)@32<=sL0x20015700,L0x20015700<=s3367617@32,
(-3367617)@32<=sL0x20015724,L0x20015724<=s3367617@32,
(-3367617)@32<=sL0x20015748,L0x20015748<=s3367617@32
,
(-3367617)@32<=sL0x200149ec,L0x200149ec<=s3367617@32,
(-3367617)@32<=sL0x20014a10,L0x20014a10<=s3367617@32,
(-3367617)@32<=sL0x20014a34,L0x20014a34<=s3367617@32,
(-3367617)@32<=sL0x2001576c,L0x2001576c<=s3367617@32,
(-3367617)@32<=sL0x20015790,L0x20015790<=s3367617@32,
(-3367617)@32<=sL0x200157b4,L0x200157b4<=s3367617@32
,
(-3367617)@32<=sL0x20014a58,L0x20014a58<=s3367617@32,
(-3367617)@32<=sL0x20014a7c,L0x20014a7c<=s3367617@32,
(-3367617)@32<=sL0x20014aa0,L0x20014aa0<=s3367617@32,
(-3367617)@32<=sL0x200157d8,L0x200157d8<=s3367617@32,
(-3367617)@32<=sL0x200157fc,L0x200157fc<=s3367617@32,
(-3367617)@32<=sL0x20015820,L0x20015820<=s3367617@32
,
(-3367617)@32<=sL0x20014ac4,L0x20014ac4<=s3367617@32,
(-3367617)@32<=sL0x20014ae8,L0x20014ae8<=s3367617@32,
(-3367617)@32<=sL0x20014b0c,L0x20014b0c<=s3367617@32,
(-3367617)@32<=sL0x20015844,L0x20015844<=s3367617@32,
(-3367617)@32<=sL0x20015868,L0x20015868<=s3367617@32,
(-3367617)@32<=sL0x2001588c,L0x2001588c<=s3367617@32
,
(-3367617)@32<=sL0x20014b30,L0x20014b30<=s3367617@32,
(-3367617)@32<=sL0x20014b54,L0x20014b54<=s3367617@32,
(-3367617)@32<=sL0x20014b78,L0x20014b78<=s3367617@32,
(-3367617)@32<=sL0x200158b0,L0x200158b0<=s3367617@32,
(-3367617)@32<=sL0x200158d4,L0x200158d4<=s3367617@32,
(-3367617)@32<=sL0x200158f8,L0x200158f8<=s3367617@32
,
(-3367617)@32<=sL0x20014b9c,L0x20014b9c<=s3367617@32,
(-3367617)@32<=sL0x20014bc0,L0x20014bc0<=s3367617@32,
(-3367617)@32<=sL0x20014be4,L0x20014be4<=s3367617@32,
(-3367617)@32<=sL0x2001591c,L0x2001591c<=s3367617@32,
(-3367617)@32<=sL0x20015940,L0x20015940<=s3367617@32,
(-3367617)@32<=sL0x20015964,L0x20015964<=s3367617@32
,
(-3367617)@32<=sL0x20014c08,L0x20014c08<=s3367617@32,
(-3367617)@32<=sL0x20014c2c,L0x20014c2c<=s3367617@32,
(-3367617)@32<=sL0x20014c50,L0x20014c50<=s3367617@32,
(-3367617)@32<=sL0x20015988,L0x20015988<=s3367617@32,
(-3367617)@32<=sL0x200159ac,L0x200159ac<=s3367617@32,
(-3367617)@32<=sL0x200159d0,L0x200159d0<=s3367617@32
,
(-3367617)@32<=sL0x20014c74,L0x20014c74<=s3367617@32,
(-3367617)@32<=sL0x20014c98,L0x20014c98<=s3367617@32,
(-3367617)@32<=sL0x20014cbc,L0x20014cbc<=s3367617@32,
(-3367617)@32<=sL0x200159f4,L0x200159f4<=s3367617@32,
(-3367617)@32<=sL0x20015a18,L0x20015a18<=s3367617@32,
(-3367617)@32<=sL0x20015a3c,L0x20015a3c<=s3367617@32
,
(-3367617)@32<=sL0x20014ce0,L0x20014ce0<=s3367617@32,
(-3367617)@32<=sL0x20014d04,L0x20014d04<=s3367617@32,
(-3367617)@32<=sL0x20014d28,L0x20014d28<=s3367617@32,
(-3367617)@32<=sL0x20015a60,L0x20015a60<=s3367617@32,
(-3367617)@32<=sL0x20015a84,L0x20015a84<=s3367617@32,
(-3367617)@32<=sL0x20015aa8,L0x20015aa8<=s3367617@32
,
(-3367617)@32<=sL0x20014d4c,L0x20014d4c<=s3367617@32,
(-3367617)@32<=sL0x20014d70,L0x20014d70<=s3367617@32,
(-3367617)@32<=sL0x20014d94,L0x20014d94<=s3367617@32,
(-3367617)@32<=sL0x20015acc,L0x20015acc<=s3367617@32,
(-3367617)@32<=sL0x20015af0,L0x20015af0<=s3367617@32,
(-3367617)@32<=sL0x20015b14,L0x20015b14<=s3367617@32
,
(-3367617)@32<=sL0x20014db8,L0x20014db8<=s3367617@32,
(-3367617)@32<=sL0x20014ddc,L0x20014ddc<=s3367617@32,
(-3367617)@32<=sL0x20014e00,L0x20014e00<=s3367617@32,
(-3367617)@32<=sL0x20015b38,L0x20015b38<=s3367617@32,
(-3367617)@32<=sL0x20015b5c,L0x20015b5c<=s3367617@32,
(-3367617)@32<=sL0x20015b80,L0x20015b80<=s3367617@32
,
(-3367617)@32<=sL0x20014e24,L0x20014e24<=s3367617@32,
(-3367617)@32<=sL0x20014e48,L0x20014e48<=s3367617@32,
(-3367617)@32<=sL0x20014e6c,L0x20014e6c<=s3367617@32,
(-3367617)@32<=sL0x20015ba4,L0x20015ba4<=s3367617@32,
(-3367617)@32<=sL0x20015bc8,L0x20015bc8<=s3367617@32,
(-3367617)@32<=sL0x20015bec,L0x20015bec<=s3367617@32
,
(-3367617)@32<=sL0x20014e90,L0x20014e90<=s3367617@32,
(-3367617)@32<=sL0x20014eb4,L0x20014eb4<=s3367617@32,
(-3367617)@32<=sL0x20014ed8,L0x20014ed8<=s3367617@32,
(-3367617)@32<=sL0x20015c10,L0x20015c10<=s3367617@32,
(-3367617)@32<=sL0x20015c34,L0x20015c34<=s3367617@32,
(-3367617)@32<=sL0x20015c58,L0x20015c58<=s3367617@32
,
(-3367617)@32<=sL0x20014efc,L0x20014efc<=s3367617@32,
(-3367617)@32<=sL0x20014f20,L0x20014f20<=s3367617@32,
(-3367617)@32<=sL0x20014f44,L0x20014f44<=s3367617@32,
(-3367617)@32<=sL0x20015c7c,L0x20015c7c<=s3367617@32,
(-3367617)@32<=sL0x20015ca0,L0x20015ca0<=s3367617@32,
(-3367617)@32<=sL0x20015cc4,L0x20015cc4<=s3367617@32
,
(-3367617)@32<=sL0x20014f68,L0x20014f68<=s3367617@32,
(-3367617)@32<=sL0x20014f8c,L0x20014f8c<=s3367617@32,
(-3367617)@32<=sL0x20014fb0,L0x20014fb0<=s3367617@32,
(-3367617)@32<=sL0x20015ce8,L0x20015ce8<=s3367617@32,
(-3367617)@32<=sL0x20015d0c,L0x20015d0c<=s3367617@32,
(-3367617)@32<=sL0x20015d30,L0x20015d30<=s3367617@32
,
(-3367617)@32<=sL0x20014fd4,L0x20014fd4<=s3367617@32,
(-3367617)@32<=sL0x20014ff8,L0x20014ff8<=s3367617@32,
(-3367617)@32<=sL0x2001501c,L0x2001501c<=s3367617@32,
(-3367617)@32<=sL0x20015d54,L0x20015d54<=s3367617@32,
(-3367617)@32<=sL0x20015d78,L0x20015d78<=s3367617@32,
(-3367617)@32<=sL0x20015d9c,L0x20015d9c<=s3367617@32
,
(-3367617)@32<=sL0x20015040,L0x20015040<=s3367617@32,
(-3367617)@32<=sL0x20015064,L0x20015064<=s3367617@32,
(-3367617)@32<=sL0x20015088,L0x20015088<=s3367617@32,
(-3367617)@32<=sL0x20015dc0,L0x20015dc0<=s3367617@32,
(-3367617)@32<=sL0x20015de4,L0x20015de4<=s3367617@32,
(-3367617)@32<=sL0x20015e08,L0x20015e08<=s3367617@32
,
(-3367617)@32<=sL0x200150ac,L0x200150ac<=s3367617@32,
(-3367617)@32<=sL0x200150d0,L0x200150d0<=s3367617@32,
(-3367617)@32<=sL0x200150f4,L0x200150f4<=s3367617@32,
(-3367617)@32<=sL0x20015e2c,L0x20015e2c<=s3367617@32,
(-3367617)@32<=sL0x20015e50,L0x20015e50<=s3367617@32,
(-3367617)@32<=sL0x20015e74,L0x20015e74<=s3367617@32
,
(-3367617)@32<=sL0x20015118,L0x20015118<=s3367617@32,
(-3367617)@32<=sL0x2001513c,L0x2001513c<=s3367617@32,
(-3367617)@32<=sL0x20015160,L0x20015160<=s3367617@32,
(-3367617)@32<=sL0x20015e98,L0x20015e98<=s3367617@32,
(-3367617)@32<=sL0x20015ebc,L0x20015ebc<=s3367617@32,
(-3367617)@32<=sL0x20015ee0,L0x20015ee0<=s3367617@32
,
(-3367617)@32<=sL0x20015184,L0x20015184<=s3367617@32,
(-3367617)@32<=sL0x200151a8,L0x200151a8<=s3367617@32,
(-3367617)@32<=sL0x200151cc,L0x200151cc<=s3367617@32,
(-3367617)@32<=sL0x20015f04,L0x20015f04<=s3367617@32,
(-3367617)@32<=sL0x20015f28,L0x20015f28<=s3367617@32,
(-3367617)@32<=sL0x20015f4c,L0x20015f4c<=s3367617@32
,
(-3367617)@32<=sL0x200151f0,L0x200151f0<=s3367617@32,
(-3367617)@32<=sL0x20015214,L0x20015214<=s3367617@32,
(-3367617)@32<=sL0x20015238,L0x20015238<=s3367617@32,
(-3367617)@32<=sL0x20015f70,L0x20015f70<=s3367617@32,
(-3367617)@32<=sL0x20015f94,L0x20015f94<=s3367617@32,
(-3367617)@32<=sL0x20015fb8,L0x20015fb8<=s3367617@32
,
(-3367617)@32<=sL0x2001525c,L0x2001525c<=s3367617@32,
(-3367617)@32<=sL0x20015280,L0x20015280<=s3367617@32,
(-3367617)@32<=sL0x200152a4,L0x200152a4<=s3367617@32,
(-3367617)@32<=sL0x20015fdc,L0x20015fdc<=s3367617@32,
(-3367617)@32<=sL0x20016000,L0x20016000<=s3367617@32,
(-3367617)@32<=sL0x20016024,L0x20016024<=s3367617@32
,
(-3367617)@32<=sL0x200152c8,L0x200152c8<=s3367617@32,
(-3367617)@32<=sL0x200152ec,L0x200152ec<=s3367617@32,
(-3367617)@32<=sL0x20015310,L0x20015310<=s3367617@32,
(-3367617)@32<=sL0x20016048,L0x20016048<=s3367617@32,
(-3367617)@32<=sL0x2001606c,L0x2001606c<=s3367617@32,
(-3367617)@32<=sL0x20016090,L0x20016090<=s3367617@32
,
(-3367617)@32<=sL0x20015334,L0x20015334<=s3367617@32,
(-3367617)@32<=sL0x20015358,L0x20015358<=s3367617@32,
(-3367617)@32<=sL0x2001537c,L0x2001537c<=s3367617@32,
(-3367617)@32<=sL0x200160b4,L0x200160b4<=s3367617@32,
(-3367617)@32<=sL0x200160d8,L0x200160d8<=s3367617@32,
(-3367617)@32<=sL0x200160fc,L0x200160fc<=s3367617@32
,
(-3367617)@32<=sL0x200153a0,L0x200153a0<=s3367617@32,
(-3367617)@32<=sL0x200153c4,L0x200153c4<=s3367617@32,
(-3367617)@32<=sL0x200153e8,L0x200153e8<=s3367617@32,
(-3367617)@32<=sL0x20016120,L0x20016120<=s3367617@32,
(-3367617)@32<=sL0x20016144,L0x20016144<=s3367617@32,
(-3367617)@32<=sL0x20016168,L0x20016168<=s3367617@32
,
(-3367617)@32<=sL0x2001540c,L0x2001540c<=s3367617@32,
(-3367617)@32<=sL0x20015430,L0x20015430<=s3367617@32,
(-3367617)@32<=sL0x20015454,L0x20015454<=s3367617@32,
(-3367617)@32<=sL0x2001618c,L0x2001618c<=s3367617@32,
(-3367617)@32<=sL0x200161b0,L0x200161b0<=s3367617@32,
(-3367617)@32<=sL0x200161d4,L0x200161d4<=s3367617@32
,
(-3367617)@32<=sL0x20015478,L0x20015478<=s3367617@32,
(-3367617)@32<=sL0x2001549c,L0x2001549c<=s3367617@32,
(-3367617)@32<=sL0x200154c0,L0x200154c0<=s3367617@32,
(-3367617)@32<=sL0x200161f8,L0x200161f8<=s3367617@32,
(-3367617)@32<=sL0x2001621c,L0x2001621c<=s3367617@32,
(-3367617)@32<=sL0x20016240,L0x20016240<=s3367617@32
,
(-3367617)@32<=sL0x200154e4,L0x200154e4<=s3367617@32,
(-3367617)@32<=sL0x20015508,L0x20015508<=s3367617@32,
(-3367617)@32<=sL0x2001552c,L0x2001552c<=s3367617@32,
(-3367617)@32<=sL0x20016264,L0x20016264<=s3367617@32,
(-3367617)@32<=sL0x20016288,L0x20016288<=s3367617@32,
(-3367617)@32<=sL0x200162ac,L0x200162ac<=s3367617@32
,
(-3367617)@32<=sL0x20015550,L0x20015550<=s3367617@32,
(-3367617)@32<=sL0x20015574,L0x20015574<=s3367617@32,
(-3367617)@32<=sL0x20015598,L0x20015598<=s3367617@32,
(-3367617)@32<=sL0x200162d0,L0x200162d0<=s3367617@32,
(-3367617)@32<=sL0x200162f4,L0x200162f4<=s3367617@32,
(-3367617)@32<=sL0x20016318,L0x20016318<=s3367617@32
,
(-3367617)@32<=sL0x200155bc,L0x200155bc<=s3367617@32,
(-3367617)@32<=sL0x200155e0,L0x200155e0<=s3367617@32,
(-3367617)@32<=sL0x20015604,L0x20015604<=s3367617@32,
(-3367617)@32<=sL0x2001633c,L0x2001633c<=s3367617@32,
(-3367617)@32<=sL0x20016360,L0x20016360<=s3367617@32,
(-3367617)@32<=sL0x20016384,L0x20016384<=s3367617@32
] prove with [ precondition ];





(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x200186f4; Value = 0x00000ffd; PC = 0x805623c *)
mov r4 L0x200186f4;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018b74; Value = 0x00000003; PC = 0x8056240 *)
mov r6 L0x20018b74;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x20018934; Value = 0x00000ffd; PC = 0x8056244 *)
mov r9 L0x20018934;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2100@sint32 : and [cf2100 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2400@sint32 : and [cf2400 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2700@sint32 : and [cf2700 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200148ac; PC = 0x8056290 *)
mov L0x200148ac r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200148d0; PC = 0x8056294 *)
mov L0x200148d0 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x200148f4; PC = 0x8056298 *)
mov L0x200148f4 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x2001562c; PC = 0x805629c *)
mov L0x2001562c r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015650; PC = 0x80562a0 *)
mov L0x20015650 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015674; PC = 0x80562a4 *)
mov L0x20015674 r9;



(******************** offset 2, 1,  0 ********************)


(**************** CUT 160, - *****************)

ecut and [
eqmod cf2100 f2100 2048, eqmod cf2400 f2400 2048, eqmod cf2700 f2700 2048,
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x200148ac*x**2*y**1*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x200148d0*x**2*y**1*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x200148f4*x**2*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x2001562c*x**2*y**1*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x20015650*x**2*y**1*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x20015674*x**2*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x2001857a; Value = 0x00000000; PC = 0x80562a8 *)
mov r4 L0x2001857a;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x200189fa; Value = 0x00030003; PC = 0x80562ac *)
mov r6 L0x200189fa;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x200187ba; Value = 0x0ffd0003; PC = 0x80562b0 *)
mov r9 L0x200187ba;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2101@sint32 : and [cf2101 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2401@sint32 : and [cf2401 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2701@sint32 : and [cf2701 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014918; PC = 0x80562fc *)
mov L0x20014918 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x2001493c; PC = 0x8056300 *)
mov L0x2001493c r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014960; PC = 0x8056304 *)
mov L0x20014960 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015698; PC = 0x8056308 *)
mov L0x20015698 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200156bc; PC = 0x805630c *)
mov L0x200156bc r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200156e0; PC = 0x8056310 *)
mov L0x200156e0 r9;



(******************** offset 2, 1,  1 ********************)


(**************** CUT 161, - *****************)

ecut and [
eqmod cf2101 f2101 2048, eqmod cf2401 f2401 2048, eqmod cf2701 f2701 2048,
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x20014918*x**2*y**1*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x2001493c*x**2*y**1*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x20014960*x**2*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x20015698*x**2*y**1*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x200156bc*x**2*y**1*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x200156e0*x**2*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018ac0; Value = 0x00000003; PC = 0x8056314 *)
mov r5 L0x20018ac0;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x20018880; Value = 0x0ffd0000; PC = 0x8056318 *)
mov r6 L0x20018880;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x20018640; Value = 0x00030ffd; PC = 0x805631c *)
mov r9 L0x20018640;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2102@sint32 : and [cf2102 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2402@sint32 : and [cf2402 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2702@sint32 : and [cf2702 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014984; PC = 0x8056370 *)
mov L0x20014984 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200149a8; PC = 0x8056374 *)
mov L0x200149a8 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200149cc; PC = 0x8056378 *)
mov L0x200149cc r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015704; PC = 0x805637c *)
mov L0x20015704 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015728; PC = 0x8056380 *)
mov L0x20015728 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x2001574c; PC = 0x8056384 *)
mov L0x2001574c r9;



(******************** offset 2, 1,  2 ********************)


(**************** CUT 162, - *****************)

ecut and [
eqmod cf2102 f2102 2048, eqmod cf2402 f2402 2048, eqmod cf2702 f2702 2048,
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x20014984*x**2*y**1*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x200149a8*x**2*y**1*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x200149cc*x**2*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x20015704*x**2*y**1*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x20015728*x**2*y**1*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x2001574c*x**2*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x20018946; Value = 0x0ffd0003; PC = 0x8056388 *)
mov r5 L0x20018946;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x20018706; Value = 0x0ffd0ffd; PC = 0x805638c *)
mov r6 L0x20018706;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018b86; Value = 0x00030003; PC = 0x8056390 *)
mov r8 L0x20018b86;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2103@sint32 : and [cf2103 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2403@sint32 : and [cf2403 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2703@sint32 : and [cf2703 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x200149f0; PC = 0x80563d0 *)
mov L0x200149f0 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014a14; PC = 0x80563d4 *)
mov L0x20014a14 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014a38; PC = 0x80563d8 *)
mov L0x20014a38 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015770; PC = 0x80563dc *)
mov L0x20015770 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015794; PC = 0x80563e0 *)
mov L0x20015794 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x200157b8; PC = 0x80563e4 *)
mov L0x200157b8 r9;



(******************** offset 2, 1,  3 ********************)


(**************** CUT 163, - *****************)

ecut and [
eqmod cf2103 f2103 2048, eqmod cf2403 f2403 2048, eqmod cf2703 f2703 2048,
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x200149f0*x**2*y**1*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x20014a14*x**2*y**1*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x20014a38*x**2*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x20015770*x**2*y**1*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x20015794*x**2*y**1*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x200157b8*x**2*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x200187cc; Value = 0x00030000; PC = 0x80563e8 *)
mov r5 L0x200187cc;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x2001858c; Value = 0x00030ffd; PC = 0x80563ec *)
mov r6 L0x2001858c;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018a0c; Value = 0x0ffd0000; PC = 0x80563f0 *)
mov r8 L0x20018a0c;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2104@sint32 : and [cf2104 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2404@sint32 : and [cf2404 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2704@sint32 : and [cf2704 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014a5c; PC = 0x8056430 *)
mov L0x20014a5c r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014a80; PC = 0x8056434 *)
mov L0x20014a80 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014aa4; PC = 0x8056438 *)
mov L0x20014aa4 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x200157dc; PC = 0x805643c *)
mov L0x200157dc r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015800; PC = 0x8056440 *)
mov L0x20015800 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015824; PC = 0x8056444 *)
mov L0x20015824 r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;



(******************** offset 2, 1,  4 ********************)


(**************** CUT 164, - *****************)

ecut and [
eqmod cf2104 f2104 2048, eqmod cf2404 f2404 2048, eqmod cf2704 f2704 2048,
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x20014a5c*x**2*y**1*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x20014a80*x**2*y**1*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x20014aa4*x**2*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x200157dc*x**2*y**1*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x20015800*x**2*y**1*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x20015824*x**2*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #222]	; 0xde                   #! EA = L0x20018652; Value = 0x00030000; PC = 0x8056454 *)
mov r5 L0x20018652;
(* ldrsh.w	r7, [lr, #1374]	; 0x55e                 #! EA = L0x20018ad2; Value = 0x00030003; PC = 0x8056458 *)
mov r7 L0x20018ad2;
(* ldrsh.w	r8, [lr, #798]	; 0x31e                  #! EA = L0x20018892; Value = 0x00000000; PC = 0x805645c *)
mov r8 L0x20018892;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056460 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2105@sint32 : and [cf2105 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056464 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2405@sint32 : and [cf2405 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056468 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2705@sint32 : and [cf2705 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x805646c *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056470 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8056474 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056478 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x805647c *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056480 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x8056482 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x8056484 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056488 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x805648c *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056490 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x8056494 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056498 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x805649c *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80564a0 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x80564a4 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x80564a8 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x80564ac *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014ac8; PC = 0x80564b0 *)
mov L0x20014ac8 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014aec; PC = 0x80564b4 *)
mov L0x20014aec r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014b10; PC = 0x80564b8 *)
mov L0x20014b10 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015848; PC = 0x80564bc *)
mov L0x20015848 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x2001586c; PC = 0x80564c0 *)
mov L0x2001586c r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015890; PC = 0x80564c4 *)
mov L0x20015890 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80564c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 2, 1,  5 ********************)


(**************** CUT 165, - *****************)

ecut and [
eqmod cf2105 f2105 2048, eqmod cf2405 f2405 2048, eqmod cf2705 f2705 2048,
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x20014ac8*x**2*y**1*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x20014aec*x**2*y**1*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x20014b10*x**2*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x20015848*x**2*y**1*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x2001586c*x**2*y**1*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x20015890*x**2*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1572]	; 0x624                 #! EA = L0x20018b98; Value = 0x00030ffd; PC = 0x80564cc *)
mov r4 L0x20018b98;
(* ldrsh.w	r7, [lr, #996]	; 0x3e4                  #! EA = L0x20018958; Value = 0x00000000; PC = 0x80564d0 *)
mov r7 L0x20018958;
(* ldrsh.w	r8, [lr, #420]	; 0x1a4                  #! EA = L0x20018718; Value = 0x00000003; PC = 0x80564d4 *)
mov r8 L0x20018718;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80564d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2106@sint32 : and [cf2106 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80564dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2406@sint32 : and [cf2406 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80564e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2706@sint32 : and [cf2706 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80564e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80564e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80564ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80564f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80564f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80564f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80564fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80564fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056500 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056504 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056508 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805650c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805650e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056510 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056514 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056518 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805651c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014b34; PC = 0x8056520 *)
mov L0x20014b34 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014b58; PC = 0x8056524 *)
mov L0x20014b58 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014b7c; PC = 0x8056528 *)
mov L0x20014b7c r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200158b4; PC = 0x805652c *)
mov L0x200158b4 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200158d8; PC = 0x8056530 *)
mov L0x200158d8 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200158fc; PC = 0x8056534 *)
mov L0x200158fc r9;



(******************** offset 2, 1,  6 ********************)


(**************** CUT 166, - *****************)

ecut and [
eqmod cf2106 f2106 2048, eqmod cf2406 f2406 2048, eqmod cf2706 f2706 2048,
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x20014b34*x**2*y**1*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x20014b58*x**2*y**1*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x20014b7c*x**2*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x200158b4*x**2*y**1*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x200158d8*x**2*y**1*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x200158fc*x**2*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1194]	; 0x4aa                 #! EA = L0x20018a1e; Value = 0x00000003; PC = 0x8056538 *)
mov r4 L0x20018a1e;
(* ldrsh.w	r7, [lr, #618]	; 0x26a                  #! EA = L0x200187de; Value = 0x00030ffd; PC = 0x805653c *)
mov r7 L0x200187de;
(* ldrsh.w	r8, [lr, #42]	; 0x2a                    #! EA = L0x2001859e; Value = 0x0ffd0000; PC = 0x8056540 *)
mov r8 L0x2001859e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056544 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2107@sint32 : and [cf2107 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056548 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2407@sint32 : and [cf2407 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805654c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2707@sint32 : and [cf2707 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056550 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056554 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056558 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805655c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056560 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056564 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056566 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056568 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805656c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056570 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056574 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056578 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805657a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805657c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056580 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056584 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056588 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014ba0; PC = 0x805658c *)
mov L0x20014ba0 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014bc4; PC = 0x8056590 *)
mov L0x20014bc4 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014be8; PC = 0x8056594 *)
mov L0x20014be8 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015920; PC = 0x8056598 *)
mov L0x20015920 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015944; PC = 0x805659c *)
mov L0x20015944 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015968; PC = 0x80565a0 *)
mov L0x20015968 r9;



(******************** offset 2, 1,  7 ********************)


(**************** CUT 167, - *****************)

ecut and [
eqmod cf2107 f2107 2048, eqmod cf2407 f2407 2048, eqmod cf2707 f2707 2048,
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20014ba0*x**2*y**1*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20014bc4*x**2*y**1*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20014be8*x**2*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20015920*x**2*y**1*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20015944*x**2*y**1*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20015968*x**2*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #816]	; 0x330                  #! EA = L0x200188a4; Value = 0x0ffd0000; PC = 0x80565a4 *)
mov r4 L0x200188a4;
(* ldrsh.w	r7, [lr, #240]	; 0xf0                   #! EA = L0x20018664; Value = 0x0ffd0000; PC = 0x80565a8 *)
mov r7 L0x20018664;
(* ldrsh.w	r9, [lr, #1392]	; 0x570                 #! EA = L0x20018ae4; Value = 0x0ffd0003; PC = 0x80565ac *)
mov r9 L0x20018ae4;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80565b0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2108@sint32 : and [cf2108 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80565b4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2408@sint32 : and [cf2408 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80565b8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2708@sint32 : and [cf2708 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x80565bc *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x80565c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x80565c4 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x80565c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x80565cc *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x80565d0 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x80565d4 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80565d8 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x80565dc *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x80565e0 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x80565e2 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80565e4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014c0c; PC = 0x80565e8 *)
mov L0x20014c0c r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014c30; PC = 0x80565ec *)
mov L0x20014c30 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014c54; PC = 0x80565f0 *)
mov L0x20014c54 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x2001598c; PC = 0x80565f4 *)
mov L0x2001598c r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x200159b0; PC = 0x80565f8 *)
mov L0x200159b0 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200159d4; PC = 0x80565fc *)
mov L0x200159d4 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056600 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056604 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805623c <_Good_loop1_inner>              #! PC = 0x8056608 *)
#b.w	0x805623c <_Good_loop1_inner>              #! 0x8056608 = 0x8056608;



(******************** offset 2, 1,  8 ********************)


(**************** CUT 168, - *****************)

ecut and [
eqmod cf2108 f2108 2048, eqmod cf2408 f2408 2048, eqmod cf2708 f2708 2048,
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x20014c0c*x**2*y**1*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x20014c30*x**2*y**1*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x20014c54*x**2*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x2001598c*x**2*y**1*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x200159b0*x**2*y**1*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x200159d4*x**2*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x2001872a; Value = 0x0ffd0ffd; PC = 0x805623c *)
mov r4 L0x2001872a;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018baa; Value = 0x0ffd0000; PC = 0x8056240 *)
mov r6 L0x20018baa;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x2001896a; Value = 0x00030000; PC = 0x8056244 *)
mov r9 L0x2001896a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2109@sint32 : and [cf2109 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2409@sint32 : and [cf2409 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2709@sint32 : and [cf2709 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014c78; PC = 0x8056290 *)
mov L0x20014c78 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014c9c; PC = 0x8056294 *)
mov L0x20014c9c r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014cc0; PC = 0x8056298 *)
mov L0x20014cc0 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200159f8; PC = 0x805629c *)
mov L0x200159f8 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015a1c; PC = 0x80562a0 *)
mov L0x20015a1c r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015a40; PC = 0x80562a4 *)
mov L0x20015a40 r9;



(******************** offset 2, 1,  9 ********************)


(**************** CUT 169, - *****************)

ecut and [
eqmod cf2109 f2109 2048, eqmod cf2409 f2409 2048, eqmod cf2709 f2709 2048,
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x20014c78*x**2*y**1*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x20014c9c*x**2*y**1*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x20014cc0*x**2*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x200159f8*x**2*y**1*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x20015a1c*x**2*y**1*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x20015a40*x**2*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x200185b0; Value = 0x00000000; PC = 0x80562a8 *)
mov r4 L0x200185b0;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x20018a30; Value = 0x00000000; PC = 0x80562ac *)
mov r6 L0x20018a30;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x200187f0; Value = 0x00030003; PC = 0x80562b0 *)
mov r9 L0x200187f0;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2110@sint32 : and [cf2110 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2410@sint32 : and [cf2410 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2710@sint32 : and [cf2710 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014ce4; PC = 0x80562fc *)
mov L0x20014ce4 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014d08; PC = 0x8056300 *)
mov L0x20014d08 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014d2c; PC = 0x8056304 *)
mov L0x20014d2c r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015a64; PC = 0x8056308 *)
mov L0x20015a64 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015a88; PC = 0x805630c *)
mov L0x20015a88 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015aac; PC = 0x8056310 *)
mov L0x20015aac r9;



(******************** offset 2, 1, 10 ********************)


(**************** CUT 170, - *****************)

ecut and [
eqmod cf2110 f2110 2048, eqmod cf2410 f2410 2048, eqmod cf2710 f2710 2048,
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20014ce4*x**2*y**1*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20014d08*x**2*y**1*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20014d2c*x**2*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20015a64*x**2*y**1*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20015a88*x**2*y**1*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20015aac*x**2*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018af6; Value = 0x0ffd0000; PC = 0x8056314 *)
mov r5 L0x20018af6;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x200188b6; Value = 0x00030ffd; PC = 0x8056318 *)
mov r6 L0x200188b6;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x20018676; Value = 0x00030ffd; PC = 0x805631c *)
mov r9 L0x20018676;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2111@sint32 : and [cf2111 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2411@sint32 : and [cf2411 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2711@sint32 : and [cf2711 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014d50; PC = 0x8056370 *)
mov L0x20014d50 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014d74; PC = 0x8056374 *)
mov L0x20014d74 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014d98; PC = 0x8056378 *)
mov L0x20014d98 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015ad0; PC = 0x805637c *)
mov L0x20015ad0 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015af4; PC = 0x8056380 *)
mov L0x20015af4 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015b18; PC = 0x8056384 *)
mov L0x20015b18 r9;



(******************** offset 2, 1, 11 ********************)


(**************** CUT 171, - *****************)

ecut and [
eqmod cf2111 f2111 2048, eqmod cf2411 f2411 2048, eqmod cf2711 f2711 2048,
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20014d50*x**2*y**1*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20014d74*x**2*y**1*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20014d98*x**2*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20015ad0*x**2*y**1*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20015af4*x**2*y**1*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20015b18*x**2*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x2001897c; Value = 0x0ffd0ffd; PC = 0x8056388 *)
mov r5 L0x2001897c;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x2001873c; Value = 0x00030ffd; PC = 0x805638c *)
mov r6 L0x2001873c;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018bbc; Value = 0x0ffd0ffd; PC = 0x8056390 *)
mov r8 L0x20018bbc;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2112@sint32 : and [cf2112 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2412@sint32 : and [cf2412 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2712@sint32 : and [cf2712 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20014dbc; PC = 0x80563d0 *)
mov L0x20014dbc r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014de0; PC = 0x80563d4 *)
mov L0x20014de0 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014e04; PC = 0x80563d8 *)
mov L0x20014e04 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015b3c; PC = 0x80563dc *)
mov L0x20015b3c r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015b60; PC = 0x80563e0 *)
mov L0x20015b60 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015b84; PC = 0x80563e4 *)
mov L0x20015b84 r9;



(******************** offset 2, 1, 12 ********************)


(**************** CUT 172, - *****************)

ecut and [
eqmod cf2112 f2112 2048, eqmod cf2412 f2412 2048, eqmod cf2712 f2712 2048,
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20014dbc*x**2*y**1*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20014de0*x**2*y**1*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20014e04*x**2*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20015b3c*x**2*y**1*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20015b60*x**2*y**1*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20015b84*x**2*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x20018802; Value = 0x00030000; PC = 0x80563e8 *)
mov r5 L0x20018802;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x200185c2; Value = 0x00000003; PC = 0x80563ec *)
mov r6 L0x200185c2;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018a42; Value = 0x00030003; PC = 0x80563f0 *)
mov r8 L0x20018a42;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2113@sint32 : and [cf2113 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2413@sint32 : and [cf2413 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2713@sint32 : and [cf2713 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014e28; PC = 0x8056430 *)
mov L0x20014e28 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014e4c; PC = 0x8056434 *)
mov L0x20014e4c r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014e70; PC = 0x8056438 *)
mov L0x20014e70 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015ba8; PC = 0x805643c *)
mov L0x20015ba8 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015bcc; PC = 0x8056440 *)
mov L0x20015bcc r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015bf0; PC = 0x8056444 *)
mov L0x20015bf0 r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;



(******************** offset 2, 1, 13 ********************)


(**************** CUT 173, - *****************)

ecut and [
eqmod cf2113 f2113 2048, eqmod cf2413 f2413 2048, eqmod cf2713 f2713 2048,
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20014e28*x**2*y**1*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20014e4c*x**2*y**1*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20014e70*x**2*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20015ba8*x**2*y**1*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20015bcc*x**2*y**1*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20015bf0*x**2*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #222]	; 0xde                   #! EA = L0x20018688; Value = 0x00030000; PC = 0x8056454 *)
mov r5 L0x20018688;
(* ldrsh.w	r7, [lr, #1374]	; 0x55e                 #! EA = L0x20018b08; Value = 0x00000ffd; PC = 0x8056458 *)
mov r7 L0x20018b08;
(* ldrsh.w	r8, [lr, #798]	; 0x31e                  #! EA = L0x200188c8; Value = 0x00030000; PC = 0x805645c *)
mov r8 L0x200188c8;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056460 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2114@sint32 : and [cf2114 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056464 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2414@sint32 : and [cf2414 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056468 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2714@sint32 : and [cf2714 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x805646c *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056470 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8056474 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056478 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x805647c *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056480 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x8056482 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x8056484 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056488 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x805648c *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056490 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x8056494 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056498 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x805649c *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80564a0 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x80564a4 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x80564a8 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x80564ac *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014e94; PC = 0x80564b0 *)
mov L0x20014e94 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014eb8; PC = 0x80564b4 *)
mov L0x20014eb8 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014edc; PC = 0x80564b8 *)
mov L0x20014edc r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015c14; PC = 0x80564bc *)
mov L0x20015c14 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015c38; PC = 0x80564c0 *)
mov L0x20015c38 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015c5c; PC = 0x80564c4 *)
mov L0x20015c5c r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80564c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 2, 1, 14 ********************)


(**************** CUT 174, - *****************)

ecut and [
eqmod cf2114 f2114 2048, eqmod cf2414 f2414 2048, eqmod cf2714 f2714 2048,
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20014e94*x**2*y**1*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20014eb8*x**2*y**1*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20014edc*x**2*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20015c14*x**2*y**1*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20015c38*x**2*y**1*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20015c5c*x**2*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1572]	; 0x624                 #! EA = L0x20018bce; Value = 0x00000000; PC = 0x80564cc *)
mov r4 L0x20018bce;
(* ldrsh.w	r7, [lr, #996]	; 0x3e4                  #! EA = L0x2001898e; Value = 0x00030ffd; PC = 0x80564d0 *)
mov r7 L0x2001898e;
(* ldrsh.w	r8, [lr, #420]	; 0x1a4                  #! EA = L0x2001874e; Value = 0x0ffd0000; PC = 0x80564d4 *)
mov r8 L0x2001874e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80564d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2115@sint32 : and [cf2115 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80564dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2415@sint32 : and [cf2415 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80564e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2715@sint32 : and [cf2715 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80564e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80564e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80564ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80564f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80564f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80564f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80564fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80564fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056500 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056504 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056508 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805650c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805650e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056510 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056514 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056518 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805651c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014f00; PC = 0x8056520 *)
mov L0x20014f00 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014f24; PC = 0x8056524 *)
mov L0x20014f24 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014f48; PC = 0x8056528 *)
mov L0x20014f48 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015c80; PC = 0x805652c *)
mov L0x20015c80 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015ca4; PC = 0x8056530 *)
mov L0x20015ca4 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015cc8; PC = 0x8056534 *)
mov L0x20015cc8 r9;



(******************** offset 2, 1, 15 ********************)


(**************** CUT 175, - *****************)

ecut and [
eqmod cf2115 f2115 2048, eqmod cf2415 f2415 2048, eqmod cf2715 f2715 2048,
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20014f00*x**2*y**1*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20014f24*x**2*y**1*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20014f48*x**2*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20015c80*x**2*y**1*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20015ca4*x**2*y**1*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20015cc8*x**2*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1194]	; 0x4aa                 #! EA = L0x20018a54; Value = 0x0ffd0ffd; PC = 0x8056538 *)
mov r4 L0x20018a54;
(* ldrsh.w	r7, [lr, #618]	; 0x26a                  #! EA = L0x20018814; Value = 0x00030000; PC = 0x805653c *)
mov r7 L0x20018814;
(* ldrsh.w	r8, [lr, #42]	; 0x2a                    #! EA = L0x200185d4; Value = 0x00000003; PC = 0x8056540 *)
mov r8 L0x200185d4;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056544 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2116@sint32 : and [cf2116 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056548 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2416@sint32 : and [cf2416 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805654c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2716@sint32 : and [cf2716 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056550 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056554 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056558 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805655c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056560 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056564 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056566 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056568 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805656c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056570 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056574 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056578 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805657a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805657c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056580 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056584 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056588 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014f6c; PC = 0x805658c *)
mov L0x20014f6c r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014f90; PC = 0x8056590 *)
mov L0x20014f90 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014fb4; PC = 0x8056594 *)
mov L0x20014fb4 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015cec; PC = 0x8056598 *)
mov L0x20015cec r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015d10; PC = 0x805659c *)
mov L0x20015d10 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015d34; PC = 0x80565a0 *)
mov L0x20015d34 r9;



(******************** offset 2, 1, 16 ********************)


(**************** CUT 176, - *****************)

ecut and [
eqmod cf2116 f2116 2048, eqmod cf2416 f2416 2048, eqmod cf2716 f2716 2048,
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20014f6c*x**2*y**1*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20014f90*x**2*y**1*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20014fb4*x**2*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20015cec*x**2*y**1*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20015d10*x**2*y**1*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20015d34*x**2*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #816]	; 0x330                  #! EA = L0x200188da; Value = 0x00000000; PC = 0x80565a4 *)
mov r4 L0x200188da;
(* ldrsh.w	r7, [lr, #240]	; 0xf0                   #! EA = L0x2001869a; Value = 0x00000ffd; PC = 0x80565a8 *)
mov r7 L0x2001869a;
(* ldrsh.w	r9, [lr, #1392]	; 0x570                 #! EA = L0x20018b1a; Value = 0x0ffd0003; PC = 0x80565ac *)
mov r9 L0x20018b1a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80565b0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2117@sint32 : and [cf2117 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80565b4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2417@sint32 : and [cf2417 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80565b8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2717@sint32 : and [cf2717 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x80565bc *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x80565c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x80565c4 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x80565c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x80565cc *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x80565d0 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x80565d4 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80565d8 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x80565dc *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x80565e0 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x80565e2 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80565e4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014fd8; PC = 0x80565e8 *)
mov L0x20014fd8 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014ffc; PC = 0x80565ec *)
mov L0x20014ffc r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015020; PC = 0x80565f0 *)
mov L0x20015020 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015d58; PC = 0x80565f4 *)
mov L0x20015d58 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015d7c; PC = 0x80565f8 *)
mov L0x20015d7c r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015da0; PC = 0x80565fc *)
mov L0x20015da0 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056600 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056604 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805623c <_Good_loop1_inner>              #! PC = 0x8056608 *)
#b.w	0x805623c <_Good_loop1_inner>              #! 0x8056608 = 0x8056608;



(******************** offset 2, 1, 17 ********************)


(**************** CUT 177, - *****************)

ecut and [
eqmod cf2117 f2117 2048, eqmod cf2417 f2417 2048, eqmod cf2717 f2717 2048,
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20014fd8*x**2*y**1*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20014ffc*x**2*y**1*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20015020*x**2*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20015d58*x**2*y**1*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20015d7c*x**2*y**1*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20015da0*x**2*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x20018760; Value = 0x0ffd0ffd; PC = 0x805623c *)
mov r4 L0x20018760;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018be0; Value = 0x00000000; PC = 0x8056240 *)
mov r6 L0x20018be0;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x200189a0; Value = 0x00000003; PC = 0x8056244 *)
mov r9 L0x200189a0;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2118@sint32 : and [cf2118 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2418@sint32 : and [cf2418 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2718@sint32 : and [cf2718 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015044; PC = 0x8056290 *)
mov L0x20015044 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015068; PC = 0x8056294 *)
mov L0x20015068 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x2001508c; PC = 0x8056298 *)
mov L0x2001508c r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015dc4; PC = 0x805629c *)
mov L0x20015dc4 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015de8; PC = 0x80562a0 *)
mov L0x20015de8 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015e0c; PC = 0x80562a4 *)
mov L0x20015e0c r9;



(******************** offset 2, 1, 18 ********************)


(**************** CUT 178, - *****************)

ecut and [
eqmod cf2118 f2118 2048, eqmod cf2418 f2418 2048, eqmod cf2718 f2718 2048,
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x20015044*x**2*y**1*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x20015068*x**2*y**1*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x2001508c*x**2*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x20015dc4*x**2*y**1*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x20015de8*x**2*y**1*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x20015e0c*x**2*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x200185e6; Value = 0x0ffd0003; PC = 0x80562a8 *)
mov r4 L0x200185e6;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x20018a66; Value = 0x0ffd0000; PC = 0x80562ac *)
mov r6 L0x20018a66;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x20018826; Value = 0x00030003; PC = 0x80562b0 *)
mov r9 L0x20018826;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2119@sint32 : and [cf2119 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2419@sint32 : and [cf2419 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2719@sint32 : and [cf2719 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x200150b0; PC = 0x80562fc *)
mov L0x200150b0 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200150d4; PC = 0x8056300 *)
mov L0x200150d4 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200150f8; PC = 0x8056304 *)
mov L0x200150f8 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015e30; PC = 0x8056308 *)
mov L0x20015e30 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015e54; PC = 0x805630c *)
mov L0x20015e54 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015e78; PC = 0x8056310 *)
mov L0x20015e78 r9;



(******************** offset 2, 1, 19 ********************)


(**************** CUT 179, - *****************)

ecut and [
eqmod cf2119 f2119 2048, eqmod cf2419 f2419 2048, eqmod cf2719 f2719 2048,
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x200150b0*x**2*y**1*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x200150d4*x**2*y**1*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x200150f8*x**2*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x20015e30*x**2*y**1*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x20015e54*x**2*y**1*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x20015e78*x**2*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018b2c; Value = 0x00000000; PC = 0x8056314 *)
mov r5 L0x20018b2c;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x200188ec; Value = 0x00030000; PC = 0x8056318 *)
mov r6 L0x200188ec;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x200186ac; Value = 0x0ffd0000; PC = 0x805631c *)
mov r9 L0x200186ac;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2120@sint32 : and [cf2120 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2420@sint32 : and [cf2420 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2720@sint32 : and [cf2720 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x2001511c; PC = 0x8056370 *)
mov L0x2001511c r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015140; PC = 0x8056374 *)
mov L0x20015140 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015164; PC = 0x8056378 *)
mov L0x20015164 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015e9c; PC = 0x805637c *)
mov L0x20015e9c r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015ec0; PC = 0x8056380 *)
mov L0x20015ec0 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015ee4; PC = 0x8056384 *)
mov L0x20015ee4 r9;



(******************** offset 2, 1, 20 ********************)


(**************** CUT 180, - *****************)

ecut and [
eqmod cf2120 f2120 2048, eqmod cf2420 f2420 2048, eqmod cf2720 f2720 2048,
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x2001511c*x**2*y**1*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x20015140*x**2*y**1*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x20015164*x**2*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x20015e9c*x**2*y**1*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x20015ec0*x**2*y**1*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x20015ee4*x**2*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x200189b2; Value = 0x00030000; PC = 0x8056388 *)
mov r5 L0x200189b2;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x20018772; Value = 0x00030ffd; PC = 0x805638c *)
mov r6 L0x20018772;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018bf2; Value = 0x00000000; PC = 0x8056390 *)
mov r8 L0x20018bf2;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2121@sint32 : and [cf2121 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2421@sint32 : and [cf2421 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2721@sint32 : and [cf2721 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015188; PC = 0x80563d0 *)
mov L0x20015188 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x200151ac; PC = 0x80563d4 *)
mov L0x200151ac r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200151d0; PC = 0x80563d8 *)
mov L0x200151d0 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015f08; PC = 0x80563dc *)
mov L0x20015f08 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015f2c; PC = 0x80563e0 *)
mov L0x20015f2c r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015f50; PC = 0x80563e4 *)
mov L0x20015f50 r9;



(******************** offset 2, 1, 21 ********************)


(**************** CUT 181, - *****************)

ecut and [
eqmod cf2121 f2121 2048, eqmod cf2421 f2421 2048, eqmod cf2721 f2721 2048,
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x20015188*x**2*y**1*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x200151ac*x**2*y**1*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x200151d0*x**2*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x20015f08*x**2*y**1*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x20015f2c*x**2*y**1*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x20015f50*x**2*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x20018838; Value = 0x00030ffd; PC = 0x80563e8 *)
mov r5 L0x20018838;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x200185f8; Value = 0x00000000; PC = 0x80563ec *)
mov r6 L0x200185f8;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018a78; Value = 0x00000000; PC = 0x80563f0 *)
mov r8 L0x20018a78;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2122@sint32 : and [cf2122 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2422@sint32 : and [cf2422 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2722@sint32 : and [cf2722 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200151f4; PC = 0x8056430 *)
mov L0x200151f4 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20015218; PC = 0x8056434 *)
mov L0x20015218 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x2001523c; PC = 0x8056438 *)
mov L0x2001523c r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015f74; PC = 0x805643c *)
mov L0x20015f74 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015f98; PC = 0x8056440 *)
mov L0x20015f98 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015fbc; PC = 0x8056444 *)
mov L0x20015fbc r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;



(******************** offset 2, 1, 22 ********************)


(**************** CUT 182, - *****************)

ecut and [
eqmod cf2122 f2122 2048, eqmod cf2422 f2422 2048, eqmod cf2722 f2722 2048,
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x200151f4*x**2*y**1*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x20015218*x**2*y**1*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x2001523c*x**2*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x20015f74*x**2*y**1*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x20015f98*x**2*y**1*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x20015fbc*x**2*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #222]	; 0xde                   #! EA = L0x200186be; Value = 0x00030000; PC = 0x8056454 *)
mov r5 L0x200186be;
(* ldrsh.w	r7, [lr, #1374]	; 0x55e                 #! EA = L0x20018b3e; Value = 0x00000ffd; PC = 0x8056458 *)
mov r7 L0x20018b3e;
(* ldrsh.w	r8, [lr, #798]	; 0x31e                  #! EA = L0x200188fe; Value = 0x00000ffd; PC = 0x805645c *)
mov r8 L0x200188fe;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056460 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2123@sint32 : and [cf2123 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056464 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2423@sint32 : and [cf2423 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056468 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2723@sint32 : and [cf2723 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x805646c *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056470 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x8056474 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x8056478 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x805647c *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x8056480 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x8056482 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x8056484 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x8056488 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x805648c *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x8056490 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x8056494 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056498 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x805649c *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x80564a0 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x80564a4 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x80564a8 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x80564ac *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20015260; PC = 0x80564b0 *)
mov L0x20015260 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20015284; PC = 0x80564b4 *)
mov L0x20015284 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x200152a8; PC = 0x80564b8 *)
mov L0x200152a8 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015fe0; PC = 0x80564bc *)
mov L0x20015fe0 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20016004; PC = 0x80564c0 *)
mov L0x20016004 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20016028; PC = 0x80564c4 *)
mov L0x20016028 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80564c8 *)
adds dc r0 r0 648@uint32;



(******************** offset 2, 1, 23 ********************)


(**************** CUT 183, - *****************)

ecut and [
eqmod cf2123 f2123 2048, eqmod cf2423 f2423 2048, eqmod cf2723 f2723 2048,
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x20015260*x**2*y**1*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x20015284*x**2*y**1*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x200152a8*x**2*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x20015fe0*x**2*y**1*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x20016004*x**2*y**1*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x20016028*x**2*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1572]	; 0x624                 #! EA = L0x20018c04; Value = 0x00000000; PC = 0x80564cc *)
mov r4 L0x20018c04;
(* ldrsh.w	r7, [lr, #996]	; 0x3e4                  #! EA = L0x200189c4; Value = 0x00000003; PC = 0x80564d0 *)
mov r7 L0x200189c4;
(* ldrsh.w	r8, [lr, #420]	; 0x1a4                  #! EA = L0x20018784; Value = 0x0ffd0ffd; PC = 0x80564d4 *)
mov r8 L0x20018784;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80564d8 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2124@sint32 : and [cf2124 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80564dc *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2424@sint32 : and [cf2424 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80564e0 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2724@sint32 : and [cf2724 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80564e4 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80564e8 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80564ec *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80564f0 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80564f4 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80564f8 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80564fa *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80564fc *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x8056500 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056504 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056508 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x805650c *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805650e *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x8056510 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056514 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056518 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x805651c *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200152cc; PC = 0x8056520 *)
mov L0x200152cc r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200152f0; PC = 0x8056524 *)
mov L0x200152f0 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015314; PC = 0x8056528 *)
mov L0x20015314 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x2001604c; PC = 0x805652c *)
mov L0x2001604c r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20016070; PC = 0x8056530 *)
mov L0x20016070 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20016094; PC = 0x8056534 *)
mov L0x20016094 r9;



(******************** offset 2, 1, 24 ********************)


(**************** CUT 184, - *****************)

ecut and [
eqmod cf2124 f2124 2048, eqmod cf2424 f2424 2048, eqmod cf2724 f2724 2048,
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x200152cc*x**2*y**1*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x200152f0*x**2*y**1*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x20015314*x**2*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x2001604c*x**2*y**1*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x20016070*x**2*y**1*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x20016094*x**2*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1194]	; 0x4aa                 #! EA = L0x20018a8a; Value = 0x0ffd0003; PC = 0x8056538 *)
mov r4 L0x20018a8a;
(* ldrsh.w	r7, [lr, #618]	; 0x26a                  #! EA = L0x2001884a; Value = 0x00000000; PC = 0x805653c *)
mov r7 L0x2001884a;
(* ldrsh.w	r8, [lr, #42]	; 0x2a                    #! EA = L0x2001860a; Value = 0x0ffd0000; PC = 0x8056540 *)
mov r8 L0x2001860a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056544 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2125@sint32 : and [cf2125 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056548 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2425@sint32 : and [cf2425 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805654c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2725@sint32 : and [cf2725 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056550 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056554 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056558 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805655c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056560 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056564 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056566 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056568 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805656c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056570 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056574 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056578 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805657a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805657c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056580 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056584 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056588 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015338; PC = 0x805658c *)
mov L0x20015338 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x2001535c; PC = 0x8056590 *)
mov L0x2001535c r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20015380; PC = 0x8056594 *)
mov L0x20015380 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200160b8; PC = 0x8056598 *)
mov L0x200160b8 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200160dc; PC = 0x805659c *)
mov L0x200160dc r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20016100; PC = 0x80565a0 *)
mov L0x20016100 r9;



(******************** offset 2, 1, 25 ********************)


(**************** CUT 185, - *****************)

ecut and [
eqmod cf2125 f2125 2048, eqmod cf2425 f2425 2048, eqmod cf2725 f2725 2048,
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x20015338*x**2*y**1*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x2001535c*x**2*y**1*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x20015380*x**2*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x200160b8*x**2*y**1*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x200160dc*x**2*y**1*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x20016100*x**2*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #816]	; 0x330                  #! EA = L0x20018910; Value = 0x00000000; PC = 0x80565a4 *)
mov r4 L0x20018910;
(* ldrsh.w	r7, [lr, #240]	; 0xf0                   #! EA = L0x200186d0; Value = 0x0ffd0000; PC = 0x80565a8 *)
mov r7 L0x200186d0;
(* ldrsh.w	r9, [lr, #1392]	; 0x570                 #! EA = L0x20018b50; Value = 0x0ffd0003; PC = 0x80565ac *)
mov r9 L0x20018b50;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80565b0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2126@sint32 : and [cf2126 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80565b4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2426@sint32 : and [cf2426 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80565b8 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2726@sint32 : and [cf2726 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x80565bc *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x80565c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x80565c4 *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x80565c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x80565cc *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x80565d0 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x80565d4 *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80565d8 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x80565dc *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x80565e0 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x80565e2 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80565e4 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200153a4; PC = 0x80565e8 *)
mov L0x200153a4 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200153c8; PC = 0x80565ec *)
mov L0x200153c8 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200153ec; PC = 0x80565f0 *)
mov L0x200153ec r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016124; PC = 0x80565f4 *)
mov L0x20016124 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016148; PC = 0x80565f8 *)
mov L0x20016148 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x2001616c; PC = 0x80565fc *)
mov L0x2001616c r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056600 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056604 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805623c <_Good_loop1_inner>              #! PC = 0x8056608 *)
#b.w	0x805623c <_Good_loop1_inner>              #! 0x8056608 = 0x8056608;



(******************** offset 2, 1, 26 ********************)


(**************** CUT 186, - *****************)

ecut and [
eqmod cf2126 f2126 2048, eqmod cf2426 f2426 2048, eqmod cf2726 f2726 2048,
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x200153a4*x**2*y**1*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x200153c8*x**2*y**1*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x200153ec*x**2*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x20016124*x**2*y**1*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x20016148*x**2*y**1*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x2001616c*x**2*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #384]	; 0x180                  #! EA = L0x20018796; Value = 0x00000ffd; PC = 0x805623c *)
mov r4 L0x20018796;
(* ldrsh.w	r6, [lr, #1536]	; 0x600                 #! EA = L0x20018c16; Value = 0x00000000; PC = 0x8056240 *)
mov r6 L0x20018c16;
(* ldrsh.w	r9, [lr, #960]	; 0x3c0                  #! EA = L0x200189d6; Value = 0x00000003; PC = 0x8056244 *)
mov r9 L0x200189d6;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056248 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2127@sint32 : and [cf2127 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x805624c *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2427@sint32 : and [cf2427 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056250 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2727@sint32 : and [cf2727 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x8056254 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056258 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x805625c *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056260 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x8056264 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056268 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x805626a *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x805626c *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056270 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x8056274 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056278 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x805627c *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x805627e *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056280 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x8056284 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056288 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x805628c *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015410; PC = 0x8056290 *)
mov L0x20015410 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015434; PC = 0x8056294 *)
mov L0x20015434 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015458; PC = 0x8056298 *)
mov L0x20015458 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016190; PC = 0x805629c *)
mov L0x20016190 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200161b4; PC = 0x80562a0 *)
mov L0x200161b4 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200161d8; PC = 0x80562a4 *)
mov L0x200161d8 r9;



(******************** offset 2, 1, 27 ********************)


(**************** CUT 187, - *****************)

ecut and [
eqmod cf2127 f2127 2048, eqmod cf2427 f2427 2048, eqmod cf2727 f2727 2048,
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x20015410*x**2*y**1*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x20015434*x**2*y**1*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x20015458*x**2*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x20016190*x**2*y**1*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x200161b4*x**2*y**1*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x200161d8*x**2*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #6]                            #! EA = L0x2001861c; Value = 0x00030000; PC = 0x80562a8 *)
mov r4 L0x2001861c;
(* ldrsh.w	r6, [lr, #1158]	; 0x486                 #! EA = L0x20018a9c; Value = 0x0ffd0003; PC = 0x80562ac *)
mov r6 L0x20018a9c;
(* ldrsh.w	r9, [lr, #582]	; 0x246                  #! EA = L0x2001885c; Value = 0x00000003; PC = 0x80562b0 *)
mov r9 L0x2001885c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80562b4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2128@sint32 : and [cf2128 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80562b8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2428@sint32 : and [cf2428 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80562bc *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2728@sint32 : and [cf2728 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80562c0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80562c4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80562c8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80562cc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80562d0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80562d4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80562d6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80562d8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80562dc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80562e0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80562e4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80562e8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80562ea *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80562ec *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80562f0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80562f4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80562f8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x2001547c; PC = 0x80562fc *)
mov L0x2001547c r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200154a0; PC = 0x8056300 *)
mov L0x200154a0 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200154c4; PC = 0x8056304 *)
mov L0x200154c4 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200161fc; PC = 0x8056308 *)
mov L0x200161fc r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20016220; PC = 0x805630c *)
mov L0x20016220 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20016244; PC = 0x8056310 *)
mov L0x20016244 r9;



(******************** offset 2, 1, 28 ********************)


(**************** CUT 188, - *****************)

ecut and [
eqmod cf2128 f2128 2048, eqmod cf2428 f2428 2048, eqmod cf2728 f2728 2048,
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x2001547c*x**2*y**1*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x200154a0*x**2*y**1*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x200154c4*x**2*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x200161fc*x**2*y**1*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x20016220*x**2*y**1*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x20016244*x**2*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1356]	; 0x54c                 #! EA = L0x20018b62; Value = 0x00000000; PC = 0x8056314 *)
mov r5 L0x20018b62;
(* ldrsh.w	r6, [lr, #780]	; 0x30c                  #! EA = L0x20018922; Value = 0x00000000; PC = 0x8056318 *)
mov r6 L0x20018922;
(* ldrsh.w	r9, [lr, #204]	; 0xcc                   #! EA = L0x200186e2; Value = 0x0ffd0000; PC = 0x805631c *)
mov r9 L0x200186e2;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056320 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2129@sint32 : and [cf2129 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056324 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2429@sint32 : and [cf2429 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056328 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2729@sint32 : and [cf2729 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x805632c *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x8056330 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056334 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056338 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x805633c *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x8056340 *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x8056342 *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x8056344 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x8056348 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x805634c *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x8056350 *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x8056354 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x8056358 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x805635c *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x8056360 *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x8056364 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x8056368 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x805636c *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200154e8; PC = 0x8056370 *)
mov L0x200154e8 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x2001550c; PC = 0x8056374 *)
mov L0x2001550c r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015530; PC = 0x8056378 *)
mov L0x20015530 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016268; PC = 0x805637c *)
mov L0x20016268 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x2001628c; PC = 0x8056380 *)
mov L0x2001628c r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200162b0; PC = 0x8056384 *)
mov L0x200162b0 r9;



(******************** offset 2, 1, 29 ********************)


(**************** CUT 189, - *****************)

ecut and [
eqmod cf2129 f2129 2048, eqmod cf2429 f2429 2048, eqmod cf2729 f2729 2048,
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x200154e8*x**2*y**1*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x2001550c*x**2*y**1*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x20015530*x**2*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x20016268*x**2*y**1*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x2001628c*x**2*y**1*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x200162b0*x**2*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #978]	; 0x3d2                  #! EA = L0x200189e8; Value = 0x00000003; PC = 0x8056388 *)
mov r5 L0x200189e8;
(* ldrsh.w	r6, [lr, #402]	; 0x192                  #! EA = L0x200187a8; Value = 0x0ffd0ffd; PC = 0x805638c *)
mov r6 L0x200187a8;
(* ldrsh.w	r8, [lr, #1554]	; 0x612                 #! EA = L0x20018c28; Value = 0x00000000; PC = 0x8056390 *)
mov r8 L0x20018c28;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x8056394 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2130@sint32 : and [cf2130 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056398 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2430@sint32 : and [cf2430 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805639c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2730@sint32 : and [cf2730 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80563a0 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x80563a4 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x80563a8 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x80563ac *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x80563b0 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x80563b4 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x80563b8 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x80563bc *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x80563c0 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x80563c4 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x80563c8 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x80563ca *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80563cc *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015554; PC = 0x80563d0 *)
mov L0x20015554 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20015578; PC = 0x80563d4 *)
mov L0x20015578 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x2001559c; PC = 0x80563d8 *)
mov L0x2001559c r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x200162d4; PC = 0x80563dc *)
mov L0x200162d4 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x200162f8; PC = 0x80563e0 *)
mov L0x200162f8 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x2001631c; PC = 0x80563e4 *)
mov L0x2001631c r9;



(******************** offset 2, 1, 30 ********************)


(**************** CUT 190, - *****************)

ecut and [
eqmod cf2130 f2130 2048, eqmod cf2430 f2430 2048, eqmod cf2730 f2730 2048,
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x20015554*x**2*y**1*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x20015578*x**2*y**1*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x2001559c*x**2*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x200162d4*x**2*y**1*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x200162f8*x**2*y**1*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x2001631c*x**2*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #600]	; 0x258                  #! EA = L0x2001886e; Value = 0x00030ffd; PC = 0x80563e8 *)
mov r5 L0x2001886e;
(* ldrsh.w	r6, [lr, #24]                           #! EA = L0x2001862e; Value = 0x00000ffd; PC = 0x80563ec *)
mov r6 L0x2001862e;
(* ldrsh.w	r8, [lr, #1176]	; 0x498                 #! EA = L0x20018aae; Value = 0x00000000; PC = 0x80563f0 *)
mov r8 L0x20018aae;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80563f4 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2131@sint32 : and [cf2131 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80563f8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2431@sint32 : and [cf2431 = r6] && true;
(* NOTE: bug fixed manually: sbfx	r8, r8, #0, #12 *)
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80563fc *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2731@sint32 : and [cf2731 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056400 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056404 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056408 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x805640c *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056410 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056414 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056418 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x805641c *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056420 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056424 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056428 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x805642a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805642c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200155c0; PC = 0x8056430 *)
mov L0x200155c0 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x200155e4; PC = 0x8056434 *)
mov L0x200155e4 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015608; PC = 0x8056438 *)
mov L0x20015608 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20016340; PC = 0x805643c *)
mov L0x20016340 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20016364; PC = 0x8056440 *)
mov L0x20016364 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20016388; PC = 0x8056444 *)
mov L0x20016388 r9;
(* vmov	r1, s3                                     #! PC = 0x8056448 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x805644c *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x805660c <_Good_loop1_inner_end>        #! PC = 0x8056450 *)
#beq.w	0x805660c <_Good_loop1_inner_end>        #! 0x8056450 = 0x8056450;
(* sub.w	lr, lr, #162	; 0xa2                       #! PC = 0x805660c *)
subs dc lr lr 162@uint32;
(* subw	r0, r0, #2916	; 0xb64                      #! PC = 0x8056610 *)
subc dc r0 r0 2916@uint32;
(* add.w	lr, lr, #2                                #! PC = 0x8056614 *)
adds dc lr lr 2@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8056618 *)
adds dc r0 r0 4@uint32;
(* cmp.w	r0, r12                                   #! PC = 0x805661c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8056234 <_Good_loop1>                  #! PC = 0x8056620 *)
#bne.w	0x8056234 <_Good_loop1>                  #! 0x8056620 = 0x8056620;
(* sub.w	lr, lr, #6                                #! PC = 0x8056624 *)
subs dc lr lr 6@uint32;
(* sub.w	r0, r0, #12                               #! PC = 0x8056628 *)
subs dc r0 r0 12@uint32;
(* add.w	r0, r0, #12                               #! PC = 0x805662c *)
adds dc r0 r0 12@uint32;
(* add.w	r12, r0, #12                              #! PC = 0x8056630 *)
adds dc r12 r0 12@uint32;
(* addw	r1, r0, #2916	; 0xb64                      #! PC = 0x8056634 *)
adds dc r1 r0 2916@uint32;
(* vmov	s3, r1                                     #! PC = 0x8056638 *)
mov s3 r1;



(******************** offset 2, 1, 31 ********************)


(**************** CUT 191, - *****************)

ecut and [
eqmod cf2131 f2131 2048, eqmod cf2431 f2431 2048, eqmod cf2731 f2731 2048,
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x200155c0*x**2*y**1*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x200155e4*x**2*y**1*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x20015608*x**2*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x20016340*x**2*y**1*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x20016364*x**2*y**1*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x20016388*x**2*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(**************** CUT -,   5 *****************)

rcut and [
(-3367617)@32<=sL0x200148ac,L0x200148ac<=s3367617@32,
(-3367617)@32<=sL0x200148d0,L0x200148d0<=s3367617@32,
(-3367617)@32<=sL0x200148f4,L0x200148f4<=s3367617@32,
(-3367617)@32<=sL0x2001562c,L0x2001562c<=s3367617@32,
(-3367617)@32<=sL0x20015650,L0x20015650<=s3367617@32,
(-3367617)@32<=sL0x20015674,L0x20015674<=s3367617@32
,
(-3367617)@32<=sL0x20014918,L0x20014918<=s3367617@32,
(-3367617)@32<=sL0x2001493c,L0x2001493c<=s3367617@32,
(-3367617)@32<=sL0x20014960,L0x20014960<=s3367617@32,
(-3367617)@32<=sL0x20015698,L0x20015698<=s3367617@32,
(-3367617)@32<=sL0x200156bc,L0x200156bc<=s3367617@32,
(-3367617)@32<=sL0x200156e0,L0x200156e0<=s3367617@32
,
(-3367617)@32<=sL0x20014984,L0x20014984<=s3367617@32,
(-3367617)@32<=sL0x200149a8,L0x200149a8<=s3367617@32,
(-3367617)@32<=sL0x200149cc,L0x200149cc<=s3367617@32,
(-3367617)@32<=sL0x20015704,L0x20015704<=s3367617@32,
(-3367617)@32<=sL0x20015728,L0x20015728<=s3367617@32,
(-3367617)@32<=sL0x2001574c,L0x2001574c<=s3367617@32
,
(-3367617)@32<=sL0x200149f0,L0x200149f0<=s3367617@32,
(-3367617)@32<=sL0x20014a14,L0x20014a14<=s3367617@32,
(-3367617)@32<=sL0x20014a38,L0x20014a38<=s3367617@32,
(-3367617)@32<=sL0x20015770,L0x20015770<=s3367617@32,
(-3367617)@32<=sL0x20015794,L0x20015794<=s3367617@32,
(-3367617)@32<=sL0x200157b8,L0x200157b8<=s3367617@32
,
(-3367617)@32<=sL0x20014a5c,L0x20014a5c<=s3367617@32,
(-3367617)@32<=sL0x20014a80,L0x20014a80<=s3367617@32,
(-3367617)@32<=sL0x20014aa4,L0x20014aa4<=s3367617@32,
(-3367617)@32<=sL0x200157dc,L0x200157dc<=s3367617@32,
(-3367617)@32<=sL0x20015800,L0x20015800<=s3367617@32,
(-3367617)@32<=sL0x20015824,L0x20015824<=s3367617@32
,
(-3367617)@32<=sL0x20014ac8,L0x20014ac8<=s3367617@32,
(-3367617)@32<=sL0x20014aec,L0x20014aec<=s3367617@32,
(-3367617)@32<=sL0x20014b10,L0x20014b10<=s3367617@32,
(-3367617)@32<=sL0x20015848,L0x20015848<=s3367617@32,
(-3367617)@32<=sL0x2001586c,L0x2001586c<=s3367617@32,
(-3367617)@32<=sL0x20015890,L0x20015890<=s3367617@32
,
(-3367617)@32<=sL0x20014b34,L0x20014b34<=s3367617@32,
(-3367617)@32<=sL0x20014b58,L0x20014b58<=s3367617@32,
(-3367617)@32<=sL0x20014b7c,L0x20014b7c<=s3367617@32,
(-3367617)@32<=sL0x200158b4,L0x200158b4<=s3367617@32,
(-3367617)@32<=sL0x200158d8,L0x200158d8<=s3367617@32,
(-3367617)@32<=sL0x200158fc,L0x200158fc<=s3367617@32
,
(-3367617)@32<=sL0x20014ba0,L0x20014ba0<=s3367617@32,
(-3367617)@32<=sL0x20014bc4,L0x20014bc4<=s3367617@32,
(-3367617)@32<=sL0x20014be8,L0x20014be8<=s3367617@32,
(-3367617)@32<=sL0x20015920,L0x20015920<=s3367617@32,
(-3367617)@32<=sL0x20015944,L0x20015944<=s3367617@32,
(-3367617)@32<=sL0x20015968,L0x20015968<=s3367617@32
,
(-3367617)@32<=sL0x20014c0c,L0x20014c0c<=s3367617@32,
(-3367617)@32<=sL0x20014c30,L0x20014c30<=s3367617@32,
(-3367617)@32<=sL0x20014c54,L0x20014c54<=s3367617@32,
(-3367617)@32<=sL0x2001598c,L0x2001598c<=s3367617@32,
(-3367617)@32<=sL0x200159b0,L0x200159b0<=s3367617@32,
(-3367617)@32<=sL0x200159d4,L0x200159d4<=s3367617@32
,
(-3367617)@32<=sL0x20014c78,L0x20014c78<=s3367617@32,
(-3367617)@32<=sL0x20014c9c,L0x20014c9c<=s3367617@32,
(-3367617)@32<=sL0x20014cc0,L0x20014cc0<=s3367617@32,
(-3367617)@32<=sL0x200159f8,L0x200159f8<=s3367617@32,
(-3367617)@32<=sL0x20015a1c,L0x20015a1c<=s3367617@32,
(-3367617)@32<=sL0x20015a40,L0x20015a40<=s3367617@32
,
(-3367617)@32<=sL0x20014ce4,L0x20014ce4<=s3367617@32,
(-3367617)@32<=sL0x20014d08,L0x20014d08<=s3367617@32,
(-3367617)@32<=sL0x20014d2c,L0x20014d2c<=s3367617@32,
(-3367617)@32<=sL0x20015a64,L0x20015a64<=s3367617@32,
(-3367617)@32<=sL0x20015a88,L0x20015a88<=s3367617@32,
(-3367617)@32<=sL0x20015aac,L0x20015aac<=s3367617@32
,
(-3367617)@32<=sL0x20014d50,L0x20014d50<=s3367617@32,
(-3367617)@32<=sL0x20014d74,L0x20014d74<=s3367617@32,
(-3367617)@32<=sL0x20014d98,L0x20014d98<=s3367617@32,
(-3367617)@32<=sL0x20015ad0,L0x20015ad0<=s3367617@32,
(-3367617)@32<=sL0x20015af4,L0x20015af4<=s3367617@32,
(-3367617)@32<=sL0x20015b18,L0x20015b18<=s3367617@32
,
(-3367617)@32<=sL0x20014dbc,L0x20014dbc<=s3367617@32,
(-3367617)@32<=sL0x20014de0,L0x20014de0<=s3367617@32,
(-3367617)@32<=sL0x20014e04,L0x20014e04<=s3367617@32,
(-3367617)@32<=sL0x20015b3c,L0x20015b3c<=s3367617@32,
(-3367617)@32<=sL0x20015b60,L0x20015b60<=s3367617@32,
(-3367617)@32<=sL0x20015b84,L0x20015b84<=s3367617@32
,
(-3367617)@32<=sL0x20014e28,L0x20014e28<=s3367617@32,
(-3367617)@32<=sL0x20014e4c,L0x20014e4c<=s3367617@32,
(-3367617)@32<=sL0x20014e70,L0x20014e70<=s3367617@32,
(-3367617)@32<=sL0x20015ba8,L0x20015ba8<=s3367617@32,
(-3367617)@32<=sL0x20015bcc,L0x20015bcc<=s3367617@32,
(-3367617)@32<=sL0x20015bf0,L0x20015bf0<=s3367617@32
,
(-3367617)@32<=sL0x20014e94,L0x20014e94<=s3367617@32,
(-3367617)@32<=sL0x20014eb8,L0x20014eb8<=s3367617@32,
(-3367617)@32<=sL0x20014edc,L0x20014edc<=s3367617@32,
(-3367617)@32<=sL0x20015c14,L0x20015c14<=s3367617@32,
(-3367617)@32<=sL0x20015c38,L0x20015c38<=s3367617@32,
(-3367617)@32<=sL0x20015c5c,L0x20015c5c<=s3367617@32
,
(-3367617)@32<=sL0x20014f00,L0x20014f00<=s3367617@32,
(-3367617)@32<=sL0x20014f24,L0x20014f24<=s3367617@32,
(-3367617)@32<=sL0x20014f48,L0x20014f48<=s3367617@32,
(-3367617)@32<=sL0x20015c80,L0x20015c80<=s3367617@32,
(-3367617)@32<=sL0x20015ca4,L0x20015ca4<=s3367617@32,
(-3367617)@32<=sL0x20015cc8,L0x20015cc8<=s3367617@32
,
(-3367617)@32<=sL0x20014f6c,L0x20014f6c<=s3367617@32,
(-3367617)@32<=sL0x20014f90,L0x20014f90<=s3367617@32,
(-3367617)@32<=sL0x20014fb4,L0x20014fb4<=s3367617@32,
(-3367617)@32<=sL0x20015cec,L0x20015cec<=s3367617@32,
(-3367617)@32<=sL0x20015d10,L0x20015d10<=s3367617@32,
(-3367617)@32<=sL0x20015d34,L0x20015d34<=s3367617@32
,
(-3367617)@32<=sL0x20014fd8,L0x20014fd8<=s3367617@32,
(-3367617)@32<=sL0x20014ffc,L0x20014ffc<=s3367617@32,
(-3367617)@32<=sL0x20015020,L0x20015020<=s3367617@32,
(-3367617)@32<=sL0x20015d58,L0x20015d58<=s3367617@32,
(-3367617)@32<=sL0x20015d7c,L0x20015d7c<=s3367617@32,
(-3367617)@32<=sL0x20015da0,L0x20015da0<=s3367617@32
,
(-3367617)@32<=sL0x20015044,L0x20015044<=s3367617@32,
(-3367617)@32<=sL0x20015068,L0x20015068<=s3367617@32,
(-3367617)@32<=sL0x2001508c,L0x2001508c<=s3367617@32,
(-3367617)@32<=sL0x20015dc4,L0x20015dc4<=s3367617@32,
(-3367617)@32<=sL0x20015de8,L0x20015de8<=s3367617@32,
(-3367617)@32<=sL0x20015e0c,L0x20015e0c<=s3367617@32
,
(-3367617)@32<=sL0x200150b0,L0x200150b0<=s3367617@32,
(-3367617)@32<=sL0x200150d4,L0x200150d4<=s3367617@32,
(-3367617)@32<=sL0x200150f8,L0x200150f8<=s3367617@32,
(-3367617)@32<=sL0x20015e30,L0x20015e30<=s3367617@32,
(-3367617)@32<=sL0x20015e54,L0x20015e54<=s3367617@32,
(-3367617)@32<=sL0x20015e78,L0x20015e78<=s3367617@32
,
(-3367617)@32<=sL0x2001511c,L0x2001511c<=s3367617@32,
(-3367617)@32<=sL0x20015140,L0x20015140<=s3367617@32,
(-3367617)@32<=sL0x20015164,L0x20015164<=s3367617@32,
(-3367617)@32<=sL0x20015e9c,L0x20015e9c<=s3367617@32,
(-3367617)@32<=sL0x20015ec0,L0x20015ec0<=s3367617@32,
(-3367617)@32<=sL0x20015ee4,L0x20015ee4<=s3367617@32
,
(-3367617)@32<=sL0x20015188,L0x20015188<=s3367617@32,
(-3367617)@32<=sL0x200151ac,L0x200151ac<=s3367617@32,
(-3367617)@32<=sL0x200151d0,L0x200151d0<=s3367617@32,
(-3367617)@32<=sL0x20015f08,L0x20015f08<=s3367617@32,
(-3367617)@32<=sL0x20015f2c,L0x20015f2c<=s3367617@32,
(-3367617)@32<=sL0x20015f50,L0x20015f50<=s3367617@32
,
(-3367617)@32<=sL0x200151f4,L0x200151f4<=s3367617@32,
(-3367617)@32<=sL0x20015218,L0x20015218<=s3367617@32,
(-3367617)@32<=sL0x2001523c,L0x2001523c<=s3367617@32,
(-3367617)@32<=sL0x20015f74,L0x20015f74<=s3367617@32,
(-3367617)@32<=sL0x20015f98,L0x20015f98<=s3367617@32,
(-3367617)@32<=sL0x20015fbc,L0x20015fbc<=s3367617@32
,
(-3367617)@32<=sL0x20015260,L0x20015260<=s3367617@32,
(-3367617)@32<=sL0x20015284,L0x20015284<=s3367617@32,
(-3367617)@32<=sL0x200152a8,L0x200152a8<=s3367617@32,
(-3367617)@32<=sL0x20015fe0,L0x20015fe0<=s3367617@32,
(-3367617)@32<=sL0x20016004,L0x20016004<=s3367617@32,
(-3367617)@32<=sL0x20016028,L0x20016028<=s3367617@32
,
(-3367617)@32<=sL0x200152cc,L0x200152cc<=s3367617@32,
(-3367617)@32<=sL0x200152f0,L0x200152f0<=s3367617@32,
(-3367617)@32<=sL0x20015314,L0x20015314<=s3367617@32,
(-3367617)@32<=sL0x2001604c,L0x2001604c<=s3367617@32,
(-3367617)@32<=sL0x20016070,L0x20016070<=s3367617@32,
(-3367617)@32<=sL0x20016094,L0x20016094<=s3367617@32
,
(-3367617)@32<=sL0x20015338,L0x20015338<=s3367617@32,
(-3367617)@32<=sL0x2001535c,L0x2001535c<=s3367617@32,
(-3367617)@32<=sL0x20015380,L0x20015380<=s3367617@32,
(-3367617)@32<=sL0x200160b8,L0x200160b8<=s3367617@32,
(-3367617)@32<=sL0x200160dc,L0x200160dc<=s3367617@32,
(-3367617)@32<=sL0x20016100,L0x20016100<=s3367617@32
,
(-3367617)@32<=sL0x200153a4,L0x200153a4<=s3367617@32,
(-3367617)@32<=sL0x200153c8,L0x200153c8<=s3367617@32,
(-3367617)@32<=sL0x200153ec,L0x200153ec<=s3367617@32,
(-3367617)@32<=sL0x20016124,L0x20016124<=s3367617@32,
(-3367617)@32<=sL0x20016148,L0x20016148<=s3367617@32,
(-3367617)@32<=sL0x2001616c,L0x2001616c<=s3367617@32
,
(-3367617)@32<=sL0x20015410,L0x20015410<=s3367617@32,
(-3367617)@32<=sL0x20015434,L0x20015434<=s3367617@32,
(-3367617)@32<=sL0x20015458,L0x20015458<=s3367617@32,
(-3367617)@32<=sL0x20016190,L0x20016190<=s3367617@32,
(-3367617)@32<=sL0x200161b4,L0x200161b4<=s3367617@32,
(-3367617)@32<=sL0x200161d8,L0x200161d8<=s3367617@32
,
(-3367617)@32<=sL0x2001547c,L0x2001547c<=s3367617@32,
(-3367617)@32<=sL0x200154a0,L0x200154a0<=s3367617@32,
(-3367617)@32<=sL0x200154c4,L0x200154c4<=s3367617@32,
(-3367617)@32<=sL0x200161fc,L0x200161fc<=s3367617@32,
(-3367617)@32<=sL0x20016220,L0x20016220<=s3367617@32,
(-3367617)@32<=sL0x20016244,L0x20016244<=s3367617@32
,
(-3367617)@32<=sL0x200154e8,L0x200154e8<=s3367617@32,
(-3367617)@32<=sL0x2001550c,L0x2001550c<=s3367617@32,
(-3367617)@32<=sL0x20015530,L0x20015530<=s3367617@32,
(-3367617)@32<=sL0x20016268,L0x20016268<=s3367617@32,
(-3367617)@32<=sL0x2001628c,L0x2001628c<=s3367617@32,
(-3367617)@32<=sL0x200162b0,L0x200162b0<=s3367617@32
,
(-3367617)@32<=sL0x20015554,L0x20015554<=s3367617@32,
(-3367617)@32<=sL0x20015578,L0x20015578<=s3367617@32,
(-3367617)@32<=sL0x2001559c,L0x2001559c<=s3367617@32,
(-3367617)@32<=sL0x200162d4,L0x200162d4<=s3367617@32,
(-3367617)@32<=sL0x200162f8,L0x200162f8<=s3367617@32,
(-3367617)@32<=sL0x2001631c,L0x2001631c<=s3367617@32
,
(-3367617)@32<=sL0x200155c0,L0x200155c0<=s3367617@32,
(-3367617)@32<=sL0x200155e4,L0x200155e4<=s3367617@32,
(-3367617)@32<=sL0x20015608,L0x20015608<=s3367617@32,
(-3367617)@32<=sL0x20016340,L0x20016340<=s3367617@32,
(-3367617)@32<=sL0x20016364,L0x20016364<=s3367617@32,
(-3367617)@32<=sL0x20016388,L0x20016388<=s3367617@32
] prove with [ precondition ];





(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x20018870; Value = 0x00000003; PC = 0x805663c *)
mov r4 L0x20018870;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x20018630; Value = 0x00030000; PC = 0x8056640 *)
mov r7 L0x20018630;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018ab0; Value = 0x00000000; PC = 0x8056644 *)
mov r9 L0x20018ab0;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0200@sint32 : and [cf0200 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0500@sint32 : and [cf0500 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0800@sint32 : and [cf0800 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200148b0; PC = 0x8056680 *)
mov L0x200148b0 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200148d4; PC = 0x8056684 *)
mov L0x200148d4 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x200148f8; PC = 0x8056688 *)
mov L0x200148f8 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015630; PC = 0x805668c *)
mov L0x20015630 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015654; PC = 0x8056690 *)
mov L0x20015654 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015678; PC = 0x8056694 *)
mov L0x20015678 r9;



(******************** offset 0, 2,  0 ********************)


(**************** CUT 192, - *****************)

ecut and [
eqmod cf0200 f0200 2048, eqmod cf0500 f0500 2048, eqmod cf0800 f0800 2048,
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x200148b0*x**0*y**2*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x200148d4*x**0*y**2*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x200148f8*x**0*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x20015630*x**0*y**2*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x20015654*x**0*y**2*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x20015678*x**0*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x200186f6; Value = 0x0ffd0000; PC = 0x8056698 *)
mov r4 L0x200186f6;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018b76; Value = 0x00000000; PC = 0x805669c *)
mov r6 L0x20018b76;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x20018936; Value = 0x00000000; PC = 0x80566a0 *)
mov r9 L0x20018936;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0201@sint32 : and [cf0201 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0501@sint32 : and [cf0501 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0801@sint32 : and [cf0801 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x2001491c; PC = 0x80566ec *)
mov L0x2001491c r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014940; PC = 0x80566f0 *)
mov L0x20014940 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014964; PC = 0x80566f4 *)
mov L0x20014964 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x2001569c; PC = 0x80566f8 *)
mov L0x2001569c r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200156c0; PC = 0x80566fc *)
mov L0x200156c0 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200156e4; PC = 0x8056700 *)
mov L0x200156e4 r9;



(******************** offset 0, 2,  1 ********************)


(**************** CUT 193, - *****************)

ecut and [
eqmod cf0201 f0201 2048, eqmod cf0501 f0501 2048, eqmod cf0801 f0801 2048,
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x2001491c*x**0*y**2*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x20014940*x**0*y**2*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x20014964*x**0*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x2001569c*x**0*y**2*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x200156c0*x**0*y**2*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x200156e4*x**0*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x2001857c; Value = 0x0ffd0000; PC = 0x8056704 *)
mov r4 L0x2001857c;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x200189fc; Value = 0x00000003; PC = 0x8056708 *)
mov r6 L0x200189fc;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x200187bc; Value = 0x00000ffd; PC = 0x805670c *)
mov r9 L0x200187bc;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0202@sint32 : and [cf0202 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0502@sint32 : and [cf0502 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0802@sint32 : and [cf0802 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014988; PC = 0x8056758 *)
mov L0x20014988 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200149ac; PC = 0x805675c *)
mov L0x200149ac r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200149d0; PC = 0x8056760 *)
mov L0x200149d0 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015708; PC = 0x8056764 *)
mov L0x20015708 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x2001572c; PC = 0x8056768 *)
mov L0x2001572c r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015750; PC = 0x805676c *)
mov L0x20015750 r9;



(******************** offset 0, 2,  2 ********************)


(**************** CUT 194, - *****************)

ecut and [
eqmod cf0202 f0202 2048, eqmod cf0502 f0502 2048, eqmod cf0802 f0802 2048,
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x20014988*x**0*y**2*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x200149ac*x**0*y**2*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x200149d0*x**0*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x20015708*x**0*y**2*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x2001572c*x**0*y**2*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x20015750*x**0*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018ac2; Value = 0x00000000; PC = 0x8056770 *)
mov r5 L0x20018ac2;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x20018882; Value = 0x00000ffd; PC = 0x8056774 *)
mov r6 L0x20018882;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x20018642; Value = 0x00030003; PC = 0x8056778 *)
mov r9 L0x20018642;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0203@sint32 : and [cf0203 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0503@sint32 : and [cf0503 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0803@sint32 : and [cf0803 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x200149f4; PC = 0x80567cc *)
mov L0x200149f4 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014a18; PC = 0x80567d0 *)
mov L0x20014a18 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014a3c; PC = 0x80567d4 *)
mov L0x20014a3c r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015774; PC = 0x80567d8 *)
mov L0x20015774 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015798; PC = 0x80567dc *)
mov L0x20015798 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x200157bc; PC = 0x80567e0 *)
mov L0x200157bc r9;



(******************** offset 0, 2,  3 ********************)


(**************** CUT 195, - *****************)

ecut and [
eqmod cf0203 f0203 2048, eqmod cf0503 f0503 2048, eqmod cf0803 f0803 2048,
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x200149f4*x**0*y**2*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x20014a18*x**0*y**2*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x20014a3c*x**0*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x20015774*x**0*y**2*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x20015798*x**0*y**2*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x200157bc*x**0*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x20018948; Value = 0x00000ffd; PC = 0x80567e4 *)
mov r5 L0x20018948;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x20018708; Value = 0x0ffd0ffd; PC = 0x80567e8 *)
mov r6 L0x20018708;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018b88; Value = 0x00000003; PC = 0x80567ec *)
mov r8 L0x20018b88;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0204@sint32 : and [cf0204 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0504@sint32 : and [cf0504 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0804@sint32 : and [cf0804 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014a60; PC = 0x805682c *)
mov L0x20014a60 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014a84; PC = 0x8056830 *)
mov L0x20014a84 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014aa8; PC = 0x8056834 *)
mov L0x20014aa8 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x200157e0; PC = 0x8056838 *)
mov L0x200157e0 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015804; PC = 0x805683c *)
mov L0x20015804 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015828; PC = 0x8056840 *)
mov L0x20015828 r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;



(******************** offset 0, 2,  4 ********************)


(**************** CUT 196, - *****************)

ecut and [
eqmod cf0204 f0204 2048, eqmod cf0504 f0504 2048, eqmod cf0804 f0804 2048,
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x20014a60*x**0*y**2*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x20014a84*x**0*y**2*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x20014aa8*x**0*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x200157e0*x**0*y**2*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x20015804*x**0*y**2*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x20015828*x**0*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #606]	; 0x25e                  #! EA = L0x200187ce; Value = 0x00000003; PC = 0x8056850 *)
mov r5 L0x200187ce;
(* ldrsh.w	r6, [lr, #30]                           #! EA = L0x2001858e; Value = 0x00030003; PC = 0x8056854 *)
mov r6 L0x2001858e;
(* ldrsh.w	r8, [lr, #1182]	; 0x49e                 #! EA = L0x20018a0e; Value = 0x00030ffd; PC = 0x8056858 *)
mov r8 L0x20018a0e;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805685c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0205@sint32 : and [cf0205 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056860 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0505@sint32 : and [cf0505 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056864 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0805@sint32 : and [cf0805 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056868 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x805686c *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056870 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056874 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056878 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x805687c *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056880 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056884 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056888 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x805688c *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056890 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056892 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056894 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014acc; PC = 0x8056898 *)
mov L0x20014acc r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014af0; PC = 0x805689c *)
mov L0x20014af0 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014b14; PC = 0x80568a0 *)
mov L0x20014b14 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x2001584c; PC = 0x80568a4 *)
mov L0x2001584c r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015870; PC = 0x80568a8 *)
mov L0x20015870 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015894; PC = 0x80568ac *)
mov L0x20015894 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80568b0 *)
adds dc r0 r0 648@uint32;



(******************** offset 0, 2,  5 ********************)


(**************** CUT 197, - *****************)

ecut and [
eqmod cf0205 f0205 2048, eqmod cf0505 f0505 2048, eqmod cf0805 f0805 2048,
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x20014acc*x**0*y**2*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x20014af0*x**0*y**2*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x20014b14*x**0*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x2001584c*x**0*y**2*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x20015870*x**0*y**2*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x20015894*x**0*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #228]	; 0xe4                   #! EA = L0x20018654; Value = 0x0ffd0003; PC = 0x80568b4 *)
mov r5 L0x20018654;
(* ldrsh.w	r7, [lr, #1380]	; 0x564                 #! EA = L0x20018ad4; Value = 0x0ffd0003; PC = 0x80568b8 *)
mov r7 L0x20018ad4;
(* ldrsh.w	r8, [lr, #804]	; 0x324                  #! EA = L0x20018894; Value = 0x00000000; PC = 0x80568bc *)
mov r8 L0x20018894;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80568c0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0206@sint32 : and [cf0206 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80568c4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0506@sint32 : and [cf0506 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80568c8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0806@sint32 : and [cf0806 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x80568cc *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x80568d0 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x80568d4 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x80568d8 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x80568dc *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x80568e0 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x80568e2 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x80568e4 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x80568e8 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x80568ec *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x80568f0 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x80568f4 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80568f8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x80568fc *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056900 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x8056904 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056908 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x805690c *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014b38; PC = 0x8056910 *)
mov L0x20014b38 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014b5c; PC = 0x8056914 *)
mov L0x20014b5c r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014b80; PC = 0x8056918 *)
mov L0x20014b80 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200158b8; PC = 0x805691c *)
mov L0x200158b8 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200158dc; PC = 0x8056920 *)
mov L0x200158dc r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015900; PC = 0x8056924 *)
mov L0x20015900 r9;



(******************** offset 0, 2,  6 ********************)


(**************** CUT 198, - *****************)

ecut and [
eqmod cf0206 f0206 2048, eqmod cf0506 f0506 2048, eqmod cf0806 f0806 2048,
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x20014b38*x**0*y**2*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x20014b5c*x**0*y**2*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x20014b80*x**0*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x200158b8*x**0*y**2*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x200158dc*x**0*y**2*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x20015900*x**0*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1578]	; 0x62a                 #! EA = L0x20018b9a; Value = 0x00000003; PC = 0x8056928 *)
mov r4 L0x20018b9a;
(* ldrsh.w	r7, [lr, #1002]	; 0x3ea                 #! EA = L0x2001895a; Value = 0x00030000; PC = 0x805692c *)
mov r7 L0x2001895a;
(* ldrsh.w	r8, [lr, #426]	; 0x1aa                  #! EA = L0x2001871a; Value = 0x00000000; PC = 0x8056930 *)
mov r8 L0x2001871a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056934 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0207@sint32 : and [cf0207 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056938 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0507@sint32 : and [cf0507 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805693c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0807@sint32 : and [cf0807 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056940 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056944 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056948 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805694c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056950 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056954 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056956 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056958 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805695c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056960 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056964 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056968 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805696a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805696c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056970 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056974 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056978 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014ba4; PC = 0x805697c *)
mov L0x20014ba4 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014bc8; PC = 0x8056980 *)
mov L0x20014bc8 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014bec; PC = 0x8056984 *)
mov L0x20014bec r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015924; PC = 0x8056988 *)
mov L0x20015924 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015948; PC = 0x805698c *)
mov L0x20015948 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x2001596c; PC = 0x8056990 *)
mov L0x2001596c r9;



(******************** offset 0, 2,  7 ********************)


(**************** CUT 199, - *****************)

ecut and [
eqmod cf0207 f0207 2048, eqmod cf0507 f0507 2048, eqmod cf0807 f0807 2048,
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x20014ba4*x**0*y**2*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x20014bc8*x**0*y**2*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x20014bec*x**0*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x20015924*x**0*y**2*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x20015948*x**0*y**2*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x2001596c*x**0*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1200]	; 0x4b0                 #! EA = L0x20018a20; Value = 0x00000000; PC = 0x8056994 *)
mov r4 L0x20018a20;
(* ldrsh.w	r7, [lr, #624]	; 0x270                  #! EA = L0x200187e0; Value = 0x00000003; PC = 0x8056998 *)
mov r7 L0x200187e0;
(* ldrsh.w	r8, [lr, #48]	; 0x30                    #! EA = L0x200185a0; Value = 0x00000ffd; PC = 0x805699c *)
mov r8 L0x200185a0;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80569a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0208@sint32 : and [cf0208 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80569a4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0508@sint32 : and [cf0508 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80569a8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0808@sint32 : and [cf0808 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80569ac *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80569b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80569b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80569b8 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80569bc *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80569c0 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80569c2 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80569c4 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x80569c8 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x80569cc *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x80569d0 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x80569d4 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x80569d6 *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80569d8 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80569dc *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80569e0 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80569e4 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014c10; PC = 0x80569e8 *)
mov L0x20014c10 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014c34; PC = 0x80569ec *)
mov L0x20014c34 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014c58; PC = 0x80569f0 *)
mov L0x20014c58 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015990; PC = 0x80569f4 *)
mov L0x20015990 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x200159b4; PC = 0x80569f8 *)
mov L0x200159b4 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200159d8; PC = 0x80569fc *)
mov L0x200159d8 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056a00 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056a04 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805663c <_Good_loop2_inner>              #! PC = 0x8056a08 *)
#b.w	0x805663c <_Good_loop2_inner>              #! 0x8056a08 = 0x8056a08;



(******************** offset 0, 2,  8 ********************)


(**************** CUT 200, - *****************)

ecut and [
eqmod cf0208 f0208 2048, eqmod cf0508 f0508 2048, eqmod cf0808 f0808 2048,
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x20014c10*x**0*y**2*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x20014c34*x**0*y**2*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x20014c58*x**0*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x20015990*x**0*y**2*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x200159b4*x**0*y**2*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x200159d8*x**0*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x200188a6; Value = 0x00030ffd; PC = 0x805663c *)
mov r4 L0x200188a6;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x20018666; Value = 0x00000ffd; PC = 0x8056640 *)
mov r7 L0x20018666;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018ae6; Value = 0x0ffd0ffd; PC = 0x8056644 *)
mov r9 L0x20018ae6;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0209@sint32 : and [cf0209 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0509@sint32 : and [cf0509 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0809@sint32 : and [cf0809 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014c7c; PC = 0x8056680 *)
mov L0x20014c7c r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014ca0; PC = 0x8056684 *)
mov L0x20014ca0 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014cc4; PC = 0x8056688 *)
mov L0x20014cc4 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200159fc; PC = 0x805668c *)
mov L0x200159fc r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015a20; PC = 0x8056690 *)
mov L0x20015a20 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015a44; PC = 0x8056694 *)
mov L0x20015a44 r9;



(******************** offset 0, 2,  9 ********************)


(**************** CUT 201, - *****************)

ecut and [
eqmod cf0209 f0209 2048, eqmod cf0509 f0509 2048, eqmod cf0809 f0809 2048,
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x20014c7c*x**0*y**2*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x20014ca0*x**0*y**2*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x20014cc4*x**0*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x200159fc*x**0*y**2*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x20015a20*x**0*y**2*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x20015a44*x**0*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x2001872c; Value = 0x0ffd0ffd; PC = 0x8056698 *)
mov r4 L0x2001872c;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018bac; Value = 0x00030ffd; PC = 0x805669c *)
mov r6 L0x20018bac;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x2001896c; Value = 0x00030003; PC = 0x80566a0 *)
mov r9 L0x2001896c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0210@sint32 : and [cf0210 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0510@sint32 : and [cf0510 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0810@sint32 : and [cf0810 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014ce8; PC = 0x80566ec *)
mov L0x20014ce8 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014d0c; PC = 0x80566f0 *)
mov L0x20014d0c r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014d30; PC = 0x80566f4 *)
mov L0x20014d30 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015a68; PC = 0x80566f8 *)
mov L0x20015a68 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015a8c; PC = 0x80566fc *)
mov L0x20015a8c r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015ab0; PC = 0x8056700 *)
mov L0x20015ab0 r9;



(******************** offset 0, 2, 10 ********************)


(**************** CUT 202, - *****************)

ecut and [
eqmod cf0210 f0210 2048, eqmod cf0510 f0510 2048, eqmod cf0810 f0810 2048,
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20014ce8*x**0*y**2*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20014d0c*x**0*y**2*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20014d30*x**0*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20015a68*x**0*y**2*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20015a8c*x**0*y**2*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20015ab0*x**0*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x200185b2; Value = 0x0ffd0000; PC = 0x8056704 *)
mov r4 L0x200185b2;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x20018a32; Value = 0x00030000; PC = 0x8056708 *)
mov r6 L0x20018a32;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x200187f2; Value = 0x00030003; PC = 0x805670c *)
mov r9 L0x200187f2;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0211@sint32 : and [cf0211 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0511@sint32 : and [cf0511 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0811@sint32 : and [cf0811 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014d54; PC = 0x8056758 *)
mov L0x20014d54 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014d78; PC = 0x805675c *)
mov L0x20014d78 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014d9c; PC = 0x8056760 *)
mov L0x20014d9c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015ad4; PC = 0x8056764 *)
mov L0x20015ad4 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015af8; PC = 0x8056768 *)
mov L0x20015af8 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015b1c; PC = 0x805676c *)
mov L0x20015b1c r9;



(******************** offset 0, 2, 11 ********************)


(**************** CUT 203, - *****************)

ecut and [
eqmod cf0211 f0211 2048, eqmod cf0511 f0511 2048, eqmod cf0811 f0811 2048,
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20014d54*x**0*y**2*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20014d78*x**0*y**2*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20014d9c*x**0*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20015ad4*x**0*y**2*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20015af8*x**0*y**2*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20015b1c*x**0*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018af8; Value = 0x0ffd0ffd; PC = 0x8056770 *)
mov r5 L0x20018af8;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x200188b8; Value = 0x0ffd0003; PC = 0x8056774 *)
mov r6 L0x200188b8;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x20018678; Value = 0x00030003; PC = 0x8056778 *)
mov r9 L0x20018678;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0212@sint32 : and [cf0212 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0512@sint32 : and [cf0512 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0812@sint32 : and [cf0812 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20014dc0; PC = 0x80567cc *)
mov L0x20014dc0 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014de4; PC = 0x80567d0 *)
mov L0x20014de4 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014e08; PC = 0x80567d4 *)
mov L0x20014e08 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015b40; PC = 0x80567d8 *)
mov L0x20015b40 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015b64; PC = 0x80567dc *)
mov L0x20015b64 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015b88; PC = 0x80567e0 *)
mov L0x20015b88 r9;



(******************** offset 0, 2, 12 ********************)


(**************** CUT 204, - *****************)

ecut and [
eqmod cf0212 f0212 2048, eqmod cf0512 f0512 2048, eqmod cf0812 f0812 2048,
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20014dc0*x**0*y**2*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20014de4*x**0*y**2*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20014e08*x**0*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20015b40*x**0*y**2*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20015b64*x**0*y**2*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20015b88*x**0*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x2001897e; Value = 0x00030ffd; PC = 0x80567e4 *)
mov r5 L0x2001897e;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x2001873e; Value = 0x0ffd0003; PC = 0x80567e8 *)
mov r6 L0x2001873e;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018bbe; Value = 0x0ffd0ffd; PC = 0x80567ec *)
mov r8 L0x20018bbe;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0213@sint32 : and [cf0213 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0513@sint32 : and [cf0513 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0813@sint32 : and [cf0813 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014e2c; PC = 0x805682c *)
mov L0x20014e2c r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014e50; PC = 0x8056830 *)
mov L0x20014e50 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014e74; PC = 0x8056834 *)
mov L0x20014e74 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015bac; PC = 0x8056838 *)
mov L0x20015bac r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015bd0; PC = 0x805683c *)
mov L0x20015bd0 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015bf4; PC = 0x8056840 *)
mov L0x20015bf4 r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;



(******************** offset 0, 2, 13 ********************)


(**************** CUT 205, - *****************)

ecut and [
eqmod cf0213 f0213 2048, eqmod cf0513 f0513 2048, eqmod cf0813 f0813 2048,
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20014e2c*x**0*y**2*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20014e50*x**0*y**2*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20014e74*x**0*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20015bac*x**0*y**2*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20015bd0*x**0*y**2*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20015bf4*x**0*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #606]	; 0x25e                  #! EA = L0x20018804; Value = 0x00030003; PC = 0x8056850 *)
mov r5 L0x20018804;
(* ldrsh.w	r6, [lr, #30]                           #! EA = L0x200185c4; Value = 0x00030000; PC = 0x8056854 *)
mov r6 L0x200185c4;
(* ldrsh.w	r8, [lr, #1182]	; 0x49e                 #! EA = L0x20018a44; Value = 0x00030003; PC = 0x8056858 *)
mov r8 L0x20018a44;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805685c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0214@sint32 : and [cf0214 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056860 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0514@sint32 : and [cf0514 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056864 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0814@sint32 : and [cf0814 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056868 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x805686c *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056870 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056874 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056878 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x805687c *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056880 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056884 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056888 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x805688c *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056890 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056892 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056894 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014e98; PC = 0x8056898 *)
mov L0x20014e98 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014ebc; PC = 0x805689c *)
mov L0x20014ebc r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014ee0; PC = 0x80568a0 *)
mov L0x20014ee0 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015c18; PC = 0x80568a4 *)
mov L0x20015c18 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015c3c; PC = 0x80568a8 *)
mov L0x20015c3c r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015c60; PC = 0x80568ac *)
mov L0x20015c60 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80568b0 *)
adds dc r0 r0 648@uint32;



(******************** offset 0, 2, 14 ********************)


(**************** CUT 206, - *****************)

ecut and [
eqmod cf0214 f0214 2048, eqmod cf0514 f0514 2048, eqmod cf0814 f0814 2048,
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20014e98*x**0*y**2*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20014ebc*x**0*y**2*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20014ee0*x**0*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20015c18*x**0*y**2*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20015c3c*x**0*y**2*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20015c60*x**0*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #228]	; 0xe4                   #! EA = L0x2001868a; Value = 0x00030003; PC = 0x80568b4 *)
mov r5 L0x2001868a;
(* ldrsh.w	r7, [lr, #1380]	; 0x564                 #! EA = L0x20018b0a; Value = 0x0ffd0000; PC = 0x80568b8 *)
mov r7 L0x20018b0a;
(* ldrsh.w	r8, [lr, #804]	; 0x324                  #! EA = L0x200188ca; Value = 0x00000003; PC = 0x80568bc *)
mov r8 L0x200188ca;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80568c0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0215@sint32 : and [cf0215 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80568c4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0515@sint32 : and [cf0515 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80568c8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0815@sint32 : and [cf0815 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x80568cc *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x80568d0 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x80568d4 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x80568d8 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x80568dc *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x80568e0 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x80568e2 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x80568e4 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x80568e8 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x80568ec *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x80568f0 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x80568f4 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80568f8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x80568fc *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056900 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x8056904 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056908 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x805690c *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014f04; PC = 0x8056910 *)
mov L0x20014f04 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014f28; PC = 0x8056914 *)
mov L0x20014f28 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014f4c; PC = 0x8056918 *)
mov L0x20014f4c r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015c84; PC = 0x805691c *)
mov L0x20015c84 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015ca8; PC = 0x8056920 *)
mov L0x20015ca8 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015ccc; PC = 0x8056924 *)
mov L0x20015ccc r9;



(******************** offset 0, 2, 15 ********************)


(**************** CUT 207, - *****************)

ecut and [
eqmod cf0215 f0215 2048, eqmod cf0515 f0515 2048, eqmod cf0815 f0815 2048,
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20014f04*x**0*y**2*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20014f28*x**0*y**2*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20014f4c*x**0*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20015c84*x**0*y**2*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20015ca8*x**0*y**2*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20015ccc*x**0*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1578]	; 0x62a                 #! EA = L0x20018bd0; Value = 0x00030000; PC = 0x8056928 *)
mov r4 L0x20018bd0;
(* ldrsh.w	r7, [lr, #1002]	; 0x3ea                 #! EA = L0x20018990; Value = 0x0ffd0003; PC = 0x805692c *)
mov r7 L0x20018990;
(* ldrsh.w	r8, [lr, #426]	; 0x1aa                  #! EA = L0x20018750; Value = 0x00000ffd; PC = 0x8056930 *)
mov r8 L0x20018750;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056934 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0216@sint32 : and [cf0216 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056938 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0516@sint32 : and [cf0516 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805693c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0816@sint32 : and [cf0816 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056940 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056944 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056948 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805694c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056950 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056954 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056956 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056958 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805695c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056960 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056964 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056968 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805696a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805696c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056970 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056974 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056978 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014f70; PC = 0x805697c *)
mov L0x20014f70 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014f94; PC = 0x8056980 *)
mov L0x20014f94 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014fb8; PC = 0x8056984 *)
mov L0x20014fb8 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015cf0; PC = 0x8056988 *)
mov L0x20015cf0 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015d14; PC = 0x805698c *)
mov L0x20015d14 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015d38; PC = 0x8056990 *)
mov L0x20015d38 r9;



(******************** offset 0, 2, 16 ********************)


(**************** CUT 208, - *****************)

ecut and [
eqmod cf0216 f0216 2048, eqmod cf0516 f0516 2048, eqmod cf0816 f0816 2048,
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20014f70*x**0*y**2*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20014f94*x**0*y**2*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20014fb8*x**0*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20015cf0*x**0*y**2*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20015d14*x**0*y**2*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20015d38*x**0*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1200]	; 0x4b0                 #! EA = L0x20018a56; Value = 0x00030ffd; PC = 0x8056994 *)
mov r4 L0x20018a56;
(* ldrsh.w	r7, [lr, #624]	; 0x270                  #! EA = L0x20018816; Value = 0x0ffd0003; PC = 0x8056998 *)
mov r7 L0x20018816;
(* ldrsh.w	r8, [lr, #48]	; 0x30                    #! EA = L0x200185d6; Value = 0x00030000; PC = 0x805699c *)
mov r8 L0x200185d6;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80569a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0217@sint32 : and [cf0217 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80569a4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0517@sint32 : and [cf0517 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80569a8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0817@sint32 : and [cf0817 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80569ac *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80569b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80569b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80569b8 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80569bc *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80569c0 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80569c2 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80569c4 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x80569c8 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x80569cc *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x80569d0 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x80569d4 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x80569d6 *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80569d8 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80569dc *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80569e0 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80569e4 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014fdc; PC = 0x80569e8 *)
mov L0x20014fdc r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015000; PC = 0x80569ec *)
mov L0x20015000 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015024; PC = 0x80569f0 *)
mov L0x20015024 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015d5c; PC = 0x80569f4 *)
mov L0x20015d5c r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015d80; PC = 0x80569f8 *)
mov L0x20015d80 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015da4; PC = 0x80569fc *)
mov L0x20015da4 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056a00 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056a04 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805663c <_Good_loop2_inner>              #! PC = 0x8056a08 *)
#b.w	0x805663c <_Good_loop2_inner>              #! 0x8056a08 = 0x8056a08;



(******************** offset 0, 2, 17 ********************)


(**************** CUT 209, - *****************)

ecut and [
eqmod cf0217 f0217 2048, eqmod cf0517 f0517 2048, eqmod cf0817 f0817 2048,
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20014fdc*x**0*y**2*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20015000*x**0*y**2*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20015024*x**0*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20015d5c*x**0*y**2*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20015d80*x**0*y**2*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20015da4*x**0*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x200188dc; Value = 0x0ffd0000; PC = 0x805663c *)
mov r4 L0x200188dc;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x2001869c; Value = 0x00000000; PC = 0x8056640 *)
mov r7 L0x2001869c;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018b1c; Value = 0x0ffd0ffd; PC = 0x8056644 *)
mov r9 L0x20018b1c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0218@sint32 : and [cf0218 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0518@sint32 : and [cf0518 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0818@sint32 : and [cf0818 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015048; PC = 0x8056680 *)
mov L0x20015048 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x2001506c; PC = 0x8056684 *)
mov L0x2001506c r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015090; PC = 0x8056688 *)
mov L0x20015090 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015dc8; PC = 0x805668c *)
mov L0x20015dc8 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015dec; PC = 0x8056690 *)
mov L0x20015dec r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015e10; PC = 0x8056694 *)
mov L0x20015e10 r9;



(******************** offset 0, 2, 18 ********************)


(**************** CUT 210, - *****************)

ecut and [
eqmod cf0218 f0218 2048, eqmod cf0518 f0518 2048, eqmod cf0818 f0818 2048,
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x20015048*x**0*y**2*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x2001506c*x**0*y**2*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x20015090*x**0*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x20015dc8*x**0*y**2*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x20015dec*x**0*y**2*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x20015e10*x**0*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x20018762; Value = 0x00000ffd; PC = 0x8056698 *)
mov r4 L0x20018762;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018be2; Value = 0x00000000; PC = 0x805669c *)
mov r6 L0x20018be2;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x200189a2; Value = 0x00030000; PC = 0x80566a0 *)
mov r9 L0x200189a2;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0219@sint32 : and [cf0219 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0519@sint32 : and [cf0519 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0819@sint32 : and [cf0819 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x200150b4; PC = 0x80566ec *)
mov L0x200150b4 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200150d8; PC = 0x80566f0 *)
mov L0x200150d8 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200150fc; PC = 0x80566f4 *)
mov L0x200150fc r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015e34; PC = 0x80566f8 *)
mov L0x20015e34 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015e58; PC = 0x80566fc *)
mov L0x20015e58 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015e7c; PC = 0x8056700 *)
mov L0x20015e7c r9;



(******************** offset 0, 2, 19 ********************)


(**************** CUT 211, - *****************)

ecut and [
eqmod cf0219 f0219 2048, eqmod cf0519 f0519 2048, eqmod cf0819 f0819 2048,
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x200150b4*x**0*y**2*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x200150d8*x**0*y**2*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x200150fc*x**0*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x20015e34*x**0*y**2*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x20015e58*x**0*y**2*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x20015e7c*x**0*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x200185e8; Value = 0x0ffd0ffd; PC = 0x8056704 *)
mov r4 L0x200185e8;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x20018a68; Value = 0x0ffd0ffd; PC = 0x8056708 *)
mov r6 L0x20018a68;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x20018828; Value = 0x0ffd0003; PC = 0x805670c *)
mov r9 L0x20018828;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0220@sint32 : and [cf0220 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0520@sint32 : and [cf0520 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0820@sint32 : and [cf0820 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20015120; PC = 0x8056758 *)
mov L0x20015120 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015144; PC = 0x805675c *)
mov L0x20015144 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015168; PC = 0x8056760 *)
mov L0x20015168 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015ea0; PC = 0x8056764 *)
mov L0x20015ea0 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015ec4; PC = 0x8056768 *)
mov L0x20015ec4 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015ee8; PC = 0x805676c *)
mov L0x20015ee8 r9;



(******************** offset 0, 2, 20 ********************)


(**************** CUT 212, - *****************)

ecut and [
eqmod cf0220 f0220 2048, eqmod cf0520 f0520 2048, eqmod cf0820 f0820 2048,
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015120*x**0*y**2*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015144*x**0*y**2*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015168*x**0*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015ea0*x**0*y**2*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015ec4*x**0*y**2*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015ee8*x**0*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018b2e; Value = 0x00000000; PC = 0x8056770 *)
mov r5 L0x20018b2e;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x200188ee; Value = 0x00030003; PC = 0x8056774 *)
mov r6 L0x200188ee;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x200186ae; Value = 0x0ffd0ffd; PC = 0x8056778 *)
mov r9 L0x200186ae;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0221@sint32 : and [cf0221 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0521@sint32 : and [cf0521 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0821@sint32 : and [cf0821 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x2001518c; PC = 0x80567cc *)
mov L0x2001518c r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x200151b0; PC = 0x80567d0 *)
mov L0x200151b0 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200151d4; PC = 0x80567d4 *)
mov L0x200151d4 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015f0c; PC = 0x80567d8 *)
mov L0x20015f0c r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015f30; PC = 0x80567dc *)
mov L0x20015f30 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015f54; PC = 0x80567e0 *)
mov L0x20015f54 r9;



(******************** offset 0, 2, 21 ********************)


(**************** CUT 213, - *****************)

ecut and [
eqmod cf0221 f0221 2048, eqmod cf0521 f0521 2048, eqmod cf0821 f0821 2048,
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x2001518c*x**0*y**2*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x200151b0*x**0*y**2*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x200151d4*x**0*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x20015f0c*x**0*y**2*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x20015f30*x**0*y**2*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x20015f54*x**0*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x200189b4; Value = 0x00000003; PC = 0x80567e4 *)
mov r5 L0x200189b4;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x20018774; Value = 0x00030003; PC = 0x80567e8 *)
mov r6 L0x20018774;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018bf4; Value = 0x00000000; PC = 0x80567ec *)
mov r8 L0x20018bf4;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0222@sint32 : and [cf0222 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0522@sint32 : and [cf0522 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0822@sint32 : and [cf0822 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200151f8; PC = 0x805682c *)
mov L0x200151f8 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x2001521c; PC = 0x8056830 *)
mov L0x2001521c r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015240; PC = 0x8056834 *)
mov L0x20015240 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015f78; PC = 0x8056838 *)
mov L0x20015f78 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015f9c; PC = 0x805683c *)
mov L0x20015f9c r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015fc0; PC = 0x8056840 *)
mov L0x20015fc0 r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;



(******************** offset 0, 2, 22 ********************)


(**************** CUT 214, - *****************)

ecut and [
eqmod cf0222 f0222 2048, eqmod cf0522 f0522 2048, eqmod cf0822 f0822 2048,
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x200151f8*x**0*y**2*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x2001521c*x**0*y**2*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x20015240*x**0*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x20015f78*x**0*y**2*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x20015f9c*x**0*y**2*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x20015fc0*x**0*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #606]	; 0x25e                  #! EA = L0x2001883a; Value = 0x00000003; PC = 0x8056850 *)
mov r5 L0x2001883a;
(* ldrsh.w	r6, [lr, #30]                           #! EA = L0x200185fa; Value = 0x0ffd0000; PC = 0x8056854 *)
mov r6 L0x200185fa;
(* ldrsh.w	r8, [lr, #1182]	; 0x49e                 #! EA = L0x20018a7a; Value = 0x00000000; PC = 0x8056858 *)
mov r8 L0x20018a7a;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805685c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0223@sint32 : and [cf0223 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056860 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0523@sint32 : and [cf0523 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056864 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0823@sint32 : and [cf0823 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056868 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x805686c *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056870 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056874 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056878 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x805687c *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056880 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056884 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056888 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x805688c *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056890 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056892 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056894 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20015264; PC = 0x8056898 *)
mov L0x20015264 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20015288; PC = 0x805689c *)
mov L0x20015288 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x200152ac; PC = 0x80568a0 *)
mov L0x200152ac r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015fe4; PC = 0x80568a4 *)
mov L0x20015fe4 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20016008; PC = 0x80568a8 *)
mov L0x20016008 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x2001602c; PC = 0x80568ac *)
mov L0x2001602c r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80568b0 *)
adds dc r0 r0 648@uint32;



(******************** offset 0, 2, 23 ********************)


(**************** CUT 215, - *****************)

ecut and [
eqmod cf0223 f0223 2048, eqmod cf0523 f0523 2048, eqmod cf0823 f0823 2048,
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x20015264*x**0*y**2*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x20015288*x**0*y**2*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x200152ac*x**0*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x20015fe4*x**0*y**2*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x20016008*x**0*y**2*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x2001602c*x**0*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #228]	; 0xe4                   #! EA = L0x200186c0; Value = 0x00030003; PC = 0x80568b4 *)
mov r5 L0x200186c0;
(* ldrsh.w	r7, [lr, #1380]	; 0x564                 #! EA = L0x20018b40; Value = 0x0ffd0000; PC = 0x80568b8 *)
mov r7 L0x20018b40;
(* ldrsh.w	r8, [lr, #804]	; 0x324                  #! EA = L0x20018900; Value = 0x0ffd0000; PC = 0x80568bc *)
mov r8 L0x20018900;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80568c0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0224@sint32 : and [cf0224 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80568c4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0524@sint32 : and [cf0524 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80568c8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0824@sint32 : and [cf0824 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x80568cc *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x80568d0 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x80568d4 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x80568d8 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x80568dc *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x80568e0 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x80568e2 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x80568e4 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x80568e8 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x80568ec *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x80568f0 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x80568f4 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80568f8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x80568fc *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056900 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x8056904 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056908 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x805690c *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200152d0; PC = 0x8056910 *)
mov L0x200152d0 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200152f4; PC = 0x8056914 *)
mov L0x200152f4 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015318; PC = 0x8056918 *)
mov L0x20015318 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016050; PC = 0x805691c *)
mov L0x20016050 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20016074; PC = 0x8056920 *)
mov L0x20016074 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20016098; PC = 0x8056924 *)
mov L0x20016098 r9;



(******************** offset 0, 2, 24 ********************)


(**************** CUT 216, - *****************)

ecut and [
eqmod cf0224 f0224 2048, eqmod cf0524 f0524 2048, eqmod cf0824 f0824 2048,
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x200152d0*x**0*y**2*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x200152f4*x**0*y**2*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x20015318*x**0*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x20016050*x**0*y**2*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x20016074*x**0*y**2*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x20016098*x**0*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1578]	; 0x62a                 #! EA = L0x20018c06; Value = 0x00000000; PC = 0x8056928 *)
mov r4 L0x20018c06;
(* ldrsh.w	r7, [lr, #1002]	; 0x3ea                 #! EA = L0x200189c6; Value = 0x00030000; PC = 0x805692c *)
mov r7 L0x200189c6;
(* ldrsh.w	r8, [lr, #426]	; 0x1aa                  #! EA = L0x20018786; Value = 0x00030ffd; PC = 0x8056930 *)
mov r8 L0x20018786;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056934 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0225@sint32 : and [cf0225 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056938 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0525@sint32 : and [cf0525 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805693c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0825@sint32 : and [cf0825 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056940 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056944 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056948 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805694c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056950 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056954 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056956 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056958 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805695c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056960 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056964 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056968 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805696a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805696c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056970 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056974 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056978 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x2001533c; PC = 0x805697c *)
mov L0x2001533c r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20015360; PC = 0x8056980 *)
mov L0x20015360 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20015384; PC = 0x8056984 *)
mov L0x20015384 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200160bc; PC = 0x8056988 *)
mov L0x200160bc r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200160e0; PC = 0x805698c *)
mov L0x200160e0 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20016104; PC = 0x8056990 *)
mov L0x20016104 r9;



(******************** offset 0, 2, 25 ********************)


(**************** CUT 217, - *****************)

ecut and [
eqmod cf0225 f0225 2048, eqmod cf0525 f0525 2048, eqmod cf0825 f0825 2048,
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x2001533c*x**0*y**2*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x20015360*x**0*y**2*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x20015384*x**0*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x200160bc*x**0*y**2*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x200160e0*x**0*y**2*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x20016104*x**0*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1200]	; 0x4b0                 #! EA = L0x20018a8c; Value = 0x00030ffd; PC = 0x8056994 *)
mov r4 L0x20018a8c;
(* ldrsh.w	r7, [lr, #624]	; 0x270                  #! EA = L0x2001884c; Value = 0x00000000; PC = 0x8056998 *)
mov r7 L0x2001884c;
(* ldrsh.w	r8, [lr, #48]	; 0x30                    #! EA = L0x2001860c; Value = 0x0ffd0ffd; PC = 0x805699c *)
mov r8 L0x2001860c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80569a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0226@sint32 : and [cf0226 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80569a4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0526@sint32 : and [cf0526 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80569a8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0826@sint32 : and [cf0826 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80569ac *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80569b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80569b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80569b8 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80569bc *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80569c0 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80569c2 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80569c4 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x80569c8 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x80569cc *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x80569d0 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x80569d4 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x80569d6 *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80569d8 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80569dc *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80569e0 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80569e4 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200153a8; PC = 0x80569e8 *)
mov L0x200153a8 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200153cc; PC = 0x80569ec *)
mov L0x200153cc r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200153f0; PC = 0x80569f0 *)
mov L0x200153f0 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016128; PC = 0x80569f4 *)
mov L0x20016128 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x2001614c; PC = 0x80569f8 *)
mov L0x2001614c r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20016170; PC = 0x80569fc *)
mov L0x20016170 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056a00 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056a04 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805663c <_Good_loop2_inner>              #! PC = 0x8056a08 *)
#b.w	0x805663c <_Good_loop2_inner>              #! 0x8056a08 = 0x8056a08;



(******************** offset 0, 2, 26 ********************)


(**************** CUT 218, - *****************)

ecut and [
eqmod cf0226 f0226 2048, eqmod cf0526 f0526 2048, eqmod cf0826 f0826 2048,
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x200153a8*x**0*y**2*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x200153cc*x**0*y**2*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x200153f0*x**0*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x20016128*x**0*y**2*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x2001614c*x**0*y**2*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x20016170*x**0*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x20018912; Value = 0x00030000; PC = 0x805663c *)
mov r4 L0x20018912;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x200186d2; Value = 0x0ffd0ffd; PC = 0x8056640 *)
mov r7 L0x200186d2;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018b52; Value = 0x0ffd0ffd; PC = 0x8056644 *)
mov r9 L0x20018b52;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0227@sint32 : and [cf0227 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf0527@sint32 : and [cf0527 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0827@sint32 : and [cf0827 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015414; PC = 0x8056680 *)
mov L0x20015414 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015438; PC = 0x8056684 *)
mov L0x20015438 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x2001545c; PC = 0x8056688 *)
mov L0x2001545c r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016194; PC = 0x805668c *)
mov L0x20016194 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200161b8; PC = 0x8056690 *)
mov L0x200161b8 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200161dc; PC = 0x8056694 *)
mov L0x200161dc r9;



(******************** offset 0, 2, 27 ********************)


(**************** CUT 219, - *****************)

ecut and [
eqmod cf0227 f0227 2048, eqmod cf0527 f0527 2048, eqmod cf0827 f0827 2048,
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x20015414*x**0*y**2*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x20015438*x**0*y**2*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x2001545c*x**0*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x20016194*x**0*y**2*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x200161b8*x**0*y**2*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x200161dc*x**0*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x20018798; Value = 0x0ffd0000; PC = 0x8056698 *)
mov r4 L0x20018798;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018c18; Value = 0x00000000; PC = 0x805669c *)
mov r6 L0x20018c18;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x200189d8; Value = 0x00030000; PC = 0x80566a0 *)
mov r9 L0x200189d8;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0228@sint32 : and [cf0228 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0528@sint32 : and [cf0528 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0828@sint32 : and [cf0828 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015480; PC = 0x80566ec *)
mov L0x20015480 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200154a4; PC = 0x80566f0 *)
mov L0x200154a4 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200154c8; PC = 0x80566f4 *)
mov L0x200154c8 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20016200; PC = 0x80566f8 *)
mov L0x20016200 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20016224; PC = 0x80566fc *)
mov L0x20016224 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20016248; PC = 0x8056700 *)
mov L0x20016248 r9;



(******************** offset 0, 2, 28 ********************)


(**************** CUT 220, - *****************)

ecut and [
eqmod cf0228 f0228 2048, eqmod cf0528 f0528 2048, eqmod cf0828 f0828 2048,
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x20015480*x**0*y**2*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x200154a4*x**0*y**2*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x200154c8*x**0*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x20016200*x**0*y**2*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x20016224*x**0*y**2*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x20016248*x**0*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x2001861e; Value = 0x00000003; PC = 0x8056704 *)
mov r4 L0x2001861e;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x20018a9e; Value = 0x0ffd0ffd; PC = 0x8056708 *)
mov r6 L0x20018a9e;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x2001885e; Value = 0x0ffd0000; PC = 0x805670c *)
mov r9 L0x2001885e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf0229@sint32 : and [cf0229 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0529@sint32 : and [cf0529 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0829@sint32 : and [cf0829 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200154ec; PC = 0x8056758 *)
mov L0x200154ec r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015510; PC = 0x805675c *)
mov L0x20015510 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015534; PC = 0x8056760 *)
mov L0x20015534 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x2001626c; PC = 0x8056764 *)
mov L0x2001626c r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016290; PC = 0x8056768 *)
mov L0x20016290 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200162b4; PC = 0x805676c *)
mov L0x200162b4 r9;



(******************** offset 0, 2, 29 ********************)


(**************** CUT 221, - *****************)

ecut and [
eqmod cf0229 f0229 2048, eqmod cf0529 f0529 2048, eqmod cf0829 f0829 2048,
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x200154ec*x**0*y**2*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x20015510*x**0*y**2*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x20015534*x**0*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x2001626c*x**0*y**2*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x20016290*x**0*y**2*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x200162b4*x**0*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018b64; Value = 0x00030000; PC = 0x8056770 *)
mov r5 L0x20018b64;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x20018924; Value = 0x00030000; PC = 0x8056774 *)
mov r6 L0x20018924;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x200186e4; Value = 0x00000ffd; PC = 0x8056778 *)
mov r9 L0x200186e4;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0230@sint32 : and [cf0230 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0530@sint32 : and [cf0530 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf0830@sint32 : and [cf0830 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015558; PC = 0x80567cc *)
mov L0x20015558 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x2001557c; PC = 0x80567d0 *)
mov L0x2001557c r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200155a0; PC = 0x80567d4 *)
mov L0x200155a0 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x200162d8; PC = 0x80567d8 *)
mov L0x200162d8 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x200162fc; PC = 0x80567dc *)
mov L0x200162fc r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20016320; PC = 0x80567e0 *)
mov L0x20016320 r9;



(******************** offset 0, 2, 30 ********************)


(**************** CUT 222, - *****************)

ecut and [
eqmod cf0230 f0230 2048, eqmod cf0530 f0530 2048, eqmod cf0830 f0830 2048,
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x20015558*x**0*y**2*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x2001557c*x**0*y**2*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x200155a0*x**0*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x200162d8*x**0*y**2*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x200162fc*x**0*y**2*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x20016320*x**0*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x200189ea; Value = 0x00030000; PC = 0x80567e4 *)
mov r5 L0x200189ea;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x200187aa; Value = 0x00030ffd; PC = 0x80567e8 *)
mov r6 L0x200187aa;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018c2a; Value = 0x00000000; PC = 0x80567ec *)
mov r8 L0x20018c2a;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf0231@sint32 : and [cf0231 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf0531@sint32 : and [cf0531 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf0831@sint32 : and [cf0831 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200155c4; PC = 0x805682c *)
mov L0x200155c4 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x200155e8; PC = 0x8056830 *)
mov L0x200155e8 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x2001560c; PC = 0x8056834 *)
mov L0x2001560c r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20016344; PC = 0x8056838 *)
mov L0x20016344 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20016368; PC = 0x805683c *)
mov L0x20016368 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x2001638c; PC = 0x8056840 *)
mov L0x2001638c r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;
(* sub.w	lr, lr, #162	; 0xa2                       #! PC = 0x8056a0c *)
subs dc lr lr 162@uint32;
(* subw	r0, r0, #2916	; 0xb64                      #! PC = 0x8056a10 *)
subc dc r0 r0 2916@uint32;
(* add.w	lr, lr, #2                                #! PC = 0x8056a14 *)
adds dc lr lr 2@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8056a18 *)
adds dc r0 r0 4@uint32;
(* cmp.w	r0, r12                                   #! PC = 0x8056a1c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8056634 <_Good_loop2>                  #! PC = 0x8056a20 *)
#bne.w	0x8056634 <_Good_loop2>                  #! 0x8056a20 = 0x8056a20;
(* addw	r1, r0, #2916	; 0xb64                      #! PC = 0x8056634 *)
adds dc r1 r0 2916@uint32;
(* vmov	s3, r1                                     #! PC = 0x8056638 *)
mov s3 r1;



(******************** offset 0, 2, 31 ********************)


(**************** CUT 223, - *****************)

ecut and [
eqmod cf0231 f0231 2048, eqmod cf0531 f0531 2048, eqmod cf0831 f0831 2048,
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x200155c4*x**0*y**2*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x200155e8*x**0*y**2*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x2001560c*x**0*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x20016344*x**0*y**2*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x20016368*x**0*y**2*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x2001638c*x**0*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(**************** CUT -,   6 *****************)

rcut and [
(-3367617)@32<=sL0x200148b0,L0x200148b0<=s3367617@32,
(-3367617)@32<=sL0x200148d4,L0x200148d4<=s3367617@32,
(-3367617)@32<=sL0x200148f8,L0x200148f8<=s3367617@32,
(-3367617)@32<=sL0x20015630,L0x20015630<=s3367617@32,
(-3367617)@32<=sL0x20015654,L0x20015654<=s3367617@32,
(-3367617)@32<=sL0x20015678,L0x20015678<=s3367617@32
,
(-3367617)@32<=sL0x2001491c,L0x2001491c<=s3367617@32,
(-3367617)@32<=sL0x20014940,L0x20014940<=s3367617@32,
(-3367617)@32<=sL0x20014964,L0x20014964<=s3367617@32,
(-3367617)@32<=sL0x2001569c,L0x2001569c<=s3367617@32,
(-3367617)@32<=sL0x200156c0,L0x200156c0<=s3367617@32,
(-3367617)@32<=sL0x200156e4,L0x200156e4<=s3367617@32
,
(-3367617)@32<=sL0x20014988,L0x20014988<=s3367617@32,
(-3367617)@32<=sL0x200149ac,L0x200149ac<=s3367617@32,
(-3367617)@32<=sL0x200149d0,L0x200149d0<=s3367617@32,
(-3367617)@32<=sL0x20015708,L0x20015708<=s3367617@32,
(-3367617)@32<=sL0x2001572c,L0x2001572c<=s3367617@32,
(-3367617)@32<=sL0x20015750,L0x20015750<=s3367617@32
,
(-3367617)@32<=sL0x200149f4,L0x200149f4<=s3367617@32,
(-3367617)@32<=sL0x20014a18,L0x20014a18<=s3367617@32,
(-3367617)@32<=sL0x20014a3c,L0x20014a3c<=s3367617@32,
(-3367617)@32<=sL0x20015774,L0x20015774<=s3367617@32,
(-3367617)@32<=sL0x20015798,L0x20015798<=s3367617@32,
(-3367617)@32<=sL0x200157bc,L0x200157bc<=s3367617@32
,
(-3367617)@32<=sL0x20014a60,L0x20014a60<=s3367617@32,
(-3367617)@32<=sL0x20014a84,L0x20014a84<=s3367617@32,
(-3367617)@32<=sL0x20014aa8,L0x20014aa8<=s3367617@32,
(-3367617)@32<=sL0x200157e0,L0x200157e0<=s3367617@32,
(-3367617)@32<=sL0x20015804,L0x20015804<=s3367617@32,
(-3367617)@32<=sL0x20015828,L0x20015828<=s3367617@32
,
(-3367617)@32<=sL0x20014acc,L0x20014acc<=s3367617@32,
(-3367617)@32<=sL0x20014af0,L0x20014af0<=s3367617@32,
(-3367617)@32<=sL0x20014b14,L0x20014b14<=s3367617@32,
(-3367617)@32<=sL0x2001584c,L0x2001584c<=s3367617@32,
(-3367617)@32<=sL0x20015870,L0x20015870<=s3367617@32,
(-3367617)@32<=sL0x20015894,L0x20015894<=s3367617@32
,
(-3367617)@32<=sL0x20014b38,L0x20014b38<=s3367617@32,
(-3367617)@32<=sL0x20014b5c,L0x20014b5c<=s3367617@32,
(-3367617)@32<=sL0x20014b80,L0x20014b80<=s3367617@32,
(-3367617)@32<=sL0x200158b8,L0x200158b8<=s3367617@32,
(-3367617)@32<=sL0x200158dc,L0x200158dc<=s3367617@32,
(-3367617)@32<=sL0x20015900,L0x20015900<=s3367617@32
,
(-3367617)@32<=sL0x20014ba4,L0x20014ba4<=s3367617@32,
(-3367617)@32<=sL0x20014bc8,L0x20014bc8<=s3367617@32,
(-3367617)@32<=sL0x20014bec,L0x20014bec<=s3367617@32,
(-3367617)@32<=sL0x20015924,L0x20015924<=s3367617@32,
(-3367617)@32<=sL0x20015948,L0x20015948<=s3367617@32,
(-3367617)@32<=sL0x2001596c,L0x2001596c<=s3367617@32
,
(-3367617)@32<=sL0x20014c10,L0x20014c10<=s3367617@32,
(-3367617)@32<=sL0x20014c34,L0x20014c34<=s3367617@32,
(-3367617)@32<=sL0x20014c58,L0x20014c58<=s3367617@32,
(-3367617)@32<=sL0x20015990,L0x20015990<=s3367617@32,
(-3367617)@32<=sL0x200159b4,L0x200159b4<=s3367617@32,
(-3367617)@32<=sL0x200159d8,L0x200159d8<=s3367617@32
,
(-3367617)@32<=sL0x20014c7c,L0x20014c7c<=s3367617@32,
(-3367617)@32<=sL0x20014ca0,L0x20014ca0<=s3367617@32,
(-3367617)@32<=sL0x20014cc4,L0x20014cc4<=s3367617@32,
(-3367617)@32<=sL0x200159fc,L0x200159fc<=s3367617@32,
(-3367617)@32<=sL0x20015a20,L0x20015a20<=s3367617@32,
(-3367617)@32<=sL0x20015a44,L0x20015a44<=s3367617@32
,
(-3367617)@32<=sL0x20014ce8,L0x20014ce8<=s3367617@32,
(-3367617)@32<=sL0x20014d0c,L0x20014d0c<=s3367617@32,
(-3367617)@32<=sL0x20014d30,L0x20014d30<=s3367617@32,
(-3367617)@32<=sL0x20015a68,L0x20015a68<=s3367617@32,
(-3367617)@32<=sL0x20015a8c,L0x20015a8c<=s3367617@32,
(-3367617)@32<=sL0x20015ab0,L0x20015ab0<=s3367617@32
,
(-3367617)@32<=sL0x20014d54,L0x20014d54<=s3367617@32,
(-3367617)@32<=sL0x20014d78,L0x20014d78<=s3367617@32,
(-3367617)@32<=sL0x20014d9c,L0x20014d9c<=s3367617@32,
(-3367617)@32<=sL0x20015ad4,L0x20015ad4<=s3367617@32,
(-3367617)@32<=sL0x20015af8,L0x20015af8<=s3367617@32,
(-3367617)@32<=sL0x20015b1c,L0x20015b1c<=s3367617@32
,
(-3367617)@32<=sL0x20014dc0,L0x20014dc0<=s3367617@32,
(-3367617)@32<=sL0x20014de4,L0x20014de4<=s3367617@32,
(-3367617)@32<=sL0x20014e08,L0x20014e08<=s3367617@32,
(-3367617)@32<=sL0x20015b40,L0x20015b40<=s3367617@32,
(-3367617)@32<=sL0x20015b64,L0x20015b64<=s3367617@32,
(-3367617)@32<=sL0x20015b88,L0x20015b88<=s3367617@32
,
(-3367617)@32<=sL0x20014e2c,L0x20014e2c<=s3367617@32,
(-3367617)@32<=sL0x20014e50,L0x20014e50<=s3367617@32,
(-3367617)@32<=sL0x20014e74,L0x20014e74<=s3367617@32,
(-3367617)@32<=sL0x20015bac,L0x20015bac<=s3367617@32,
(-3367617)@32<=sL0x20015bd0,L0x20015bd0<=s3367617@32,
(-3367617)@32<=sL0x20015bf4,L0x20015bf4<=s3367617@32
,
(-3367617)@32<=sL0x20014e98,L0x20014e98<=s3367617@32,
(-3367617)@32<=sL0x20014ebc,L0x20014ebc<=s3367617@32,
(-3367617)@32<=sL0x20014ee0,L0x20014ee0<=s3367617@32,
(-3367617)@32<=sL0x20015c18,L0x20015c18<=s3367617@32,
(-3367617)@32<=sL0x20015c3c,L0x20015c3c<=s3367617@32,
(-3367617)@32<=sL0x20015c60,L0x20015c60<=s3367617@32
,
(-3367617)@32<=sL0x20014f04,L0x20014f04<=s3367617@32,
(-3367617)@32<=sL0x20014f28,L0x20014f28<=s3367617@32,
(-3367617)@32<=sL0x20014f4c,L0x20014f4c<=s3367617@32,
(-3367617)@32<=sL0x20015c84,L0x20015c84<=s3367617@32,
(-3367617)@32<=sL0x20015ca8,L0x20015ca8<=s3367617@32,
(-3367617)@32<=sL0x20015ccc,L0x20015ccc<=s3367617@32
,
(-3367617)@32<=sL0x20014f70,L0x20014f70<=s3367617@32,
(-3367617)@32<=sL0x20014f94,L0x20014f94<=s3367617@32,
(-3367617)@32<=sL0x20014fb8,L0x20014fb8<=s3367617@32,
(-3367617)@32<=sL0x20015cf0,L0x20015cf0<=s3367617@32,
(-3367617)@32<=sL0x20015d14,L0x20015d14<=s3367617@32,
(-3367617)@32<=sL0x20015d38,L0x20015d38<=s3367617@32
,
(-3367617)@32<=sL0x20014fdc,L0x20014fdc<=s3367617@32,
(-3367617)@32<=sL0x20015000,L0x20015000<=s3367617@32,
(-3367617)@32<=sL0x20015024,L0x20015024<=s3367617@32,
(-3367617)@32<=sL0x20015d5c,L0x20015d5c<=s3367617@32,
(-3367617)@32<=sL0x20015d80,L0x20015d80<=s3367617@32,
(-3367617)@32<=sL0x20015da4,L0x20015da4<=s3367617@32
,
(-3367617)@32<=sL0x20015048,L0x20015048<=s3367617@32,
(-3367617)@32<=sL0x2001506c,L0x2001506c<=s3367617@32,
(-3367617)@32<=sL0x20015090,L0x20015090<=s3367617@32,
(-3367617)@32<=sL0x20015dc8,L0x20015dc8<=s3367617@32,
(-3367617)@32<=sL0x20015dec,L0x20015dec<=s3367617@32,
(-3367617)@32<=sL0x20015e10,L0x20015e10<=s3367617@32
,
(-3367617)@32<=sL0x200150b4,L0x200150b4<=s3367617@32,
(-3367617)@32<=sL0x200150d8,L0x200150d8<=s3367617@32,
(-3367617)@32<=sL0x200150fc,L0x200150fc<=s3367617@32,
(-3367617)@32<=sL0x20015e34,L0x20015e34<=s3367617@32,
(-3367617)@32<=sL0x20015e58,L0x20015e58<=s3367617@32,
(-3367617)@32<=sL0x20015e7c,L0x20015e7c<=s3367617@32
,
(-3367617)@32<=sL0x20015120,L0x20015120<=s3367617@32,
(-3367617)@32<=sL0x20015144,L0x20015144<=s3367617@32,
(-3367617)@32<=sL0x20015168,L0x20015168<=s3367617@32,
(-3367617)@32<=sL0x20015ea0,L0x20015ea0<=s3367617@32,
(-3367617)@32<=sL0x20015ec4,L0x20015ec4<=s3367617@32,
(-3367617)@32<=sL0x20015ee8,L0x20015ee8<=s3367617@32
,
(-3367617)@32<=sL0x2001518c,L0x2001518c<=s3367617@32,
(-3367617)@32<=sL0x200151b0,L0x200151b0<=s3367617@32,
(-3367617)@32<=sL0x200151d4,L0x200151d4<=s3367617@32,
(-3367617)@32<=sL0x20015f0c,L0x20015f0c<=s3367617@32,
(-3367617)@32<=sL0x20015f30,L0x20015f30<=s3367617@32,
(-3367617)@32<=sL0x20015f54,L0x20015f54<=s3367617@32
,
(-3367617)@32<=sL0x200151f8,L0x200151f8<=s3367617@32,
(-3367617)@32<=sL0x2001521c,L0x2001521c<=s3367617@32,
(-3367617)@32<=sL0x20015240,L0x20015240<=s3367617@32,
(-3367617)@32<=sL0x20015f78,L0x20015f78<=s3367617@32,
(-3367617)@32<=sL0x20015f9c,L0x20015f9c<=s3367617@32,
(-3367617)@32<=sL0x20015fc0,L0x20015fc0<=s3367617@32
,
(-3367617)@32<=sL0x20015264,L0x20015264<=s3367617@32,
(-3367617)@32<=sL0x20015288,L0x20015288<=s3367617@32,
(-3367617)@32<=sL0x200152ac,L0x200152ac<=s3367617@32,
(-3367617)@32<=sL0x20015fe4,L0x20015fe4<=s3367617@32,
(-3367617)@32<=sL0x20016008,L0x20016008<=s3367617@32,
(-3367617)@32<=sL0x2001602c,L0x2001602c<=s3367617@32
,
(-3367617)@32<=sL0x200152d0,L0x200152d0<=s3367617@32,
(-3367617)@32<=sL0x200152f4,L0x200152f4<=s3367617@32,
(-3367617)@32<=sL0x20015318,L0x20015318<=s3367617@32,
(-3367617)@32<=sL0x20016050,L0x20016050<=s3367617@32,
(-3367617)@32<=sL0x20016074,L0x20016074<=s3367617@32,
(-3367617)@32<=sL0x20016098,L0x20016098<=s3367617@32
,
(-3367617)@32<=sL0x2001533c,L0x2001533c<=s3367617@32,
(-3367617)@32<=sL0x20015360,L0x20015360<=s3367617@32,
(-3367617)@32<=sL0x20015384,L0x20015384<=s3367617@32,
(-3367617)@32<=sL0x200160bc,L0x200160bc<=s3367617@32,
(-3367617)@32<=sL0x200160e0,L0x200160e0<=s3367617@32,
(-3367617)@32<=sL0x20016104,L0x20016104<=s3367617@32
,
(-3367617)@32<=sL0x200153a8,L0x200153a8<=s3367617@32,
(-3367617)@32<=sL0x200153cc,L0x200153cc<=s3367617@32,
(-3367617)@32<=sL0x200153f0,L0x200153f0<=s3367617@32,
(-3367617)@32<=sL0x20016128,L0x20016128<=s3367617@32,
(-3367617)@32<=sL0x2001614c,L0x2001614c<=s3367617@32,
(-3367617)@32<=sL0x20016170,L0x20016170<=s3367617@32
,
(-3367617)@32<=sL0x20015414,L0x20015414<=s3367617@32,
(-3367617)@32<=sL0x20015438,L0x20015438<=s3367617@32,
(-3367617)@32<=sL0x2001545c,L0x2001545c<=s3367617@32,
(-3367617)@32<=sL0x20016194,L0x20016194<=s3367617@32,
(-3367617)@32<=sL0x200161b8,L0x200161b8<=s3367617@32,
(-3367617)@32<=sL0x200161dc,L0x200161dc<=s3367617@32
,
(-3367617)@32<=sL0x20015480,L0x20015480<=s3367617@32,
(-3367617)@32<=sL0x200154a4,L0x200154a4<=s3367617@32,
(-3367617)@32<=sL0x200154c8,L0x200154c8<=s3367617@32,
(-3367617)@32<=sL0x20016200,L0x20016200<=s3367617@32,
(-3367617)@32<=sL0x20016224,L0x20016224<=s3367617@32,
(-3367617)@32<=sL0x20016248,L0x20016248<=s3367617@32
,
(-3367617)@32<=sL0x200154ec,L0x200154ec<=s3367617@32,
(-3367617)@32<=sL0x20015510,L0x20015510<=s3367617@32,
(-3367617)@32<=sL0x20015534,L0x20015534<=s3367617@32,
(-3367617)@32<=sL0x2001626c,L0x2001626c<=s3367617@32,
(-3367617)@32<=sL0x20016290,L0x20016290<=s3367617@32,
(-3367617)@32<=sL0x200162b4,L0x200162b4<=s3367617@32
,
(-3367617)@32<=sL0x20015558,L0x20015558<=s3367617@32,
(-3367617)@32<=sL0x2001557c,L0x2001557c<=s3367617@32,
(-3367617)@32<=sL0x200155a0,L0x200155a0<=s3367617@32,
(-3367617)@32<=sL0x200162d8,L0x200162d8<=s3367617@32,
(-3367617)@32<=sL0x200162fc,L0x200162fc<=s3367617@32,
(-3367617)@32<=sL0x20016320,L0x20016320<=s3367617@32
,
(-3367617)@32<=sL0x200155c4,L0x200155c4<=s3367617@32,
(-3367617)@32<=sL0x200155e8,L0x200155e8<=s3367617@32,
(-3367617)@32<=sL0x2001560c,L0x2001560c<=s3367617@32,
(-3367617)@32<=sL0x20016344,L0x20016344<=s3367617@32,
(-3367617)@32<=sL0x20016368,L0x20016368<=s3367617@32,
(-3367617)@32<=sL0x2001638c,L0x2001638c<=s3367617@32
] prove with [ precondition ];





(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x20018872; Value = 0x00030000; PC = 0x805663c *)
mov r4 L0x20018872;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x20018632; Value = 0x0ffd0003; PC = 0x8056640 *)
mov r7 L0x20018632;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018ab2; Value = 0x00030000; PC = 0x8056644 *)
mov r9 L0x20018ab2;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1200@sint32 : and [cf1200 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1500@sint32 : and [cf1500 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1800@sint32 : and [cf1800 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200148b4; PC = 0x8056680 *)
mov L0x200148b4 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200148d8; PC = 0x8056684 *)
mov L0x200148d8 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x200148fc; PC = 0x8056688 *)
mov L0x200148fc r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015634; PC = 0x805668c *)
mov L0x20015634 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015658; PC = 0x8056690 *)
mov L0x20015658 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x2001567c; PC = 0x8056694 *)
mov L0x2001567c r9;



(******************** offset 1, 2,  0 ********************)


(**************** CUT 224, - *****************)

ecut and [
eqmod cf1200 f1200 2048, eqmod cf1500 f1500 2048, eqmod cf1800 f1800 2048,
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x200148b4*x**1*y**2*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x200148d8*x**1*y**2*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x200148fc*x**1*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x20015634*x**1*y**2*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x20015658*x**1*y**2*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x2001567c*x**1*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x200186f8; Value = 0x00000ffd; PC = 0x8056698 *)
mov r4 L0x200186f8;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018b78; Value = 0x00000000; PC = 0x805669c *)
mov r6 L0x20018b78;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x20018938; Value = 0x00000000; PC = 0x80566a0 *)
mov r9 L0x20018938;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1201@sint32 : and [cf1201 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1501@sint32 : and [cf1501 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1801@sint32 : and [cf1801 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014920; PC = 0x80566ec *)
mov L0x20014920 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014944; PC = 0x80566f0 *)
mov L0x20014944 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014968; PC = 0x80566f4 *)
mov L0x20014968 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200156a0; PC = 0x80566f8 *)
mov L0x200156a0 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200156c4; PC = 0x80566fc *)
mov L0x200156c4 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200156e8; PC = 0x8056700 *)
mov L0x200156e8 r9;



(******************** offset 1, 2,  1 ********************)


(**************** CUT 225, - *****************)

ecut and [
eqmod cf1201 f1201 2048, eqmod cf1501 f1501 2048, eqmod cf1801 f1801 2048,
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x20014920*x**1*y**2*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x20014944*x**1*y**2*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x20014968*x**1*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x200156a0*x**1*y**2*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x200156c4*x**1*y**2*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x200156e8*x**1*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x2001857e; Value = 0x00030ffd; PC = 0x8056704 *)
mov r4 L0x2001857e;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x200189fe; Value = 0x00030000; PC = 0x8056708 *)
mov r6 L0x200189fe;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x200187be; Value = 0x0ffd0000; PC = 0x805670c *)
mov r9 L0x200187be;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1202@sint32 : and [cf1202 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1502@sint32 : and [cf1502 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1802@sint32 : and [cf1802 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x2001498c; PC = 0x8056758 *)
mov L0x2001498c r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200149b0; PC = 0x805675c *)
mov L0x200149b0 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200149d4; PC = 0x8056760 *)
mov L0x200149d4 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x2001570c; PC = 0x8056764 *)
mov L0x2001570c r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015730; PC = 0x8056768 *)
mov L0x20015730 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015754; PC = 0x805676c *)
mov L0x20015754 r9;



(******************** offset 1, 2,  2 ********************)


(**************** CUT 226, - *****************)

ecut and [
eqmod cf1202 f1202 2048, eqmod cf1502 f1502 2048, eqmod cf1802 f1802 2048,
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x2001498c*x**1*y**2*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x200149b0*x**1*y**2*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x200149d4*x**1*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x2001570c*x**1*y**2*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x20015730*x**1*y**2*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x20015754*x**1*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018ac4; Value = 0x00030000; PC = 0x8056770 *)
mov r5 L0x20018ac4;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x20018884; Value = 0x00030000; PC = 0x8056774 *)
mov r6 L0x20018884;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x20018644; Value = 0x00030003; PC = 0x8056778 *)
mov r9 L0x20018644;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1203@sint32 : and [cf1203 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1503@sint32 : and [cf1503 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1803@sint32 : and [cf1803 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x200149f8; PC = 0x80567cc *)
mov L0x200149f8 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014a1c; PC = 0x80567d0 *)
mov L0x20014a1c r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014a40; PC = 0x80567d4 *)
mov L0x20014a40 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015778; PC = 0x80567d8 *)
mov L0x20015778 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x2001579c; PC = 0x80567dc *)
mov L0x2001579c r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x200157c0; PC = 0x80567e0 *)
mov L0x200157c0 r9;



(******************** offset 1, 2,  3 ********************)


(**************** CUT 227, - *****************)

ecut and [
eqmod cf1203 f1203 2048, eqmod cf1503 f1503 2048, eqmod cf1803 f1803 2048,
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x200149f8*x**1*y**2*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x20014a1c*x**1*y**2*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x20014a40*x**1*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x20015778*x**1*y**2*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x2001579c*x**1*y**2*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x200157c0*x**1*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x2001894a; Value = 0x00000000; PC = 0x80567e4 *)
mov r5 L0x2001894a;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x2001870a; Value = 0x00030ffd; PC = 0x80567e8 *)
mov r6 L0x2001870a;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018b8a; Value = 0x00030000; PC = 0x80567ec *)
mov r8 L0x20018b8a;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1204@sint32 : and [cf1204 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1504@sint32 : and [cf1504 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1804@sint32 : and [cf1804 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014a64; PC = 0x805682c *)
mov L0x20014a64 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014a88; PC = 0x8056830 *)
mov L0x20014a88 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014aac; PC = 0x8056834 *)
mov L0x20014aac r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x200157e4; PC = 0x8056838 *)
mov L0x200157e4 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015808; PC = 0x805683c *)
mov L0x20015808 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x2001582c; PC = 0x8056840 *)
mov L0x2001582c r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;



(******************** offset 1, 2,  4 ********************)


(**************** CUT 228, - *****************)

ecut and [
eqmod cf1204 f1204 2048, eqmod cf1504 f1504 2048, eqmod cf1804 f1804 2048,
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x20014a64*x**1*y**2*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x20014a88*x**1*y**2*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x20014aac*x**1*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x200157e4*x**1*y**2*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x20015808*x**1*y**2*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x2001582c*x**1*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #606]	; 0x25e                  #! EA = L0x200187d0; Value = 0x0ffd0000; PC = 0x8056850 *)
mov r5 L0x200187d0;
(* ldrsh.w	r6, [lr, #30]                           #! EA = L0x20018590; Value = 0x0ffd0003; PC = 0x8056854 *)
mov r6 L0x20018590;
(* ldrsh.w	r8, [lr, #1182]	; 0x49e                 #! EA = L0x20018a10; Value = 0x0ffd0003; PC = 0x8056858 *)
mov r8 L0x20018a10;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805685c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1205@sint32 : and [cf1205 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056860 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1505@sint32 : and [cf1505 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056864 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1805@sint32 : and [cf1805 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056868 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x805686c *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056870 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056874 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056878 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x805687c *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056880 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056884 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056888 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x805688c *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056890 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056892 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056894 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014ad0; PC = 0x8056898 *)
mov L0x20014ad0 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014af4; PC = 0x805689c *)
mov L0x20014af4 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014b18; PC = 0x80568a0 *)
mov L0x20014b18 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015850; PC = 0x80568a4 *)
mov L0x20015850 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015874; PC = 0x80568a8 *)
mov L0x20015874 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015898; PC = 0x80568ac *)
mov L0x20015898 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80568b0 *)
adds dc r0 r0 648@uint32;



(******************** offset 1, 2,  5 ********************)


(**************** CUT 229, - *****************)

ecut and [
eqmod cf1205 f1205 2048, eqmod cf1505 f1505 2048, eqmod cf1805 f1805 2048,
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20014ad0*x**1*y**2*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20014af4*x**1*y**2*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20014b18*x**1*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20015850*x**1*y**2*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20015874*x**1*y**2*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20015898*x**1*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #228]	; 0xe4                   #! EA = L0x20018656; Value = 0x0ffd0ffd; PC = 0x80568b4 *)
mov r5 L0x20018656;
(* ldrsh.w	r7, [lr, #1380]	; 0x564                 #! EA = L0x20018ad6; Value = 0x00000ffd; PC = 0x80568b8 *)
mov r7 L0x20018ad6;
(* ldrsh.w	r8, [lr, #804]	; 0x324                  #! EA = L0x20018896; Value = 0x0ffd0000; PC = 0x80568bc *)
mov r8 L0x20018896;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80568c0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1206@sint32 : and [cf1206 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80568c4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1506@sint32 : and [cf1506 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80568c8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1806@sint32 : and [cf1806 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x80568cc *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x80568d0 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x80568d4 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x80568d8 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x80568dc *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x80568e0 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x80568e2 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x80568e4 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x80568e8 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x80568ec *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x80568f0 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x80568f4 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80568f8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x80568fc *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056900 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x8056904 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056908 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x805690c *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014b3c; PC = 0x8056910 *)
mov L0x20014b3c r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014b60; PC = 0x8056914 *)
mov L0x20014b60 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014b84; PC = 0x8056918 *)
mov L0x20014b84 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200158bc; PC = 0x805691c *)
mov L0x200158bc r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200158e0; PC = 0x8056920 *)
mov L0x200158e0 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015904; PC = 0x8056924 *)
mov L0x20015904 r9;



(******************** offset 1, 2,  6 ********************)


(**************** CUT 230, - *****************)

ecut and [
eqmod cf1206 f1206 2048, eqmod cf1506 f1506 2048, eqmod cf1806 f1806 2048,
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x20014b3c*x**1*y**2*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x20014b60*x**1*y**2*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x20014b84*x**1*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x200158bc*x**1*y**2*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x200158e0*x**1*y**2*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x20015904*x**1*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1578]	; 0x62a                 #! EA = L0x20018b9c; Value = 0x00030000; PC = 0x8056928 *)
mov r4 L0x20018b9c;
(* ldrsh.w	r7, [lr, #1002]	; 0x3ea                 #! EA = L0x2001895c; Value = 0x00030003; PC = 0x805692c *)
mov r7 L0x2001895c;
(* ldrsh.w	r8, [lr, #426]	; 0x1aa                  #! EA = L0x2001871c; Value = 0x00030000; PC = 0x8056930 *)
mov r8 L0x2001871c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056934 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1207@sint32 : and [cf1207 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056938 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1507@sint32 : and [cf1507 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805693c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1807@sint32 : and [cf1807 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056940 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056944 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056948 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805694c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056950 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056954 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056956 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056958 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805695c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056960 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056964 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056968 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805696a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805696c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056970 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056974 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056978 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014ba8; PC = 0x805697c *)
mov L0x20014ba8 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014bcc; PC = 0x8056980 *)
mov L0x20014bcc r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014bf0; PC = 0x8056984 *)
mov L0x20014bf0 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015928; PC = 0x8056988 *)
mov L0x20015928 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x2001594c; PC = 0x805698c *)
mov L0x2001594c r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015970; PC = 0x8056990 *)
mov L0x20015970 r9;



(******************** offset 1, 2,  7 ********************)


(**************** CUT 231, - *****************)

ecut and [
eqmod cf1207 f1207 2048, eqmod cf1507 f1507 2048, eqmod cf1807 f1807 2048,
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x20014ba8*x**1*y**2*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x20014bcc*x**1*y**2*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x20014bf0*x**1*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x20015928*x**1*y**2*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x2001594c*x**1*y**2*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x20015970*x**1*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1200]	; 0x4b0                 #! EA = L0x20018a22; Value = 0x00000000; PC = 0x8056994 *)
mov r4 L0x20018a22;
(* ldrsh.w	r7, [lr, #624]	; 0x270                  #! EA = L0x200187e2; Value = 0x00030000; PC = 0x8056998 *)
mov r7 L0x200187e2;
(* ldrsh.w	r8, [lr, #48]	; 0x30                    #! EA = L0x200185a2; Value = 0x00000000; PC = 0x805699c *)
mov r8 L0x200185a2;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80569a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1208@sint32 : and [cf1208 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80569a4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1508@sint32 : and [cf1508 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80569a8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1808@sint32 : and [cf1808 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80569ac *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80569b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80569b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80569b8 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80569bc *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80569c0 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80569c2 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80569c4 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x80569c8 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x80569cc *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x80569d0 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x80569d4 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x80569d6 *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80569d8 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80569dc *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80569e0 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80569e4 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014c14; PC = 0x80569e8 *)
mov L0x20014c14 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014c38; PC = 0x80569ec *)
mov L0x20014c38 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014c5c; PC = 0x80569f0 *)
mov L0x20014c5c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015994; PC = 0x80569f4 *)
mov L0x20015994 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x200159b8; PC = 0x80569f8 *)
mov L0x200159b8 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200159dc; PC = 0x80569fc *)
mov L0x200159dc r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056a00 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056a04 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805663c <_Good_loop2_inner>              #! PC = 0x8056a08 *)
#b.w	0x805663c <_Good_loop2_inner>              #! 0x8056a08 = 0x8056a08;



(******************** offset 1, 2,  8 ********************)


(**************** CUT 232, - *****************)

ecut and [
eqmod cf1208 f1208 2048, eqmod cf1508 f1508 2048, eqmod cf1808 f1808 2048,
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x20014c14*x**1*y**2*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x20014c38*x**1*y**2*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x20014c5c*x**1*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x20015994*x**1*y**2*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x200159b8*x**1*y**2*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x200159dc*x**1*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x200188a8; Value = 0x0ffd0003; PC = 0x805663c *)
mov r4 L0x200188a8;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x20018668; Value = 0x0ffd0000; PC = 0x8056640 *)
mov r7 L0x20018668;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018ae8; Value = 0x00000ffd; PC = 0x8056644 *)
mov r9 L0x20018ae8;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1209@sint32 : and [cf1209 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1509@sint32 : and [cf1509 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1809@sint32 : and [cf1809 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014c80; PC = 0x8056680 *)
mov L0x20014c80 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014ca4; PC = 0x8056684 *)
mov L0x20014ca4 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014cc8; PC = 0x8056688 *)
mov L0x20014cc8 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015a00; PC = 0x805668c *)
mov L0x20015a00 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015a24; PC = 0x8056690 *)
mov L0x20015a24 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015a48; PC = 0x8056694 *)
mov L0x20015a48 r9;



(******************** offset 1, 2,  9 ********************)


(**************** CUT 233, - *****************)

ecut and [
eqmod cf1209 f1209 2048, eqmod cf1509 f1509 2048, eqmod cf1809 f1809 2048,
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20014c80*x**1*y**2*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20014ca4*x**1*y**2*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20014cc8*x**1*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20015a00*x**1*y**2*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20015a24*x**1*y**2*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20015a48*x**1*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x2001872e; Value = 0x00000ffd; PC = 0x8056698 *)
mov r4 L0x2001872e;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018bae; Value = 0x0ffd0003; PC = 0x805669c *)
mov r6 L0x20018bae;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x2001896e; Value = 0x00030003; PC = 0x80566a0 *)
mov r9 L0x2001896e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1210@sint32 : and [cf1210 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1510@sint32 : and [cf1510 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1810@sint32 : and [cf1810 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014cec; PC = 0x80566ec *)
mov L0x20014cec r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014d10; PC = 0x80566f0 *)
mov L0x20014d10 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014d34; PC = 0x80566f4 *)
mov L0x20014d34 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015a6c; PC = 0x80566f8 *)
mov L0x20015a6c r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015a90; PC = 0x80566fc *)
mov L0x20015a90 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015ab4; PC = 0x8056700 *)
mov L0x20015ab4 r9;



(******************** offset 1, 2, 10 ********************)


(**************** CUT 234, - *****************)

ecut and [
eqmod cf1210 f1210 2048, eqmod cf1510 f1510 2048, eqmod cf1810 f1810 2048,
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20014cec*x**1*y**2*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20014d10*x**1*y**2*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20014d34*x**1*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20015a6c*x**1*y**2*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20015a90*x**1*y**2*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20015ab4*x**1*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x200185b4; Value = 0x00030ffd; PC = 0x8056704 *)
mov r4 L0x200185b4;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x20018a34; Value = 0x00030003; PC = 0x8056708 *)
mov r6 L0x20018a34;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x200187f4; Value = 0x00000003; PC = 0x805670c *)
mov r9 L0x200187f4;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1211@sint32 : and [cf1211 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1511@sint32 : and [cf1511 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1811@sint32 : and [cf1811 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014d58; PC = 0x8056758 *)
mov L0x20014d58 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014d7c; PC = 0x805675c *)
mov L0x20014d7c r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014da0; PC = 0x8056760 *)
mov L0x20014da0 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015ad8; PC = 0x8056764 *)
mov L0x20015ad8 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015afc; PC = 0x8056768 *)
mov L0x20015afc r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015b20; PC = 0x805676c *)
mov L0x20015b20 r9;



(******************** offset 1, 2, 11 ********************)


(**************** CUT 235, - *****************)

ecut and [
eqmod cf1211 f1211 2048, eqmod cf1511 f1511 2048, eqmod cf1811 f1811 2048,
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20014d58*x**1*y**2*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20014d7c*x**1*y**2*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20014da0*x**1*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20015ad8*x**1*y**2*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20015afc*x**1*y**2*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20015b20*x**1*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018afa; Value = 0x0ffd0ffd; PC = 0x8056770 *)
mov r5 L0x20018afa;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x200188ba; Value = 0x0ffd0ffd; PC = 0x8056774 *)
mov r6 L0x200188ba;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x2001867a; Value = 0x0ffd0003; PC = 0x8056778 *)
mov r9 L0x2001867a;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1212@sint32 : and [cf1212 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1512@sint32 : and [cf1512 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1812@sint32 : and [cf1812 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20014dc4; PC = 0x80567cc *)
mov L0x20014dc4 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014de8; PC = 0x80567d0 *)
mov L0x20014de8 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014e0c; PC = 0x80567d4 *)
mov L0x20014e0c r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015b44; PC = 0x80567d8 *)
mov L0x20015b44 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015b68; PC = 0x80567dc *)
mov L0x20015b68 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015b8c; PC = 0x80567e0 *)
mov L0x20015b8c r9;



(******************** offset 1, 2, 12 ********************)


(**************** CUT 236, - *****************)

ecut and [
eqmod cf1212 f1212 2048, eqmod cf1512 f1512 2048, eqmod cf1812 f1812 2048,
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20014dc4*x**1*y**2*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20014de8*x**1*y**2*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20014e0c*x**1*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20015b44*x**1*y**2*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20015b68*x**1*y**2*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20015b8c*x**1*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x20018980; Value = 0x00000003; PC = 0x80567e4 *)
mov r5 L0x20018980;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x20018740; Value = 0x00030ffd; PC = 0x80567e8 *)
mov r6 L0x20018740;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018bc0; Value = 0x0ffd0ffd; PC = 0x80567ec *)
mov r8 L0x20018bc0;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1213@sint32 : and [cf1213 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1513@sint32 : and [cf1513 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1813@sint32 : and [cf1813 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014e30; PC = 0x805682c *)
mov L0x20014e30 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014e54; PC = 0x8056830 *)
mov L0x20014e54 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014e78; PC = 0x8056834 *)
mov L0x20014e78 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015bb0; PC = 0x8056838 *)
mov L0x20015bb0 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015bd4; PC = 0x805683c *)
mov L0x20015bd4 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015bf8; PC = 0x8056840 *)
mov L0x20015bf8 r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;



(******************** offset 1, 2, 13 ********************)


(**************** CUT 237, - *****************)

ecut and [
eqmod cf1213 f1213 2048, eqmod cf1513 f1513 2048, eqmod cf1813 f1813 2048,
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20014e30*x**1*y**2*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20014e54*x**1*y**2*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20014e78*x**1*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20015bb0*x**1*y**2*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20015bd4*x**1*y**2*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20015bf8*x**1*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #606]	; 0x25e                  #! EA = L0x20018806; Value = 0x00030003; PC = 0x8056850 *)
mov r5 L0x20018806;
(* ldrsh.w	r6, [lr, #30]                           #! EA = L0x200185c6; Value = 0x00000003; PC = 0x8056854 *)
mov r6 L0x200185c6;
(* ldrsh.w	r8, [lr, #1182]	; 0x49e                 #! EA = L0x20018a46; Value = 0x00030003; PC = 0x8056858 *)
mov r8 L0x20018a46;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805685c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1214@sint32 : and [cf1214 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056860 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1514@sint32 : and [cf1514 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056864 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1814@sint32 : and [cf1814 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056868 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x805686c *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056870 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056874 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056878 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x805687c *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056880 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056884 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056888 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x805688c *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056890 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056892 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056894 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014e9c; PC = 0x8056898 *)
mov L0x20014e9c r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014ec0; PC = 0x805689c *)
mov L0x20014ec0 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014ee4; PC = 0x80568a0 *)
mov L0x20014ee4 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015c1c; PC = 0x80568a4 *)
mov L0x20015c1c r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015c40; PC = 0x80568a8 *)
mov L0x20015c40 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015c64; PC = 0x80568ac *)
mov L0x20015c64 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80568b0 *)
adds dc r0 r0 648@uint32;



(******************** offset 1, 2, 14 ********************)


(**************** CUT 238, - *****************)

ecut and [
eqmod cf1214 f1214 2048, eqmod cf1514 f1514 2048, eqmod cf1814 f1814 2048,
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20014e9c*x**1*y**2*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20014ec0*x**1*y**2*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20014ee4*x**1*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20015c1c*x**1*y**2*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20015c40*x**1*y**2*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20015c64*x**1*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #228]	; 0xe4                   #! EA = L0x2001868c; Value = 0x00000003; PC = 0x80568b4 *)
mov r5 L0x2001868c;
(* ldrsh.w	r7, [lr, #1380]	; 0x564                 #! EA = L0x20018b0c; Value = 0x00000ffd; PC = 0x80568b8 *)
mov r7 L0x20018b0c;
(* ldrsh.w	r8, [lr, #804]	; 0x324                  #! EA = L0x200188cc; Value = 0x00030000; PC = 0x80568bc *)
mov r8 L0x200188cc;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80568c0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1215@sint32 : and [cf1215 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80568c4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1515@sint32 : and [cf1515 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80568c8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1815@sint32 : and [cf1815 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x80568cc *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x80568d0 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x80568d4 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x80568d8 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x80568dc *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x80568e0 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x80568e2 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x80568e4 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x80568e8 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x80568ec *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x80568f0 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x80568f4 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80568f8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x80568fc *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056900 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x8056904 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056908 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x805690c *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014f08; PC = 0x8056910 *)
mov L0x20014f08 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014f2c; PC = 0x8056914 *)
mov L0x20014f2c r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014f50; PC = 0x8056918 *)
mov L0x20014f50 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015c88; PC = 0x805691c *)
mov L0x20015c88 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015cac; PC = 0x8056920 *)
mov L0x20015cac r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015cd0; PC = 0x8056924 *)
mov L0x20015cd0 r9;



(******************** offset 1, 2, 15 ********************)


(**************** CUT 239, - *****************)

ecut and [
eqmod cf1215 f1215 2048, eqmod cf1515 f1515 2048, eqmod cf1815 f1815 2048,
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20014f08*x**1*y**2*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20014f2c*x**1*y**2*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20014f50*x**1*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20015c88*x**1*y**2*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20015cac*x**1*y**2*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20015cd0*x**1*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1578]	; 0x62a                 #! EA = L0x20018bd2; Value = 0x00000003; PC = 0x8056928 *)
mov r4 L0x20018bd2;
(* ldrsh.w	r7, [lr, #1002]	; 0x3ea                 #! EA = L0x20018992; Value = 0x00030ffd; PC = 0x805692c *)
mov r7 L0x20018992;
(* ldrsh.w	r8, [lr, #426]	; 0x1aa                  #! EA = L0x20018752; Value = 0x0ffd0000; PC = 0x8056930 *)
mov r8 L0x20018752;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056934 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1216@sint32 : and [cf1216 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056938 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1516@sint32 : and [cf1516 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805693c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1816@sint32 : and [cf1816 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056940 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056944 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056948 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805694c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056950 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056954 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056956 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056958 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805695c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056960 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056964 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056968 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805696a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805696c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056970 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056974 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056978 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014f74; PC = 0x805697c *)
mov L0x20014f74 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014f98; PC = 0x8056980 *)
mov L0x20014f98 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014fbc; PC = 0x8056984 *)
mov L0x20014fbc r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015cf4; PC = 0x8056988 *)
mov L0x20015cf4 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015d18; PC = 0x805698c *)
mov L0x20015d18 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015d3c; PC = 0x8056990 *)
mov L0x20015d3c r9;



(******************** offset 1, 2, 16 ********************)


(**************** CUT 240, - *****************)

ecut and [
eqmod cf1216 f1216 2048, eqmod cf1516 f1516 2048, eqmod cf1816 f1816 2048,
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20014f74*x**1*y**2*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20014f98*x**1*y**2*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20014fbc*x**1*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20015cf4*x**1*y**2*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20015d18*x**1*y**2*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20015d3c*x**1*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1200]	; 0x4b0                 #! EA = L0x20018a58; Value = 0x00000003; PC = 0x8056994 *)
mov r4 L0x20018a58;
(* ldrsh.w	r7, [lr, #624]	; 0x270                  #! EA = L0x20018818; Value = 0x00000ffd; PC = 0x8056998 *)
mov r7 L0x20018818;
(* ldrsh.w	r8, [lr, #48]	; 0x30                    #! EA = L0x200185d8; Value = 0x0ffd0003; PC = 0x805699c *)
mov r8 L0x200185d8;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80569a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1217@sint32 : and [cf1217 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80569a4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1517@sint32 : and [cf1517 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80569a8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1817@sint32 : and [cf1817 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80569ac *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80569b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80569b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80569b8 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80569bc *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80569c0 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80569c2 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80569c4 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x80569c8 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x80569cc *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x80569d0 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x80569d4 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x80569d6 *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80569d8 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80569dc *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80569e0 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80569e4 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014fe0; PC = 0x80569e8 *)
mov L0x20014fe0 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015004; PC = 0x80569ec *)
mov L0x20015004 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015028; PC = 0x80569f0 *)
mov L0x20015028 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015d60; PC = 0x80569f4 *)
mov L0x20015d60 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015d84; PC = 0x80569f8 *)
mov L0x20015d84 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015da8; PC = 0x80569fc *)
mov L0x20015da8 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056a00 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056a04 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805663c <_Good_loop2_inner>              #! PC = 0x8056a08 *)
#b.w	0x805663c <_Good_loop2_inner>              #! 0x8056a08 = 0x8056a08;



(******************** offset 1, 2, 17 ********************)


(**************** CUT 241, - *****************)

ecut and [
eqmod cf1217 f1217 2048, eqmod cf1517 f1517 2048, eqmod cf1817 f1817 2048,
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20014fe0*x**1*y**2*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20015004*x**1*y**2*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20015028*x**1*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20015d60*x**1*y**2*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20015d84*x**1*y**2*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20015da8*x**1*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x200188de; Value = 0x00000ffd; PC = 0x805663c *)
mov r4 L0x200188de;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x2001869e; Value = 0x00030000; PC = 0x8056640 *)
mov r7 L0x2001869e;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018b1e; Value = 0x00000ffd; PC = 0x8056644 *)
mov r9 L0x20018b1e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1218@sint32 : and [cf1218 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1518@sint32 : and [cf1518 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1818@sint32 : and [cf1818 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x2001504c; PC = 0x8056680 *)
mov L0x2001504c r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015070; PC = 0x8056684 *)
mov L0x20015070 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015094; PC = 0x8056688 *)
mov L0x20015094 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015dcc; PC = 0x805668c *)
mov L0x20015dcc r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015df0; PC = 0x8056690 *)
mov L0x20015df0 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015e14; PC = 0x8056694 *)
mov L0x20015e14 r9;



(******************** offset 1, 2, 18 ********************)


(**************** CUT 242, - *****************)

ecut and [
eqmod cf1218 f1218 2048, eqmod cf1518 f1518 2048, eqmod cf1818 f1818 2048,
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x2001504c*x**1*y**2*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x20015070*x**1*y**2*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x20015094*x**1*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x20015dcc*x**1*y**2*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x20015df0*x**1*y**2*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x20015e14*x**1*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x20018764; Value = 0x00030000; PC = 0x8056698 *)
mov r4 L0x20018764;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018be4; Value = 0x00000000; PC = 0x805669c *)
mov r6 L0x20018be4;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x200189a4; Value = 0x0ffd0003; PC = 0x80566a0 *)
mov r9 L0x200189a4;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1219@sint32 : and [cf1219 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1519@sint32 : and [cf1519 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1819@sint32 : and [cf1819 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x200150b8; PC = 0x80566ec *)
mov L0x200150b8 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200150dc; PC = 0x80566f0 *)
mov L0x200150dc r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20015100; PC = 0x80566f4 *)
mov L0x20015100 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015e38; PC = 0x80566f8 *)
mov L0x20015e38 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015e5c; PC = 0x80566fc *)
mov L0x20015e5c r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015e80; PC = 0x8056700 *)
mov L0x20015e80 r9;



(******************** offset 1, 2, 19 ********************)


(**************** CUT 243, - *****************)

ecut and [
eqmod cf1219 f1219 2048, eqmod cf1519 f1519 2048, eqmod cf1819 f1819 2048,
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x200150b8*x**1*y**2*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x200150dc*x**1*y**2*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x20015100*x**1*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x20015e38*x**1*y**2*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x20015e5c*x**1*y**2*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x20015e80*x**1*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x200185ea; Value = 0x00000ffd; PC = 0x8056704 *)
mov r4 L0x200185ea;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x20018a6a; Value = 0x00000ffd; PC = 0x8056708 *)
mov r6 L0x20018a6a;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x2001882a; Value = 0x00000ffd; PC = 0x805670c *)
mov r9 L0x2001882a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1220@sint32 : and [cf1220 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1520@sint32 : and [cf1520 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1820@sint32 : and [cf1820 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20015124; PC = 0x8056758 *)
mov L0x20015124 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015148; PC = 0x805675c *)
mov L0x20015148 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x2001516c; PC = 0x8056760 *)
mov L0x2001516c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015ea4; PC = 0x8056764 *)
mov L0x20015ea4 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015ec8; PC = 0x8056768 *)
mov L0x20015ec8 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015eec; PC = 0x805676c *)
mov L0x20015eec r9;



(******************** offset 1, 2, 20 ********************)


(**************** CUT 244, - *****************)

ecut and [
eqmod cf1220 f1220 2048, eqmod cf1520 f1520 2048, eqmod cf1820 f1820 2048,
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x20015124*x**1*y**2*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x20015148*x**1*y**2*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x2001516c*x**1*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x20015ea4*x**1*y**2*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x20015ec8*x**1*y**2*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x20015eec*x**1*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018b30; Value = 0x00000000; PC = 0x8056770 *)
mov r5 L0x20018b30;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x200188f0; Value = 0x00030003; PC = 0x8056774 *)
mov r6 L0x200188f0;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x200186b0; Value = 0x00000ffd; PC = 0x8056778 *)
mov r9 L0x200186b0;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1221@sint32 : and [cf1221 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1521@sint32 : and [cf1521 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1821@sint32 : and [cf1821 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015190; PC = 0x80567cc *)
mov L0x20015190 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x200151b4; PC = 0x80567d0 *)
mov L0x200151b4 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200151d8; PC = 0x80567d4 *)
mov L0x200151d8 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015f10; PC = 0x80567d8 *)
mov L0x20015f10 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015f34; PC = 0x80567dc *)
mov L0x20015f34 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015f58; PC = 0x80567e0 *)
mov L0x20015f58 r9;



(******************** offset 1, 2, 21 ********************)


(**************** CUT 245, - *****************)

ecut and [
eqmod cf1221 f1221 2048, eqmod cf1521 f1521 2048, eqmod cf1821 f1821 2048,
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x20015190*x**1*y**2*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x200151b4*x**1*y**2*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x200151d8*x**1*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x20015f10*x**1*y**2*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x20015f34*x**1*y**2*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x20015f58*x**1*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x200189b6; Value = 0x0ffd0000; PC = 0x80567e4 *)
mov r5 L0x200189b6;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x20018776; Value = 0x00000003; PC = 0x80567e8 *)
mov r6 L0x20018776;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018bf6; Value = 0x00000000; PC = 0x80567ec *)
mov r8 L0x20018bf6;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1222@sint32 : and [cf1222 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1522@sint32 : and [cf1522 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1822@sint32 : and [cf1822 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200151fc; PC = 0x805682c *)
mov L0x200151fc r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20015220; PC = 0x8056830 *)
mov L0x20015220 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015244; PC = 0x8056834 *)
mov L0x20015244 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015f7c; PC = 0x8056838 *)
mov L0x20015f7c r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015fa0; PC = 0x805683c *)
mov L0x20015fa0 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015fc4; PC = 0x8056840 *)
mov L0x20015fc4 r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;



(******************** offset 1, 2, 22 ********************)


(**************** CUT 246, - *****************)

ecut and [
eqmod cf1222 f1222 2048, eqmod cf1522 f1522 2048, eqmod cf1822 f1822 2048,
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x200151fc*x**1*y**2*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x20015220*x**1*y**2*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x20015244*x**1*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x20015f7c*x**1*y**2*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x20015fa0*x**1*y**2*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x20015fc4*x**1*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #606]	; 0x25e                  #! EA = L0x2001883c; Value = 0x00030000; PC = 0x8056850 *)
mov r5 L0x2001883c;
(* ldrsh.w	r6, [lr, #30]                           #! EA = L0x200185fc; Value = 0x00030ffd; PC = 0x8056854 *)
mov r6 L0x200185fc;
(* ldrsh.w	r8, [lr, #1182]	; 0x49e                 #! EA = L0x20018a7c; Value = 0x00000000; PC = 0x8056858 *)
mov r8 L0x20018a7c;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805685c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1223@sint32 : and [cf1223 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056860 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1523@sint32 : and [cf1523 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056864 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1823@sint32 : and [cf1823 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056868 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x805686c *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056870 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056874 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056878 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x805687c *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056880 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056884 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056888 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x805688c *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056890 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056892 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056894 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20015268; PC = 0x8056898 *)
mov L0x20015268 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x2001528c; PC = 0x805689c *)
mov L0x2001528c r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x200152b0; PC = 0x80568a0 *)
mov L0x200152b0 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015fe8; PC = 0x80568a4 *)
mov L0x20015fe8 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x2001600c; PC = 0x80568a8 *)
mov L0x2001600c r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20016030; PC = 0x80568ac *)
mov L0x20016030 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80568b0 *)
adds dc r0 r0 648@uint32;



(******************** offset 1, 2, 23 ********************)


(**************** CUT 247, - *****************)

ecut and [
eqmod cf1223 f1223 2048, eqmod cf1523 f1523 2048, eqmod cf1823 f1823 2048,
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x20015268*x**1*y**2*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x2001528c*x**1*y**2*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x200152b0*x**1*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x20015fe8*x**1*y**2*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x2001600c*x**1*y**2*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x20016030*x**1*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #228]	; 0xe4                   #! EA = L0x200186c2; Value = 0x00000003; PC = 0x80568b4 *)
mov r5 L0x200186c2;
(* ldrsh.w	r7, [lr, #1380]	; 0x564                 #! EA = L0x20018b42; Value = 0x0ffd0ffd; PC = 0x80568b8 *)
mov r7 L0x20018b42;
(* ldrsh.w	r8, [lr, #804]	; 0x324                  #! EA = L0x20018902; Value = 0x00030ffd; PC = 0x80568bc *)
mov r8 L0x20018902;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80568c0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1224@sint32 : and [cf1224 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80568c4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1524@sint32 : and [cf1524 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80568c8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1824@sint32 : and [cf1824 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x80568cc *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x80568d0 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x80568d4 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x80568d8 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x80568dc *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x80568e0 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x80568e2 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x80568e4 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x80568e8 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x80568ec *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x80568f0 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x80568f4 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80568f8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x80568fc *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056900 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x8056904 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056908 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x805690c *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200152d4; PC = 0x8056910 *)
mov L0x200152d4 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200152f8; PC = 0x8056914 *)
mov L0x200152f8 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x2001531c; PC = 0x8056918 *)
mov L0x2001531c r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016054; PC = 0x805691c *)
mov L0x20016054 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20016078; PC = 0x8056920 *)
mov L0x20016078 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x2001609c; PC = 0x8056924 *)
mov L0x2001609c r9;



(******************** offset 1, 2, 24 ********************)


(**************** CUT 248, - *****************)

ecut and [
eqmod cf1224 f1224 2048, eqmod cf1524 f1524 2048, eqmod cf1824 f1824 2048,
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x200152d4*x**1*y**2*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x200152f8*x**1*y**2*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x2001531c*x**1*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x20016054*x**1*y**2*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x20016078*x**1*y**2*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x2001609c*x**1*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1578]	; 0x62a                 #! EA = L0x20018c08; Value = 0x00000000; PC = 0x8056928 *)
mov r4 L0x20018c08;
(* ldrsh.w	r7, [lr, #1002]	; 0x3ea                 #! EA = L0x200189c8; Value = 0x0ffd0003; PC = 0x805692c *)
mov r7 L0x200189c8;
(* ldrsh.w	r8, [lr, #426]	; 0x1aa                  #! EA = L0x20018788; Value = 0x00000003; PC = 0x8056930 *)
mov r8 L0x20018788;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056934 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1225@sint32 : and [cf1225 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056938 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1525@sint32 : and [cf1525 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805693c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1825@sint32 : and [cf1825 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056940 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056944 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056948 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805694c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056950 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056954 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056956 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056958 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805695c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056960 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056964 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056968 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805696a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805696c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056970 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056974 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056978 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015340; PC = 0x805697c *)
mov L0x20015340 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20015364; PC = 0x8056980 *)
mov L0x20015364 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20015388; PC = 0x8056984 *)
mov L0x20015388 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200160c0; PC = 0x8056988 *)
mov L0x200160c0 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200160e4; PC = 0x805698c *)
mov L0x200160e4 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20016108; PC = 0x8056990 *)
mov L0x20016108 r9;



(******************** offset 1, 2, 25 ********************)


(**************** CUT 249, - *****************)

ecut and [
eqmod cf1225 f1225 2048, eqmod cf1525 f1525 2048, eqmod cf1825 f1825 2048,
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x20015340*x**1*y**2*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x20015364*x**1*y**2*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x20015388*x**1*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x200160c0*x**1*y**2*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x200160e4*x**1*y**2*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x20016108*x**1*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1200]	; 0x4b0                 #! EA = L0x20018a8e; Value = 0x0ffd0003; PC = 0x8056994 *)
mov r4 L0x20018a8e;
(* ldrsh.w	r7, [lr, #624]	; 0x270                  #! EA = L0x2001884e; Value = 0x0ffd0000; PC = 0x8056998 *)
mov r7 L0x2001884e;
(* ldrsh.w	r8, [lr, #48]	; 0x30                    #! EA = L0x2001860e; Value = 0x00030ffd; PC = 0x805699c *)
mov r8 L0x2001860e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80569a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1226@sint32 : and [cf1226 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80569a4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1526@sint32 : and [cf1526 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80569a8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1826@sint32 : and [cf1826 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80569ac *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80569b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80569b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80569b8 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80569bc *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80569c0 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80569c2 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80569c4 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x80569c8 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x80569cc *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x80569d0 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x80569d4 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x80569d6 *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80569d8 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80569dc *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80569e0 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80569e4 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200153ac; PC = 0x80569e8 *)
mov L0x200153ac r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200153d0; PC = 0x80569ec *)
mov L0x200153d0 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200153f4; PC = 0x80569f0 *)
mov L0x200153f4 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x2001612c; PC = 0x80569f4 *)
mov L0x2001612c r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016150; PC = 0x80569f8 *)
mov L0x20016150 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20016174; PC = 0x80569fc *)
mov L0x20016174 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056a00 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056a04 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805663c <_Good_loop2_inner>              #! PC = 0x8056a08 *)
#b.w	0x805663c <_Good_loop2_inner>              #! 0x8056a08 = 0x8056a08;



(******************** offset 1, 2, 26 ********************)


(**************** CUT 250, - *****************)

ecut and [
eqmod cf1226 f1226 2048, eqmod cf1526 f1526 2048, eqmod cf1826 f1826 2048,
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x200153ac*x**1*y**2*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x200153d0*x**1*y**2*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x200153f4*x**1*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x2001612c*x**1*y**2*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x20016150*x**1*y**2*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x20016174*x**1*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x20018914; Value = 0x00000003; PC = 0x805663c *)
mov r4 L0x20018914;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x200186d4; Value = 0x00030ffd; PC = 0x8056640 *)
mov r7 L0x200186d4;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018b54; Value = 0x00000ffd; PC = 0x8056644 *)
mov r9 L0x20018b54;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1227@sint32 : and [cf1227 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf1527@sint32 : and [cf1527 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1827@sint32 : and [cf1827 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015418; PC = 0x8056680 *)
mov L0x20015418 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x2001543c; PC = 0x8056684 *)
mov L0x2001543c r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015460; PC = 0x8056688 *)
mov L0x20015460 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016198; PC = 0x805668c *)
mov L0x20016198 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200161bc; PC = 0x8056690 *)
mov L0x200161bc r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200161e0; PC = 0x8056694 *)
mov L0x200161e0 r9;



(******************** offset 1, 2, 27 ********************)


(**************** CUT 251, - *****************)

ecut and [
eqmod cf1227 f1227 2048, eqmod cf1527 f1527 2048, eqmod cf1827 f1827 2048,
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x20015418*x**1*y**2*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x2001543c*x**1*y**2*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x20015460*x**1*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x20016198*x**1*y**2*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x200161bc*x**1*y**2*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x200161e0*x**1*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x2001879a; Value = 0x00000ffd; PC = 0x8056698 *)
mov r4 L0x2001879a;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018c1a; Value = 0x00000000; PC = 0x805669c *)
mov r6 L0x20018c1a;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x200189da; Value = 0x0ffd0003; PC = 0x80566a0 *)
mov r9 L0x200189da;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1228@sint32 : and [cf1228 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1528@sint32 : and [cf1528 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1828@sint32 : and [cf1828 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015484; PC = 0x80566ec *)
mov L0x20015484 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200154a8; PC = 0x80566f0 *)
mov L0x200154a8 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200154cc; PC = 0x80566f4 *)
mov L0x200154cc r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20016204; PC = 0x80566f8 *)
mov L0x20016204 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20016228; PC = 0x80566fc *)
mov L0x20016228 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x2001624c; PC = 0x8056700 *)
mov L0x2001624c r9;



(******************** offset 1, 2, 28 ********************)


(**************** CUT 252, - *****************)

ecut and [
eqmod cf1228 f1228 2048, eqmod cf1528 f1528 2048, eqmod cf1828 f1828 2048,
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x20015484*x**1*y**2*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x200154a8*x**1*y**2*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x200154cc*x**1*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x20016204*x**1*y**2*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x20016228*x**1*y**2*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x2001624c*x**1*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x20018620; Value = 0x00030000; PC = 0x8056704 *)
mov r4 L0x20018620;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x20018aa0; Value = 0x0ffd0ffd; PC = 0x8056708 *)
mov r6 L0x20018aa0;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x20018860; Value = 0x00000ffd; PC = 0x805670c *)
mov r9 L0x20018860;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf1229@sint32 : and [cf1229 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1529@sint32 : and [cf1529 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1829@sint32 : and [cf1829 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200154f0; PC = 0x8056758 *)
mov L0x200154f0 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015514; PC = 0x805675c *)
mov L0x20015514 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015538; PC = 0x8056760 *)
mov L0x20015538 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016270; PC = 0x8056764 *)
mov L0x20016270 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016294; PC = 0x8056768 *)
mov L0x20016294 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200162b8; PC = 0x805676c *)
mov L0x200162b8 r9;



(******************** offset 1, 2, 29 ********************)


(**************** CUT 253, - *****************)

ecut and [
eqmod cf1229 f1229 2048, eqmod cf1529 f1529 2048, eqmod cf1829 f1829 2048,
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x200154f0*x**1*y**2*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x20015514*x**1*y**2*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x20015538*x**1*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x20016270*x**1*y**2*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x20016294*x**1*y**2*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x200162b8*x**1*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018b66; Value = 0x00000003; PC = 0x8056770 *)
mov r5 L0x20018b66;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x20018926; Value = 0x00030003; PC = 0x8056774 *)
mov r6 L0x20018926;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x200186e6; Value = 0x00000000; PC = 0x8056778 *)
mov r9 L0x200186e6;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1230@sint32 : and [cf1230 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1530@sint32 : and [cf1530 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf1830@sint32 : and [cf1830 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x2001555c; PC = 0x80567cc *)
mov L0x2001555c r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20015580; PC = 0x80567d0 *)
mov L0x20015580 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200155a4; PC = 0x80567d4 *)
mov L0x200155a4 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x200162dc; PC = 0x80567d8 *)
mov L0x200162dc r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20016300; PC = 0x80567dc *)
mov L0x20016300 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20016324; PC = 0x80567e0 *)
mov L0x20016324 r9;



(******************** offset 1, 2, 30 ********************)


(**************** CUT 254, - *****************)

ecut and [
eqmod cf1230 f1230 2048, eqmod cf1530 f1530 2048, eqmod cf1830 f1830 2048,
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x2001555c*x**1*y**2*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x20015580*x**1*y**2*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x200155a4*x**1*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x200162dc*x**1*y**2*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x20016300*x**1*y**2*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x20016324*x**1*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x200189ec; Value = 0x0ffd0003; PC = 0x80567e4 *)
mov r5 L0x200189ec;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x200187ac; Value = 0x00030003; PC = 0x80567e8 *)
mov r6 L0x200187ac;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018c2c; Value = 0x00000000; PC = 0x80567ec *)
mov r8 L0x20018c2c;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf1231@sint32 : and [cf1231 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf1531@sint32 : and [cf1531 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf1831@sint32 : and [cf1831 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200155c8; PC = 0x805682c *)
mov L0x200155c8 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x200155ec; PC = 0x8056830 *)
mov L0x200155ec r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015610; PC = 0x8056834 *)
mov L0x20015610 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20016348; PC = 0x8056838 *)
mov L0x20016348 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x2001636c; PC = 0x805683c *)
mov L0x2001636c r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20016390; PC = 0x8056840 *)
mov L0x20016390 r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;
(* sub.w	lr, lr, #162	; 0xa2                       #! PC = 0x8056a0c *)
subs dc lr lr 162@uint32;
(* subw	r0, r0, #2916	; 0xb64                      #! PC = 0x8056a10 *)
subc dc r0 r0 2916@uint32;
(* add.w	lr, lr, #2                                #! PC = 0x8056a14 *)
adds dc lr lr 2@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8056a18 *)
adds dc r0 r0 4@uint32;
(* cmp.w	r0, r12                                   #! PC = 0x8056a1c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8056634 <_Good_loop2>                  #! PC = 0x8056a20 *)
#bne.w	0x8056634 <_Good_loop2>                  #! 0x8056a20 = 0x8056a20;
(* addw	r1, r0, #2916	; 0xb64                      #! PC = 0x8056634 *)
adds dc r1 r0 2916@uint32;
(* vmov	s3, r1                                     #! PC = 0x8056638 *)
mov s3 r1;



(******************** offset 1, 2, 31 ********************)


(**************** CUT 255, - *****************)

ecut and [
eqmod cf1231 f1231 2048, eqmod cf1531 f1531 2048, eqmod cf1831 f1831 2048,
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x200155c8*x**1*y**2*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x200155ec*x**1*y**2*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x20015610*x**1*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x20016348*x**1*y**2*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x2001636c*x**1*y**2*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x20016390*x**1*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(**************** CUT -,   7 *****************)

rcut and [
(-3367617)@32<=sL0x200148b4,L0x200148b4<=s3367617@32,
(-3367617)@32<=sL0x200148d8,L0x200148d8<=s3367617@32,
(-3367617)@32<=sL0x200148fc,L0x200148fc<=s3367617@32,
(-3367617)@32<=sL0x20015634,L0x20015634<=s3367617@32,
(-3367617)@32<=sL0x20015658,L0x20015658<=s3367617@32,
(-3367617)@32<=sL0x2001567c,L0x2001567c<=s3367617@32
,
(-3367617)@32<=sL0x20014920,L0x20014920<=s3367617@32,
(-3367617)@32<=sL0x20014944,L0x20014944<=s3367617@32,
(-3367617)@32<=sL0x20014968,L0x20014968<=s3367617@32,
(-3367617)@32<=sL0x200156a0,L0x200156a0<=s3367617@32,
(-3367617)@32<=sL0x200156c4,L0x200156c4<=s3367617@32,
(-3367617)@32<=sL0x200156e8,L0x200156e8<=s3367617@32
,
(-3367617)@32<=sL0x2001498c,L0x2001498c<=s3367617@32,
(-3367617)@32<=sL0x200149b0,L0x200149b0<=s3367617@32,
(-3367617)@32<=sL0x200149d4,L0x200149d4<=s3367617@32,
(-3367617)@32<=sL0x2001570c,L0x2001570c<=s3367617@32,
(-3367617)@32<=sL0x20015730,L0x20015730<=s3367617@32,
(-3367617)@32<=sL0x20015754,L0x20015754<=s3367617@32
,
(-3367617)@32<=sL0x200149f8,L0x200149f8<=s3367617@32,
(-3367617)@32<=sL0x20014a1c,L0x20014a1c<=s3367617@32,
(-3367617)@32<=sL0x20014a40,L0x20014a40<=s3367617@32,
(-3367617)@32<=sL0x20015778,L0x20015778<=s3367617@32,
(-3367617)@32<=sL0x2001579c,L0x2001579c<=s3367617@32,
(-3367617)@32<=sL0x200157c0,L0x200157c0<=s3367617@32
,
(-3367617)@32<=sL0x20014a64,L0x20014a64<=s3367617@32,
(-3367617)@32<=sL0x20014a88,L0x20014a88<=s3367617@32,
(-3367617)@32<=sL0x20014aac,L0x20014aac<=s3367617@32,
(-3367617)@32<=sL0x200157e4,L0x200157e4<=s3367617@32,
(-3367617)@32<=sL0x20015808,L0x20015808<=s3367617@32,
(-3367617)@32<=sL0x2001582c,L0x2001582c<=s3367617@32
,
(-3367617)@32<=sL0x20014ad0,L0x20014ad0<=s3367617@32,
(-3367617)@32<=sL0x20014af4,L0x20014af4<=s3367617@32,
(-3367617)@32<=sL0x20014b18,L0x20014b18<=s3367617@32,
(-3367617)@32<=sL0x20015850,L0x20015850<=s3367617@32,
(-3367617)@32<=sL0x20015874,L0x20015874<=s3367617@32,
(-3367617)@32<=sL0x20015898,L0x20015898<=s3367617@32
,
(-3367617)@32<=sL0x20014b3c,L0x20014b3c<=s3367617@32,
(-3367617)@32<=sL0x20014b60,L0x20014b60<=s3367617@32,
(-3367617)@32<=sL0x20014b84,L0x20014b84<=s3367617@32,
(-3367617)@32<=sL0x200158bc,L0x200158bc<=s3367617@32,
(-3367617)@32<=sL0x200158e0,L0x200158e0<=s3367617@32,
(-3367617)@32<=sL0x20015904,L0x20015904<=s3367617@32
,
(-3367617)@32<=sL0x20014ba8,L0x20014ba8<=s3367617@32,
(-3367617)@32<=sL0x20014bcc,L0x20014bcc<=s3367617@32,
(-3367617)@32<=sL0x20014bf0,L0x20014bf0<=s3367617@32,
(-3367617)@32<=sL0x20015928,L0x20015928<=s3367617@32,
(-3367617)@32<=sL0x2001594c,L0x2001594c<=s3367617@32,
(-3367617)@32<=sL0x20015970,L0x20015970<=s3367617@32
,
(-3367617)@32<=sL0x20014c14,L0x20014c14<=s3367617@32,
(-3367617)@32<=sL0x20014c38,L0x20014c38<=s3367617@32,
(-3367617)@32<=sL0x20014c5c,L0x20014c5c<=s3367617@32,
(-3367617)@32<=sL0x20015994,L0x20015994<=s3367617@32,
(-3367617)@32<=sL0x200159b8,L0x200159b8<=s3367617@32,
(-3367617)@32<=sL0x200159dc,L0x200159dc<=s3367617@32
,
(-3367617)@32<=sL0x20014c80,L0x20014c80<=s3367617@32,
(-3367617)@32<=sL0x20014ca4,L0x20014ca4<=s3367617@32,
(-3367617)@32<=sL0x20014cc8,L0x20014cc8<=s3367617@32,
(-3367617)@32<=sL0x20015a00,L0x20015a00<=s3367617@32,
(-3367617)@32<=sL0x20015a24,L0x20015a24<=s3367617@32,
(-3367617)@32<=sL0x20015a48,L0x20015a48<=s3367617@32
,
(-3367617)@32<=sL0x20014cec,L0x20014cec<=s3367617@32,
(-3367617)@32<=sL0x20014d10,L0x20014d10<=s3367617@32,
(-3367617)@32<=sL0x20014d34,L0x20014d34<=s3367617@32,
(-3367617)@32<=sL0x20015a6c,L0x20015a6c<=s3367617@32,
(-3367617)@32<=sL0x20015a90,L0x20015a90<=s3367617@32,
(-3367617)@32<=sL0x20015ab4,L0x20015ab4<=s3367617@32
,
(-3367617)@32<=sL0x20014d58,L0x20014d58<=s3367617@32,
(-3367617)@32<=sL0x20014d7c,L0x20014d7c<=s3367617@32,
(-3367617)@32<=sL0x20014da0,L0x20014da0<=s3367617@32,
(-3367617)@32<=sL0x20015ad8,L0x20015ad8<=s3367617@32,
(-3367617)@32<=sL0x20015afc,L0x20015afc<=s3367617@32,
(-3367617)@32<=sL0x20015b20,L0x20015b20<=s3367617@32
,
(-3367617)@32<=sL0x20014dc4,L0x20014dc4<=s3367617@32,
(-3367617)@32<=sL0x20014de8,L0x20014de8<=s3367617@32,
(-3367617)@32<=sL0x20014e0c,L0x20014e0c<=s3367617@32,
(-3367617)@32<=sL0x20015b44,L0x20015b44<=s3367617@32,
(-3367617)@32<=sL0x20015b68,L0x20015b68<=s3367617@32,
(-3367617)@32<=sL0x20015b8c,L0x20015b8c<=s3367617@32
,
(-3367617)@32<=sL0x20014e30,L0x20014e30<=s3367617@32,
(-3367617)@32<=sL0x20014e54,L0x20014e54<=s3367617@32,
(-3367617)@32<=sL0x20014e78,L0x20014e78<=s3367617@32,
(-3367617)@32<=sL0x20015bb0,L0x20015bb0<=s3367617@32,
(-3367617)@32<=sL0x20015bd4,L0x20015bd4<=s3367617@32,
(-3367617)@32<=sL0x20015bf8,L0x20015bf8<=s3367617@32
,
(-3367617)@32<=sL0x20014e9c,L0x20014e9c<=s3367617@32,
(-3367617)@32<=sL0x20014ec0,L0x20014ec0<=s3367617@32,
(-3367617)@32<=sL0x20014ee4,L0x20014ee4<=s3367617@32,
(-3367617)@32<=sL0x20015c1c,L0x20015c1c<=s3367617@32,
(-3367617)@32<=sL0x20015c40,L0x20015c40<=s3367617@32,
(-3367617)@32<=sL0x20015c64,L0x20015c64<=s3367617@32
,
(-3367617)@32<=sL0x20014f08,L0x20014f08<=s3367617@32,
(-3367617)@32<=sL0x20014f2c,L0x20014f2c<=s3367617@32,
(-3367617)@32<=sL0x20014f50,L0x20014f50<=s3367617@32,
(-3367617)@32<=sL0x20015c88,L0x20015c88<=s3367617@32,
(-3367617)@32<=sL0x20015cac,L0x20015cac<=s3367617@32,
(-3367617)@32<=sL0x20015cd0,L0x20015cd0<=s3367617@32
,
(-3367617)@32<=sL0x20014f74,L0x20014f74<=s3367617@32,
(-3367617)@32<=sL0x20014f98,L0x20014f98<=s3367617@32,
(-3367617)@32<=sL0x20014fbc,L0x20014fbc<=s3367617@32,
(-3367617)@32<=sL0x20015cf4,L0x20015cf4<=s3367617@32,
(-3367617)@32<=sL0x20015d18,L0x20015d18<=s3367617@32,
(-3367617)@32<=sL0x20015d3c,L0x20015d3c<=s3367617@32
,
(-3367617)@32<=sL0x20014fe0,L0x20014fe0<=s3367617@32,
(-3367617)@32<=sL0x20015004,L0x20015004<=s3367617@32,
(-3367617)@32<=sL0x20015028,L0x20015028<=s3367617@32,
(-3367617)@32<=sL0x20015d60,L0x20015d60<=s3367617@32,
(-3367617)@32<=sL0x20015d84,L0x20015d84<=s3367617@32,
(-3367617)@32<=sL0x20015da8,L0x20015da8<=s3367617@32
,
(-3367617)@32<=sL0x2001504c,L0x2001504c<=s3367617@32,
(-3367617)@32<=sL0x20015070,L0x20015070<=s3367617@32,
(-3367617)@32<=sL0x20015094,L0x20015094<=s3367617@32,
(-3367617)@32<=sL0x20015dcc,L0x20015dcc<=s3367617@32,
(-3367617)@32<=sL0x20015df0,L0x20015df0<=s3367617@32,
(-3367617)@32<=sL0x20015e14,L0x20015e14<=s3367617@32
,
(-3367617)@32<=sL0x200150b8,L0x200150b8<=s3367617@32,
(-3367617)@32<=sL0x200150dc,L0x200150dc<=s3367617@32,
(-3367617)@32<=sL0x20015100,L0x20015100<=s3367617@32,
(-3367617)@32<=sL0x20015e38,L0x20015e38<=s3367617@32,
(-3367617)@32<=sL0x20015e5c,L0x20015e5c<=s3367617@32,
(-3367617)@32<=sL0x20015e80,L0x20015e80<=s3367617@32
,
(-3367617)@32<=sL0x20015124,L0x20015124<=s3367617@32,
(-3367617)@32<=sL0x20015148,L0x20015148<=s3367617@32,
(-3367617)@32<=sL0x2001516c,L0x2001516c<=s3367617@32,
(-3367617)@32<=sL0x20015ea4,L0x20015ea4<=s3367617@32,
(-3367617)@32<=sL0x20015ec8,L0x20015ec8<=s3367617@32,
(-3367617)@32<=sL0x20015eec,L0x20015eec<=s3367617@32
,
(-3367617)@32<=sL0x20015190,L0x20015190<=s3367617@32,
(-3367617)@32<=sL0x200151b4,L0x200151b4<=s3367617@32,
(-3367617)@32<=sL0x200151d8,L0x200151d8<=s3367617@32,
(-3367617)@32<=sL0x20015f10,L0x20015f10<=s3367617@32,
(-3367617)@32<=sL0x20015f34,L0x20015f34<=s3367617@32,
(-3367617)@32<=sL0x20015f58,L0x20015f58<=s3367617@32
,
(-3367617)@32<=sL0x200151fc,L0x200151fc<=s3367617@32,
(-3367617)@32<=sL0x20015220,L0x20015220<=s3367617@32,
(-3367617)@32<=sL0x20015244,L0x20015244<=s3367617@32,
(-3367617)@32<=sL0x20015f7c,L0x20015f7c<=s3367617@32,
(-3367617)@32<=sL0x20015fa0,L0x20015fa0<=s3367617@32,
(-3367617)@32<=sL0x20015fc4,L0x20015fc4<=s3367617@32
,
(-3367617)@32<=sL0x20015268,L0x20015268<=s3367617@32,
(-3367617)@32<=sL0x2001528c,L0x2001528c<=s3367617@32,
(-3367617)@32<=sL0x200152b0,L0x200152b0<=s3367617@32,
(-3367617)@32<=sL0x20015fe8,L0x20015fe8<=s3367617@32,
(-3367617)@32<=sL0x2001600c,L0x2001600c<=s3367617@32,
(-3367617)@32<=sL0x20016030,L0x20016030<=s3367617@32
,
(-3367617)@32<=sL0x200152d4,L0x200152d4<=s3367617@32,
(-3367617)@32<=sL0x200152f8,L0x200152f8<=s3367617@32,
(-3367617)@32<=sL0x2001531c,L0x2001531c<=s3367617@32,
(-3367617)@32<=sL0x20016054,L0x20016054<=s3367617@32,
(-3367617)@32<=sL0x20016078,L0x20016078<=s3367617@32,
(-3367617)@32<=sL0x2001609c,L0x2001609c<=s3367617@32
,
(-3367617)@32<=sL0x20015340,L0x20015340<=s3367617@32,
(-3367617)@32<=sL0x20015364,L0x20015364<=s3367617@32,
(-3367617)@32<=sL0x20015388,L0x20015388<=s3367617@32,
(-3367617)@32<=sL0x200160c0,L0x200160c0<=s3367617@32,
(-3367617)@32<=sL0x200160e4,L0x200160e4<=s3367617@32,
(-3367617)@32<=sL0x20016108,L0x20016108<=s3367617@32
,
(-3367617)@32<=sL0x200153ac,L0x200153ac<=s3367617@32,
(-3367617)@32<=sL0x200153d0,L0x200153d0<=s3367617@32,
(-3367617)@32<=sL0x200153f4,L0x200153f4<=s3367617@32,
(-3367617)@32<=sL0x2001612c,L0x2001612c<=s3367617@32,
(-3367617)@32<=sL0x20016150,L0x20016150<=s3367617@32,
(-3367617)@32<=sL0x20016174,L0x20016174<=s3367617@32
,
(-3367617)@32<=sL0x20015418,L0x20015418<=s3367617@32,
(-3367617)@32<=sL0x2001543c,L0x2001543c<=s3367617@32,
(-3367617)@32<=sL0x20015460,L0x20015460<=s3367617@32,
(-3367617)@32<=sL0x20016198,L0x20016198<=s3367617@32,
(-3367617)@32<=sL0x200161bc,L0x200161bc<=s3367617@32,
(-3367617)@32<=sL0x200161e0,L0x200161e0<=s3367617@32
,
(-3367617)@32<=sL0x20015484,L0x20015484<=s3367617@32,
(-3367617)@32<=sL0x200154a8,L0x200154a8<=s3367617@32,
(-3367617)@32<=sL0x200154cc,L0x200154cc<=s3367617@32,
(-3367617)@32<=sL0x20016204,L0x20016204<=s3367617@32,
(-3367617)@32<=sL0x20016228,L0x20016228<=s3367617@32,
(-3367617)@32<=sL0x2001624c,L0x2001624c<=s3367617@32
,
(-3367617)@32<=sL0x200154f0,L0x200154f0<=s3367617@32,
(-3367617)@32<=sL0x20015514,L0x20015514<=s3367617@32,
(-3367617)@32<=sL0x20015538,L0x20015538<=s3367617@32,
(-3367617)@32<=sL0x20016270,L0x20016270<=s3367617@32,
(-3367617)@32<=sL0x20016294,L0x20016294<=s3367617@32,
(-3367617)@32<=sL0x200162b8,L0x200162b8<=s3367617@32
,
(-3367617)@32<=sL0x2001555c,L0x2001555c<=s3367617@32,
(-3367617)@32<=sL0x20015580,L0x20015580<=s3367617@32,
(-3367617)@32<=sL0x200155a4,L0x200155a4<=s3367617@32,
(-3367617)@32<=sL0x200162dc,L0x200162dc<=s3367617@32,
(-3367617)@32<=sL0x20016300,L0x20016300<=s3367617@32,
(-3367617)@32<=sL0x20016324,L0x20016324<=s3367617@32
,
(-3367617)@32<=sL0x200155c8,L0x200155c8<=s3367617@32,
(-3367617)@32<=sL0x200155ec,L0x200155ec<=s3367617@32,
(-3367617)@32<=sL0x20015610,L0x20015610<=s3367617@32,
(-3367617)@32<=sL0x20016348,L0x20016348<=s3367617@32,
(-3367617)@32<=sL0x2001636c,L0x2001636c<=s3367617@32,
(-3367617)@32<=sL0x20016390,L0x20016390<=s3367617@32
] prove with [ precondition ];





(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x20018874; Value = 0x00000003; PC = 0x805663c *)
mov r4 L0x20018874;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x20018634; Value = 0x00000ffd; PC = 0x8056640 *)
mov r7 L0x20018634;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018ab4; Value = 0x00000003; PC = 0x8056644 *)
mov r9 L0x20018ab4;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2200@sint32 : and [cf2200 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2500@sint32 : and [cf2500 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2800@sint32 : and [cf2800 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200148b8; PC = 0x8056680 *)
mov L0x200148b8 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200148dc; PC = 0x8056684 *)
mov L0x200148dc r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014900; PC = 0x8056688 *)
mov L0x20014900 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015638; PC = 0x805668c *)
mov L0x20015638 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x2001565c; PC = 0x8056690 *)
mov L0x2001565c r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015680; PC = 0x8056694 *)
mov L0x20015680 r9;



(******************** offset 2, 2,  0 ********************)


(**************** CUT 256, - *****************)

ecut and [
eqmod cf2200 f2200 2048, eqmod cf2500 f2500 2048, eqmod cf2800 f2800 2048,
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x200148b8*x**2*y**2*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x200148dc*x**2*y**2*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x20014900*x**2*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x20015638*x**2*y**2*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x2001565c*x**2*y**2*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x20015680*x**2*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x200186fa; Value = 0x00030000; PC = 0x8056698 *)
mov r4 L0x200186fa;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018b7a; Value = 0x0ffd0000; PC = 0x805669c *)
mov r6 L0x20018b7a;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x2001893a; Value = 0x0ffd0000; PC = 0x80566a0 *)
mov r9 L0x2001893a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2201@sint32 : and [cf2201 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2501@sint32 : and [cf2501 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2801@sint32 : and [cf2801 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014924; PC = 0x80566ec *)
mov L0x20014924 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014948; PC = 0x80566f0 *)
mov L0x20014948 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x2001496c; PC = 0x80566f4 *)
mov L0x2001496c r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200156a4; PC = 0x80566f8 *)
mov L0x200156a4 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200156c8; PC = 0x80566fc *)
mov L0x200156c8 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x200156ec; PC = 0x8056700 *)
mov L0x200156ec r9;



(******************** offset 2, 2,  1 ********************)


(**************** CUT 257, - *****************)

ecut and [
eqmod cf2201 f2201 2048, eqmod cf2501 f2501 2048, eqmod cf2801 f2801 2048,
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x20014924*x**2*y**2*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x20014948*x**2*y**2*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x2001496c*x**2*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x200156a4*x**2*y**2*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x200156c8*x**2*y**2*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x200156ec*x**2*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x20018580; Value = 0x00030003; PC = 0x8056704 *)
mov r4 L0x20018580;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x20018a00; Value = 0x00000003; PC = 0x8056708 *)
mov r6 L0x20018a00;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x200187c0; Value = 0x0ffd0ffd; PC = 0x805670c *)
mov r9 L0x200187c0;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2202@sint32 : and [cf2202 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2502@sint32 : and [cf2502 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2802@sint32 : and [cf2802 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014990; PC = 0x8056758 *)
mov L0x20014990 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200149b4; PC = 0x805675c *)
mov L0x200149b4 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200149d8; PC = 0x8056760 *)
mov L0x200149d8 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015710; PC = 0x8056764 *)
mov L0x20015710 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015734; PC = 0x8056768 *)
mov L0x20015734 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015758; PC = 0x805676c *)
mov L0x20015758 r9;



(******************** offset 2, 2,  2 ********************)


(**************** CUT 258, - *****************)

ecut and [
eqmod cf2202 f2202 2048, eqmod cf2502 f2502 2048, eqmod cf2802 f2802 2048,
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x20014990*x**2*y**2*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x200149b4*x**2*y**2*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x200149d8*x**2*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x20015710*x**2*y**2*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x20015734*x**2*y**2*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x20015758*x**2*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018ac6; Value = 0x0ffd0003; PC = 0x8056770 *)
mov r5 L0x20018ac6;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x20018886; Value = 0x0ffd0003; PC = 0x8056774 *)
mov r6 L0x20018886;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x20018646; Value = 0x00000003; PC = 0x8056778 *)
mov r9 L0x20018646;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2203@sint32 : and [cf2203 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2503@sint32 : and [cf2503 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2803@sint32 : and [cf2803 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x200149fc; PC = 0x80567cc *)
mov L0x200149fc r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014a20; PC = 0x80567d0 *)
mov L0x20014a20 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014a44; PC = 0x80567d4 *)
mov L0x20014a44 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x2001577c; PC = 0x80567d8 *)
mov L0x2001577c r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x200157a0; PC = 0x80567dc *)
mov L0x200157a0 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x200157c4; PC = 0x80567e0 *)
mov L0x200157c4 r9;



(******************** offset 2, 2,  3 ********************)


(**************** CUT 259, - *****************)

ecut and [
eqmod cf2203 f2203 2048, eqmod cf2503 f2503 2048, eqmod cf2803 f2803 2048,
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x200149fc*x**2*y**2*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x20014a20*x**2*y**2*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x20014a44*x**2*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x2001577c*x**2*y**2*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x200157a0*x**2*y**2*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x200157c4*x**2*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x2001894c; Value = 0x0ffd0000; PC = 0x80567e4 *)
mov r5 L0x2001894c;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x2001870c; Value = 0x0ffd0003; PC = 0x80567e8 *)
mov r6 L0x2001870c;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018b8c; Value = 0x00000003; PC = 0x80567ec *)
mov r8 L0x20018b8c;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2204@sint32 : and [cf2204 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2504@sint32 : and [cf2504 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2804@sint32 : and [cf2804 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014a68; PC = 0x805682c *)
mov L0x20014a68 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014a8c; PC = 0x8056830 *)
mov L0x20014a8c r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014ab0; PC = 0x8056834 *)
mov L0x20014ab0 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x200157e8; PC = 0x8056838 *)
mov L0x200157e8 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x2001580c; PC = 0x805683c *)
mov L0x2001580c r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015830; PC = 0x8056840 *)
mov L0x20015830 r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;



(******************** offset 2, 2,  4 ********************)


(**************** CUT 260, - *****************)

ecut and [
eqmod cf2204 f2204 2048, eqmod cf2504 f2504 2048, eqmod cf2804 f2804 2048,
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x20014a68*x**2*y**2*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x20014a8c*x**2*y**2*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x20014ab0*x**2*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x200157e8*x**2*y**2*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x2001580c*x**2*y**2*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x20015830*x**2*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #606]	; 0x25e                  #! EA = L0x200187d2; Value = 0x00000ffd; PC = 0x8056850 *)
mov r5 L0x200187d2;
(* ldrsh.w	r6, [lr, #30]                           #! EA = L0x20018592; Value = 0x00030ffd; PC = 0x8056854 *)
mov r6 L0x20018592;
(* ldrsh.w	r8, [lr, #1182]	; 0x49e                 #! EA = L0x20018a12; Value = 0x00030ffd; PC = 0x8056858 *)
mov r8 L0x20018a12;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805685c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2205@sint32 : and [cf2205 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056860 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2505@sint32 : and [cf2505 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056864 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2805@sint32 : and [cf2805 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056868 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x805686c *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056870 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056874 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056878 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x805687c *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056880 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056884 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056888 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x805688c *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056890 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056892 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056894 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014ad4; PC = 0x8056898 *)
mov L0x20014ad4 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014af8; PC = 0x805689c *)
mov L0x20014af8 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014b1c; PC = 0x80568a0 *)
mov L0x20014b1c r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015854; PC = 0x80568a4 *)
mov L0x20015854 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015878; PC = 0x80568a8 *)
mov L0x20015878 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x2001589c; PC = 0x80568ac *)
mov L0x2001589c r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80568b0 *)
adds dc r0 r0 648@uint32;



(******************** offset 2, 2,  5 ********************)


(**************** CUT 261, - *****************)

ecut and [
eqmod cf2205 f2205 2048, eqmod cf2505 f2505 2048, eqmod cf2805 f2805 2048,
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x20014ad4*x**2*y**2*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x20014af8*x**2*y**2*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x20014b1c*x**2*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x20015854*x**2*y**2*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x20015878*x**2*y**2*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x2001589c*x**2*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #228]	; 0xe4                   #! EA = L0x20018658; Value = 0x00000ffd; PC = 0x80568b4 *)
mov r5 L0x20018658;
(* ldrsh.w	r7, [lr, #1380]	; 0x564                 #! EA = L0x20018ad8; Value = 0x00000000; PC = 0x80568b8 *)
mov r7 L0x20018ad8;
(* ldrsh.w	r8, [lr, #804]	; 0x324                  #! EA = L0x20018898; Value = 0x0ffd0ffd; PC = 0x80568bc *)
mov r8 L0x20018898;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80568c0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2206@sint32 : and [cf2206 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80568c4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2506@sint32 : and [cf2506 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80568c8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2806@sint32 : and [cf2806 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x80568cc *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x80568d0 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x80568d4 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x80568d8 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x80568dc *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x80568e0 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x80568e2 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x80568e4 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x80568e8 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x80568ec *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x80568f0 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x80568f4 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80568f8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x80568fc *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056900 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x8056904 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056908 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x805690c *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014b40; PC = 0x8056910 *)
mov L0x20014b40 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014b64; PC = 0x8056914 *)
mov L0x20014b64 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014b88; PC = 0x8056918 *)
mov L0x20014b88 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x200158c0; PC = 0x805691c *)
mov L0x200158c0 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200158e4; PC = 0x8056920 *)
mov L0x200158e4 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015908; PC = 0x8056924 *)
mov L0x20015908 r9;



(******************** offset 2, 2,  6 ********************)


(**************** CUT 262, - *****************)

ecut and [
eqmod cf2206 f2206 2048, eqmod cf2506 f2506 2048, eqmod cf2806 f2806 2048,
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x20014b40*x**2*y**2*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x20014b64*x**2*y**2*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x20014b88*x**2*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x200158c0*x**2*y**2*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x200158e4*x**2*y**2*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x20015908*x**2*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1578]	; 0x62a                 #! EA = L0x20018b9e; Value = 0x00030003; PC = 0x8056928 *)
mov r4 L0x20018b9e;
(* ldrsh.w	r7, [lr, #1002]	; 0x3ea                 #! EA = L0x2001895e; Value = 0x00030003; PC = 0x805692c *)
mov r7 L0x2001895e;
(* ldrsh.w	r8, [lr, #426]	; 0x1aa                  #! EA = L0x2001871e; Value = 0x00030003; PC = 0x8056930 *)
mov r8 L0x2001871e;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056934 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2207@sint32 : and [cf2207 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056938 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2507@sint32 : and [cf2507 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805693c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2807@sint32 : and [cf2807 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056940 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056944 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056948 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805694c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056950 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056954 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056956 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056958 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805695c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056960 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056964 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056968 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805696a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805696c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056970 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056974 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056978 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014bac; PC = 0x805697c *)
mov L0x20014bac r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014bd0; PC = 0x8056980 *)
mov L0x20014bd0 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014bf4; PC = 0x8056984 *)
mov L0x20014bf4 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x2001592c; PC = 0x8056988 *)
mov L0x2001592c r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015950; PC = 0x805698c *)
mov L0x20015950 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015974; PC = 0x8056990 *)
mov L0x20015974 r9;



(******************** offset 2, 2,  7 ********************)


(**************** CUT 263, - *****************)

ecut and [
eqmod cf2207 f2207 2048, eqmod cf2507 f2507 2048, eqmod cf2807 f2807 2048,
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x20014bac*x**2*y**2*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x20014bd0*x**2*y**2*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x20014bf4*x**2*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x2001592c*x**2*y**2*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x20015950*x**2*y**2*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x20015974*x**2*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1200]	; 0x4b0                 #! EA = L0x20018a24; Value = 0x0ffd0000; PC = 0x8056994 *)
mov r4 L0x20018a24;
(* ldrsh.w	r7, [lr, #624]	; 0x270                  #! EA = L0x200187e4; Value = 0x00030003; PC = 0x8056998 *)
mov r7 L0x200187e4;
(* ldrsh.w	r8, [lr, #48]	; 0x30                    #! EA = L0x200185a4; Value = 0x0ffd0000; PC = 0x805699c *)
mov r8 L0x200185a4;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80569a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2208@sint32 : and [cf2208 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80569a4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2508@sint32 : and [cf2508 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80569a8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2808@sint32 : and [cf2808 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80569ac *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80569b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80569b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80569b8 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80569bc *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80569c0 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80569c2 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80569c4 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x80569c8 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x80569cc *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x80569d0 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x80569d4 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x80569d6 *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80569d8 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80569dc *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80569e0 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80569e4 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014c18; PC = 0x80569e8 *)
mov L0x20014c18 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014c3c; PC = 0x80569ec *)
mov L0x20014c3c r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014c60; PC = 0x80569f0 *)
mov L0x20014c60 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015998; PC = 0x80569f4 *)
mov L0x20015998 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x200159bc; PC = 0x80569f8 *)
mov L0x200159bc r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200159e0; PC = 0x80569fc *)
mov L0x200159e0 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056a00 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056a04 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805663c <_Good_loop2_inner>              #! PC = 0x8056a08 *)
#b.w	0x805663c <_Good_loop2_inner>              #! 0x8056a08 = 0x8056a08;



(******************** offset 2, 2,  8 ********************)


(**************** CUT 264, - *****************)

ecut and [
eqmod cf2208 f2208 2048, eqmod cf2508 f2508 2048, eqmod cf2808 f2808 2048,
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x20014c18*x**2*y**2*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x20014c3c*x**2*y**2*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x20014c60*x**2*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x20015998*x**2*y**2*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x200159bc*x**2*y**2*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x200159e0*x**2*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x200188aa; Value = 0x00000ffd; PC = 0x805663c *)
mov r4 L0x200188aa;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x2001866a; Value = 0x00000ffd; PC = 0x8056640 *)
mov r7 L0x2001866a;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018aea; Value = 0x00000000; PC = 0x8056644 *)
mov r9 L0x20018aea;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2209@sint32 : and [cf2209 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2509@sint32 : and [cf2509 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2809@sint32 : and [cf2809 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014c84; PC = 0x8056680 *)
mov L0x20014c84 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014ca8; PC = 0x8056684 *)
mov L0x20014ca8 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014ccc; PC = 0x8056688 *)
mov L0x20014ccc r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015a04; PC = 0x805668c *)
mov L0x20015a04 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015a28; PC = 0x8056690 *)
mov L0x20015a28 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015a4c; PC = 0x8056694 *)
mov L0x20015a4c r9;



(******************** offset 2, 2,  9 ********************)


(**************** CUT 265, - *****************)

ecut and [
eqmod cf2209 f2209 2048, eqmod cf2509 f2509 2048, eqmod cf2809 f2809 2048,
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20014c84*x**2*y**2*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20014ca8*x**2*y**2*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20014ccc*x**2*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20015a04*x**2*y**2*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20015a28*x**2*y**2*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20015a4c*x**2*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x20018730; Value = 0x00030000; PC = 0x8056698 *)
mov r4 L0x20018730;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018bb0; Value = 0x0ffd0ffd; PC = 0x805669c *)
mov r6 L0x20018bb0;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x20018970; Value = 0x00030003; PC = 0x80566a0 *)
mov r9 L0x20018970;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2210@sint32 : and [cf2210 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2510@sint32 : and [cf2510 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2810@sint32 : and [cf2810 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014cf0; PC = 0x80566ec *)
mov L0x20014cf0 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014d14; PC = 0x80566f0 *)
mov L0x20014d14 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014d38; PC = 0x80566f4 *)
mov L0x20014d38 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015a70; PC = 0x80566f8 *)
mov L0x20015a70 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015a94; PC = 0x80566fc *)
mov L0x20015a94 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015ab8; PC = 0x8056700 *)
mov L0x20015ab8 r9;



(******************** offset 2, 2, 10 ********************)


(**************** CUT 266, - *****************)

ecut and [
eqmod cf2210 f2210 2048, eqmod cf2510 f2510 2048, eqmod cf2810 f2810 2048,
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20014cf0*x**2*y**2*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20014d14*x**2*y**2*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20014d38*x**2*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20015a70*x**2*y**2*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20015a94*x**2*y**2*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20015ab8*x**2*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x200185b6; Value = 0x00000003; PC = 0x8056704 *)
mov r4 L0x200185b6;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x20018a36; Value = 0x00000003; PC = 0x8056708 *)
mov r6 L0x20018a36;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x200187f6; Value = 0x0ffd0000; PC = 0x805670c *)
mov r9 L0x200187f6;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2211@sint32 : and [cf2211 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2511@sint32 : and [cf2511 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2811@sint32 : and [cf2811 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014d5c; PC = 0x8056758 *)
mov L0x20014d5c r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20014d80; PC = 0x805675c *)
mov L0x20014d80 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20014da4; PC = 0x8056760 *)
mov L0x20014da4 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015adc; PC = 0x8056764 *)
mov L0x20015adc r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015b00; PC = 0x8056768 *)
mov L0x20015b00 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015b24; PC = 0x805676c *)
mov L0x20015b24 r9;



(******************** offset 2, 2, 11 ********************)


(**************** CUT 267, - *****************)

ecut and [
eqmod cf2211 f2211 2048, eqmod cf2511 f2511 2048, eqmod cf2811 f2811 2048,
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20014d5c*x**2*y**2*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20014d80*x**2*y**2*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20014da4*x**2*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20015adc*x**2*y**2*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20015b00*x**2*y**2*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20015b24*x**2*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018afc; Value = 0x00000ffd; PC = 0x8056770 *)
mov r5 L0x20018afc;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x200188bc; Value = 0x00000ffd; PC = 0x8056774 *)
mov r6 L0x200188bc;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x2001867c; Value = 0x0ffd0ffd; PC = 0x8056778 *)
mov r9 L0x2001867c;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2212@sint32 : and [cf2212 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2512@sint32 : and [cf2512 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2812@sint32 : and [cf2812 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20014dc8; PC = 0x80567cc *)
mov L0x20014dc8 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20014dec; PC = 0x80567d0 *)
mov L0x20014dec r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x20014e10; PC = 0x80567d4 *)
mov L0x20014e10 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015b48; PC = 0x80567d8 *)
mov L0x20015b48 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015b6c; PC = 0x80567dc *)
mov L0x20015b6c r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015b90; PC = 0x80567e0 *)
mov L0x20015b90 r9;



(******************** offset 2, 2, 12 ********************)


(**************** CUT 268, - *****************)

ecut and [
eqmod cf2212 f2212 2048, eqmod cf2512 f2512 2048, eqmod cf2812 f2812 2048,
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20014dc8*x**2*y**2*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20014dec*x**2*y**2*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20014e10*x**2*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20015b48*x**2*y**2*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20015b6c*x**2*y**2*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20015b90*x**2*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x20018982; Value = 0x0ffd0000; PC = 0x80567e4 *)
mov r5 L0x20018982;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x20018742; Value = 0x00000003; PC = 0x80567e8 *)
mov r6 L0x20018742;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018bc2; Value = 0x00030ffd; PC = 0x80567ec *)
mov r8 L0x20018bc2;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2213@sint32 : and [cf2213 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2513@sint32 : and [cf2513 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2813@sint32 : and [cf2813 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20014e34; PC = 0x805682c *)
mov L0x20014e34 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20014e58; PC = 0x8056830 *)
mov L0x20014e58 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20014e7c; PC = 0x8056834 *)
mov L0x20014e7c r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015bb4; PC = 0x8056838 *)
mov L0x20015bb4 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015bd8; PC = 0x805683c *)
mov L0x20015bd8 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015bfc; PC = 0x8056840 *)
mov L0x20015bfc r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;



(******************** offset 2, 2, 13 ********************)


(**************** CUT 269, - *****************)

ecut and [
eqmod cf2213 f2213 2048, eqmod cf2513 f2513 2048, eqmod cf2813 f2813 2048,
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20014e34*x**2*y**2*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20014e58*x**2*y**2*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20014e7c*x**2*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20015bb4*x**2*y**2*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20015bd8*x**2*y**2*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20015bfc*x**2*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #606]	; 0x25e                  #! EA = L0x20018808; Value = 0x00000003; PC = 0x8056850 *)
mov r5 L0x20018808;
(* ldrsh.w	r6, [lr, #30]                           #! EA = L0x200185c8; Value = 0x00030000; PC = 0x8056854 *)
mov r6 L0x200185c8;
(* ldrsh.w	r8, [lr, #1182]	; 0x49e                 #! EA = L0x20018a48; Value = 0x00030003; PC = 0x8056858 *)
mov r8 L0x20018a48;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805685c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2214@sint32 : and [cf2214 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056860 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2514@sint32 : and [cf2514 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056864 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2814@sint32 : and [cf2814 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056868 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x805686c *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056870 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056874 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056878 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x805687c *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056880 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056884 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056888 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x805688c *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056890 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056892 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056894 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x20014ea0; PC = 0x8056898 *)
mov L0x20014ea0 r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20014ec4; PC = 0x805689c *)
mov L0x20014ec4 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x20014ee8; PC = 0x80568a0 *)
mov L0x20014ee8 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015c20; PC = 0x80568a4 *)
mov L0x20015c20 r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20015c44; PC = 0x80568a8 *)
mov L0x20015c44 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20015c68; PC = 0x80568ac *)
mov L0x20015c68 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80568b0 *)
adds dc r0 r0 648@uint32;



(******************** offset 2, 2, 14 ********************)


(**************** CUT 270, - *****************)

ecut and [
eqmod cf2214 f2214 2048, eqmod cf2514 f2514 2048, eqmod cf2814 f2814 2048,
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20014ea0*x**2*y**2*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20014ec4*x**2*y**2*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20014ee8*x**2*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20015c20*x**2*y**2*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20015c44*x**2*y**2*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20015c68*x**2*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #228]	; 0xe4                   #! EA = L0x2001868e; Value = 0x00000000; PC = 0x80568b4 *)
mov r5 L0x2001868e;
(* ldrsh.w	r7, [lr, #1380]	; 0x564                 #! EA = L0x20018b0e; Value = 0x0ffd0000; PC = 0x80568b8 *)
mov r7 L0x20018b0e;
(* ldrsh.w	r8, [lr, #804]	; 0x324                  #! EA = L0x200188ce; Value = 0x00000003; PC = 0x80568bc *)
mov r8 L0x200188ce;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80568c0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2215@sint32 : and [cf2215 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80568c4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2515@sint32 : and [cf2515 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80568c8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2815@sint32 : and [cf2815 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x80568cc *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x80568d0 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x80568d4 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x80568d8 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x80568dc *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x80568e0 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x80568e2 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x80568e4 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x80568e8 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x80568ec *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x80568f0 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x80568f4 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80568f8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x80568fc *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056900 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x8056904 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056908 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x805690c *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20014f0c; PC = 0x8056910 *)
mov L0x20014f0c r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20014f30; PC = 0x8056914 *)
mov L0x20014f30 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20014f54; PC = 0x8056918 *)
mov L0x20014f54 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015c8c; PC = 0x805691c *)
mov L0x20015c8c r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015cb0; PC = 0x8056920 *)
mov L0x20015cb0 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015cd4; PC = 0x8056924 *)
mov L0x20015cd4 r9;



(******************** offset 2, 2, 15 ********************)


(**************** CUT 271, - *****************)

ecut and [
eqmod cf2215 f2215 2048, eqmod cf2515 f2515 2048, eqmod cf2815 f2815 2048,
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20014f0c*x**2*y**2*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20014f30*x**2*y**2*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20014f54*x**2*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20015c8c*x**2*y**2*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20015cb0*x**2*y**2*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20015cd4*x**2*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1578]	; 0x62a                 #! EA = L0x20018bd4; Value = 0x00000000; PC = 0x8056928 *)
mov r4 L0x20018bd4;
(* ldrsh.w	r7, [lr, #1002]	; 0x3ea                 #! EA = L0x20018994; Value = 0x0ffd0003; PC = 0x805692c *)
mov r7 L0x20018994;
(* ldrsh.w	r8, [lr, #426]	; 0x1aa                  #! EA = L0x20018754; Value = 0x00000ffd; PC = 0x8056930 *)
mov r8 L0x20018754;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056934 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2216@sint32 : and [cf2216 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056938 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2516@sint32 : and [cf2516 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805693c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2816@sint32 : and [cf2816 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056940 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056944 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056948 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805694c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056950 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056954 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056956 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056958 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805695c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056960 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056964 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056968 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805696a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805696c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056970 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056974 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056978 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20014f78; PC = 0x805697c *)
mov L0x20014f78 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20014f9c; PC = 0x8056980 *)
mov L0x20014f9c r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20014fc0; PC = 0x8056984 *)
mov L0x20014fc0 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015cf8; PC = 0x8056988 *)
mov L0x20015cf8 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015d1c; PC = 0x805698c *)
mov L0x20015d1c r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015d40; PC = 0x8056990 *)
mov L0x20015d40 r9;



(******************** offset 2, 2, 16 ********************)


(**************** CUT 272, - *****************)

ecut and [
eqmod cf2216 f2216 2048, eqmod cf2516 f2516 2048, eqmod cf2816 f2816 2048,
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20014f78*x**2*y**2*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20014f9c*x**2*y**2*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20014fc0*x**2*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20015cf8*x**2*y**2*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20015d1c*x**2*y**2*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20015d40*x**2*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1200]	; 0x4b0                 #! EA = L0x20018a5a; Value = 0x00000000; PC = 0x8056994 *)
mov r4 L0x20018a5a;
(* ldrsh.w	r7, [lr, #624]	; 0x270                  #! EA = L0x2001881a; Value = 0x0ffd0000; PC = 0x8056998 *)
mov r7 L0x2001881a;
(* ldrsh.w	r8, [lr, #48]	; 0x30                    #! EA = L0x200185da; Value = 0x00000ffd; PC = 0x805699c *)
mov r8 L0x200185da;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80569a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2217@sint32 : and [cf2217 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80569a4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2517@sint32 : and [cf2517 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80569a8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2817@sint32 : and [cf2817 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80569ac *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80569b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80569b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80569b8 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80569bc *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80569c0 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80569c2 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80569c4 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x80569c8 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x80569cc *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x80569d0 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x80569d4 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x80569d6 *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80569d8 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80569dc *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80569e0 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80569e4 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20014fe4; PC = 0x80569e8 *)
mov L0x20014fe4 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015008; PC = 0x80569ec *)
mov L0x20015008 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x2001502c; PC = 0x80569f0 *)
mov L0x2001502c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015d64; PC = 0x80569f4 *)
mov L0x20015d64 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015d88; PC = 0x80569f8 *)
mov L0x20015d88 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015dac; PC = 0x80569fc *)
mov L0x20015dac r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056a00 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056a04 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805663c <_Good_loop2_inner>              #! PC = 0x8056a08 *)
#b.w	0x805663c <_Good_loop2_inner>              #! 0x8056a08 = 0x8056a08;



(******************** offset 2, 2, 17 ********************)


(**************** CUT 273, - *****************)

ecut and [
eqmod cf2217 f2217 2048, eqmod cf2517 f2517 2048, eqmod cf2817 f2817 2048,
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x20014fe4*x**2*y**2*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x20015008*x**2*y**2*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x2001502c*x**2*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x20015d64*x**2*y**2*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x20015d88*x**2*y**2*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x20015dac*x**2*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x200188e0; Value = 0x00030000; PC = 0x805663c *)
mov r4 L0x200188e0;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x200186a0; Value = 0x0ffd0003; PC = 0x8056640 *)
mov r7 L0x200186a0;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018b20; Value = 0x00000000; PC = 0x8056644 *)
mov r9 L0x20018b20;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2218@sint32 : and [cf2218 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2518@sint32 : and [cf2518 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2818@sint32 : and [cf2818 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x20015050; PC = 0x8056680 *)
mov L0x20015050 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015074; PC = 0x8056684 *)
mov L0x20015074 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015098; PC = 0x8056688 *)
mov L0x20015098 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20015dd0; PC = 0x805668c *)
mov L0x20015dd0 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x20015df4; PC = 0x8056690 *)
mov L0x20015df4 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x20015e18; PC = 0x8056694 *)
mov L0x20015e18 r9;



(******************** offset 2, 2, 18 ********************)


(**************** CUT 274, - *****************)

ecut and [
eqmod cf2218 f2218 2048, eqmod cf2518 f2518 2048, eqmod cf2818 f2818 2048,
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015050*x**2*y**2*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015074*x**2*y**2*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015098*x**2*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015dd0*x**2*y**2*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015df4*x**2*y**2*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015e18*x**2*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x20018766; Value = 0x00000003; PC = 0x8056698 *)
mov r4 L0x20018766;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018be6; Value = 0x00000000; PC = 0x805669c *)
mov r6 L0x20018be6;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x200189a6; Value = 0x00030ffd; PC = 0x80566a0 *)
mov r9 L0x200189a6;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2219@sint32 : and [cf2219 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2519@sint32 : and [cf2519 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2819@sint32 : and [cf2819 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x200150bc; PC = 0x80566ec *)
mov L0x200150bc r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200150e0; PC = 0x80566f0 *)
mov L0x200150e0 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x20015104; PC = 0x80566f4 *)
mov L0x20015104 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20015e3c; PC = 0x80566f8 *)
mov L0x20015e3c r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x20015e60; PC = 0x80566fc *)
mov L0x20015e60 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20015e84; PC = 0x8056700 *)
mov L0x20015e84 r9;



(******************** offset 2, 2, 19 ********************)


(**************** CUT 275, - *****************)

ecut and [
eqmod cf2219 f2219 2048, eqmod cf2519 f2519 2048, eqmod cf2819 f2819 2048,
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x200150bc*x**2*y**2*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x200150e0*x**2*y**2*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x20015104*x**2*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x20015e3c*x**2*y**2*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x20015e60*x**2*y**2*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x20015e84*x**2*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x200185ec; Value = 0x0ffd0000; PC = 0x8056704 *)
mov r4 L0x200185ec;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x20018a6c; Value = 0x00000000; PC = 0x8056708 *)
mov r6 L0x20018a6c;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x2001882c; Value = 0x0ffd0000; PC = 0x805670c *)
mov r9 L0x2001882c;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2220@sint32 : and [cf2220 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2520@sint32 : and [cf2520 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2820@sint32 : and [cf2820 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x20015128; PC = 0x8056758 *)
mov L0x20015128 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x2001514c; PC = 0x805675c *)
mov L0x2001514c r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x20015170; PC = 0x8056760 *)
mov L0x20015170 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20015ea8; PC = 0x8056764 *)
mov L0x20015ea8 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20015ecc; PC = 0x8056768 *)
mov L0x20015ecc r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20015ef0; PC = 0x805676c *)
mov L0x20015ef0 r9;



(******************** offset 2, 2, 20 ********************)


(**************** CUT 276, - *****************)

ecut and [
eqmod cf2220 f2220 2048, eqmod cf2520 f2520 2048, eqmod cf2820 f2820 2048,
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x20015128*x**2*y**2*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x2001514c*x**2*y**2*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x20015170*x**2*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x20015ea8*x**2*y**2*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x20015ecc*x**2*y**2*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x20015ef0*x**2*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018b32; Value = 0x00000000; PC = 0x8056770 *)
mov r5 L0x20018b32;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x200188f2; Value = 0x00000003; PC = 0x8056774 *)
mov r6 L0x200188f2;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x200186b2; Value = 0x00030000; PC = 0x8056778 *)
mov r9 L0x200186b2;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2221@sint32 : and [cf2221 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2521@sint32 : and [cf2521 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2821@sint32 : and [cf2821 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015194; PC = 0x80567cc *)
mov L0x20015194 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x200151b8; PC = 0x80567d0 *)
mov L0x200151b8 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200151dc; PC = 0x80567d4 *)
mov L0x200151dc r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x20015f14; PC = 0x80567d8 *)
mov L0x20015f14 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20015f38; PC = 0x80567dc *)
mov L0x20015f38 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20015f5c; PC = 0x80567e0 *)
mov L0x20015f5c r9;



(******************** offset 2, 2, 21 ********************)


(**************** CUT 277, - *****************)

ecut and [
eqmod cf2221 f2221 2048, eqmod cf2521 f2521 2048, eqmod cf2821 f2821 2048,
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x20015194*x**2*y**2*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x200151b8*x**2*y**2*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x200151dc*x**2*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x20015f14*x**2*y**2*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x20015f38*x**2*y**2*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x20015f5c*x**2*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x200189b8; Value = 0x0ffd0ffd; PC = 0x80567e4 *)
mov r5 L0x200189b8;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x20018778; Value = 0x0ffd0000; PC = 0x80567e8 *)
mov r6 L0x20018778;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018bf8; Value = 0x00000000; PC = 0x80567ec *)
mov r8 L0x20018bf8;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2222@sint32 : and [cf2222 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2522@sint32 : and [cf2522 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2822@sint32 : and [cf2822 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x20015200; PC = 0x805682c *)
mov L0x20015200 r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x20015224; PC = 0x8056830 *)
mov L0x20015224 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015248; PC = 0x8056834 *)
mov L0x20015248 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x20015f80; PC = 0x8056838 *)
mov L0x20015f80 r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20015fa4; PC = 0x805683c *)
mov L0x20015fa4 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20015fc8; PC = 0x8056840 *)
mov L0x20015fc8 r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;



(******************** offset 2, 2, 22 ********************)


(**************** CUT 278, - *****************)

ecut and [
eqmod cf2222 f2222 2048, eqmod cf2522 f2522 2048, eqmod cf2822 f2822 2048,
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015200*x**2*y**2*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015224*x**2*y**2*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015248*x**2*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015f80*x**2*y**2*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015fa4*x**2*y**2*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015fc8*x**2*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #606]	; 0x25e                  #! EA = L0x2001883e; Value = 0x00000003; PC = 0x8056850 *)
mov r5 L0x2001883e;
(* ldrsh.w	r6, [lr, #30]                           #! EA = L0x200185fe; Value = 0x0ffd0003; PC = 0x8056854 *)
mov r6 L0x200185fe;
(* ldrsh.w	r8, [lr, #1182]	; 0x49e                 #! EA = L0x20018a7e; Value = 0x0ffd0000; PC = 0x8056858 *)
mov r8 L0x20018a7e;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805685c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2223@sint32 : and [cf2223 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056860 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2523@sint32 : and [cf2523 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x8056864 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2823@sint32 : and [cf2823 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x8056868 *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x805686c *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056870 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056874 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x8056878 *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x805687c *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056880 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056884 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x8056888 *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x805688c *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056890 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056892 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056894 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #540]	; 0x21c                    #! EA = L0x2001526c; PC = 0x8056898 *)
mov L0x2001526c r4;
(* str.w	r6, [r0, #576]	; 0x240                    #! EA = L0x20015290; PC = 0x805689c *)
mov L0x20015290 r6;
(* str.w	r8, [r0, #612]	; 0x264                    #! EA = L0x200152b4; PC = 0x80568a0 *)
mov L0x200152b4 r8;
(* str.w	r5, [r0, #3996]	; 0xf9c                   #! EA = L0x20015fec; PC = 0x80568a4 *)
mov L0x20015fec r5;
(* str.w	r7, [r0, #4032]	; 0xfc0                   #! EA = L0x20016010; PC = 0x80568a8 *)
mov L0x20016010 r7;
(* str.w	r9, [r0, #4068]	; 0xfe4                   #! EA = L0x20016034; PC = 0x80568ac *)
mov L0x20016034 r9;
(* add.w	r0, r0, #648	; 0x288                      #! PC = 0x80568b0 *)
adds dc r0 r0 648@uint32;



(******************** offset 2, 2, 23 ********************)


(**************** CUT 279, - *****************)

ecut and [
eqmod cf2223 f2223 2048, eqmod cf2523 f2523 2048, eqmod cf2823 f2823 2048,
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x2001526c*x**2*y**2*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x20015290*x**2*y**2*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x200152b4*x**2*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x20015fec*x**2*y**2*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x20016010*x**2*y**2*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x20016034*x**2*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #228]	; 0xe4                   #! EA = L0x200186c4; Value = 0x0ffd0000; PC = 0x80568b4 *)
mov r5 L0x200186c4;
(* ldrsh.w	r7, [lr, #1380]	; 0x564                 #! EA = L0x20018b44; Value = 0x00000ffd; PC = 0x80568b8 *)
mov r7 L0x20018b44;
(* ldrsh.w	r8, [lr, #804]	; 0x324                  #! EA = L0x20018904; Value = 0x0ffd0003; PC = 0x80568bc *)
mov r8 L0x20018904;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80568c0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2224@sint32 : and [cf2224 = r5] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80568c4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2524@sint32 : and [cf2524 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80568c8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2824@sint32 : and [cf2824 = r8] && true;
(* smull	r1, r6, r7, r10                           #! PC = 0x80568cc *)
smull r6 r1 r7 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x80568d0 *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r6, r4, r3                            #! PC = 0x80568d4 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r9, r6, r7                                #! PC = 0x80568d8 *)
add r9 r6 r7;
(* sub.w	r9, r5, r9                                #! PC = 0x80568dc *)
sub r9 r5 r9;
(* add	r6, r5                                      #! PC = 0x80568e0 *)
add r6 r6 r5;
(* add	r5, r7                                      #! PC = 0x80568e2 *)
add r5 r5 r7;
(* smull	r4, r1, r8, r10                           #! PC = 0x80568e4 *)
smull r1 r4 r8 r10;
(* mul.w	r7, r4, r2                                #! PC = 0x80568e8 *)
mull dontcare r7 r4 r2;
cast r7@sint32 r7;
(* smlal	r4, r1, r7, r3                            #! PC = 0x80568ec *)
smull tmpml_h tmpml_l r7 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r7, r8, r1                                #! PC = 0x80568f0 *)
add r7 r8 r1;
(* add.w	r4, r8, r5                                #! PC = 0x80568f4 *)
add r4 r8 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x80568f8 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* add.w	r8, r1, r9                                #! PC = 0x80568fc *)
add r8 r1 r9;
(* sub.w	r9, r8, r9, lsl #1                        #! PC = 0x8056900 *)
shl tmpx2 r9 1;
sub r9 r8 tmpx2;
(* rsb	r7, r7, #0                                  #! PC = 0x8056904 *)
sub r7 0@sint32 r7;
(* sub.w	r7, r7, r6                                #! PC = 0x8056908 *)
sub r7 r7 r6;
(* add.w	r6, r7, r6, lsl #1                        #! PC = 0x805690c *)
shl tmpx2 r6 1;
add r6 r7 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x200152d8; PC = 0x8056910 *)
mov L0x200152d8 r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x200152fc; PC = 0x8056914 *)
mov L0x200152fc r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015320; PC = 0x8056918 *)
mov L0x20015320 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x20016058; PC = 0x805691c *)
mov L0x20016058 r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x2001607c; PC = 0x8056920 *)
mov L0x2001607c r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200160a0; PC = 0x8056924 *)
mov L0x200160a0 r9;



(******************** offset 2, 2, 24 ********************)


(**************** CUT 280, - *****************)

ecut and [
eqmod cf2224 f2224 2048, eqmod cf2524 f2524 2048, eqmod cf2824 f2824 2048,
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x200152d8*x**2*y**2*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x200152fc*x**2*y**2*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x20015320*x**2*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x20016058*x**2*y**2*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x2001607c*x**2*y**2*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x200160a0*x**2*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1578]	; 0x62a                 #! EA = L0x20018c0a; Value = 0x00000000; PC = 0x8056928 *)
mov r4 L0x20018c0a;
(* ldrsh.w	r7, [lr, #1002]	; 0x3ea                 #! EA = L0x200189ca; Value = 0x0ffd0ffd; PC = 0x805692c *)
mov r7 L0x200189ca;
(* ldrsh.w	r8, [lr, #426]	; 0x1aa                  #! EA = L0x2001878a; Value = 0x0ffd0000; PC = 0x8056930 *)
mov r8 L0x2001878a;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056934 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2225@sint32 : and [cf2225 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x8056938 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2525@sint32 : and [cf2525 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x805693c *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2825@sint32 : and [cf2825 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x8056940 *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056944 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x8056948 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x805694c *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056950 *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x8056954 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x8056956 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x8056958 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x805695c *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x8056960 *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x8056964 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x8056968 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x805696a *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x805696c *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x8056970 *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x8056974 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x8056978 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015344; PC = 0x805697c *)
mov L0x20015344 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x20015368; PC = 0x8056980 *)
mov L0x20015368 r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x2001538c; PC = 0x8056984 *)
mov L0x2001538c r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x200160c4; PC = 0x8056988 *)
mov L0x200160c4 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x200160e8; PC = 0x805698c *)
mov L0x200160e8 r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x2001610c; PC = 0x8056990 *)
mov L0x2001610c r9;



(******************** offset 2, 2, 25 ********************)


(**************** CUT 281, - *****************)

ecut and [
eqmod cf2225 f2225 2048, eqmod cf2525 f2525 2048, eqmod cf2825 f2825 2048,
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x20015344*x**2*y**2*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x20015368*x**2*y**2*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x2001538c*x**2*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x200160c4*x**2*y**2*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x200160e8*x**2*y**2*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x2001610c*x**2*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #1200]	; 0x4b0                 #! EA = L0x20018a90; Value = 0x0ffd0ffd; PC = 0x8056994 *)
mov r4 L0x20018a90;
(* ldrsh.w	r7, [lr, #624]	; 0x270                  #! EA = L0x20018850; Value = 0x0ffd0ffd; PC = 0x8056998 *)
mov r7 L0x20018850;
(* ldrsh.w	r8, [lr, #48]	; 0x30                    #! EA = L0x20018610; Value = 0x00030003; PC = 0x805699c *)
mov r8 L0x20018610;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80569a0 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2226@sint32 : and [cf2226 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x80569a4 *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2526@sint32 : and [cf2526 = r7] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80569a8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2826@sint32 : and [cf2826 = r8] && true;
(* smull	r1, r9, r8, r10                           #! PC = 0x80569ac *)
smull r9 r1 r8 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80569b0 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r9, r5, r3                            #! PC = 0x80569b4 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r6, r8, r9                                #! PC = 0x80569b8 *)
add r6 r8 r9;
(* add.w	r9, r9, r4                                #! PC = 0x80569bc *)
add r9 r9 r4;
(* subs	r6, r4, r6                                 #! PC = 0x80569c0 *)
subc carry r6 r4 r6;
(* add	r4, r8                                      #! PC = 0x80569c2 *)
add r4 r4 r8;
(* smull	r5, r1, r7, r10                           #! PC = 0x80569c4 *)
smull r1 r5 r7 r10;
(* mul.w	r8, r5, r2                                #! PC = 0x80569c8 *)
mull dontcare r8 r5 r2;
cast r8@sint32 r8;
(* smlal	r5, r1, r8, r3                            #! PC = 0x80569cc *)
smull tmpml_h tmpml_l r8 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r8, r7, r1                                #! PC = 0x80569d0 *)
add r8 r7 r1;
(* add	r4, r7                                      #! PC = 0x80569d4 *)
add r4 r4 r7;
(* add	r6, r1                                      #! PC = 0x80569d6 *)
add r6 r6 r1;
(* sub.w	r5, r4, r7, lsl #1                        #! PC = 0x80569d8 *)
shl tmpx2 r7 1;
sub r5 r4 tmpx2;
(* sub.w	r7, r6, r1, lsl #1                        #! PC = 0x80569dc *)
shl tmpx2 r1 1;
sub r7 r6 tmpx2;
(* add.w	r9, r9, r8                                #! PC = 0x80569e0 *)
add r9 r9 r8;
(* sub.w	r8, r9, r8, lsl #1                        #! PC = 0x80569e4 *)
shl tmpx2 r8 1;
sub r8 r9 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200153b0; PC = 0x80569e8 *)
mov L0x200153b0 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x200153d4; PC = 0x80569ec *)
mov L0x200153d4 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x200153f8; PC = 0x80569f0 *)
mov L0x200153f8 r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016130; PC = 0x80569f4 *)
mov L0x20016130 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016154; PC = 0x80569f8 *)
mov L0x20016154 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x20016178; PC = 0x80569fc *)
mov L0x20016178 r9;
(* add.w	lr, lr, #54	; 0x36                        #! PC = 0x8056a00 *)
adds dc lr lr 54@uint32;
(* add.w	r0, r0, #324	; 0x144                      #! PC = 0x8056a04 *)
adds dc r0 r0 324@uint32;
(* #b.w	0x805663c <_Good_loop2_inner>              #! PC = 0x8056a08 *)
#b.w	0x805663c <_Good_loop2_inner>              #! 0x8056a08 = 0x8056a08;



(******************** offset 2, 2, 26 ********************)


(**************** CUT 282, - *****************)

ecut and [
eqmod cf2226 f2226 2048, eqmod cf2526 f2526 2048, eqmod cf2826 f2826 2048,
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x200153b0*x**2*y**2*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x200153d4*x**2*y**2*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x200153f8*x**2*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x20016130*x**2*y**2*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x20016154*x**2*y**2*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x20016178*x**2*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #768]	; 0x300                  #! EA = L0x20018916; Value = 0x00030000; PC = 0x805663c *)
mov r4 L0x20018916;
(* ldrsh.w	r7, [lr, #192]	; 0xc0                   #! EA = L0x200186d6; Value = 0x00030003; PC = 0x8056640 *)
mov r7 L0x200186d6;
(* ldrsh.w	r9, [lr, #1344]	; 0x540                 #! EA = L0x20018b56; Value = 0x00030000; PC = 0x8056644 *)
mov r9 L0x20018b56;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056648 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2227@sint32 : and [cf2227 = r4] && true;
(* sbfx	r7, r7, #0, #12                            #! PC = 0x805664c *)
mov r7_o r7;
split dontcare sbfxlow r7 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r7 sbfxhi sbfxlow;
ghost cf2527@sint32 : and [cf2527 = r7] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056650 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2827@sint32 : and [cf2827 = r9] && true;
(* smull	r8, r6, r7, r10                           #! PC = 0x8056654 *)
smull r6 r8 r7 r10;
(* smlal	r8, r6, r9, r11                           #! PC = 0x8056658 *)
smull tmpml_h tmpml_l r9 r11;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
(* mul.w	r5, r8, r2                                #! PC = 0x805665c *)
mull dontcare r5 r8 r2;
cast r5@sint32 r5;
(* smlal	r8, r6, r5, r3                            #! PC = 0x8056660 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r8 r8 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r8 0 (2**32) && true;
assume r8 = 0 && true;
(* add.w	r5, r7, r9                                #! PC = 0x8056664 *)
add r5 r7 r9;
(* add.w	r9, r6, r5                                #! PC = 0x8056668 *)
add r9 r6 r5;
(* sub.w	r8, r4, r9                                #! PC = 0x805666c *)
sub r8 r4 r9;
(* add.w	r9, r9, r4                                #! PC = 0x8056670 *)
add r9 r9 r4;
(* sub.w	r7, r4, r6                                #! PC = 0x8056674 *)
sub r7 r4 r6;
(* add	r6, r4                                      #! PC = 0x8056678 *)
add r6 r6 r4;
(* add	r4, r5                                      #! PC = 0x805667a *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x805667c *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0]                                  #! EA = L0x2001541c; PC = 0x8056680 *)
mov L0x2001541c r4;
(* str.w	r6, [r0, #36]	; 0x24                      #! EA = L0x20015440; PC = 0x8056684 *)
mov L0x20015440 r6;
(* str.w	r8, [r0, #72]	; 0x48                      #! EA = L0x20015464; PC = 0x8056688 *)
mov L0x20015464 r8;
(* str.w	r5, [r0, #3456]	; 0xd80                   #! EA = L0x2001619c; PC = 0x805668c *)
mov L0x2001619c r5;
(* str.w	r7, [r0, #3492]	; 0xda4                   #! EA = L0x200161c0; PC = 0x8056690 *)
mov L0x200161c0 r7;
(* str.w	r9, [r0, #3528]	; 0xdc8                   #! EA = L0x200161e4; PC = 0x8056694 *)
mov L0x200161e4 r9;



(******************** offset 2, 2, 27 ********************)


(**************** CUT 283, - *****************)

ecut and [
eqmod cf2227 f2227 2048, eqmod cf2527 f2527 2048, eqmod cf2827 f2827 2048,
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x2001541c*x**2*y**2*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x20015440*x**2*y**2*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x20015464*x**2*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x2001619c*x**2*y**2*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x200161c0*x**2*y**2*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x200161e4*x**2*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #390]	; 0x186                  #! EA = L0x2001879c; Value = 0x00000000; PC = 0x8056698 *)
mov r4 L0x2001879c;
(* ldrsh.w	r6, [lr, #1542]	; 0x606                 #! EA = L0x20018c1c; Value = 0x00000000; PC = 0x805669c *)
mov r6 L0x20018c1c;
(* ldrsh.w	r9, [lr, #966]	; 0x3c6                  #! EA = L0x200189dc; Value = 0x00000ffd; PC = 0x80566a0 *)
mov r9 L0x200189dc;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x80566a4 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2228@sint32 : and [cf2228 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80566a8 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2528@sint32 : and [cf2528 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x80566ac *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2828@sint32 : and [cf2828 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x80566b0 *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x80566b4 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x80566b8 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x80566bc *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x80566c0 *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x80566c4 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x80566c6 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x80566c8 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x80566cc *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x80566d0 *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x80566d4 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x80566d8 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x80566da *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x80566dc *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x80566e0 *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x80566e4 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x80566e8 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #108]	; 0x6c                     #! EA = L0x20015488; PC = 0x80566ec *)
mov L0x20015488 r4;
(* str.w	r6, [r0, #144]	; 0x90                     #! EA = L0x200154ac; PC = 0x80566f0 *)
mov L0x200154ac r6;
(* str.w	r8, [r0, #180]	; 0xb4                     #! EA = L0x200154d0; PC = 0x80566f4 *)
mov L0x200154d0 r8;
(* str.w	r5, [r0, #3564]	; 0xdec                   #! EA = L0x20016208; PC = 0x80566f8 *)
mov L0x20016208 r5;
(* str.w	r7, [r0, #3600]	; 0xe10                   #! EA = L0x2001622c; PC = 0x80566fc *)
mov L0x2001622c r7;
(* str.w	r9, [r0, #3636]	; 0xe34                   #! EA = L0x20016250; PC = 0x8056700 *)
mov L0x20016250 r9;



(******************** offset 2, 2, 28 ********************)


(**************** CUT 284, - *****************)

ecut and [
eqmod cf2228 f2228 2048, eqmod cf2528 f2528 2048, eqmod cf2828 f2828 2048,
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x20015488*x**2*y**2*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x200154ac*x**2*y**2*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x200154d0*x**2*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x20016208*x**2*y**2*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x2001622c*x**2*y**2*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x20016250*x**2*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r4, [lr, #12]                           #! EA = L0x20018622; Value = 0x00000003; PC = 0x8056704 *)
mov r4 L0x20018622;
(* ldrsh.w	r6, [lr, #1164]	; 0x48c                 #! EA = L0x20018aa2; Value = 0x0ffd0ffd; PC = 0x8056708 *)
mov r6 L0x20018aa2;
(* ldrsh.w	r9, [lr, #588]	; 0x24c                  #! EA = L0x20018862; Value = 0x0ffd0000; PC = 0x805670c *)
mov r9 L0x20018862;
(* sbfx	r4, r4, #0, #12                            #! PC = 0x8056710 *)
mov r4_o r4;
split dontcare sbfxlow r4 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r4 sbfxhi sbfxlow;
ghost cf2229@sint32 : and [cf2229 = r4] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056714 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2529@sint32 : and [cf2529 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056718 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2829@sint32 : and [cf2829 = r9] && true;
(* smull	r1, r7, r6, r10                           #! PC = 0x805671c *)
smull r7 r1 r6 r10;
(* mul.w	r5, r1, r2                                #! PC = 0x8056720 *)
mull dontcare r5 r1 r2;
cast r5@sint32 r5;
(* smlal	r1, r7, r5, r3                            #! PC = 0x8056724 *)
smull tmpml_h tmpml_l r5 r3;
adds carry r1 r1 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r8, r6, r7                                #! PC = 0x8056728 *)
add r8 r6 r7;
(* sub.w	r8, r4, r8                                #! PC = 0x805672c *)
sub r8 r4 r8;
(* add	r7, r4                                      #! PC = 0x8056730 *)
add r7 r7 r4;
(* add	r4, r6                                      #! PC = 0x8056732 *)
add r4 r4 r6;
(* smull	r5, r1, r9, r10                           #! PC = 0x8056734 *)
smull r1 r5 r9 r10;
(* mul.w	r6, r5, r2                                #! PC = 0x8056738 *)
mull dontcare r6 r5 r2;
cast r6@sint32 r6;
(* smlal	r5, r1, r6, r3                            #! PC = 0x805673c *)
smull tmpml_h tmpml_l r6 r3;
adds carry r5 r5 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r5 0 (2**32) && true;
assume r5 = 0 && true;
(* add.w	r6, r9, r1                                #! PC = 0x8056740 *)
add r6 r9 r1;
(* add	r4, r9                                      #! PC = 0x8056744 *)
add r4 r4 r9;
(* add	r7, r6                                      #! PC = 0x8056746 *)
add r7 r7 r6;
(* sub.w	r5, r4, r9, lsl #1                        #! PC = 0x8056748 *)
shl tmpx2 r9 1;
sub r5 r4 tmpx2;
(* sub.w	r6, r7, r6, lsl #1                        #! PC = 0x805674c *)
shl tmpx2 r6 1;
sub r6 r7 tmpx2;
(* add.w	r8, r8, r1                                #! PC = 0x8056750 *)
add r8 r8 r1;
(* sub.w	r9, r8, r1, lsl #1                        #! PC = 0x8056754 *)
shl tmpx2 r1 1;
sub r9 r8 tmpx2;
(* str.w	r4, [r0, #216]	; 0xd8                     #! EA = L0x200154f4; PC = 0x8056758 *)
mov L0x200154f4 r4;
(* str.w	r6, [r0, #252]	; 0xfc                     #! EA = L0x20015518; PC = 0x805675c *)
mov L0x20015518 r6;
(* str.w	r8, [r0, #288]	; 0x120                    #! EA = L0x2001553c; PC = 0x8056760 *)
mov L0x2001553c r8;
(* str.w	r5, [r0, #3672]	; 0xe58                   #! EA = L0x20016274; PC = 0x8056764 *)
mov L0x20016274 r5;
(* str.w	r7, [r0, #3708]	; 0xe7c                   #! EA = L0x20016298; PC = 0x8056768 *)
mov L0x20016298 r7;
(* str.w	r9, [r0, #3744]	; 0xea0                   #! EA = L0x200162bc; PC = 0x805676c *)
mov L0x200162bc r9;



(******************** offset 2, 2, 29 ********************)


(**************** CUT 285, - *****************)

ecut and [
eqmod cf2229 f2229 2048, eqmod cf2529 f2529 2048, eqmod cf2829 f2829 2048,
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x200154f4*x**2*y**2*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x20015518*x**2*y**2*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x2001553c*x**2*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x20016274*x**2*y**2*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x20016298*x**2*y**2*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x200162bc*x**2*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #1362]	; 0x552                 #! EA = L0x20018b68; Value = 0x00000000; PC = 0x8056770 *)
mov r5 L0x20018b68;
(* ldrsh.w	r6, [lr, #786]	; 0x312                  #! EA = L0x20018928; Value = 0x00030003; PC = 0x8056774 *)
mov r6 L0x20018928;
(* ldrsh.w	r9, [lr, #210]	; 0xd2                   #! EA = L0x200186e8; Value = 0x00030000; PC = 0x8056778 *)
mov r9 L0x200186e8;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x805677c *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2230@sint32 : and [cf2230 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x8056780 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2530@sint32 : and [cf2530 = r6] && true;
(* sbfx	r9, r9, #0, #12                            #! PC = 0x8056784 *)
mov r9_o r9;
split dontcare sbfxlow r9 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r9 sbfxhi sbfxlow;
ghost cf2830@sint32 : and [cf2830 = r9] && true;
(* smull	r1, r8, r9, r10                           #! PC = 0x8056788 *)
smull r8 r1 r9 r10;
(* mul.w	r4, r1, r2                                #! PC = 0x805678c *)
mull dontcare r4 r1 r2;
cast r4@sint32 r4;
(* smlal	r1, r8, r4, r3                            #! PC = 0x8056790 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r1 r1 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r1 0 (2**32) && true;
assume r1 = 0 && true;
(* add.w	r7, r8, r9                                #! PC = 0x8056794 *)
add r7 r8 r9;
(* add.w	r8, r8, r5                                #! PC = 0x8056798 *)
add r8 r8 r5;
(* subs	r7, r5, r7                                 #! PC = 0x805679c *)
subc carry r7 r5 r7;
(* add	r5, r9                                      #! PC = 0x805679e *)
add r5 r5 r9;
(* smull	r4, r1, r6, r10                           #! PC = 0x80567a0 *)
smull r1 r4 r6 r10;
(* mul.w	r9, r4, r2                                #! PC = 0x80567a4 *)
mull dontcare r9 r4 r2;
cast r9@sint32 r9;
(* smlal	r4, r1, r9, r3                            #! PC = 0x80567a8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry r4 r4 tmpml_l;
adc r1 r1 tmpml_h carry;
assert eqmod r4 0 (2**32) && true;
assume r4 = 0 && true;
(* add.w	r9, r6, r1                                #! PC = 0x80567ac *)
add r9 r6 r1;
(* add.w	r4, r6, r5                                #! PC = 0x80567b0 *)
add r4 r6 r5;
(* sub.w	r5, r6, r5                                #! PC = 0x80567b4 *)
sub r5 r6 r5;
(* add.w	r6, r1, r7                                #! PC = 0x80567b8 *)
add r6 r1 r7;
(* sub.w	r7, r6, r7, lsl #1                        #! PC = 0x80567bc *)
shl tmpx2 r7 1;
sub r7 r6 tmpx2;
(* sub.w	r8, r8, r9                                #! PC = 0x80567c0 *)
sub r8 r8 r9;
(* add.w	r9, r8, r9, lsl #1                        #! PC = 0x80567c4 *)
shl tmpx2 r9 1;
add r9 r8 tmpx2;
(* rsb	r9, r9, #0                                  #! PC = 0x80567c8 *)
sub r9 0@sint32 r9;
(* str.w	r4, [r0, #324]	; 0x144                    #! EA = L0x20015560; PC = 0x80567cc *)
mov L0x20015560 r4;
(* str.w	r6, [r0, #360]	; 0x168                    #! EA = L0x20015584; PC = 0x80567d0 *)
mov L0x20015584 r6;
(* str.w	r8, [r0, #396]	; 0x18c                    #! EA = L0x200155a8; PC = 0x80567d4 *)
mov L0x200155a8 r8;
(* str.w	r5, [r0, #3780]	; 0xec4                   #! EA = L0x200162e0; PC = 0x80567d8 *)
mov L0x200162e0 r5;
(* str.w	r7, [r0, #3816]	; 0xee8                   #! EA = L0x20016304; PC = 0x80567dc *)
mov L0x20016304 r7;
(* str.w	r9, [r0, #3852]	; 0xf0c                   #! EA = L0x20016328; PC = 0x80567e0 *)
mov L0x20016328 r9;



(******************** offset 2, 2, 30 ********************)


(**************** CUT 286, - *****************)

ecut and [
eqmod cf2230 f2230 2048, eqmod cf2530 f2530 2048, eqmod cf2830 f2830 2048,
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x20015560*x**2*y**2*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x20015584*x**2*y**2*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x200155a8*x**2*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x200162e0*x**2*y**2*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x20016304*x**2*y**2*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x20016328*x**2*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(* ldrsh.w	r5, [lr, #984]	; 0x3d8                  #! EA = L0x200189ee; Value = 0x0ffd0ffd; PC = 0x80567e4 *)
mov r5 L0x200189ee;
(* ldrsh.w	r6, [lr, #408]	; 0x198                  #! EA = L0x200187ae; Value = 0x00000003; PC = 0x80567e8 *)
mov r6 L0x200187ae;
(* ldrsh.w	r8, [lr, #1560]	; 0x618                 #! EA = L0x20018c2e; Value = 0x00010000; PC = 0x80567ec *)
mov r8 L0x20018c2e;
(* sbfx	r5, r5, #0, #12                            #! PC = 0x80567f0 *)
mov r5_o r5;
split dontcare sbfxlow r5 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r5 sbfxhi sbfxlow;
ghost cf2231@sint32 : and [cf2231 = r5] && true;
(* sbfx	r6, r6, #0, #12                            #! PC = 0x80567f4 *)
mov r6_o r6;
split dontcare sbfxlow r6 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r6 sbfxhi sbfxlow;
ghost cf2531@sint32 : and [cf2531 = r6] && true;
(* sbfx	r8, r8, #0, #12                            #! PC = 0x80567f8 *)
mov r8_o r8;
split dontcare sbfxlow r8 12;
split sbfxmsb dontcare sbfxlow 11;
cast sbfxlow@sint32 sbfxlow;
cast sbfxmsb@bit sbfxmsb;
cmov sbfxhi sbfxmsb 0xfffff000@sint32 0x0@sint32;
add r8 sbfxhi sbfxlow;
ghost cf2831@sint32 : and [cf2831 = r8] && true;
(* smull	r9, r7, r6, r10                           #! PC = 0x80567fc *)
smull r7 r9 r6 r10;
(* smlal	r9, r7, r8, r11                           #! PC = 0x8056800 *)
smull tmpml_h tmpml_l r8 r11;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
(* mul.w	r4, r9, r2                                #! PC = 0x8056804 *)
mull dontcare r4 r9 r2;
cast r4@sint32 r4;
(* smlal	r9, r7, r4, r3                            #! PC = 0x8056808 *)
smull tmpml_h tmpml_l r4 r3;
adds carry r9 r9 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r9 0 (2**32) && true;
assume r9 = 0 && true;
(* add.w	r4, r6, r8                                #! PC = 0x805680c *)
add r4 r6 r8;
(* add.w	r8, r7, r4                                #! PC = 0x8056810 *)
add r8 r7 r4;
(* add.w	r9, r5, r8                                #! PC = 0x8056814 *)
add r9 r5 r8;
(* rsb	r9, r9, #0                                  #! PC = 0x8056818 *)
sub r9 0@sint32 r9;
(* sub.w	r8, r5, r8                                #! PC = 0x805681c *)
sub r8 r5 r8;
(* add.w	r6, r7, r5                                #! PC = 0x8056820 *)
add r6 r7 r5;
(* subs	r7, r7, r5                                 #! PC = 0x8056824 *)
subc carry r7 r7 r5;
(* add	r4, r5                                      #! PC = 0x8056826 *)
add r4 r4 r5;
(* sub.w	r5, r4, r5, lsl #1                        #! PC = 0x8056828 *)
shl tmpx2 r5 1;
sub r5 r4 tmpx2;
(* str.w	r4, [r0, #432]	; 0x1b0                    #! EA = L0x200155cc; PC = 0x805682c *)
mov L0x200155cc r4;
(* str.w	r6, [r0, #468]	; 0x1d4                    #! EA = L0x200155f0; PC = 0x8056830 *)
mov L0x200155f0 r6;
(* str.w	r8, [r0, #504]	; 0x1f8                    #! EA = L0x20015614; PC = 0x8056834 *)
mov L0x20015614 r8;
(* str.w	r5, [r0, #3888]	; 0xf30                   #! EA = L0x2001634c; PC = 0x8056838 *)
mov L0x2001634c r5;
(* str.w	r7, [r0, #3924]	; 0xf54                   #! EA = L0x20016370; PC = 0x805683c *)
mov L0x20016370 r7;
(* str.w	r9, [r0, #3960]	; 0xf78                   #! EA = L0x20016394; PC = 0x8056840 *)
mov L0x20016394 r9;
(* vmov	r1, s3                                     #! PC = 0x8056844 *)
mov r1 s3;
(* cmp.w	r0, r1                                    #! PC = 0x8056848 *)
(* cmp.w r0, r1 *)nop;
(* #beq.w	0x8056a0c <_Good_loop2_inner_end>        #! PC = 0x805684c *)
#beq.w	0x8056a0c <_Good_loop2_inner_end>        #! 0x805684c = 0x805684c;
(* sub.w	lr, lr, #162	; 0xa2                       #! PC = 0x8056a0c *)
subs dc lr lr 162@uint32;
(* subw	r0, r0, #2916	; 0xb64                      #! PC = 0x8056a10 *)
subc dc r0 r0 2916@uint32;
(* add.w	lr, lr, #2                                #! PC = 0x8056a14 *)
adds dc lr lr 2@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8056a18 *)
adds dc r0 r0 4@uint32;
(* cmp.w	r0, r12                                   #! PC = 0x8056a1c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8056634 <_Good_loop2>                  #! PC = 0x8056a20 *)
#bne.w	0x8056634 <_Good_loop2>                  #! 0x8056a20 = 0x8056a20;
(* #ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, pc}#! EA = L0x20014868; Value = 0x20014898; PC = 0x8056a24 *)
#ldmia.w	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, pc}#! L0x20014868 = L0x20014868; 0x20014898 = 0x20014898; 0x8056a24 = 0x8056a24;
(* #add.w	r7, sp, #6912	; 0x1b00                    #! PC = 0x8003a20 *)
#add.w	%%r7, sp, #6912	; 0x1b00                    #! 0x8003a20 = 0x8003a20;
(* #mov	r0, r4                                      #! PC = 0x8003a24 *)
#mov	%%r0, %%r4                                      #! 0x8003a24 = 0x8003a24;

(******************** offset 2, 2, 31 ********************)


(**************** CUT 287, - *****************)

ecut and [
eqmod cf2231 f2231 2048, eqmod cf2531 f2531 2048, eqmod cf2831 f2831 2048,
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x200155cc*x**2*y**2*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x200155f0*x**2*y**2*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x20015614*x**2*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x2001634c*x**2*y**2*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x20016370*x**2*y**2*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x20016394*x**2*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
];



(**************** CUT -,   8 *****************)

rcut and [
(-3367617)@32<=sL0x200148b8,L0x200148b8<=s3367617@32,
(-3367617)@32<=sL0x200148dc,L0x200148dc<=s3367617@32,
(-3367617)@32<=sL0x20014900,L0x20014900<=s3367617@32,
(-3367617)@32<=sL0x20015638,L0x20015638<=s3367617@32,
(-3367617)@32<=sL0x2001565c,L0x2001565c<=s3367617@32,
(-3367617)@32<=sL0x20015680,L0x20015680<=s3367617@32
,
(-3367617)@32<=sL0x20014924,L0x20014924<=s3367617@32,
(-3367617)@32<=sL0x20014948,L0x20014948<=s3367617@32,
(-3367617)@32<=sL0x2001496c,L0x2001496c<=s3367617@32,
(-3367617)@32<=sL0x200156a4,L0x200156a4<=s3367617@32,
(-3367617)@32<=sL0x200156c8,L0x200156c8<=s3367617@32,
(-3367617)@32<=sL0x200156ec,L0x200156ec<=s3367617@32
,
(-3367617)@32<=sL0x20014990,L0x20014990<=s3367617@32,
(-3367617)@32<=sL0x200149b4,L0x200149b4<=s3367617@32,
(-3367617)@32<=sL0x200149d8,L0x200149d8<=s3367617@32,
(-3367617)@32<=sL0x20015710,L0x20015710<=s3367617@32,
(-3367617)@32<=sL0x20015734,L0x20015734<=s3367617@32,
(-3367617)@32<=sL0x20015758,L0x20015758<=s3367617@32
,
(-3367617)@32<=sL0x200149fc,L0x200149fc<=s3367617@32,
(-3367617)@32<=sL0x20014a20,L0x20014a20<=s3367617@32,
(-3367617)@32<=sL0x20014a44,L0x20014a44<=s3367617@32,
(-3367617)@32<=sL0x2001577c,L0x2001577c<=s3367617@32,
(-3367617)@32<=sL0x200157a0,L0x200157a0<=s3367617@32,
(-3367617)@32<=sL0x200157c4,L0x200157c4<=s3367617@32
,
(-3367617)@32<=sL0x20014a68,L0x20014a68<=s3367617@32,
(-3367617)@32<=sL0x20014a8c,L0x20014a8c<=s3367617@32,
(-3367617)@32<=sL0x20014ab0,L0x20014ab0<=s3367617@32,
(-3367617)@32<=sL0x200157e8,L0x200157e8<=s3367617@32,
(-3367617)@32<=sL0x2001580c,L0x2001580c<=s3367617@32,
(-3367617)@32<=sL0x20015830,L0x20015830<=s3367617@32
,
(-3367617)@32<=sL0x20014ad4,L0x20014ad4<=s3367617@32,
(-3367617)@32<=sL0x20014af8,L0x20014af8<=s3367617@32,
(-3367617)@32<=sL0x20014b1c,L0x20014b1c<=s3367617@32,
(-3367617)@32<=sL0x20015854,L0x20015854<=s3367617@32,
(-3367617)@32<=sL0x20015878,L0x20015878<=s3367617@32,
(-3367617)@32<=sL0x2001589c,L0x2001589c<=s3367617@32
,
(-3367617)@32<=sL0x20014b40,L0x20014b40<=s3367617@32,
(-3367617)@32<=sL0x20014b64,L0x20014b64<=s3367617@32,
(-3367617)@32<=sL0x20014b88,L0x20014b88<=s3367617@32,
(-3367617)@32<=sL0x200158c0,L0x200158c0<=s3367617@32,
(-3367617)@32<=sL0x200158e4,L0x200158e4<=s3367617@32,
(-3367617)@32<=sL0x20015908,L0x20015908<=s3367617@32
,
(-3367617)@32<=sL0x20014bac,L0x20014bac<=s3367617@32,
(-3367617)@32<=sL0x20014bd0,L0x20014bd0<=s3367617@32,
(-3367617)@32<=sL0x20014bf4,L0x20014bf4<=s3367617@32,
(-3367617)@32<=sL0x2001592c,L0x2001592c<=s3367617@32,
(-3367617)@32<=sL0x20015950,L0x20015950<=s3367617@32,
(-3367617)@32<=sL0x20015974,L0x20015974<=s3367617@32
,
(-3367617)@32<=sL0x20014c18,L0x20014c18<=s3367617@32,
(-3367617)@32<=sL0x20014c3c,L0x20014c3c<=s3367617@32,
(-3367617)@32<=sL0x20014c60,L0x20014c60<=s3367617@32,
(-3367617)@32<=sL0x20015998,L0x20015998<=s3367617@32,
(-3367617)@32<=sL0x200159bc,L0x200159bc<=s3367617@32,
(-3367617)@32<=sL0x200159e0,L0x200159e0<=s3367617@32
,
(-3367617)@32<=sL0x20014c84,L0x20014c84<=s3367617@32,
(-3367617)@32<=sL0x20014ca8,L0x20014ca8<=s3367617@32,
(-3367617)@32<=sL0x20014ccc,L0x20014ccc<=s3367617@32,
(-3367617)@32<=sL0x20015a04,L0x20015a04<=s3367617@32,
(-3367617)@32<=sL0x20015a28,L0x20015a28<=s3367617@32,
(-3367617)@32<=sL0x20015a4c,L0x20015a4c<=s3367617@32
,
(-3367617)@32<=sL0x20014cf0,L0x20014cf0<=s3367617@32,
(-3367617)@32<=sL0x20014d14,L0x20014d14<=s3367617@32,
(-3367617)@32<=sL0x20014d38,L0x20014d38<=s3367617@32,
(-3367617)@32<=sL0x20015a70,L0x20015a70<=s3367617@32,
(-3367617)@32<=sL0x20015a94,L0x20015a94<=s3367617@32,
(-3367617)@32<=sL0x20015ab8,L0x20015ab8<=s3367617@32
,
(-3367617)@32<=sL0x20014d5c,L0x20014d5c<=s3367617@32,
(-3367617)@32<=sL0x20014d80,L0x20014d80<=s3367617@32,
(-3367617)@32<=sL0x20014da4,L0x20014da4<=s3367617@32,
(-3367617)@32<=sL0x20015adc,L0x20015adc<=s3367617@32,
(-3367617)@32<=sL0x20015b00,L0x20015b00<=s3367617@32,
(-3367617)@32<=sL0x20015b24,L0x20015b24<=s3367617@32
,
(-3367617)@32<=sL0x20014dc8,L0x20014dc8<=s3367617@32,
(-3367617)@32<=sL0x20014dec,L0x20014dec<=s3367617@32,
(-3367617)@32<=sL0x20014e10,L0x20014e10<=s3367617@32,
(-3367617)@32<=sL0x20015b48,L0x20015b48<=s3367617@32,
(-3367617)@32<=sL0x20015b6c,L0x20015b6c<=s3367617@32,
(-3367617)@32<=sL0x20015b90,L0x20015b90<=s3367617@32
,
(-3367617)@32<=sL0x20014e34,L0x20014e34<=s3367617@32,
(-3367617)@32<=sL0x20014e58,L0x20014e58<=s3367617@32,
(-3367617)@32<=sL0x20014e7c,L0x20014e7c<=s3367617@32,
(-3367617)@32<=sL0x20015bb4,L0x20015bb4<=s3367617@32,
(-3367617)@32<=sL0x20015bd8,L0x20015bd8<=s3367617@32,
(-3367617)@32<=sL0x20015bfc,L0x20015bfc<=s3367617@32
,
(-3367617)@32<=sL0x20014ea0,L0x20014ea0<=s3367617@32,
(-3367617)@32<=sL0x20014ec4,L0x20014ec4<=s3367617@32,
(-3367617)@32<=sL0x20014ee8,L0x20014ee8<=s3367617@32,
(-3367617)@32<=sL0x20015c20,L0x20015c20<=s3367617@32,
(-3367617)@32<=sL0x20015c44,L0x20015c44<=s3367617@32,
(-3367617)@32<=sL0x20015c68,L0x20015c68<=s3367617@32
,
(-3367617)@32<=sL0x20014f0c,L0x20014f0c<=s3367617@32,
(-3367617)@32<=sL0x20014f30,L0x20014f30<=s3367617@32,
(-3367617)@32<=sL0x20014f54,L0x20014f54<=s3367617@32,
(-3367617)@32<=sL0x20015c8c,L0x20015c8c<=s3367617@32,
(-3367617)@32<=sL0x20015cb0,L0x20015cb0<=s3367617@32,
(-3367617)@32<=sL0x20015cd4,L0x20015cd4<=s3367617@32
,
(-3367617)@32<=sL0x20014f78,L0x20014f78<=s3367617@32,
(-3367617)@32<=sL0x20014f9c,L0x20014f9c<=s3367617@32,
(-3367617)@32<=sL0x20014fc0,L0x20014fc0<=s3367617@32,
(-3367617)@32<=sL0x20015cf8,L0x20015cf8<=s3367617@32,
(-3367617)@32<=sL0x20015d1c,L0x20015d1c<=s3367617@32,
(-3367617)@32<=sL0x20015d40,L0x20015d40<=s3367617@32
,
(-3367617)@32<=sL0x20014fe4,L0x20014fe4<=s3367617@32,
(-3367617)@32<=sL0x20015008,L0x20015008<=s3367617@32,
(-3367617)@32<=sL0x2001502c,L0x2001502c<=s3367617@32,
(-3367617)@32<=sL0x20015d64,L0x20015d64<=s3367617@32,
(-3367617)@32<=sL0x20015d88,L0x20015d88<=s3367617@32,
(-3367617)@32<=sL0x20015dac,L0x20015dac<=s3367617@32
,
(-3367617)@32<=sL0x20015050,L0x20015050<=s3367617@32,
(-3367617)@32<=sL0x20015074,L0x20015074<=s3367617@32,
(-3367617)@32<=sL0x20015098,L0x20015098<=s3367617@32,
(-3367617)@32<=sL0x20015dd0,L0x20015dd0<=s3367617@32,
(-3367617)@32<=sL0x20015df4,L0x20015df4<=s3367617@32,
(-3367617)@32<=sL0x20015e18,L0x20015e18<=s3367617@32
,
(-3367617)@32<=sL0x200150bc,L0x200150bc<=s3367617@32,
(-3367617)@32<=sL0x200150e0,L0x200150e0<=s3367617@32,
(-3367617)@32<=sL0x20015104,L0x20015104<=s3367617@32,
(-3367617)@32<=sL0x20015e3c,L0x20015e3c<=s3367617@32,
(-3367617)@32<=sL0x20015e60,L0x20015e60<=s3367617@32,
(-3367617)@32<=sL0x20015e84,L0x20015e84<=s3367617@32
,
(-3367617)@32<=sL0x20015128,L0x20015128<=s3367617@32,
(-3367617)@32<=sL0x2001514c,L0x2001514c<=s3367617@32,
(-3367617)@32<=sL0x20015170,L0x20015170<=s3367617@32,
(-3367617)@32<=sL0x20015ea8,L0x20015ea8<=s3367617@32,
(-3367617)@32<=sL0x20015ecc,L0x20015ecc<=s3367617@32,
(-3367617)@32<=sL0x20015ef0,L0x20015ef0<=s3367617@32
,
(-3367617)@32<=sL0x20015194,L0x20015194<=s3367617@32,
(-3367617)@32<=sL0x200151b8,L0x200151b8<=s3367617@32,
(-3367617)@32<=sL0x200151dc,L0x200151dc<=s3367617@32,
(-3367617)@32<=sL0x20015f14,L0x20015f14<=s3367617@32,
(-3367617)@32<=sL0x20015f38,L0x20015f38<=s3367617@32,
(-3367617)@32<=sL0x20015f5c,L0x20015f5c<=s3367617@32
,
(-3367617)@32<=sL0x20015200,L0x20015200<=s3367617@32,
(-3367617)@32<=sL0x20015224,L0x20015224<=s3367617@32,
(-3367617)@32<=sL0x20015248,L0x20015248<=s3367617@32,
(-3367617)@32<=sL0x20015f80,L0x20015f80<=s3367617@32,
(-3367617)@32<=sL0x20015fa4,L0x20015fa4<=s3367617@32,
(-3367617)@32<=sL0x20015fc8,L0x20015fc8<=s3367617@32
,
(-3367617)@32<=sL0x2001526c,L0x2001526c<=s3367617@32,
(-3367617)@32<=sL0x20015290,L0x20015290<=s3367617@32,
(-3367617)@32<=sL0x200152b4,L0x200152b4<=s3367617@32,
(-3367617)@32<=sL0x20015fec,L0x20015fec<=s3367617@32,
(-3367617)@32<=sL0x20016010,L0x20016010<=s3367617@32,
(-3367617)@32<=sL0x20016034,L0x20016034<=s3367617@32
,
(-3367617)@32<=sL0x200152d8,L0x200152d8<=s3367617@32,
(-3367617)@32<=sL0x200152fc,L0x200152fc<=s3367617@32,
(-3367617)@32<=sL0x20015320,L0x20015320<=s3367617@32,
(-3367617)@32<=sL0x20016058,L0x20016058<=s3367617@32,
(-3367617)@32<=sL0x2001607c,L0x2001607c<=s3367617@32,
(-3367617)@32<=sL0x200160a0,L0x200160a0<=s3367617@32
,
(-3367617)@32<=sL0x20015344,L0x20015344<=s3367617@32,
(-3367617)@32<=sL0x20015368,L0x20015368<=s3367617@32,
(-3367617)@32<=sL0x2001538c,L0x2001538c<=s3367617@32,
(-3367617)@32<=sL0x200160c4,L0x200160c4<=s3367617@32,
(-3367617)@32<=sL0x200160e8,L0x200160e8<=s3367617@32,
(-3367617)@32<=sL0x2001610c,L0x2001610c<=s3367617@32
,
(-3367617)@32<=sL0x200153b0,L0x200153b0<=s3367617@32,
(-3367617)@32<=sL0x200153d4,L0x200153d4<=s3367617@32,
(-3367617)@32<=sL0x200153f8,L0x200153f8<=s3367617@32,
(-3367617)@32<=sL0x20016130,L0x20016130<=s3367617@32,
(-3367617)@32<=sL0x20016154,L0x20016154<=s3367617@32,
(-3367617)@32<=sL0x20016178,L0x20016178<=s3367617@32
,
(-3367617)@32<=sL0x2001541c,L0x2001541c<=s3367617@32,
(-3367617)@32<=sL0x20015440,L0x20015440<=s3367617@32,
(-3367617)@32<=sL0x20015464,L0x20015464<=s3367617@32,
(-3367617)@32<=sL0x2001619c,L0x2001619c<=s3367617@32,
(-3367617)@32<=sL0x200161c0,L0x200161c0<=s3367617@32,
(-3367617)@32<=sL0x200161e4,L0x200161e4<=s3367617@32
,
(-3367617)@32<=sL0x20015488,L0x20015488<=s3367617@32,
(-3367617)@32<=sL0x200154ac,L0x200154ac<=s3367617@32,
(-3367617)@32<=sL0x200154d0,L0x200154d0<=s3367617@32,
(-3367617)@32<=sL0x20016208,L0x20016208<=s3367617@32,
(-3367617)@32<=sL0x2001622c,L0x2001622c<=s3367617@32,
(-3367617)@32<=sL0x20016250,L0x20016250<=s3367617@32
,
(-3367617)@32<=sL0x200154f4,L0x200154f4<=s3367617@32,
(-3367617)@32<=sL0x20015518,L0x20015518<=s3367617@32,
(-3367617)@32<=sL0x2001553c,L0x2001553c<=s3367617@32,
(-3367617)@32<=sL0x20016274,L0x20016274<=s3367617@32,
(-3367617)@32<=sL0x20016298,L0x20016298<=s3367617@32,
(-3367617)@32<=sL0x200162bc,L0x200162bc<=s3367617@32
,
(-3367617)@32<=sL0x20015560,L0x20015560<=s3367617@32,
(-3367617)@32<=sL0x20015584,L0x20015584<=s3367617@32,
(-3367617)@32<=sL0x200155a8,L0x200155a8<=s3367617@32,
(-3367617)@32<=sL0x200162e0,L0x200162e0<=s3367617@32,
(-3367617)@32<=sL0x20016304,L0x20016304<=s3367617@32,
(-3367617)@32<=sL0x20016328,L0x20016328<=s3367617@32
,
(-3367617)@32<=sL0x200155cc,L0x200155cc<=s3367617@32,
(-3367617)@32<=sL0x200155f0,L0x200155f0<=s3367617@32,
(-3367617)@32<=sL0x20015614,L0x20015614<=s3367617@32,
(-3367617)@32<=sL0x2001634c,L0x2001634c<=s3367617@32,
(-3367617)@32<=sL0x20016370,L0x20016370<=s3367617@32,
(-3367617)@32<=sL0x20016394,L0x20016394<=s3367617@32
] prove with [ precondition ];


(******************** collect eqmods ********************)


(**************** CUT 288, - *****************)

ecut and [
eqmod cf0000 f0000 2048, eqmod cf0001 f0001 2048, eqmod cf0002 f0002 2048,
eqmod cf0003 f0003 2048, eqmod cf0004 f0004 2048, eqmod cf0005 f0005 2048,
eqmod cf0006 f0006 2048, eqmod cf0007 f0007 2048, eqmod cf0008 f0008 2048,
eqmod cf0009 f0009 2048, eqmod cf0010 f0010 2048, eqmod cf0011 f0011 2048,
eqmod cf0012 f0012 2048, eqmod cf0013 f0013 2048, eqmod cf0014 f0014 2048,
eqmod cf0015 f0015 2048, eqmod cf0016 f0016 2048, eqmod cf0017 f0017 2048,
eqmod cf0018 f0018 2048, eqmod cf0019 f0019 2048, eqmod cf0020 f0020 2048,
eqmod cf0021 f0021 2048, eqmod cf0022 f0022 2048, eqmod cf0023 f0023 2048,
eqmod cf0024 f0024 2048, eqmod cf0025 f0025 2048, eqmod cf0026 f0026 2048,
eqmod cf0027 f0027 2048, eqmod cf0028 f0028 2048, eqmod cf0029 f0029 2048,
eqmod cf0030 f0030 2048, eqmod cf0031 f0031 2048, eqmod cf0100 f0100 2048,
eqmod cf0101 f0101 2048, eqmod cf0102 f0102 2048, eqmod cf0103 f0103 2048,
eqmod cf0104 f0104 2048, eqmod cf0105 f0105 2048, eqmod cf0106 f0106 2048,
eqmod cf0107 f0107 2048, eqmod cf0108 f0108 2048, eqmod cf0109 f0109 2048,
eqmod cf0110 f0110 2048, eqmod cf0111 f0111 2048, eqmod cf0112 f0112 2048,
eqmod cf0113 f0113 2048, eqmod cf0114 f0114 2048, eqmod cf0115 f0115 2048,
eqmod cf0116 f0116 2048, eqmod cf0117 f0117 2048, eqmod cf0118 f0118 2048,
eqmod cf0119 f0119 2048, eqmod cf0120 f0120 2048, eqmod cf0121 f0121 2048,
eqmod cf0122 f0122 2048, eqmod cf0123 f0123 2048, eqmod cf0124 f0124 2048,
eqmod cf0125 f0125 2048, eqmod cf0126 f0126 2048, eqmod cf0127 f0127 2048,
eqmod cf0128 f0128 2048, eqmod cf0129 f0129 2048, eqmod cf0130 f0130 2048,
eqmod cf0131 f0131 2048, eqmod cf0200 f0200 2048, eqmod cf0201 f0201 2048,
eqmod cf0202 f0202 2048, eqmod cf0203 f0203 2048, eqmod cf0204 f0204 2048,
eqmod cf0205 f0205 2048, eqmod cf0206 f0206 2048, eqmod cf0207 f0207 2048,
eqmod cf0208 f0208 2048, eqmod cf0209 f0209 2048, eqmod cf0210 f0210 2048,
eqmod cf0211 f0211 2048, eqmod cf0212 f0212 2048, eqmod cf0213 f0213 2048,
eqmod cf0214 f0214 2048, eqmod cf0215 f0215 2048, eqmod cf0216 f0216 2048,
eqmod cf0217 f0217 2048, eqmod cf0218 f0218 2048, eqmod cf0219 f0219 2048,
eqmod cf0220 f0220 2048, eqmod cf0221 f0221 2048, eqmod cf0222 f0222 2048,
eqmod cf0223 f0223 2048, eqmod cf0224 f0224 2048, eqmod cf0225 f0225 2048,
eqmod cf0226 f0226 2048, eqmod cf0227 f0227 2048, eqmod cf0228 f0228 2048,
eqmod cf0229 f0229 2048, eqmod cf0230 f0230 2048, eqmod cf0231 f0231 2048,
eqmod cf0300 f0300 2048, eqmod cf0301 f0301 2048, eqmod cf0302 f0302 2048,
eqmod cf0303 f0303 2048, eqmod cf0304 f0304 2048, eqmod cf0305 f0305 2048,
eqmod cf0306 f0306 2048, eqmod cf0307 f0307 2048, eqmod cf0308 f0308 2048,
eqmod cf0309 f0309 2048, eqmod cf0310 f0310 2048, eqmod cf0311 f0311 2048,
eqmod cf0312 f0312 2048, eqmod cf0313 f0313 2048, eqmod cf0314 f0314 2048,
eqmod cf0315 f0315 2048, eqmod cf0316 f0316 2048, eqmod cf0317 f0317 2048,
eqmod cf0318 f0318 2048, eqmod cf0319 f0319 2048, eqmod cf0320 f0320 2048,
eqmod cf0321 f0321 2048, eqmod cf0322 f0322 2048, eqmod cf0323 f0323 2048,
eqmod cf0324 f0324 2048, eqmod cf0325 f0325 2048, eqmod cf0326 f0326 2048,
eqmod cf0327 f0327 2048, eqmod cf0328 f0328 2048, eqmod cf0329 f0329 2048,
eqmod cf0330 f0330 2048, eqmod cf0331 f0331 2048, eqmod cf0400 f0400 2048,
eqmod cf0401 f0401 2048, eqmod cf0402 f0402 2048, eqmod cf0403 f0403 2048,
eqmod cf0404 f0404 2048, eqmod cf0405 f0405 2048, eqmod cf0406 f0406 2048,
eqmod cf0407 f0407 2048, eqmod cf0408 f0408 2048, eqmod cf0409 f0409 2048,
eqmod cf0410 f0410 2048, eqmod cf0411 f0411 2048, eqmod cf0412 f0412 2048,
eqmod cf0413 f0413 2048, eqmod cf0414 f0414 2048, eqmod cf0415 f0415 2048,
eqmod cf0416 f0416 2048, eqmod cf0417 f0417 2048, eqmod cf0418 f0418 2048,
eqmod cf0419 f0419 2048, eqmod cf0420 f0420 2048, eqmod cf0421 f0421 2048,
eqmod cf0422 f0422 2048, eqmod cf0423 f0423 2048, eqmod cf0424 f0424 2048,
eqmod cf0425 f0425 2048, eqmod cf0426 f0426 2048, eqmod cf0427 f0427 2048,
eqmod cf0428 f0428 2048, eqmod cf0429 f0429 2048, eqmod cf0430 f0430 2048,
eqmod cf0431 f0431 2048, eqmod cf0500 f0500 2048, eqmod cf0501 f0501 2048,
eqmod cf0502 f0502 2048, eqmod cf0503 f0503 2048, eqmod cf0504 f0504 2048,
eqmod cf0505 f0505 2048, eqmod cf0506 f0506 2048, eqmod cf0507 f0507 2048,
eqmod cf0508 f0508 2048, eqmod cf0509 f0509 2048, eqmod cf0510 f0510 2048,
eqmod cf0511 f0511 2048, eqmod cf0512 f0512 2048, eqmod cf0513 f0513 2048,
eqmod cf0514 f0514 2048, eqmod cf0515 f0515 2048, eqmod cf0516 f0516 2048,
eqmod cf0517 f0517 2048, eqmod cf0518 f0518 2048, eqmod cf0519 f0519 2048,
eqmod cf0520 f0520 2048, eqmod cf0521 f0521 2048, eqmod cf0522 f0522 2048,
eqmod cf0523 f0523 2048, eqmod cf0524 f0524 2048, eqmod cf0525 f0525 2048,
eqmod cf0526 f0526 2048, eqmod cf0527 f0527 2048, eqmod cf0528 f0528 2048,
eqmod cf0529 f0529 2048, eqmod cf0530 f0530 2048, eqmod cf0531 f0531 2048,
eqmod cf0600 f0600 2048, eqmod cf0601 f0601 2048, eqmod cf0602 f0602 2048,
eqmod cf0603 f0603 2048, eqmod cf0604 f0604 2048, eqmod cf0605 f0605 2048,
eqmod cf0606 f0606 2048, eqmod cf0607 f0607 2048, eqmod cf0608 f0608 2048,
eqmod cf0609 f0609 2048, eqmod cf0610 f0610 2048, eqmod cf0611 f0611 2048,
eqmod cf0612 f0612 2048, eqmod cf0613 f0613 2048, eqmod cf0614 f0614 2048,
eqmod cf0615 f0615 2048, eqmod cf0616 f0616 2048, eqmod cf0617 f0617 2048,
eqmod cf0618 f0618 2048, eqmod cf0619 f0619 2048, eqmod cf0620 f0620 2048,
eqmod cf0621 f0621 2048, eqmod cf0622 f0622 2048, eqmod cf0623 f0623 2048,
eqmod cf0624 f0624 2048, eqmod cf0625 f0625 2048, eqmod cf0626 f0626 2048,
eqmod cf0627 f0627 2048, eqmod cf0628 f0628 2048, eqmod cf0629 f0629 2048,
eqmod cf0630 f0630 2048, eqmod cf0631 f0631 2048, eqmod cf0700 f0700 2048,
eqmod cf0701 f0701 2048, eqmod cf0702 f0702 2048, eqmod cf0703 f0703 2048,
eqmod cf0704 f0704 2048, eqmod cf0705 f0705 2048, eqmod cf0706 f0706 2048,
eqmod cf0707 f0707 2048, eqmod cf0708 f0708 2048, eqmod cf0709 f0709 2048,
eqmod cf0710 f0710 2048, eqmod cf0711 f0711 2048, eqmod cf0712 f0712 2048,
eqmod cf0713 f0713 2048, eqmod cf0714 f0714 2048, eqmod cf0715 f0715 2048,
eqmod cf0716 f0716 2048, eqmod cf0717 f0717 2048, eqmod cf0718 f0718 2048,
eqmod cf0719 f0719 2048, eqmod cf0720 f0720 2048, eqmod cf0721 f0721 2048,
eqmod cf0722 f0722 2048, eqmod cf0723 f0723 2048, eqmod cf0724 f0724 2048,
eqmod cf0725 f0725 2048, eqmod cf0726 f0726 2048, eqmod cf0727 f0727 2048,
eqmod cf0728 f0728 2048, eqmod cf0729 f0729 2048, eqmod cf0730 f0730 2048,
eqmod cf0731 f0731 2048, eqmod cf0800 f0800 2048, eqmod cf0801 f0801 2048,
eqmod cf0802 f0802 2048, eqmod cf0803 f0803 2048, eqmod cf0804 f0804 2048,
eqmod cf0805 f0805 2048, eqmod cf0806 f0806 2048, eqmod cf0807 f0807 2048,
eqmod cf0808 f0808 2048, eqmod cf0809 f0809 2048, eqmod cf0810 f0810 2048,
eqmod cf0811 f0811 2048, eqmod cf0812 f0812 2048, eqmod cf0813 f0813 2048,
eqmod cf0814 f0814 2048, eqmod cf0815 f0815 2048, eqmod cf0816 f0816 2048,
eqmod cf0817 f0817 2048, eqmod cf0818 f0818 2048, eqmod cf0819 f0819 2048,
eqmod cf0820 f0820 2048, eqmod cf0821 f0821 2048, eqmod cf0822 f0822 2048,
eqmod cf0823 f0823 2048, eqmod cf0824 f0824 2048, eqmod cf0825 f0825 2048,
eqmod cf0826 f0826 2048, eqmod cf0827 f0827 2048, eqmod cf0828 f0828 2048,
eqmod cf0829 f0829 2048, eqmod cf0830 f0830 2048, eqmod cf0831 f0831 2048,
eqmod cf1000 f1000 2048, eqmod cf1001 f1001 2048, eqmod cf1002 f1002 2048,
eqmod cf1003 f1003 2048, eqmod cf1004 f1004 2048, eqmod cf1005 f1005 2048,
eqmod cf1006 f1006 2048, eqmod cf1007 f1007 2048, eqmod cf1008 f1008 2048,
eqmod cf1009 f1009 2048, eqmod cf1010 f1010 2048, eqmod cf1011 f1011 2048,
eqmod cf1012 f1012 2048, eqmod cf1013 f1013 2048, eqmod cf1014 f1014 2048,
eqmod cf1015 f1015 2048, eqmod cf1016 f1016 2048, eqmod cf1017 f1017 2048,
eqmod cf1018 f1018 2048, eqmod cf1019 f1019 2048, eqmod cf1020 f1020 2048,
eqmod cf1021 f1021 2048, eqmod cf1022 f1022 2048, eqmod cf1023 f1023 2048,
eqmod cf1024 f1024 2048, eqmod cf1025 f1025 2048, eqmod cf1026 f1026 2048,
eqmod cf1027 f1027 2048, eqmod cf1028 f1028 2048, eqmod cf1029 f1029 2048,
eqmod cf1030 f1030 2048, eqmod cf1031 f1031 2048, eqmod cf1100 f1100 2048,
eqmod cf1101 f1101 2048, eqmod cf1102 f1102 2048, eqmod cf1103 f1103 2048,
eqmod cf1104 f1104 2048, eqmod cf1105 f1105 2048, eqmod cf1106 f1106 2048,
eqmod cf1107 f1107 2048, eqmod cf1108 f1108 2048, eqmod cf1109 f1109 2048,
eqmod cf1110 f1110 2048, eqmod cf1111 f1111 2048, eqmod cf1112 f1112 2048,
eqmod cf1113 f1113 2048, eqmod cf1114 f1114 2048, eqmod cf1115 f1115 2048,
eqmod cf1116 f1116 2048, eqmod cf1117 f1117 2048, eqmod cf1118 f1118 2048,
eqmod cf1119 f1119 2048, eqmod cf1120 f1120 2048, eqmod cf1121 f1121 2048,
eqmod cf1122 f1122 2048, eqmod cf1123 f1123 2048, eqmod cf1124 f1124 2048,
eqmod cf1125 f1125 2048, eqmod cf1126 f1126 2048, eqmod cf1127 f1127 2048,
eqmod cf1128 f1128 2048, eqmod cf1129 f1129 2048, eqmod cf1130 f1130 2048,
eqmod cf1131 f1131 2048, eqmod cf1200 f1200 2048, eqmod cf1201 f1201 2048,
eqmod cf1202 f1202 2048, eqmod cf1203 f1203 2048, eqmod cf1204 f1204 2048,
eqmod cf1205 f1205 2048, eqmod cf1206 f1206 2048, eqmod cf1207 f1207 2048,
eqmod cf1208 f1208 2048, eqmod cf1209 f1209 2048, eqmod cf1210 f1210 2048,
eqmod cf1211 f1211 2048, eqmod cf1212 f1212 2048, eqmod cf1213 f1213 2048,
eqmod cf1214 f1214 2048, eqmod cf1215 f1215 2048, eqmod cf1216 f1216 2048,
eqmod cf1217 f1217 2048, eqmod cf1218 f1218 2048, eqmod cf1219 f1219 2048,
eqmod cf1220 f1220 2048, eqmod cf1221 f1221 2048, eqmod cf1222 f1222 2048,
eqmod cf1223 f1223 2048, eqmod cf1224 f1224 2048, eqmod cf1225 f1225 2048,
eqmod cf1226 f1226 2048, eqmod cf1227 f1227 2048, eqmod cf1228 f1228 2048,
eqmod cf1229 f1229 2048, eqmod cf1230 f1230 2048, eqmod cf1231 f1231 2048,
eqmod cf1300 f1300 2048, eqmod cf1301 f1301 2048, eqmod cf1302 f1302 2048,
eqmod cf1303 f1303 2048, eqmod cf1304 f1304 2048, eqmod cf1305 f1305 2048,
eqmod cf1306 f1306 2048, eqmod cf1307 f1307 2048, eqmod cf1308 f1308 2048,
eqmod cf1309 f1309 2048, eqmod cf1310 f1310 2048, eqmod cf1311 f1311 2048,
eqmod cf1312 f1312 2048, eqmod cf1313 f1313 2048, eqmod cf1314 f1314 2048,
eqmod cf1315 f1315 2048, eqmod cf1316 f1316 2048, eqmod cf1317 f1317 2048,
eqmod cf1318 f1318 2048, eqmod cf1319 f1319 2048, eqmod cf1320 f1320 2048,
eqmod cf1321 f1321 2048, eqmod cf1322 f1322 2048, eqmod cf1323 f1323 2048,
eqmod cf1324 f1324 2048, eqmod cf1325 f1325 2048, eqmod cf1326 f1326 2048,
eqmod cf1327 f1327 2048, eqmod cf1328 f1328 2048, eqmod cf1329 f1329 2048,
eqmod cf1330 f1330 2048, eqmod cf1331 f1331 2048, eqmod cf1400 f1400 2048,
eqmod cf1401 f1401 2048, eqmod cf1402 f1402 2048, eqmod cf1403 f1403 2048,
eqmod cf1404 f1404 2048, eqmod cf1405 f1405 2048, eqmod cf1406 f1406 2048,
eqmod cf1407 f1407 2048, eqmod cf1408 f1408 2048, eqmod cf1409 f1409 2048,
eqmod cf1410 f1410 2048, eqmod cf1411 f1411 2048, eqmod cf1412 f1412 2048,
eqmod cf1413 f1413 2048, eqmod cf1414 f1414 2048, eqmod cf1415 f1415 2048,
eqmod cf1416 f1416 2048, eqmod cf1417 f1417 2048, eqmod cf1418 f1418 2048,
eqmod cf1419 f1419 2048, eqmod cf1420 f1420 2048, eqmod cf1421 f1421 2048,
eqmod cf1422 f1422 2048, eqmod cf1423 f1423 2048, eqmod cf1424 f1424 2048,
eqmod cf1425 f1425 2048, eqmod cf1426 f1426 2048, eqmod cf1427 f1427 2048,
eqmod cf1428 f1428 2048, eqmod cf1429 f1429 2048, eqmod cf1430 f1430 2048,
eqmod cf1431 f1431 2048, eqmod cf1500 f1500 2048, eqmod cf1501 f1501 2048,
eqmod cf1502 f1502 2048, eqmod cf1503 f1503 2048, eqmod cf1504 f1504 2048,
eqmod cf1505 f1505 2048, eqmod cf1506 f1506 2048, eqmod cf1507 f1507 2048,
eqmod cf1508 f1508 2048, eqmod cf1509 f1509 2048, eqmod cf1510 f1510 2048,
eqmod cf1511 f1511 2048, eqmod cf1512 f1512 2048, eqmod cf1513 f1513 2048,
eqmod cf1514 f1514 2048, eqmod cf1515 f1515 2048, eqmod cf1516 f1516 2048,
eqmod cf1517 f1517 2048, eqmod cf1518 f1518 2048, eqmod cf1519 f1519 2048,
eqmod cf1520 f1520 2048, eqmod cf1521 f1521 2048, eqmod cf1522 f1522 2048,
eqmod cf1523 f1523 2048, eqmod cf1524 f1524 2048, eqmod cf1525 f1525 2048,
eqmod cf1526 f1526 2048, eqmod cf1527 f1527 2048, eqmod cf1528 f1528 2048,
eqmod cf1529 f1529 2048, eqmod cf1530 f1530 2048, eqmod cf1531 f1531 2048,
eqmod cf1600 f1600 2048, eqmod cf1601 f1601 2048, eqmod cf1602 f1602 2048,
eqmod cf1603 f1603 2048, eqmod cf1604 f1604 2048, eqmod cf1605 f1605 2048,
eqmod cf1606 f1606 2048, eqmod cf1607 f1607 2048, eqmod cf1608 f1608 2048,
eqmod cf1609 f1609 2048, eqmod cf1610 f1610 2048, eqmod cf1611 f1611 2048,
eqmod cf1612 f1612 2048, eqmod cf1613 f1613 2048, eqmod cf1614 f1614 2048,
eqmod cf1615 f1615 2048, eqmod cf1616 f1616 2048, eqmod cf1617 f1617 2048,
eqmod cf1618 f1618 2048, eqmod cf1619 f1619 2048, eqmod cf1620 f1620 2048,
eqmod cf1621 f1621 2048, eqmod cf1622 f1622 2048, eqmod cf1623 f1623 2048,
eqmod cf1624 f1624 2048, eqmod cf1625 f1625 2048, eqmod cf1626 f1626 2048,
eqmod cf1627 f1627 2048, eqmod cf1628 f1628 2048, eqmod cf1629 f1629 2048,
eqmod cf1630 f1630 2048, eqmod cf1631 f1631 2048, eqmod cf1700 f1700 2048,
eqmod cf1701 f1701 2048, eqmod cf1702 f1702 2048, eqmod cf1703 f1703 2048,
eqmod cf1704 f1704 2048, eqmod cf1705 f1705 2048, eqmod cf1706 f1706 2048,
eqmod cf1707 f1707 2048, eqmod cf1708 f1708 2048, eqmod cf1709 f1709 2048,
eqmod cf1710 f1710 2048, eqmod cf1711 f1711 2048, eqmod cf1712 f1712 2048,
eqmod cf1713 f1713 2048, eqmod cf1714 f1714 2048, eqmod cf1715 f1715 2048,
eqmod cf1716 f1716 2048, eqmod cf1717 f1717 2048, eqmod cf1718 f1718 2048,
eqmod cf1719 f1719 2048, eqmod cf1720 f1720 2048, eqmod cf1721 f1721 2048,
eqmod cf1722 f1722 2048, eqmod cf1723 f1723 2048, eqmod cf1724 f1724 2048,
eqmod cf1725 f1725 2048, eqmod cf1726 f1726 2048, eqmod cf1727 f1727 2048,
eqmod cf1728 f1728 2048, eqmod cf1729 f1729 2048, eqmod cf1730 f1730 2048,
eqmod cf1731 f1731 2048, eqmod cf1800 f1800 2048, eqmod cf1801 f1801 2048,
eqmod cf1802 f1802 2048, eqmod cf1803 f1803 2048, eqmod cf1804 f1804 2048,
eqmod cf1805 f1805 2048, eqmod cf1806 f1806 2048, eqmod cf1807 f1807 2048,
eqmod cf1808 f1808 2048, eqmod cf1809 f1809 2048, eqmod cf1810 f1810 2048,
eqmod cf1811 f1811 2048, eqmod cf1812 f1812 2048, eqmod cf1813 f1813 2048,
eqmod cf1814 f1814 2048, eqmod cf1815 f1815 2048, eqmod cf1816 f1816 2048,
eqmod cf1817 f1817 2048, eqmod cf1818 f1818 2048, eqmod cf1819 f1819 2048,
eqmod cf1820 f1820 2048, eqmod cf1821 f1821 2048, eqmod cf1822 f1822 2048,
eqmod cf1823 f1823 2048, eqmod cf1824 f1824 2048, eqmod cf1825 f1825 2048,
eqmod cf1826 f1826 2048, eqmod cf1827 f1827 2048, eqmod cf1828 f1828 2048,
eqmod cf1829 f1829 2048, eqmod cf1830 f1830 2048, eqmod cf1831 f1831 2048,
eqmod cf2000 f2000 2048, eqmod cf2001 f2001 2048, eqmod cf2002 f2002 2048,
eqmod cf2003 f2003 2048, eqmod cf2004 f2004 2048, eqmod cf2005 f2005 2048,
eqmod cf2006 f2006 2048, eqmod cf2007 f2007 2048, eqmod cf2008 f2008 2048,
eqmod cf2009 f2009 2048, eqmod cf2010 f2010 2048, eqmod cf2011 f2011 2048,
eqmod cf2012 f2012 2048, eqmod cf2013 f2013 2048, eqmod cf2014 f2014 2048,
eqmod cf2015 f2015 2048, eqmod cf2016 f2016 2048, eqmod cf2017 f2017 2048,
eqmod cf2018 f2018 2048, eqmod cf2019 f2019 2048, eqmod cf2020 f2020 2048,
eqmod cf2021 f2021 2048, eqmod cf2022 f2022 2048, eqmod cf2023 f2023 2048,
eqmod cf2024 f2024 2048, eqmod cf2025 f2025 2048, eqmod cf2026 f2026 2048,
eqmod cf2027 f2027 2048, eqmod cf2028 f2028 2048, eqmod cf2029 f2029 2048,
eqmod cf2030 f2030 2048, eqmod cf2031 f2031 2048, eqmod cf2100 f2100 2048,
eqmod cf2101 f2101 2048, eqmod cf2102 f2102 2048, eqmod cf2103 f2103 2048,
eqmod cf2104 f2104 2048, eqmod cf2105 f2105 2048, eqmod cf2106 f2106 2048,
eqmod cf2107 f2107 2048, eqmod cf2108 f2108 2048, eqmod cf2109 f2109 2048,
eqmod cf2110 f2110 2048, eqmod cf2111 f2111 2048, eqmod cf2112 f2112 2048,
eqmod cf2113 f2113 2048, eqmod cf2114 f2114 2048, eqmod cf2115 f2115 2048,
eqmod cf2116 f2116 2048, eqmod cf2117 f2117 2048, eqmod cf2118 f2118 2048,
eqmod cf2119 f2119 2048, eqmod cf2120 f2120 2048, eqmod cf2121 f2121 2048,
eqmod cf2122 f2122 2048, eqmod cf2123 f2123 2048, eqmod cf2124 f2124 2048,
eqmod cf2125 f2125 2048, eqmod cf2126 f2126 2048, eqmod cf2127 f2127 2048,
eqmod cf2128 f2128 2048, eqmod cf2129 f2129 2048, eqmod cf2130 f2130 2048,
eqmod cf2131 f2131 2048, eqmod cf2200 f2200 2048, eqmod cf2201 f2201 2048,
eqmod cf2202 f2202 2048, eqmod cf2203 f2203 2048, eqmod cf2204 f2204 2048,
eqmod cf2205 f2205 2048, eqmod cf2206 f2206 2048, eqmod cf2207 f2207 2048,
eqmod cf2208 f2208 2048, eqmod cf2209 f2209 2048, eqmod cf2210 f2210 2048,
eqmod cf2211 f2211 2048, eqmod cf2212 f2212 2048, eqmod cf2213 f2213 2048,
eqmod cf2214 f2214 2048, eqmod cf2215 f2215 2048, eqmod cf2216 f2216 2048,
eqmod cf2217 f2217 2048, eqmod cf2218 f2218 2048, eqmod cf2219 f2219 2048,
eqmod cf2220 f2220 2048, eqmod cf2221 f2221 2048, eqmod cf2222 f2222 2048,
eqmod cf2223 f2223 2048, eqmod cf2224 f2224 2048, eqmod cf2225 f2225 2048,
eqmod cf2226 f2226 2048, eqmod cf2227 f2227 2048, eqmod cf2228 f2228 2048,
eqmod cf2229 f2229 2048, eqmod cf2230 f2230 2048, eqmod cf2231 f2231 2048,
eqmod cf2300 f2300 2048, eqmod cf2301 f2301 2048, eqmod cf2302 f2302 2048,
eqmod cf2303 f2303 2048, eqmod cf2304 f2304 2048, eqmod cf2305 f2305 2048,
eqmod cf2306 f2306 2048, eqmod cf2307 f2307 2048, eqmod cf2308 f2308 2048,
eqmod cf2309 f2309 2048, eqmod cf2310 f2310 2048, eqmod cf2311 f2311 2048,
eqmod cf2312 f2312 2048, eqmod cf2313 f2313 2048, eqmod cf2314 f2314 2048,
eqmod cf2315 f2315 2048, eqmod cf2316 f2316 2048, eqmod cf2317 f2317 2048,
eqmod cf2318 f2318 2048, eqmod cf2319 f2319 2048, eqmod cf2320 f2320 2048,
eqmod cf2321 f2321 2048, eqmod cf2322 f2322 2048, eqmod cf2323 f2323 2048,
eqmod cf2324 f2324 2048, eqmod cf2325 f2325 2048, eqmod cf2326 f2326 2048,
eqmod cf2327 f2327 2048, eqmod cf2328 f2328 2048, eqmod cf2329 f2329 2048,
eqmod cf2330 f2330 2048, eqmod cf2331 f2331 2048, eqmod cf2400 f2400 2048,
eqmod cf2401 f2401 2048, eqmod cf2402 f2402 2048, eqmod cf2403 f2403 2048,
eqmod cf2404 f2404 2048, eqmod cf2405 f2405 2048, eqmod cf2406 f2406 2048,
eqmod cf2407 f2407 2048, eqmod cf2408 f2408 2048, eqmod cf2409 f2409 2048,
eqmod cf2410 f2410 2048, eqmod cf2411 f2411 2048, eqmod cf2412 f2412 2048,
eqmod cf2413 f2413 2048, eqmod cf2414 f2414 2048, eqmod cf2415 f2415 2048,
eqmod cf2416 f2416 2048, eqmod cf2417 f2417 2048, eqmod cf2418 f2418 2048,
eqmod cf2419 f2419 2048, eqmod cf2420 f2420 2048, eqmod cf2421 f2421 2048,
eqmod cf2422 f2422 2048, eqmod cf2423 f2423 2048, eqmod cf2424 f2424 2048,
eqmod cf2425 f2425 2048, eqmod cf2426 f2426 2048, eqmod cf2427 f2427 2048,
eqmod cf2428 f2428 2048, eqmod cf2429 f2429 2048, eqmod cf2430 f2430 2048,
eqmod cf2431 f2431 2048, eqmod cf2500 f2500 2048, eqmod cf2501 f2501 2048,
eqmod cf2502 f2502 2048, eqmod cf2503 f2503 2048, eqmod cf2504 f2504 2048,
eqmod cf2505 f2505 2048, eqmod cf2506 f2506 2048, eqmod cf2507 f2507 2048,
eqmod cf2508 f2508 2048, eqmod cf2509 f2509 2048, eqmod cf2510 f2510 2048,
eqmod cf2511 f2511 2048, eqmod cf2512 f2512 2048, eqmod cf2513 f2513 2048,
eqmod cf2514 f2514 2048, eqmod cf2515 f2515 2048, eqmod cf2516 f2516 2048,
eqmod cf2517 f2517 2048, eqmod cf2518 f2518 2048, eqmod cf2519 f2519 2048,
eqmod cf2520 f2520 2048, eqmod cf2521 f2521 2048, eqmod cf2522 f2522 2048,
eqmod cf2523 f2523 2048, eqmod cf2524 f2524 2048, eqmod cf2525 f2525 2048,
eqmod cf2526 f2526 2048, eqmod cf2527 f2527 2048, eqmod cf2528 f2528 2048,
eqmod cf2529 f2529 2048, eqmod cf2530 f2530 2048, eqmod cf2531 f2531 2048,
eqmod cf2600 f2600 2048, eqmod cf2601 f2601 2048, eqmod cf2602 f2602 2048,
eqmod cf2603 f2603 2048, eqmod cf2604 f2604 2048, eqmod cf2605 f2605 2048,
eqmod cf2606 f2606 2048, eqmod cf2607 f2607 2048, eqmod cf2608 f2608 2048,
eqmod cf2609 f2609 2048, eqmod cf2610 f2610 2048, eqmod cf2611 f2611 2048,
eqmod cf2612 f2612 2048, eqmod cf2613 f2613 2048, eqmod cf2614 f2614 2048,
eqmod cf2615 f2615 2048, eqmod cf2616 f2616 2048, eqmod cf2617 f2617 2048,
eqmod cf2618 f2618 2048, eqmod cf2619 f2619 2048, eqmod cf2620 f2620 2048,
eqmod cf2621 f2621 2048, eqmod cf2622 f2622 2048, eqmod cf2623 f2623 2048,
eqmod cf2624 f2624 2048, eqmod cf2625 f2625 2048, eqmod cf2626 f2626 2048,
eqmod cf2627 f2627 2048, eqmod cf2628 f2628 2048, eqmod cf2629 f2629 2048,
eqmod cf2630 f2630 2048, eqmod cf2631 f2631 2048, eqmod cf2700 f2700 2048,
eqmod cf2701 f2701 2048, eqmod cf2702 f2702 2048, eqmod cf2703 f2703 2048,
eqmod cf2704 f2704 2048, eqmod cf2705 f2705 2048, eqmod cf2706 f2706 2048,
eqmod cf2707 f2707 2048, eqmod cf2708 f2708 2048, eqmod cf2709 f2709 2048,
eqmod cf2710 f2710 2048, eqmod cf2711 f2711 2048, eqmod cf2712 f2712 2048,
eqmod cf2713 f2713 2048, eqmod cf2714 f2714 2048, eqmod cf2715 f2715 2048,
eqmod cf2716 f2716 2048, eqmod cf2717 f2717 2048, eqmod cf2718 f2718 2048,
eqmod cf2719 f2719 2048, eqmod cf2720 f2720 2048, eqmod cf2721 f2721 2048,
eqmod cf2722 f2722 2048, eqmod cf2723 f2723 2048, eqmod cf2724 f2724 2048,
eqmod cf2725 f2725 2048, eqmod cf2726 f2726 2048, eqmod cf2727 f2727 2048,
eqmod cf2728 f2728 2048, eqmod cf2729 f2729 2048, eqmod cf2730 f2730 2048,
eqmod cf2731 f2731 2048, eqmod cf2800 f2800 2048, eqmod cf2801 f2801 2048,
eqmod cf2802 f2802 2048, eqmod cf2803 f2803 2048, eqmod cf2804 f2804 2048,
eqmod cf2805 f2805 2048, eqmod cf2806 f2806 2048, eqmod cf2807 f2807 2048,
eqmod cf2808 f2808 2048, eqmod cf2809 f2809 2048, eqmod cf2810 f2810 2048,
eqmod cf2811 f2811 2048, eqmod cf2812 f2812 2048, eqmod cf2813 f2813 2048,
eqmod cf2814 f2814 2048, eqmod cf2815 f2815 2048, eqmod cf2816 f2816 2048,
eqmod cf2817 f2817 2048, eqmod cf2818 f2818 2048, eqmod cf2819 f2819 2048,
eqmod cf2820 f2820 2048, eqmod cf2821 f2821 2048, eqmod cf2822 f2822 2048,
eqmod cf2823 f2823 2048, eqmod cf2824 f2824 2048, eqmod cf2825 f2825 2048,
eqmod cf2826 f2826 2048, eqmod cf2827 f2827 2048, eqmod cf2828 f2828 2048,
eqmod cf2829 f2829 2048, eqmod cf2830 f2830 2048, eqmod cf2831 f2831 2048
] prove with [ all cuts ];



(**************** CUT 289, - *****************)

ecut and [
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x20014898*x**0*y**0*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x200148a4*x**0*y**1*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x200148b0*x**0*y**2*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x20014904*x**0*y**0*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x20014910*x**0*y**1*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x2001491c*x**0*y**2*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x20014970*x**0*y**0*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x2001497c*x**0*y**1*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x20014988*x**0*y**2*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x200149dc*x**0*y**0*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x200149e8*x**0*y**1*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x200149f4*x**0*y**2*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x20014a48*x**0*y**0*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x20014a54*x**0*y**1*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x20014a60*x**0*y**2*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x20014ab4*x**0*y**0*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20014ac0*x**0*y**1*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x20014acc*x**0*y**2*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x20014b20*x**0*y**0*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x20014b2c*x**0*y**1*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x20014b38*x**0*y**2*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x20014b8c*x**0*y**0*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x20014b98*x**0*y**1*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x20014ba4*x**0*y**2*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x20014bf8*x**0*y**0*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x20014c04*x**0*y**1*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x20014c10*x**0*y**2*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x20014c64*x**0*y**0*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x20014c70*x**0*y**1*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x20014c7c*x**0*y**2*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20014cd0*x**0*y**0*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20014cdc*x**0*y**1*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20014ce8*x**0*y**2*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20014d3c*x**0*y**0*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20014d48*x**0*y**1*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20014d54*x**0*y**2*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20014da8*x**0*y**0*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20014db4*x**0*y**1*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20014dc0*x**0*y**2*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20014e14*x**0*y**0*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20014e20*x**0*y**1*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20014e2c*x**0*y**2*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20014e80*x**0*y**0*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20014e8c*x**0*y**1*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20014e98*x**0*y**2*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20014eec*x**0*y**0*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20014ef8*x**0*y**1*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20014f04*x**0*y**2*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20014f58*x**0*y**0*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20014f64*x**0*y**1*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20014f70*x**0*y**2*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x20014fc4*x**0*y**0*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20014fd0*x**0*y**1*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20014fdc*x**0*y**2*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015030*x**0*y**0*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x2001503c*x**0*y**1*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x20015048*x**0*y**2*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x2001509c*x**0*y**0*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x200150a8*x**0*y**1*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x200150b4*x**0*y**2*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x20015108*x**0*y**0*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x20015114*x**0*y**1*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015120*x**0*y**2*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x20015174*x**0*y**0*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x20015180*x**0*y**1*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x2001518c*x**0*y**2*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x200151e0*x**0*y**0*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x200151ec*x**0*y**1*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x200151f8*x**0*y**2*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x2001524c*x**0*y**0*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x20015258*x**0*y**1*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x20015264*x**0*y**2*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x200152b8*x**0*y**0*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x200152c4*x**0*y**1*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x200152d0*x**0*y**2*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x20015324*x**0*y**0*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x20015330*x**0*y**1*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x2001533c*x**0*y**2*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x20015390*x**0*y**0*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x2001539c*x**0*y**1*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x200153a8*x**0*y**2*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x200153fc*x**0*y**0*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x20015408*x**0*y**1*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x20015414*x**0*y**2*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x20015468*x**0*y**0*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x20015474*x**0*y**1*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x20015480*x**0*y**2*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x200154d4*x**0*y**0*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x200154e0*x**0*y**1*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x200154ec*x**0*y**2*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x20015540*x**0*y**0*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x2001554c*x**0*y**1*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x20015558*x**0*y**2*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x200155ac*x**0*y**0*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x200155b8*x**0*y**1*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x200155c4*x**0*y**2*z**31 [ 3365569, y**3 -       1, z**32 -       1 ]
] prove with [ all cuts ];



(**************** CUT 290, - *****************)

ecut and [
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x2001489c*x**1*y**0*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x200148a8*x**1*y**1*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x200148b4*x**1*y**2*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x20014908*x**1*y**0*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x20014914*x**1*y**1*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x20014920*x**1*y**2*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x20014974*x**1*y**0*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x20014980*x**1*y**1*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x2001498c*x**1*y**2*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x200149e0*x**1*y**0*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x200149ec*x**1*y**1*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x200149f8*x**1*y**2*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x20014a4c*x**1*y**0*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x20014a58*x**1*y**1*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x20014a64*x**1*y**2*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x20014ab8*x**1*y**0*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x20014ac4*x**1*y**1*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20014ad0*x**1*y**2*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x20014b24*x**1*y**0*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x20014b30*x**1*y**1*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x20014b3c*x**1*y**2*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20014b90*x**1*y**0*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x20014b9c*x**1*y**1*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x20014ba8*x**1*y**2*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x20014bfc*x**1*y**0*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x20014c08*x**1*y**1*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x20014c14*x**1*y**2*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x20014c68*x**1*y**0*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x20014c74*x**1*y**1*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20014c80*x**1*y**2*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20014cd4*x**1*y**0*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20014ce0*x**1*y**1*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20014cec*x**1*y**2*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20014d40*x**1*y**0*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20014d4c*x**1*y**1*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20014d58*x**1*y**2*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20014dac*x**1*y**0*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20014db8*x**1*y**1*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20014dc4*x**1*y**2*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20014e18*x**1*y**0*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20014e24*x**1*y**1*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20014e30*x**1*y**2*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20014e84*x**1*y**0*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20014e90*x**1*y**1*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20014e9c*x**1*y**2*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20014ef0*x**1*y**0*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20014efc*x**1*y**1*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20014f08*x**1*y**2*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20014f5c*x**1*y**0*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20014f68*x**1*y**1*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20014f74*x**1*y**2*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20014fc8*x**1*y**0*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x20014fd4*x**1*y**1*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20014fe0*x**1*y**2*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x20015034*x**1*y**0*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015040*x**1*y**1*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x2001504c*x**1*y**2*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x200150a0*x**1*y**0*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x200150ac*x**1*y**1*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x200150b8*x**1*y**2*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x2001510c*x**1*y**0*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x20015118*x**1*y**1*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x20015124*x**1*y**2*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x20015178*x**1*y**0*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x20015184*x**1*y**1*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x20015190*x**1*y**2*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x200151e4*x**1*y**0*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x200151f0*x**1*y**1*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x200151fc*x**1*y**2*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20015250*x**1*y**0*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x2001525c*x**1*y**1*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x20015268*x**1*y**2*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x200152bc*x**1*y**0*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x200152c8*x**1*y**1*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x200152d4*x**1*y**2*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x20015328*x**1*y**0*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x20015334*x**1*y**1*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x20015340*x**1*y**2*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x20015394*x**1*y**0*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x200153a0*x**1*y**1*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x200153ac*x**1*y**2*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x20015400*x**1*y**0*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x2001540c*x**1*y**1*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x20015418*x**1*y**2*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x2001546c*x**1*y**0*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x20015478*x**1*y**1*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x20015484*x**1*y**2*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x200154d8*x**1*y**0*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x200154e4*x**1*y**1*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x200154f0*x**1*y**2*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x20015544*x**1*y**0*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x20015550*x**1*y**1*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x2001555c*x**1*y**2*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x200155b0*x**1*y**0*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x200155bc*x**1*y**1*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x200155c8*x**1*y**2*z**31 [ 3365569, y**3 -       1, z**32 -       1 ]
] prove with [ all cuts ];



(**************** CUT 291, - *****************)

ecut and [
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x200148a0*x**2*y**0*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x200148ac*x**2*y**1*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x200148b8*x**2*y**2*z**0 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x2001490c*x**2*y**0*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x20014918*x**2*y**1*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x20014924*x**2*y**2*z**1 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x20014978*x**2*y**0*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x20014984*x**2*y**1*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x20014990*x**2*y**2*z**2 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x200149e4*x**2*y**0*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x200149f0*x**2*y**1*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x200149fc*x**2*y**2*z**3 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x20014a50*x**2*y**0*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x20014a5c*x**2*y**1*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x20014a68*x**2*y**2*z**4 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x20014abc*x**2*y**0*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x20014ac8*x**2*y**1*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x20014ad4*x**2*y**2*z**5 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x20014b28*x**2*y**0*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x20014b34*x**2*y**1*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x20014b40*x**2*y**2*z**6 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x20014b94*x**2*y**0*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20014ba0*x**2*y**1*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x20014bac*x**2*y**2*z**7 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x20014c00*x**2*y**0*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x20014c0c*x**2*y**1*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x20014c18*x**2*y**2*z**8 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x20014c6c*x**2*y**0*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x20014c78*x**2*y**1*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20014c84*x**2*y**2*z**9 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20014cd8*x**2*y**0*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20014ce4*x**2*y**1*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20014cf0*x**2*y**2*z**10 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20014d44*x**2*y**0*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20014d50*x**2*y**1*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20014d5c*x**2*y**2*z**11 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20014db0*x**2*y**0*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20014dbc*x**2*y**1*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20014dc8*x**2*y**2*z**12 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20014e1c*x**2*y**0*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20014e28*x**2*y**1*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20014e34*x**2*y**2*z**13 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20014e88*x**2*y**0*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20014e94*x**2*y**1*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20014ea0*x**2*y**2*z**14 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20014ef4*x**2*y**0*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20014f00*x**2*y**1*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20014f0c*x**2*y**2*z**15 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20014f60*x**2*y**0*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20014f6c*x**2*y**1*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20014f78*x**2*y**2*z**16 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20014fcc*x**2*y**0*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20014fd8*x**2*y**1*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x20014fe4*x**2*y**2*z**17 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x20015038*x**2*y**0*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x20015044*x**2*y**1*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015050*x**2*y**2*z**18 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x200150a4*x**2*y**0*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x200150b0*x**2*y**1*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x200150bc*x**2*y**2*z**19 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015110*x**2*y**0*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x2001511c*x**2*y**1*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x20015128*x**2*y**2*z**20 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x2001517c*x**2*y**0*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x20015188*x**2*y**1*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x20015194*x**2*y**2*z**21 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x200151e8*x**2*y**0*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x200151f4*x**2*y**1*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015200*x**2*y**2*z**22 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x20015254*x**2*y**0*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x20015260*x**2*y**1*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x2001526c*x**2*y**2*z**23 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x200152c0*x**2*y**0*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x200152cc*x**2*y**1*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x200152d8*x**2*y**2*z**24 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x2001532c*x**2*y**0*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x20015338*x**2*y**1*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x20015344*x**2*y**2*z**25 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x20015398*x**2*y**0*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x200153a4*x**2*y**1*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x200153b0*x**2*y**2*z**26 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x20015404*x**2*y**0*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x20015410*x**2*y**1*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x2001541c*x**2*y**2*z**27 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x20015470*x**2*y**0*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x2001547c*x**2*y**1*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x20015488*x**2*y**2*z**28 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x200154dc*x**2*y**0*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x200154e8*x**2*y**1*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x200154f4*x**2*y**2*z**29 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x20015548*x**2*y**0*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x20015554*x**2*y**1*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x20015560*x**2*y**2*z**30 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x200155b4*x**2*y**0*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x200155c0*x**2*y**1*z**31 [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x200155cc*x**2*y**2*z**31 [ 3365569, y**3 -       1, z**32 -       1 ]
] prove with [ all cuts ];



(**************** CUT 292, - *****************)

ecut and [
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x200148bc*x**0*y**0*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x200148c8*x**0*y**1*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x200148d4*x**0*y**2*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x20014928*x**0*y**0*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x20014934*x**0*y**1*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x20014940*x**0*y**2*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x20014994*x**0*y**0*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x200149a0*x**0*y**1*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x200149ac*x**0*y**2*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x20014a00*x**0*y**0*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x20014a0c*x**0*y**1*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x20014a18*x**0*y**2*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x20014a6c*x**0*y**0*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x20014a78*x**0*y**1*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x20014a84*x**0*y**2*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x20014ad8*x**0*y**0*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20014ae4*x**0*y**1*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x20014af0*x**0*y**2*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x20014b44*x**0*y**0*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x20014b50*x**0*y**1*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x20014b5c*x**0*y**2*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x20014bb0*x**0*y**0*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x20014bbc*x**0*y**1*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x20014bc8*x**0*y**2*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x20014c1c*x**0*y**0*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x20014c28*x**0*y**1*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x20014c34*x**0*y**2*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x20014c88*x**0*y**0*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x20014c94*x**0*y**1*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x20014ca0*x**0*y**2*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20014cf4*x**0*y**0*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20014d00*x**0*y**1*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20014d0c*x**0*y**2*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20014d60*x**0*y**0*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20014d6c*x**0*y**1*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20014d78*x**0*y**2*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20014dcc*x**0*y**0*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20014dd8*x**0*y**1*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20014de4*x**0*y**2*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20014e38*x**0*y**0*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20014e44*x**0*y**1*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20014e50*x**0*y**2*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20014ea4*x**0*y**0*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20014eb0*x**0*y**1*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20014ebc*x**0*y**2*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20014f10*x**0*y**0*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20014f1c*x**0*y**1*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20014f28*x**0*y**2*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20014f7c*x**0*y**0*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20014f88*x**0*y**1*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20014f94*x**0*y**2*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x20014fe8*x**0*y**0*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20014ff4*x**0*y**1*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20015000*x**0*y**2*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015054*x**0*y**0*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x20015060*x**0*y**1*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x2001506c*x**0*y**2*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x200150c0*x**0*y**0*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x200150cc*x**0*y**1*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x200150d8*x**0*y**2*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x2001512c*x**0*y**0*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x20015138*x**0*y**1*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015144*x**0*y**2*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x20015198*x**0*y**0*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x200151a4*x**0*y**1*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x200151b0*x**0*y**2*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x20015204*x**0*y**0*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x20015210*x**0*y**1*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x2001521c*x**0*y**2*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x20015270*x**0*y**0*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x2001527c*x**0*y**1*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x20015288*x**0*y**2*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x200152dc*x**0*y**0*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x200152e8*x**0*y**1*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x200152f4*x**0*y**2*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x20015348*x**0*y**0*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x20015354*x**0*y**1*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x20015360*x**0*y**2*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x200153b4*x**0*y**0*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x200153c0*x**0*y**1*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x200153cc*x**0*y**2*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x20015420*x**0*y**0*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x2001542c*x**0*y**1*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x20015438*x**0*y**2*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x2001548c*x**0*y**0*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x20015498*x**0*y**1*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x200154a4*x**0*y**2*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x200154f8*x**0*y**0*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x20015504*x**0*y**1*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x20015510*x**0*y**2*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x20015564*x**0*y**0*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x20015570*x**0*y**1*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x2001557c*x**0*y**2*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x200155d0*x**0*y**0*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x200155dc*x**0*y**1*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x200155e8*x**0*y**2*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ]
] prove with [ all cuts ];



(**************** CUT 293, - *****************)

ecut and [
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x200148c0*x**1*y**0*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x200148cc*x**1*y**1*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x200148d8*x**1*y**2*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x2001492c*x**1*y**0*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x20014938*x**1*y**1*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x20014944*x**1*y**2*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x20014998*x**1*y**0*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x200149a4*x**1*y**1*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x200149b0*x**1*y**2*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x20014a04*x**1*y**0*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x20014a10*x**1*y**1*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x20014a1c*x**1*y**2*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x20014a70*x**1*y**0*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x20014a7c*x**1*y**1*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x20014a88*x**1*y**2*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x20014adc*x**1*y**0*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x20014ae8*x**1*y**1*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20014af4*x**1*y**2*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x20014b48*x**1*y**0*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x20014b54*x**1*y**1*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x20014b60*x**1*y**2*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20014bb4*x**1*y**0*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x20014bc0*x**1*y**1*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x20014bcc*x**1*y**2*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x20014c20*x**1*y**0*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x20014c2c*x**1*y**1*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x20014c38*x**1*y**2*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x20014c8c*x**1*y**0*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x20014c98*x**1*y**1*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20014ca4*x**1*y**2*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20014cf8*x**1*y**0*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20014d04*x**1*y**1*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20014d10*x**1*y**2*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20014d64*x**1*y**0*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20014d70*x**1*y**1*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20014d7c*x**1*y**2*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20014dd0*x**1*y**0*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20014ddc*x**1*y**1*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20014de8*x**1*y**2*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20014e3c*x**1*y**0*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20014e48*x**1*y**1*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20014e54*x**1*y**2*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20014ea8*x**1*y**0*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20014eb4*x**1*y**1*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20014ec0*x**1*y**2*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20014f14*x**1*y**0*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20014f20*x**1*y**1*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20014f2c*x**1*y**2*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20014f80*x**1*y**0*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20014f8c*x**1*y**1*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20014f98*x**1*y**2*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20014fec*x**1*y**0*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x20014ff8*x**1*y**1*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20015004*x**1*y**2*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x20015058*x**1*y**0*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015064*x**1*y**1*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x20015070*x**1*y**2*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x200150c4*x**1*y**0*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x200150d0*x**1*y**1*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x200150dc*x**1*y**2*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x20015130*x**1*y**0*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x2001513c*x**1*y**1*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x20015148*x**1*y**2*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x2001519c*x**1*y**0*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x200151a8*x**1*y**1*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x200151b4*x**1*y**2*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x20015208*x**1*y**0*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x20015214*x**1*y**1*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x20015220*x**1*y**2*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20015274*x**1*y**0*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x20015280*x**1*y**1*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x2001528c*x**1*y**2*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x200152e0*x**1*y**0*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x200152ec*x**1*y**1*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x200152f8*x**1*y**2*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x2001534c*x**1*y**0*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x20015358*x**1*y**1*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x20015364*x**1*y**2*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x200153b8*x**1*y**0*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x200153c4*x**1*y**1*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x200153d0*x**1*y**2*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x20015424*x**1*y**0*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x20015430*x**1*y**1*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x2001543c*x**1*y**2*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x20015490*x**1*y**0*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x2001549c*x**1*y**1*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x200154a8*x**1*y**2*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x200154fc*x**1*y**0*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x20015508*x**1*y**1*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x20015514*x**1*y**2*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x20015568*x**1*y**0*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x20015574*x**1*y**1*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x20015580*x**1*y**2*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x200155d4*x**1*y**0*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x200155e0*x**1*y**1*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x200155ec*x**1*y**2*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ]
] prove with [ all cuts ];



(**************** CUT 294, - *****************)

ecut and [
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x200148c4*x**2*y**0*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x200148d0*x**2*y**1*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x200148dc*x**2*y**2*z**0 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x20014930*x**2*y**0*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x2001493c*x**2*y**1*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x20014948*x**2*y**2*z**1 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x2001499c*x**2*y**0*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x200149a8*x**2*y**1*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x200149b4*x**2*y**2*z**2 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x20014a08*x**2*y**0*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x20014a14*x**2*y**1*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x20014a20*x**2*y**2*z**3 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x20014a74*x**2*y**0*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x20014a80*x**2*y**1*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x20014a8c*x**2*y**2*z**4 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x20014ae0*x**2*y**0*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x20014aec*x**2*y**1*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x20014af8*x**2*y**2*z**5 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x20014b4c*x**2*y**0*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x20014b58*x**2*y**1*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x20014b64*x**2*y**2*z**6 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x20014bb8*x**2*y**0*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20014bc4*x**2*y**1*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x20014bd0*x**2*y**2*z**7 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x20014c24*x**2*y**0*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x20014c30*x**2*y**1*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x20014c3c*x**2*y**2*z**8 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x20014c90*x**2*y**0*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x20014c9c*x**2*y**1*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20014ca8*x**2*y**2*z**9 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20014cfc*x**2*y**0*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20014d08*x**2*y**1*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20014d14*x**2*y**2*z**10 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20014d68*x**2*y**0*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20014d74*x**2*y**1*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20014d80*x**2*y**2*z**11 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20014dd4*x**2*y**0*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20014de0*x**2*y**1*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20014dec*x**2*y**2*z**12 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20014e40*x**2*y**0*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20014e4c*x**2*y**1*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20014e58*x**2*y**2*z**13 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20014eac*x**2*y**0*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20014eb8*x**2*y**1*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20014ec4*x**2*y**2*z**14 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20014f18*x**2*y**0*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20014f24*x**2*y**1*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20014f30*x**2*y**2*z**15 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20014f84*x**2*y**0*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20014f90*x**2*y**1*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20014f9c*x**2*y**2*z**16 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20014ff0*x**2*y**0*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20014ffc*x**2*y**1*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x20015008*x**2*y**2*z**17 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x2001505c*x**2*y**0*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x20015068*x**2*y**1*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015074*x**2*y**2*z**18 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x200150c8*x**2*y**0*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x200150d4*x**2*y**1*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x200150e0*x**2*y**2*z**19 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015134*x**2*y**0*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x20015140*x**2*y**1*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x2001514c*x**2*y**2*z**20 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x200151a0*x**2*y**0*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x200151ac*x**2*y**1*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x200151b8*x**2*y**2*z**21 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x2001520c*x**2*y**0*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x20015218*x**2*y**1*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015224*x**2*y**2*z**22 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x20015278*x**2*y**0*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x20015284*x**2*y**1*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x20015290*x**2*y**2*z**23 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x200152e4*x**2*y**0*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x200152f0*x**2*y**1*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x200152fc*x**2*y**2*z**24 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x20015350*x**2*y**0*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x2001535c*x**2*y**1*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x20015368*x**2*y**2*z**25 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x200153bc*x**2*y**0*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x200153c8*x**2*y**1*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x200153d4*x**2*y**2*z**26 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x20015428*x**2*y**0*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x20015434*x**2*y**1*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x20015440*x**2*y**2*z**27 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x20015494*x**2*y**0*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x200154a0*x**2*y**1*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x200154ac*x**2*y**2*z**28 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x20015500*x**2*y**0*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x2001550c*x**2*y**1*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x20015518*x**2*y**2*z**29 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x2001556c*x**2*y**0*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x20015578*x**2*y**1*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x20015584*x**2*y**2*z**30 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x200155d8*x**2*y**0*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x200155e4*x**2*y**1*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x200155f0*x**2*y**2*z**31 [ 3365569, y**3 -  452650, z**32 -       1 ]
] prove with [ all cuts ];



(**************** CUT 295, - *****************)

ecut and [
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x200148e0*x**0*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x200148ec*x**0*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x200148f8*x**0*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x2001494c*x**0*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x20014958*x**0*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x20014964*x**0*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x200149b8*x**0*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x200149c4*x**0*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x200149d0*x**0*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x20014a24*x**0*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x20014a30*x**0*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x20014a3c*x**0*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x20014a90*x**0*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x20014a9c*x**0*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x20014aa8*x**0*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x20014afc*x**0*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20014b08*x**0*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x20014b14*x**0*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x20014b68*x**0*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x20014b74*x**0*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x20014b80*x**0*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x20014bd4*x**0*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x20014be0*x**0*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x20014bec*x**0*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x20014c40*x**0*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x20014c4c*x**0*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x20014c58*x**0*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x20014cac*x**0*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x20014cb8*x**0*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x20014cc4*x**0*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20014d18*x**0*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20014d24*x**0*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20014d30*x**0*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20014d84*x**0*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20014d90*x**0*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20014d9c*x**0*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20014df0*x**0*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20014dfc*x**0*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20014e08*x**0*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20014e5c*x**0*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20014e68*x**0*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20014e74*x**0*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20014ec8*x**0*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20014ed4*x**0*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20014ee0*x**0*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20014f34*x**0*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20014f40*x**0*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20014f4c*x**0*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20014fa0*x**0*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20014fac*x**0*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20014fb8*x**0*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x2001500c*x**0*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20015018*x**0*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20015024*x**0*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015078*x**0*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x20015084*x**0*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x20015090*x**0*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x200150e4*x**0*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x200150f0*x**0*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x200150fc*x**0*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x20015150*x**0*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x2001515c*x**0*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015168*x**0*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x200151bc*x**0*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x200151c8*x**0*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x200151d4*x**0*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x20015228*x**0*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x20015234*x**0*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x20015240*x**0*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x20015294*x**0*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x200152a0*x**0*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x200152ac*x**0*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x20015300*x**0*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x2001530c*x**0*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x20015318*x**0*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x2001536c*x**0*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x20015378*x**0*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x20015384*x**0*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x200153d8*x**0*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x200153e4*x**0*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x200153f0*x**0*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x20015444*x**0*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x20015450*x**0*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x2001545c*x**0*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x200154b0*x**0*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x200154bc*x**0*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x200154c8*x**0*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x2001551c*x**0*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x20015528*x**0*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x20015534*x**0*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x20015588*x**0*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x20015594*x**0*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x200155a0*x**0*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x200155f4*x**0*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x20015600*x**0*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x2001560c*x**0*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ]
] prove with [ all cuts ];



(**************** CUT 296, - *****************)

ecut and [
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x200148e4*x**1*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x200148f0*x**1*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x200148fc*x**1*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x20014950*x**1*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x2001495c*x**1*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x20014968*x**1*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x200149bc*x**1*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x200149c8*x**1*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x200149d4*x**1*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x20014a28*x**1*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x20014a34*x**1*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x20014a40*x**1*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x20014a94*x**1*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x20014aa0*x**1*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x20014aac*x**1*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x20014b00*x**1*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x20014b0c*x**1*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20014b18*x**1*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x20014b6c*x**1*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x20014b78*x**1*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x20014b84*x**1*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20014bd8*x**1*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x20014be4*x**1*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x20014bf0*x**1*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x20014c44*x**1*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x20014c50*x**1*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x20014c5c*x**1*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x20014cb0*x**1*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x20014cbc*x**1*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20014cc8*x**1*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20014d1c*x**1*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20014d28*x**1*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20014d34*x**1*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20014d88*x**1*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20014d94*x**1*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20014da0*x**1*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20014df4*x**1*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20014e00*x**1*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20014e0c*x**1*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20014e60*x**1*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20014e6c*x**1*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20014e78*x**1*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20014ecc*x**1*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20014ed8*x**1*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20014ee4*x**1*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20014f38*x**1*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20014f44*x**1*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20014f50*x**1*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20014fa4*x**1*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20014fb0*x**1*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20014fbc*x**1*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20015010*x**1*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x2001501c*x**1*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20015028*x**1*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x2001507c*x**1*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015088*x**1*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x20015094*x**1*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x200150e8*x**1*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x200150f4*x**1*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x20015100*x**1*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x20015154*x**1*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x20015160*x**1*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x2001516c*x**1*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x200151c0*x**1*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x200151cc*x**1*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x200151d8*x**1*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x2001522c*x**1*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x20015238*x**1*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x20015244*x**1*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20015298*x**1*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x200152a4*x**1*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x200152b0*x**1*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x20015304*x**1*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x20015310*x**1*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x2001531c*x**1*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x20015370*x**1*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x2001537c*x**1*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x20015388*x**1*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x200153dc*x**1*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x200153e8*x**1*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x200153f4*x**1*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x20015448*x**1*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x20015454*x**1*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x20015460*x**1*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x200154b4*x**1*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x200154c0*x**1*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x200154cc*x**1*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x20015520*x**1*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x2001552c*x**1*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x20015538*x**1*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x2001558c*x**1*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x20015598*x**1*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x200155a4*x**1*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x200155f8*x**1*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x20015604*x**1*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x20015610*x**1*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ]
] prove with [ all cuts ];



(**************** CUT 297, - *****************)

ecut and [
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x200148e8*x**2*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x200148f4*x**2*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x20014900*x**2*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x20014954*x**2*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x20014960*x**2*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x2001496c*x**2*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x200149c0*x**2*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x200149cc*x**2*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x200149d8*x**2*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x20014a2c*x**2*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x20014a38*x**2*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x20014a44*x**2*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x20014a98*x**2*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x20014aa4*x**2*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x20014ab0*x**2*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x20014b04*x**2*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x20014b10*x**2*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x20014b1c*x**2*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x20014b70*x**2*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x20014b7c*x**2*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x20014b88*x**2*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x20014bdc*x**2*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20014be8*x**2*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x20014bf4*x**2*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x20014c48*x**2*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x20014c54*x**2*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x20014c60*x**2*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x20014cb4*x**2*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x20014cc0*x**2*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20014ccc*x**2*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20014d20*x**2*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20014d2c*x**2*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20014d38*x**2*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20014d8c*x**2*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20014d98*x**2*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20014da4*x**2*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20014df8*x**2*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20014e04*x**2*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20014e10*x**2*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20014e64*x**2*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20014e70*x**2*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20014e7c*x**2*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20014ed0*x**2*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20014edc*x**2*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20014ee8*x**2*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20014f3c*x**2*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20014f48*x**2*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20014f54*x**2*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20014fa8*x**2*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20014fb4*x**2*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20014fc0*x**2*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20015014*x**2*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20015020*x**2*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x2001502c*x**2*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x20015080*x**2*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x2001508c*x**2*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015098*x**2*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x200150ec*x**2*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x200150f8*x**2*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x20015104*x**2*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015158*x**2*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x20015164*x**2*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x20015170*x**2*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x200151c4*x**2*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x200151d0*x**2*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x200151dc*x**2*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x20015230*x**2*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x2001523c*x**2*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015248*x**2*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x2001529c*x**2*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x200152a8*x**2*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x200152b4*x**2*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x20015308*x**2*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x20015314*x**2*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x20015320*x**2*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x20015374*x**2*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x20015380*x**2*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x2001538c*x**2*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x200153e0*x**2*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x200153ec*x**2*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x200153f8*x**2*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x2001544c*x**2*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x20015458*x**2*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x20015464*x**2*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x200154b8*x**2*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x200154c4*x**2*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x200154d0*x**2*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x20015524*x**2*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x20015530*x**2*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x2001553c*x**2*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x20015590*x**2*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x2001559c*x**2*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x200155a8*x**2*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x200155fc*x**2*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x20015608*x**2*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x20015614*x**2*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 -       1 ]
] prove with [ all cuts ];



(**************** CUT 298, - *****************)

ecut and [
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x20015618*x**0*y**0*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x20015624*x**0*y**1*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x20015630*x**0*y**2*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x20015684*x**0*y**0*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x20015690*x**0*y**1*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x2001569c*x**0*y**2*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x200156f0*x**0*y**0*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x200156fc*x**0*y**1*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x20015708*x**0*y**2*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x2001575c*x**0*y**0*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x20015768*x**0*y**1*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x20015774*x**0*y**2*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x200157c8*x**0*y**0*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x200157d4*x**0*y**1*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x200157e0*x**0*y**2*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x20015834*x**0*y**0*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20015840*x**0*y**1*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x2001584c*x**0*y**2*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x200158a0*x**0*y**0*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x200158ac*x**0*y**1*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x200158b8*x**0*y**2*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x2001590c*x**0*y**0*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x20015918*x**0*y**1*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x20015924*x**0*y**2*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x20015978*x**0*y**0*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x20015984*x**0*y**1*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x20015990*x**0*y**2*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x200159e4*x**0*y**0*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x200159f0*x**0*y**1*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x200159fc*x**0*y**2*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20015a50*x**0*y**0*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20015a5c*x**0*y**1*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20015a68*x**0*y**2*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20015abc*x**0*y**0*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20015ac8*x**0*y**1*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20015ad4*x**0*y**2*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20015b28*x**0*y**0*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20015b34*x**0*y**1*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20015b40*x**0*y**2*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20015b94*x**0*y**0*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20015ba0*x**0*y**1*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20015bac*x**0*y**2*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20015c00*x**0*y**0*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20015c0c*x**0*y**1*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20015c18*x**0*y**2*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20015c6c*x**0*y**0*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20015c78*x**0*y**1*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20015c84*x**0*y**2*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20015cd8*x**0*y**0*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20015ce4*x**0*y**1*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20015cf0*x**0*y**2*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x20015d44*x**0*y**0*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20015d50*x**0*y**1*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20015d5c*x**0*y**2*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015db0*x**0*y**0*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x20015dbc*x**0*y**1*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x20015dc8*x**0*y**2*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x20015e1c*x**0*y**0*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x20015e28*x**0*y**1*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x20015e34*x**0*y**2*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x20015e88*x**0*y**0*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x20015e94*x**0*y**1*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015ea0*x**0*y**2*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x20015ef4*x**0*y**0*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x20015f00*x**0*y**1*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x20015f0c*x**0*y**2*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x20015f60*x**0*y**0*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x20015f6c*x**0*y**1*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x20015f78*x**0*y**2*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x20015fcc*x**0*y**0*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x20015fd8*x**0*y**1*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x20015fe4*x**0*y**2*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x20016038*x**0*y**0*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x20016044*x**0*y**1*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x20016050*x**0*y**2*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x200160a4*x**0*y**0*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x200160b0*x**0*y**1*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x200160bc*x**0*y**2*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x20016110*x**0*y**0*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x2001611c*x**0*y**1*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x20016128*x**0*y**2*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x2001617c*x**0*y**0*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x20016188*x**0*y**1*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x20016194*x**0*y**2*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x200161e8*x**0*y**0*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x200161f4*x**0*y**1*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x20016200*x**0*y**2*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x20016254*x**0*y**0*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x20016260*x**0*y**1*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x2001626c*x**0*y**2*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x200162c0*x**0*y**0*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x200162cc*x**0*y**1*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x200162d8*x**0*y**2*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x2001632c*x**0*y**0*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x20016338*x**0*y**1*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x20016344*x**0*y**2*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ]
] prove with [ all cuts ];



(**************** CUT 299, - *****************)

ecut and [
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x2001561c*x**1*y**0*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x20015628*x**1*y**1*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x20015634*x**1*y**2*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x20015688*x**1*y**0*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x20015694*x**1*y**1*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x200156a0*x**1*y**2*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x200156f4*x**1*y**0*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x20015700*x**1*y**1*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x2001570c*x**1*y**2*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x20015760*x**1*y**0*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x2001576c*x**1*y**1*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x20015778*x**1*y**2*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x200157cc*x**1*y**0*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x200157d8*x**1*y**1*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x200157e4*x**1*y**2*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x20015838*x**1*y**0*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x20015844*x**1*y**1*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20015850*x**1*y**2*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x200158a4*x**1*y**0*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x200158b0*x**1*y**1*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x200158bc*x**1*y**2*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20015910*x**1*y**0*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x2001591c*x**1*y**1*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x20015928*x**1*y**2*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x2001597c*x**1*y**0*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x20015988*x**1*y**1*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x20015994*x**1*y**2*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x200159e8*x**1*y**0*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x200159f4*x**1*y**1*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20015a00*x**1*y**2*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20015a54*x**1*y**0*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20015a60*x**1*y**1*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20015a6c*x**1*y**2*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20015ac0*x**1*y**0*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20015acc*x**1*y**1*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20015ad8*x**1*y**2*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20015b2c*x**1*y**0*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20015b38*x**1*y**1*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20015b44*x**1*y**2*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20015b98*x**1*y**0*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20015ba4*x**1*y**1*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20015bb0*x**1*y**2*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20015c04*x**1*y**0*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20015c10*x**1*y**1*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20015c1c*x**1*y**2*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20015c70*x**1*y**0*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20015c7c*x**1*y**1*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20015c88*x**1*y**2*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20015cdc*x**1*y**0*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20015ce8*x**1*y**1*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20015cf4*x**1*y**2*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20015d48*x**1*y**0*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x20015d54*x**1*y**1*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20015d60*x**1*y**2*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x20015db4*x**1*y**0*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015dc0*x**1*y**1*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x20015dcc*x**1*y**2*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x20015e20*x**1*y**0*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x20015e2c*x**1*y**1*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x20015e38*x**1*y**2*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x20015e8c*x**1*y**0*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x20015e98*x**1*y**1*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x20015ea4*x**1*y**2*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x20015ef8*x**1*y**0*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x20015f04*x**1*y**1*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x20015f10*x**1*y**2*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x20015f64*x**1*y**0*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x20015f70*x**1*y**1*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x20015f7c*x**1*y**2*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20015fd0*x**1*y**0*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x20015fdc*x**1*y**1*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x20015fe8*x**1*y**2*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x2001603c*x**1*y**0*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x20016048*x**1*y**1*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x20016054*x**1*y**2*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x200160a8*x**1*y**0*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x200160b4*x**1*y**1*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x200160c0*x**1*y**2*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x20016114*x**1*y**0*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x20016120*x**1*y**1*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x2001612c*x**1*y**2*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x20016180*x**1*y**0*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x2001618c*x**1*y**1*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x20016198*x**1*y**2*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x200161ec*x**1*y**0*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x200161f8*x**1*y**1*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x20016204*x**1*y**2*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x20016258*x**1*y**0*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x20016264*x**1*y**1*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x20016270*x**1*y**2*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x200162c4*x**1*y**0*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x200162d0*x**1*y**1*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x200162dc*x**1*y**2*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x20016330*x**1*y**0*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x2001633c*x**1*y**1*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x20016348*x**1*y**2*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ]
] prove with [ all cuts ];



(**************** CUT 300, - *****************)

ecut and [
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x20015620*x**2*y**0*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x2001562c*x**2*y**1*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x20015638*x**2*y**2*z**0 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x2001568c*x**2*y**0*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x20015698*x**2*y**1*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x200156a4*x**2*y**2*z**1 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x200156f8*x**2*y**0*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x20015704*x**2*y**1*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x20015710*x**2*y**2*z**2 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x20015764*x**2*y**0*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x20015770*x**2*y**1*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x2001577c*x**2*y**2*z**3 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x200157d0*x**2*y**0*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x200157dc*x**2*y**1*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x200157e8*x**2*y**2*z**4 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x2001583c*x**2*y**0*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x20015848*x**2*y**1*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x20015854*x**2*y**2*z**5 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x200158a8*x**2*y**0*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x200158b4*x**2*y**1*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x200158c0*x**2*y**2*z**6 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x20015914*x**2*y**0*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20015920*x**2*y**1*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x2001592c*x**2*y**2*z**7 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x20015980*x**2*y**0*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x2001598c*x**2*y**1*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x20015998*x**2*y**2*z**8 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x200159ec*x**2*y**0*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x200159f8*x**2*y**1*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20015a04*x**2*y**2*z**9 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20015a58*x**2*y**0*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20015a64*x**2*y**1*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20015a70*x**2*y**2*z**10 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20015ac4*x**2*y**0*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20015ad0*x**2*y**1*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20015adc*x**2*y**2*z**11 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20015b30*x**2*y**0*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20015b3c*x**2*y**1*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20015b48*x**2*y**2*z**12 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20015b9c*x**2*y**0*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20015ba8*x**2*y**1*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20015bb4*x**2*y**2*z**13 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20015c08*x**2*y**0*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20015c14*x**2*y**1*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20015c20*x**2*y**2*z**14 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20015c74*x**2*y**0*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20015c80*x**2*y**1*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20015c8c*x**2*y**2*z**15 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20015ce0*x**2*y**0*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20015cec*x**2*y**1*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20015cf8*x**2*y**2*z**16 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20015d4c*x**2*y**0*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20015d58*x**2*y**1*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x20015d64*x**2*y**2*z**17 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x20015db8*x**2*y**0*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x20015dc4*x**2*y**1*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015dd0*x**2*y**2*z**18 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x20015e24*x**2*y**0*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x20015e30*x**2*y**1*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x20015e3c*x**2*y**2*z**19 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015e90*x**2*y**0*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x20015e9c*x**2*y**1*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x20015ea8*x**2*y**2*z**20 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x20015efc*x**2*y**0*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x20015f08*x**2*y**1*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x20015f14*x**2*y**2*z**21 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x20015f68*x**2*y**0*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x20015f74*x**2*y**1*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015f80*x**2*y**2*z**22 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x20015fd4*x**2*y**0*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x20015fe0*x**2*y**1*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x20015fec*x**2*y**2*z**23 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x20016040*x**2*y**0*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x2001604c*x**2*y**1*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x20016058*x**2*y**2*z**24 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x200160ac*x**2*y**0*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x200160b8*x**2*y**1*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x200160c4*x**2*y**2*z**25 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x20016118*x**2*y**0*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x20016124*x**2*y**1*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x20016130*x**2*y**2*z**26 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x20016184*x**2*y**0*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x20016190*x**2*y**1*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x2001619c*x**2*y**2*z**27 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x200161f0*x**2*y**0*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x200161fc*x**2*y**1*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x20016208*x**2*y**2*z**28 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x2001625c*x**2*y**0*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x20016268*x**2*y**1*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x20016274*x**2*y**2*z**29 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x200162c8*x**2*y**0*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x200162d4*x**2*y**1*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x200162e0*x**2*y**2*z**30 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x20016334*x**2*y**0*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x20016340*x**2*y**1*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x2001634c*x**2*y**2*z**31 [ 3365569, y**3 -       1, z**32 - 3365568 ]
] prove with [ all cuts ];



(**************** CUT 301, - *****************)

ecut and [
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x2001563c*x**0*y**0*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x20015648*x**0*y**1*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x20015654*x**0*y**2*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x200156a8*x**0*y**0*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x200156b4*x**0*y**1*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x200156c0*x**0*y**2*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x20015714*x**0*y**0*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x20015720*x**0*y**1*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x2001572c*x**0*y**2*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x20015780*x**0*y**0*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x2001578c*x**0*y**1*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x20015798*x**0*y**2*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x200157ec*x**0*y**0*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x200157f8*x**0*y**1*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x20015804*x**0*y**2*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x20015858*x**0*y**0*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20015864*x**0*y**1*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x20015870*x**0*y**2*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x200158c4*x**0*y**0*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x200158d0*x**0*y**1*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x200158dc*x**0*y**2*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x20015930*x**0*y**0*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x2001593c*x**0*y**1*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x20015948*x**0*y**2*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x2001599c*x**0*y**0*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x200159a8*x**0*y**1*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x200159b4*x**0*y**2*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x20015a08*x**0*y**0*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x20015a14*x**0*y**1*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x20015a20*x**0*y**2*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20015a74*x**0*y**0*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20015a80*x**0*y**1*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20015a8c*x**0*y**2*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20015ae0*x**0*y**0*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20015aec*x**0*y**1*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20015af8*x**0*y**2*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20015b4c*x**0*y**0*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20015b58*x**0*y**1*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20015b64*x**0*y**2*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20015bb8*x**0*y**0*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20015bc4*x**0*y**1*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20015bd0*x**0*y**2*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20015c24*x**0*y**0*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20015c30*x**0*y**1*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20015c3c*x**0*y**2*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20015c90*x**0*y**0*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20015c9c*x**0*y**1*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20015ca8*x**0*y**2*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20015cfc*x**0*y**0*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20015d08*x**0*y**1*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20015d14*x**0*y**2*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x20015d68*x**0*y**0*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20015d74*x**0*y**1*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20015d80*x**0*y**2*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015dd4*x**0*y**0*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x20015de0*x**0*y**1*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x20015dec*x**0*y**2*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x20015e40*x**0*y**0*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x20015e4c*x**0*y**1*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x20015e58*x**0*y**2*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x20015eac*x**0*y**0*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x20015eb8*x**0*y**1*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015ec4*x**0*y**2*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x20015f18*x**0*y**0*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x20015f24*x**0*y**1*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x20015f30*x**0*y**2*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x20015f84*x**0*y**0*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x20015f90*x**0*y**1*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x20015f9c*x**0*y**2*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x20015ff0*x**0*y**0*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x20015ffc*x**0*y**1*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x20016008*x**0*y**2*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x2001605c*x**0*y**0*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x20016068*x**0*y**1*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x20016074*x**0*y**2*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x200160c8*x**0*y**0*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x200160d4*x**0*y**1*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x200160e0*x**0*y**2*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x20016134*x**0*y**0*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x20016140*x**0*y**1*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x2001614c*x**0*y**2*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x200161a0*x**0*y**0*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x200161ac*x**0*y**1*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x200161b8*x**0*y**2*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x2001620c*x**0*y**0*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x20016218*x**0*y**1*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x20016224*x**0*y**2*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x20016278*x**0*y**0*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x20016284*x**0*y**1*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x20016290*x**0*y**2*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x200162e4*x**0*y**0*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x200162f0*x**0*y**1*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x200162fc*x**0*y**2*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x20016350*x**0*y**0*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x2001635c*x**0*y**1*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x20016368*x**0*y**2*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ]
] prove with [ all cuts ];



(**************** CUT 302, - *****************)

ecut and [
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x20015640*x**1*y**0*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x2001564c*x**1*y**1*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x20015658*x**1*y**2*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x200156ac*x**1*y**0*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x200156b8*x**1*y**1*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x200156c4*x**1*y**2*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x20015718*x**1*y**0*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x20015724*x**1*y**1*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x20015730*x**1*y**2*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x20015784*x**1*y**0*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x20015790*x**1*y**1*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x2001579c*x**1*y**2*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x200157f0*x**1*y**0*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x200157fc*x**1*y**1*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x20015808*x**1*y**2*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x2001585c*x**1*y**0*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x20015868*x**1*y**1*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20015874*x**1*y**2*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x200158c8*x**1*y**0*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x200158d4*x**1*y**1*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x200158e0*x**1*y**2*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20015934*x**1*y**0*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x20015940*x**1*y**1*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x2001594c*x**1*y**2*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x200159a0*x**1*y**0*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x200159ac*x**1*y**1*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x200159b8*x**1*y**2*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x20015a0c*x**1*y**0*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x20015a18*x**1*y**1*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20015a24*x**1*y**2*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20015a78*x**1*y**0*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20015a84*x**1*y**1*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20015a90*x**1*y**2*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20015ae4*x**1*y**0*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20015af0*x**1*y**1*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20015afc*x**1*y**2*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20015b50*x**1*y**0*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20015b5c*x**1*y**1*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20015b68*x**1*y**2*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20015bbc*x**1*y**0*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20015bc8*x**1*y**1*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20015bd4*x**1*y**2*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20015c28*x**1*y**0*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20015c34*x**1*y**1*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20015c40*x**1*y**2*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20015c94*x**1*y**0*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20015ca0*x**1*y**1*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20015cac*x**1*y**2*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20015d00*x**1*y**0*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20015d0c*x**1*y**1*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20015d18*x**1*y**2*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20015d6c*x**1*y**0*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x20015d78*x**1*y**1*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20015d84*x**1*y**2*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x20015dd8*x**1*y**0*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015de4*x**1*y**1*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x20015df0*x**1*y**2*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x20015e44*x**1*y**0*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x20015e50*x**1*y**1*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x20015e5c*x**1*y**2*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x20015eb0*x**1*y**0*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x20015ebc*x**1*y**1*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x20015ec8*x**1*y**2*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x20015f1c*x**1*y**0*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x20015f28*x**1*y**1*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x20015f34*x**1*y**2*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x20015f88*x**1*y**0*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x20015f94*x**1*y**1*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x20015fa0*x**1*y**2*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20015ff4*x**1*y**0*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x20016000*x**1*y**1*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x2001600c*x**1*y**2*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x20016060*x**1*y**0*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x2001606c*x**1*y**1*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x20016078*x**1*y**2*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x200160cc*x**1*y**0*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x200160d8*x**1*y**1*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x200160e4*x**1*y**2*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x20016138*x**1*y**0*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x20016144*x**1*y**1*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x20016150*x**1*y**2*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x200161a4*x**1*y**0*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x200161b0*x**1*y**1*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x200161bc*x**1*y**2*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x20016210*x**1*y**0*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x2001621c*x**1*y**1*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x20016228*x**1*y**2*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x2001627c*x**1*y**0*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x20016288*x**1*y**1*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x20016294*x**1*y**2*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x200162e8*x**1*y**0*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x200162f4*x**1*y**1*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x20016300*x**1*y**2*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x20016354*x**1*y**0*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x20016360*x**1*y**1*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x2001636c*x**1*y**2*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ]
] prove with [ all cuts ];



(**************** CUT 303, - *****************)

ecut and [
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x20015644*x**2*y**0*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x20015650*x**2*y**1*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x2001565c*x**2*y**2*z**0 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x200156b0*x**2*y**0*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x200156bc*x**2*y**1*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x200156c8*x**2*y**2*z**1 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x2001571c*x**2*y**0*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x20015728*x**2*y**1*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x20015734*x**2*y**2*z**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x20015788*x**2*y**0*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x20015794*x**2*y**1*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x200157a0*x**2*y**2*z**3 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x200157f4*x**2*y**0*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x20015800*x**2*y**1*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x2001580c*x**2*y**2*z**4 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x20015860*x**2*y**0*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x2001586c*x**2*y**1*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x20015878*x**2*y**2*z**5 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x200158cc*x**2*y**0*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x200158d8*x**2*y**1*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x200158e4*x**2*y**2*z**6 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x20015938*x**2*y**0*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20015944*x**2*y**1*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x20015950*x**2*y**2*z**7 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x200159a4*x**2*y**0*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x200159b0*x**2*y**1*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x200159bc*x**2*y**2*z**8 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x20015a10*x**2*y**0*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x20015a1c*x**2*y**1*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20015a28*x**2*y**2*z**9 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20015a7c*x**2*y**0*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20015a88*x**2*y**1*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20015a94*x**2*y**2*z**10 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20015ae8*x**2*y**0*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20015af4*x**2*y**1*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20015b00*x**2*y**2*z**11 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20015b54*x**2*y**0*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20015b60*x**2*y**1*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20015b6c*x**2*y**2*z**12 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20015bc0*x**2*y**0*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20015bcc*x**2*y**1*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20015bd8*x**2*y**2*z**13 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20015c2c*x**2*y**0*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20015c38*x**2*y**1*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20015c44*x**2*y**2*z**14 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20015c98*x**2*y**0*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20015ca4*x**2*y**1*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20015cb0*x**2*y**2*z**15 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20015d04*x**2*y**0*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20015d10*x**2*y**1*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20015d1c*x**2*y**2*z**16 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20015d70*x**2*y**0*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20015d7c*x**2*y**1*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x20015d88*x**2*y**2*z**17 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x20015ddc*x**2*y**0*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x20015de8*x**2*y**1*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015df4*x**2*y**2*z**18 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x20015e48*x**2*y**0*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x20015e54*x**2*y**1*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x20015e60*x**2*y**2*z**19 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015eb4*x**2*y**0*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x20015ec0*x**2*y**1*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x20015ecc*x**2*y**2*z**20 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x20015f20*x**2*y**0*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x20015f2c*x**2*y**1*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x20015f38*x**2*y**2*z**21 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x20015f8c*x**2*y**0*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x20015f98*x**2*y**1*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015fa4*x**2*y**2*z**22 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x20015ff8*x**2*y**0*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x20016004*x**2*y**1*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x20016010*x**2*y**2*z**23 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x20016064*x**2*y**0*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x20016070*x**2*y**1*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x2001607c*x**2*y**2*z**24 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x200160d0*x**2*y**0*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x200160dc*x**2*y**1*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x200160e8*x**2*y**2*z**25 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x2001613c*x**2*y**0*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x20016148*x**2*y**1*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x20016154*x**2*y**2*z**26 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x200161a8*x**2*y**0*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x200161b4*x**2*y**1*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x200161c0*x**2*y**2*z**27 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x20016214*x**2*y**0*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x20016220*x**2*y**1*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x2001622c*x**2*y**2*z**28 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x20016280*x**2*y**0*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x2001628c*x**2*y**1*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x20016298*x**2*y**2*z**29 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x200162ec*x**2*y**0*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x200162f8*x**2*y**1*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x20016304*x**2*y**2*z**30 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x20016358*x**2*y**0*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x20016364*x**2*y**1*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x20016370*x**2*y**2*z**31 [ 3365569, y**3 -  452650, z**32 - 3365568 ]
] prove with [ all cuts ];



(**************** CUT 304, - *****************)

ecut and [
eqmod (cf0000*x**0*(y*z)**  0+cf0300*x**0*(y*z)**192+cf0600*x**0*(y*z)** 96)
      L0x20015660*x**0*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0100*x**0*(y*z)** 64+cf0400*x**0*(y*z)**256+cf0700*x**0*(y*z)**160)
      L0x2001566c*x**0*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0200*x**0*(y*z)**128+cf0500*x**0*(y*z)** 32+cf0800*x**0*(y*z)**224)
      L0x20015678*x**0*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0001*x**0*(y*z)**225+cf0301*x**0*(y*z)**129+cf0601*x**0*(y*z)** 33)
      L0x200156cc*x**0*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0101*x**0*(y*z)**  1+cf0401*x**0*(y*z)**193+cf0701*x**0*(y*z)** 97)
      L0x200156d8*x**0*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0201*x**0*(y*z)** 65+cf0501*x**0*(y*z)**257+cf0801*x**0*(y*z)**161)
      L0x200156e4*x**0*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0002*x**0*(y*z)**162+cf0302*x**0*(y*z)** 66+cf0602*x**0*(y*z)**258)
      L0x20015738*x**0*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0102*x**0*(y*z)**226+cf0402*x**0*(y*z)**130+cf0702*x**0*(y*z)** 34)
      L0x20015744*x**0*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0202*x**0*(y*z)**  2+cf0502*x**0*(y*z)**194+cf0802*x**0*(y*z)** 98)
      L0x20015750*x**0*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0003*x**0*(y*z)** 99+cf0303*x**0*(y*z)**  3+cf0603*x**0*(y*z)**195)
      L0x200157a4*x**0*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0103*x**0*(y*z)**163+cf0403*x**0*(y*z)** 67+cf0703*x**0*(y*z)**259)
      L0x200157b0*x**0*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0203*x**0*(y*z)**227+cf0503*x**0*(y*z)**131+cf0803*x**0*(y*z)** 35)
      L0x200157bc*x**0*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0004*x**0*(y*z)** 36+cf0304*x**0*(y*z)**228+cf0604*x**0*(y*z)**132)
      L0x20015810*x**0*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0104*x**0*(y*z)**100+cf0404*x**0*(y*z)**  4+cf0704*x**0*(y*z)**196)
      L0x2001581c*x**0*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0204*x**0*(y*z)**164+cf0504*x**0*(y*z)** 68+cf0804*x**0*(y*z)**260)
      L0x20015828*x**0*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0005*x**0*(y*z)**261+cf0305*x**0*(y*z)**165+cf0605*x**0*(y*z)** 69)
      L0x2001587c*x**0*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0105*x**0*(y*z)** 37+cf0405*x**0*(y*z)**229+cf0705*x**0*(y*z)**133)
      L0x20015888*x**0*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0205*x**0*(y*z)**101+cf0505*x**0*(y*z)**  5+cf0805*x**0*(y*z)**197)
      L0x20015894*x**0*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0006*x**0*(y*z)**198+cf0306*x**0*(y*z)**102+cf0606*x**0*(y*z)**  6)
      L0x200158e8*x**0*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0106*x**0*(y*z)**262+cf0406*x**0*(y*z)**166+cf0706*x**0*(y*z)** 70)
      L0x200158f4*x**0*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0206*x**0*(y*z)** 38+cf0506*x**0*(y*z)**230+cf0806*x**0*(y*z)**134)
      L0x20015900*x**0*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0007*x**0*(y*z)**135+cf0307*x**0*(y*z)** 39+cf0607*x**0*(y*z)**231)
      L0x20015954*x**0*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0107*x**0*(y*z)**199+cf0407*x**0*(y*z)**103+cf0707*x**0*(y*z)**  7)
      L0x20015960*x**0*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0207*x**0*(y*z)**263+cf0507*x**0*(y*z)**167+cf0807*x**0*(y*z)** 71)
      L0x2001596c*x**0*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0008*x**0*(y*z)** 72+cf0308*x**0*(y*z)**264+cf0608*x**0*(y*z)**168)
      L0x200159c0*x**0*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0108*x**0*(y*z)**136+cf0408*x**0*(y*z)** 40+cf0708*x**0*(y*z)**232)
      L0x200159cc*x**0*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0208*x**0*(y*z)**200+cf0508*x**0*(y*z)**104+cf0808*x**0*(y*z)**  8)
      L0x200159d8*x**0*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0009*x**0*(y*z)**  9+cf0309*x**0*(y*z)**201+cf0609*x**0*(y*z)**105)
      L0x20015a2c*x**0*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0109*x**0*(y*z)** 73+cf0409*x**0*(y*z)**265+cf0709*x**0*(y*z)**169)
      L0x20015a38*x**0*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0209*x**0*(y*z)**137+cf0509*x**0*(y*z)** 41+cf0809*x**0*(y*z)**233)
      L0x20015a44*x**0*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0010*x**0*(y*z)**234+cf0310*x**0*(y*z)**138+cf0610*x**0*(y*z)** 42)
      L0x20015a98*x**0*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0110*x**0*(y*z)** 10+cf0410*x**0*(y*z)**202+cf0710*x**0*(y*z)**106)
      L0x20015aa4*x**0*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0210*x**0*(y*z)** 74+cf0510*x**0*(y*z)**266+cf0810*x**0*(y*z)**170)
      L0x20015ab0*x**0*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0011*x**0*(y*z)**171+cf0311*x**0*(y*z)** 75+cf0611*x**0*(y*z)**267)
      L0x20015b04*x**0*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0111*x**0*(y*z)**235+cf0411*x**0*(y*z)**139+cf0711*x**0*(y*z)** 43)
      L0x20015b10*x**0*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0211*x**0*(y*z)** 11+cf0511*x**0*(y*z)**203+cf0811*x**0*(y*z)**107)
      L0x20015b1c*x**0*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0012*x**0*(y*z)**108+cf0312*x**0*(y*z)** 12+cf0612*x**0*(y*z)**204)
      L0x20015b70*x**0*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0112*x**0*(y*z)**172+cf0412*x**0*(y*z)** 76+cf0712*x**0*(y*z)**268)
      L0x20015b7c*x**0*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0212*x**0*(y*z)**236+cf0512*x**0*(y*z)**140+cf0812*x**0*(y*z)** 44)
      L0x20015b88*x**0*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0013*x**0*(y*z)** 45+cf0313*x**0*(y*z)**237+cf0613*x**0*(y*z)**141)
      L0x20015bdc*x**0*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0113*x**0*(y*z)**109+cf0413*x**0*(y*z)** 13+cf0713*x**0*(y*z)**205)
      L0x20015be8*x**0*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0213*x**0*(y*z)**173+cf0513*x**0*(y*z)** 77+cf0813*x**0*(y*z)**269)
      L0x20015bf4*x**0*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0014*x**0*(y*z)**270+cf0314*x**0*(y*z)**174+cf0614*x**0*(y*z)** 78)
      L0x20015c48*x**0*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0114*x**0*(y*z)** 46+cf0414*x**0*(y*z)**238+cf0714*x**0*(y*z)**142)
      L0x20015c54*x**0*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0214*x**0*(y*z)**110+cf0514*x**0*(y*z)** 14+cf0814*x**0*(y*z)**206)
      L0x20015c60*x**0*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0015*x**0*(y*z)**207+cf0315*x**0*(y*z)**111+cf0615*x**0*(y*z)** 15)
      L0x20015cb4*x**0*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0115*x**0*(y*z)**271+cf0415*x**0*(y*z)**175+cf0715*x**0*(y*z)** 79)
      L0x20015cc0*x**0*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0215*x**0*(y*z)** 47+cf0515*x**0*(y*z)**239+cf0815*x**0*(y*z)**143)
      L0x20015ccc*x**0*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0016*x**0*(y*z)**144+cf0316*x**0*(y*z)** 48+cf0616*x**0*(y*z)**240)
      L0x20015d20*x**0*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0116*x**0*(y*z)**208+cf0416*x**0*(y*z)**112+cf0716*x**0*(y*z)** 16)
      L0x20015d2c*x**0*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0216*x**0*(y*z)**272+cf0516*x**0*(y*z)**176+cf0816*x**0*(y*z)** 80)
      L0x20015d38*x**0*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0017*x**0*(y*z)** 81+cf0317*x**0*(y*z)**273+cf0617*x**0*(y*z)**177)
      L0x20015d8c*x**0*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0117*x**0*(y*z)**145+cf0417*x**0*(y*z)** 49+cf0717*x**0*(y*z)**241)
      L0x20015d98*x**0*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0217*x**0*(y*z)**209+cf0517*x**0*(y*z)**113+cf0817*x**0*(y*z)** 17)
      L0x20015da4*x**0*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0018*x**0*(y*z)** 18+cf0318*x**0*(y*z)**210+cf0618*x**0*(y*z)**114)
      L0x20015df8*x**0*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0118*x**0*(y*z)** 82+cf0418*x**0*(y*z)**274+cf0718*x**0*(y*z)**178)
      L0x20015e04*x**0*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0218*x**0*(y*z)**146+cf0518*x**0*(y*z)** 50+cf0818*x**0*(y*z)**242)
      L0x20015e10*x**0*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0019*x**0*(y*z)**243+cf0319*x**0*(y*z)**147+cf0619*x**0*(y*z)** 51)
      L0x20015e64*x**0*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0119*x**0*(y*z)** 19+cf0419*x**0*(y*z)**211+cf0719*x**0*(y*z)**115)
      L0x20015e70*x**0*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0219*x**0*(y*z)** 83+cf0519*x**0*(y*z)**275+cf0819*x**0*(y*z)**179)
      L0x20015e7c*x**0*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0020*x**0*(y*z)**180+cf0320*x**0*(y*z)** 84+cf0620*x**0*(y*z)**276)
      L0x20015ed0*x**0*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0120*x**0*(y*z)**244+cf0420*x**0*(y*z)**148+cf0720*x**0*(y*z)** 52)
      L0x20015edc*x**0*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0220*x**0*(y*z)** 20+cf0520*x**0*(y*z)**212+cf0820*x**0*(y*z)**116)
      L0x20015ee8*x**0*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0021*x**0*(y*z)**117+cf0321*x**0*(y*z)** 21+cf0621*x**0*(y*z)**213)
      L0x20015f3c*x**0*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0121*x**0*(y*z)**181+cf0421*x**0*(y*z)** 85+cf0721*x**0*(y*z)**277)
      L0x20015f48*x**0*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0221*x**0*(y*z)**245+cf0521*x**0*(y*z)**149+cf0821*x**0*(y*z)** 53)
      L0x20015f54*x**0*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0022*x**0*(y*z)** 54+cf0322*x**0*(y*z)**246+cf0622*x**0*(y*z)**150)
      L0x20015fa8*x**0*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0122*x**0*(y*z)**118+cf0422*x**0*(y*z)** 22+cf0722*x**0*(y*z)**214)
      L0x20015fb4*x**0*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0222*x**0*(y*z)**182+cf0522*x**0*(y*z)** 86+cf0822*x**0*(y*z)**278)
      L0x20015fc0*x**0*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0023*x**0*(y*z)**279+cf0323*x**0*(y*z)**183+cf0623*x**0*(y*z)** 87)
      L0x20016014*x**0*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0123*x**0*(y*z)** 55+cf0423*x**0*(y*z)**247+cf0723*x**0*(y*z)**151)
      L0x20016020*x**0*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0223*x**0*(y*z)**119+cf0523*x**0*(y*z)** 23+cf0823*x**0*(y*z)**215)
      L0x2001602c*x**0*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0024*x**0*(y*z)**216+cf0324*x**0*(y*z)**120+cf0624*x**0*(y*z)** 24)
      L0x20016080*x**0*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0124*x**0*(y*z)**280+cf0424*x**0*(y*z)**184+cf0724*x**0*(y*z)** 88)
      L0x2001608c*x**0*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0224*x**0*(y*z)** 56+cf0524*x**0*(y*z)**248+cf0824*x**0*(y*z)**152)
      L0x20016098*x**0*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0025*x**0*(y*z)**153+cf0325*x**0*(y*z)** 57+cf0625*x**0*(y*z)**249)
      L0x200160ec*x**0*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0125*x**0*(y*z)**217+cf0425*x**0*(y*z)**121+cf0725*x**0*(y*z)** 25)
      L0x200160f8*x**0*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0225*x**0*(y*z)**281+cf0525*x**0*(y*z)**185+cf0825*x**0*(y*z)** 89)
      L0x20016104*x**0*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0026*x**0*(y*z)** 90+cf0326*x**0*(y*z)**282+cf0626*x**0*(y*z)**186)
      L0x20016158*x**0*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0126*x**0*(y*z)**154+cf0426*x**0*(y*z)** 58+cf0726*x**0*(y*z)**250)
      L0x20016164*x**0*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0226*x**0*(y*z)**218+cf0526*x**0*(y*z)**122+cf0826*x**0*(y*z)** 26)
      L0x20016170*x**0*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0027*x**0*(y*z)** 27+cf0327*x**0*(y*z)**219+cf0627*x**0*(y*z)**123)
      L0x200161c4*x**0*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0127*x**0*(y*z)** 91+cf0427*x**0*(y*z)**283+cf0727*x**0*(y*z)**187)
      L0x200161d0*x**0*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0227*x**0*(y*z)**155+cf0527*x**0*(y*z)** 59+cf0827*x**0*(y*z)**251)
      L0x200161dc*x**0*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0028*x**0*(y*z)**252+cf0328*x**0*(y*z)**156+cf0628*x**0*(y*z)** 60)
      L0x20016230*x**0*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0128*x**0*(y*z)** 28+cf0428*x**0*(y*z)**220+cf0728*x**0*(y*z)**124)
      L0x2001623c*x**0*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0228*x**0*(y*z)** 92+cf0528*x**0*(y*z)**284+cf0828*x**0*(y*z)**188)
      L0x20016248*x**0*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0029*x**0*(y*z)**189+cf0329*x**0*(y*z)** 93+cf0629*x**0*(y*z)**285)
      L0x2001629c*x**0*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0129*x**0*(y*z)**253+cf0429*x**0*(y*z)**157+cf0729*x**0*(y*z)** 61)
      L0x200162a8*x**0*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0229*x**0*(y*z)** 29+cf0529*x**0*(y*z)**221+cf0829*x**0*(y*z)**125)
      L0x200162b4*x**0*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0030*x**0*(y*z)**126+cf0330*x**0*(y*z)** 30+cf0630*x**0*(y*z)**222)
      L0x20016308*x**0*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0130*x**0*(y*z)**190+cf0430*x**0*(y*z)** 94+cf0730*x**0*(y*z)**286)
      L0x20016314*x**0*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0230*x**0*(y*z)**254+cf0530*x**0*(y*z)**158+cf0830*x**0*(y*z)** 62)
      L0x20016320*x**0*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0031*x**0*(y*z)** 63+cf0331*x**0*(y*z)**255+cf0631*x**0*(y*z)**159)
      L0x20016374*x**0*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0131*x**0*(y*z)**127+cf0431*x**0*(y*z)** 31+cf0731*x**0*(y*z)**223)
      L0x20016380*x**0*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf0231*x**0*(y*z)**191+cf0531*x**0*(y*z)** 95+cf0831*x**0*(y*z)**287)
      L0x2001638c*x**0*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
] prove with [ all cuts ];



(**************** CUT 305, - *****************)

ecut and [
eqmod (cf1000*x**1*(y*z)**  0+cf1300*x**1*(y*z)**192+cf1600*x**1*(y*z)** 96)
      L0x20015664*x**1*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1100*x**1*(y*z)** 64+cf1400*x**1*(y*z)**256+cf1700*x**1*(y*z)**160)
      L0x20015670*x**1*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1200*x**1*(y*z)**128+cf1500*x**1*(y*z)** 32+cf1800*x**1*(y*z)**224)
      L0x2001567c*x**1*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1001*x**1*(y*z)**225+cf1301*x**1*(y*z)**129+cf1601*x**1*(y*z)** 33)
      L0x200156d0*x**1*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1101*x**1*(y*z)**  1+cf1401*x**1*(y*z)**193+cf1701*x**1*(y*z)** 97)
      L0x200156dc*x**1*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1201*x**1*(y*z)** 65+cf1501*x**1*(y*z)**257+cf1801*x**1*(y*z)**161)
      L0x200156e8*x**1*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1002*x**1*(y*z)**162+cf1302*x**1*(y*z)** 66+cf1602*x**1*(y*z)**258)
      L0x2001573c*x**1*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1102*x**1*(y*z)**226+cf1402*x**1*(y*z)**130+cf1702*x**1*(y*z)** 34)
      L0x20015748*x**1*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1202*x**1*(y*z)**  2+cf1502*x**1*(y*z)**194+cf1802*x**1*(y*z)** 98)
      L0x20015754*x**1*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1003*x**1*(y*z)** 99+cf1303*x**1*(y*z)**  3+cf1603*x**1*(y*z)**195)
      L0x200157a8*x**1*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1103*x**1*(y*z)**163+cf1403*x**1*(y*z)** 67+cf1703*x**1*(y*z)**259)
      L0x200157b4*x**1*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1203*x**1*(y*z)**227+cf1503*x**1*(y*z)**131+cf1803*x**1*(y*z)** 35)
      L0x200157c0*x**1*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1004*x**1*(y*z)** 36+cf1304*x**1*(y*z)**228+cf1604*x**1*(y*z)**132)
      L0x20015814*x**1*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1104*x**1*(y*z)**100+cf1404*x**1*(y*z)**  4+cf1704*x**1*(y*z)**196)
      L0x20015820*x**1*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1204*x**1*(y*z)**164+cf1504*x**1*(y*z)** 68+cf1804*x**1*(y*z)**260)
      L0x2001582c*x**1*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1005*x**1*(y*z)**261+cf1305*x**1*(y*z)**165+cf1605*x**1*(y*z)** 69)
      L0x20015880*x**1*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1105*x**1*(y*z)** 37+cf1405*x**1*(y*z)**229+cf1705*x**1*(y*z)**133)
      L0x2001588c*x**1*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1205*x**1*(y*z)**101+cf1505*x**1*(y*z)**  5+cf1805*x**1*(y*z)**197)
      L0x20015898*x**1*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1006*x**1*(y*z)**198+cf1306*x**1*(y*z)**102+cf1606*x**1*(y*z)**  6)
      L0x200158ec*x**1*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1106*x**1*(y*z)**262+cf1406*x**1*(y*z)**166+cf1706*x**1*(y*z)** 70)
      L0x200158f8*x**1*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1206*x**1*(y*z)** 38+cf1506*x**1*(y*z)**230+cf1806*x**1*(y*z)**134)
      L0x20015904*x**1*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1007*x**1*(y*z)**135+cf1307*x**1*(y*z)** 39+cf1607*x**1*(y*z)**231)
      L0x20015958*x**1*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1107*x**1*(y*z)**199+cf1407*x**1*(y*z)**103+cf1707*x**1*(y*z)**  7)
      L0x20015964*x**1*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1207*x**1*(y*z)**263+cf1507*x**1*(y*z)**167+cf1807*x**1*(y*z)** 71)
      L0x20015970*x**1*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1008*x**1*(y*z)** 72+cf1308*x**1*(y*z)**264+cf1608*x**1*(y*z)**168)
      L0x200159c4*x**1*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1108*x**1*(y*z)**136+cf1408*x**1*(y*z)** 40+cf1708*x**1*(y*z)**232)
      L0x200159d0*x**1*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1208*x**1*(y*z)**200+cf1508*x**1*(y*z)**104+cf1808*x**1*(y*z)**  8)
      L0x200159dc*x**1*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1009*x**1*(y*z)**  9+cf1309*x**1*(y*z)**201+cf1609*x**1*(y*z)**105)
      L0x20015a30*x**1*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1109*x**1*(y*z)** 73+cf1409*x**1*(y*z)**265+cf1709*x**1*(y*z)**169)
      L0x20015a3c*x**1*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1209*x**1*(y*z)**137+cf1509*x**1*(y*z)** 41+cf1809*x**1*(y*z)**233)
      L0x20015a48*x**1*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1010*x**1*(y*z)**234+cf1310*x**1*(y*z)**138+cf1610*x**1*(y*z)** 42)
      L0x20015a9c*x**1*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1110*x**1*(y*z)** 10+cf1410*x**1*(y*z)**202+cf1710*x**1*(y*z)**106)
      L0x20015aa8*x**1*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1210*x**1*(y*z)** 74+cf1510*x**1*(y*z)**266+cf1810*x**1*(y*z)**170)
      L0x20015ab4*x**1*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1011*x**1*(y*z)**171+cf1311*x**1*(y*z)** 75+cf1611*x**1*(y*z)**267)
      L0x20015b08*x**1*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1111*x**1*(y*z)**235+cf1411*x**1*(y*z)**139+cf1711*x**1*(y*z)** 43)
      L0x20015b14*x**1*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1211*x**1*(y*z)** 11+cf1511*x**1*(y*z)**203+cf1811*x**1*(y*z)**107)
      L0x20015b20*x**1*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1012*x**1*(y*z)**108+cf1312*x**1*(y*z)** 12+cf1612*x**1*(y*z)**204)
      L0x20015b74*x**1*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1112*x**1*(y*z)**172+cf1412*x**1*(y*z)** 76+cf1712*x**1*(y*z)**268)
      L0x20015b80*x**1*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1212*x**1*(y*z)**236+cf1512*x**1*(y*z)**140+cf1812*x**1*(y*z)** 44)
      L0x20015b8c*x**1*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1013*x**1*(y*z)** 45+cf1313*x**1*(y*z)**237+cf1613*x**1*(y*z)**141)
      L0x20015be0*x**1*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1113*x**1*(y*z)**109+cf1413*x**1*(y*z)** 13+cf1713*x**1*(y*z)**205)
      L0x20015bec*x**1*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1213*x**1*(y*z)**173+cf1513*x**1*(y*z)** 77+cf1813*x**1*(y*z)**269)
      L0x20015bf8*x**1*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1014*x**1*(y*z)**270+cf1314*x**1*(y*z)**174+cf1614*x**1*(y*z)** 78)
      L0x20015c4c*x**1*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1114*x**1*(y*z)** 46+cf1414*x**1*(y*z)**238+cf1714*x**1*(y*z)**142)
      L0x20015c58*x**1*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1214*x**1*(y*z)**110+cf1514*x**1*(y*z)** 14+cf1814*x**1*(y*z)**206)
      L0x20015c64*x**1*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1015*x**1*(y*z)**207+cf1315*x**1*(y*z)**111+cf1615*x**1*(y*z)** 15)
      L0x20015cb8*x**1*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1115*x**1*(y*z)**271+cf1415*x**1*(y*z)**175+cf1715*x**1*(y*z)** 79)
      L0x20015cc4*x**1*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1215*x**1*(y*z)** 47+cf1515*x**1*(y*z)**239+cf1815*x**1*(y*z)**143)
      L0x20015cd0*x**1*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1016*x**1*(y*z)**144+cf1316*x**1*(y*z)** 48+cf1616*x**1*(y*z)**240)
      L0x20015d24*x**1*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1116*x**1*(y*z)**208+cf1416*x**1*(y*z)**112+cf1716*x**1*(y*z)** 16)
      L0x20015d30*x**1*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1216*x**1*(y*z)**272+cf1516*x**1*(y*z)**176+cf1816*x**1*(y*z)** 80)
      L0x20015d3c*x**1*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1017*x**1*(y*z)** 81+cf1317*x**1*(y*z)**273+cf1617*x**1*(y*z)**177)
      L0x20015d90*x**1*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1117*x**1*(y*z)**145+cf1417*x**1*(y*z)** 49+cf1717*x**1*(y*z)**241)
      L0x20015d9c*x**1*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1217*x**1*(y*z)**209+cf1517*x**1*(y*z)**113+cf1817*x**1*(y*z)** 17)
      L0x20015da8*x**1*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1018*x**1*(y*z)** 18+cf1318*x**1*(y*z)**210+cf1618*x**1*(y*z)**114)
      L0x20015dfc*x**1*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1118*x**1*(y*z)** 82+cf1418*x**1*(y*z)**274+cf1718*x**1*(y*z)**178)
      L0x20015e08*x**1*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1218*x**1*(y*z)**146+cf1518*x**1*(y*z)** 50+cf1818*x**1*(y*z)**242)
      L0x20015e14*x**1*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1019*x**1*(y*z)**243+cf1319*x**1*(y*z)**147+cf1619*x**1*(y*z)** 51)
      L0x20015e68*x**1*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1119*x**1*(y*z)** 19+cf1419*x**1*(y*z)**211+cf1719*x**1*(y*z)**115)
      L0x20015e74*x**1*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1219*x**1*(y*z)** 83+cf1519*x**1*(y*z)**275+cf1819*x**1*(y*z)**179)
      L0x20015e80*x**1*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1020*x**1*(y*z)**180+cf1320*x**1*(y*z)** 84+cf1620*x**1*(y*z)**276)
      L0x20015ed4*x**1*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1120*x**1*(y*z)**244+cf1420*x**1*(y*z)**148+cf1720*x**1*(y*z)** 52)
      L0x20015ee0*x**1*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1220*x**1*(y*z)** 20+cf1520*x**1*(y*z)**212+cf1820*x**1*(y*z)**116)
      L0x20015eec*x**1*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1021*x**1*(y*z)**117+cf1321*x**1*(y*z)** 21+cf1621*x**1*(y*z)**213)
      L0x20015f40*x**1*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1121*x**1*(y*z)**181+cf1421*x**1*(y*z)** 85+cf1721*x**1*(y*z)**277)
      L0x20015f4c*x**1*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1221*x**1*(y*z)**245+cf1521*x**1*(y*z)**149+cf1821*x**1*(y*z)** 53)
      L0x20015f58*x**1*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1022*x**1*(y*z)** 54+cf1322*x**1*(y*z)**246+cf1622*x**1*(y*z)**150)
      L0x20015fac*x**1*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1122*x**1*(y*z)**118+cf1422*x**1*(y*z)** 22+cf1722*x**1*(y*z)**214)
      L0x20015fb8*x**1*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1222*x**1*(y*z)**182+cf1522*x**1*(y*z)** 86+cf1822*x**1*(y*z)**278)
      L0x20015fc4*x**1*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1023*x**1*(y*z)**279+cf1323*x**1*(y*z)**183+cf1623*x**1*(y*z)** 87)
      L0x20016018*x**1*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1123*x**1*(y*z)** 55+cf1423*x**1*(y*z)**247+cf1723*x**1*(y*z)**151)
      L0x20016024*x**1*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1223*x**1*(y*z)**119+cf1523*x**1*(y*z)** 23+cf1823*x**1*(y*z)**215)
      L0x20016030*x**1*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1024*x**1*(y*z)**216+cf1324*x**1*(y*z)**120+cf1624*x**1*(y*z)** 24)
      L0x20016084*x**1*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1124*x**1*(y*z)**280+cf1424*x**1*(y*z)**184+cf1724*x**1*(y*z)** 88)
      L0x20016090*x**1*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1224*x**1*(y*z)** 56+cf1524*x**1*(y*z)**248+cf1824*x**1*(y*z)**152)
      L0x2001609c*x**1*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1025*x**1*(y*z)**153+cf1325*x**1*(y*z)** 57+cf1625*x**1*(y*z)**249)
      L0x200160f0*x**1*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1125*x**1*(y*z)**217+cf1425*x**1*(y*z)**121+cf1725*x**1*(y*z)** 25)
      L0x200160fc*x**1*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1225*x**1*(y*z)**281+cf1525*x**1*(y*z)**185+cf1825*x**1*(y*z)** 89)
      L0x20016108*x**1*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1026*x**1*(y*z)** 90+cf1326*x**1*(y*z)**282+cf1626*x**1*(y*z)**186)
      L0x2001615c*x**1*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1126*x**1*(y*z)**154+cf1426*x**1*(y*z)** 58+cf1726*x**1*(y*z)**250)
      L0x20016168*x**1*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1226*x**1*(y*z)**218+cf1526*x**1*(y*z)**122+cf1826*x**1*(y*z)** 26)
      L0x20016174*x**1*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1027*x**1*(y*z)** 27+cf1327*x**1*(y*z)**219+cf1627*x**1*(y*z)**123)
      L0x200161c8*x**1*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1127*x**1*(y*z)** 91+cf1427*x**1*(y*z)**283+cf1727*x**1*(y*z)**187)
      L0x200161d4*x**1*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1227*x**1*(y*z)**155+cf1527*x**1*(y*z)** 59+cf1827*x**1*(y*z)**251)
      L0x200161e0*x**1*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1028*x**1*(y*z)**252+cf1328*x**1*(y*z)**156+cf1628*x**1*(y*z)** 60)
      L0x20016234*x**1*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1128*x**1*(y*z)** 28+cf1428*x**1*(y*z)**220+cf1728*x**1*(y*z)**124)
      L0x20016240*x**1*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1228*x**1*(y*z)** 92+cf1528*x**1*(y*z)**284+cf1828*x**1*(y*z)**188)
      L0x2001624c*x**1*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1029*x**1*(y*z)**189+cf1329*x**1*(y*z)** 93+cf1629*x**1*(y*z)**285)
      L0x200162a0*x**1*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1129*x**1*(y*z)**253+cf1429*x**1*(y*z)**157+cf1729*x**1*(y*z)** 61)
      L0x200162ac*x**1*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1229*x**1*(y*z)** 29+cf1529*x**1*(y*z)**221+cf1829*x**1*(y*z)**125)
      L0x200162b8*x**1*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1030*x**1*(y*z)**126+cf1330*x**1*(y*z)** 30+cf1630*x**1*(y*z)**222)
      L0x2001630c*x**1*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1130*x**1*(y*z)**190+cf1430*x**1*(y*z)** 94+cf1730*x**1*(y*z)**286)
      L0x20016318*x**1*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1230*x**1*(y*z)**254+cf1530*x**1*(y*z)**158+cf1830*x**1*(y*z)** 62)
      L0x20016324*x**1*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1031*x**1*(y*z)** 63+cf1331*x**1*(y*z)**255+cf1631*x**1*(y*z)**159)
      L0x20016378*x**1*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1131*x**1*(y*z)**127+cf1431*x**1*(y*z)** 31+cf1731*x**1*(y*z)**223)
      L0x20016384*x**1*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf1231*x**1*(y*z)**191+cf1531*x**1*(y*z)** 95+cf1831*x**1*(y*z)**287)
      L0x20016390*x**1*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
] prove with [ all cuts ];



(**************** CUT 306, - *****************)

ecut and [
eqmod (cf2000*x**2*(y*z)**  0+cf2300*x**2*(y*z)**192+cf2600*x**2*(y*z)** 96)
      L0x20015668*x**2*y**0*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2100*x**2*(y*z)** 64+cf2400*x**2*(y*z)**256+cf2700*x**2*(y*z)**160)
      L0x20015674*x**2*y**1*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2200*x**2*(y*z)**128+cf2500*x**2*(y*z)** 32+cf2800*x**2*(y*z)**224)
      L0x20015680*x**2*y**2*z**0 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2001*x**2*(y*z)**225+cf2301*x**2*(y*z)**129+cf2601*x**2*(y*z)** 33)
      L0x200156d4*x**2*y**0*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2101*x**2*(y*z)**  1+cf2401*x**2*(y*z)**193+cf2701*x**2*(y*z)** 97)
      L0x200156e0*x**2*y**1*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2201*x**2*(y*z)** 65+cf2501*x**2*(y*z)**257+cf2801*x**2*(y*z)**161)
      L0x200156ec*x**2*y**2*z**1 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2002*x**2*(y*z)**162+cf2302*x**2*(y*z)** 66+cf2602*x**2*(y*z)**258)
      L0x20015740*x**2*y**0*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2102*x**2*(y*z)**226+cf2402*x**2*(y*z)**130+cf2702*x**2*(y*z)** 34)
      L0x2001574c*x**2*y**1*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2202*x**2*(y*z)**  2+cf2502*x**2*(y*z)**194+cf2802*x**2*(y*z)** 98)
      L0x20015758*x**2*y**2*z**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2003*x**2*(y*z)** 99+cf2303*x**2*(y*z)**  3+cf2603*x**2*(y*z)**195)
      L0x200157ac*x**2*y**0*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2103*x**2*(y*z)**163+cf2403*x**2*(y*z)** 67+cf2703*x**2*(y*z)**259)
      L0x200157b8*x**2*y**1*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2203*x**2*(y*z)**227+cf2503*x**2*(y*z)**131+cf2803*x**2*(y*z)** 35)
      L0x200157c4*x**2*y**2*z**3 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2004*x**2*(y*z)** 36+cf2304*x**2*(y*z)**228+cf2604*x**2*(y*z)**132)
      L0x20015818*x**2*y**0*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2104*x**2*(y*z)**100+cf2404*x**2*(y*z)**  4+cf2704*x**2*(y*z)**196)
      L0x20015824*x**2*y**1*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2204*x**2*(y*z)**164+cf2504*x**2*(y*z)** 68+cf2804*x**2*(y*z)**260)
      L0x20015830*x**2*y**2*z**4 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2005*x**2*(y*z)**261+cf2305*x**2*(y*z)**165+cf2605*x**2*(y*z)** 69)
      L0x20015884*x**2*y**0*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2105*x**2*(y*z)** 37+cf2405*x**2*(y*z)**229+cf2705*x**2*(y*z)**133)
      L0x20015890*x**2*y**1*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2205*x**2*(y*z)**101+cf2505*x**2*(y*z)**  5+cf2805*x**2*(y*z)**197)
      L0x2001589c*x**2*y**2*z**5 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2006*x**2*(y*z)**198+cf2306*x**2*(y*z)**102+cf2606*x**2*(y*z)**  6)
      L0x200158f0*x**2*y**0*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2106*x**2*(y*z)**262+cf2406*x**2*(y*z)**166+cf2706*x**2*(y*z)** 70)
      L0x200158fc*x**2*y**1*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2206*x**2*(y*z)** 38+cf2506*x**2*(y*z)**230+cf2806*x**2*(y*z)**134)
      L0x20015908*x**2*y**2*z**6 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2007*x**2*(y*z)**135+cf2307*x**2*(y*z)** 39+cf2607*x**2*(y*z)**231)
      L0x2001595c*x**2*y**0*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2107*x**2*(y*z)**199+cf2407*x**2*(y*z)**103+cf2707*x**2*(y*z)**  7)
      L0x20015968*x**2*y**1*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2207*x**2*(y*z)**263+cf2507*x**2*(y*z)**167+cf2807*x**2*(y*z)** 71)
      L0x20015974*x**2*y**2*z**7 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2008*x**2*(y*z)** 72+cf2308*x**2*(y*z)**264+cf2608*x**2*(y*z)**168)
      L0x200159c8*x**2*y**0*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2108*x**2*(y*z)**136+cf2408*x**2*(y*z)** 40+cf2708*x**2*(y*z)**232)
      L0x200159d4*x**2*y**1*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2208*x**2*(y*z)**200+cf2508*x**2*(y*z)**104+cf2808*x**2*(y*z)**  8)
      L0x200159e0*x**2*y**2*z**8 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2009*x**2*(y*z)**  9+cf2309*x**2*(y*z)**201+cf2609*x**2*(y*z)**105)
      L0x20015a34*x**2*y**0*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2109*x**2*(y*z)** 73+cf2409*x**2*(y*z)**265+cf2709*x**2*(y*z)**169)
      L0x20015a40*x**2*y**1*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2209*x**2*(y*z)**137+cf2509*x**2*(y*z)** 41+cf2809*x**2*(y*z)**233)
      L0x20015a4c*x**2*y**2*z**9 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2010*x**2*(y*z)**234+cf2310*x**2*(y*z)**138+cf2610*x**2*(y*z)** 42)
      L0x20015aa0*x**2*y**0*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2110*x**2*(y*z)** 10+cf2410*x**2*(y*z)**202+cf2710*x**2*(y*z)**106)
      L0x20015aac*x**2*y**1*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2210*x**2*(y*z)** 74+cf2510*x**2*(y*z)**266+cf2810*x**2*(y*z)**170)
      L0x20015ab8*x**2*y**2*z**10 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2011*x**2*(y*z)**171+cf2311*x**2*(y*z)** 75+cf2611*x**2*(y*z)**267)
      L0x20015b0c*x**2*y**0*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2111*x**2*(y*z)**235+cf2411*x**2*(y*z)**139+cf2711*x**2*(y*z)** 43)
      L0x20015b18*x**2*y**1*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2211*x**2*(y*z)** 11+cf2511*x**2*(y*z)**203+cf2811*x**2*(y*z)**107)
      L0x20015b24*x**2*y**2*z**11 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2012*x**2*(y*z)**108+cf2312*x**2*(y*z)** 12+cf2612*x**2*(y*z)**204)
      L0x20015b78*x**2*y**0*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2112*x**2*(y*z)**172+cf2412*x**2*(y*z)** 76+cf2712*x**2*(y*z)**268)
      L0x20015b84*x**2*y**1*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2212*x**2*(y*z)**236+cf2512*x**2*(y*z)**140+cf2812*x**2*(y*z)** 44)
      L0x20015b90*x**2*y**2*z**12 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2013*x**2*(y*z)** 45+cf2313*x**2*(y*z)**237+cf2613*x**2*(y*z)**141)
      L0x20015be4*x**2*y**0*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2113*x**2*(y*z)**109+cf2413*x**2*(y*z)** 13+cf2713*x**2*(y*z)**205)
      L0x20015bf0*x**2*y**1*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2213*x**2*(y*z)**173+cf2513*x**2*(y*z)** 77+cf2813*x**2*(y*z)**269)
      L0x20015bfc*x**2*y**2*z**13 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2014*x**2*(y*z)**270+cf2314*x**2*(y*z)**174+cf2614*x**2*(y*z)** 78)
      L0x20015c50*x**2*y**0*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2114*x**2*(y*z)** 46+cf2414*x**2*(y*z)**238+cf2714*x**2*(y*z)**142)
      L0x20015c5c*x**2*y**1*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2214*x**2*(y*z)**110+cf2514*x**2*(y*z)** 14+cf2814*x**2*(y*z)**206)
      L0x20015c68*x**2*y**2*z**14 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2015*x**2*(y*z)**207+cf2315*x**2*(y*z)**111+cf2615*x**2*(y*z)** 15)
      L0x20015cbc*x**2*y**0*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2115*x**2*(y*z)**271+cf2415*x**2*(y*z)**175+cf2715*x**2*(y*z)** 79)
      L0x20015cc8*x**2*y**1*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2215*x**2*(y*z)** 47+cf2515*x**2*(y*z)**239+cf2815*x**2*(y*z)**143)
      L0x20015cd4*x**2*y**2*z**15 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2016*x**2*(y*z)**144+cf2316*x**2*(y*z)** 48+cf2616*x**2*(y*z)**240)
      L0x20015d28*x**2*y**0*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2116*x**2*(y*z)**208+cf2416*x**2*(y*z)**112+cf2716*x**2*(y*z)** 16)
      L0x20015d34*x**2*y**1*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2216*x**2*(y*z)**272+cf2516*x**2*(y*z)**176+cf2816*x**2*(y*z)** 80)
      L0x20015d40*x**2*y**2*z**16 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2017*x**2*(y*z)** 81+cf2317*x**2*(y*z)**273+cf2617*x**2*(y*z)**177)
      L0x20015d94*x**2*y**0*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2117*x**2*(y*z)**145+cf2417*x**2*(y*z)** 49+cf2717*x**2*(y*z)**241)
      L0x20015da0*x**2*y**1*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2217*x**2*(y*z)**209+cf2517*x**2*(y*z)**113+cf2817*x**2*(y*z)** 17)
      L0x20015dac*x**2*y**2*z**17 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2018*x**2*(y*z)** 18+cf2318*x**2*(y*z)**210+cf2618*x**2*(y*z)**114)
      L0x20015e00*x**2*y**0*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2118*x**2*(y*z)** 82+cf2418*x**2*(y*z)**274+cf2718*x**2*(y*z)**178)
      L0x20015e0c*x**2*y**1*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2218*x**2*(y*z)**146+cf2518*x**2*(y*z)** 50+cf2818*x**2*(y*z)**242)
      L0x20015e18*x**2*y**2*z**18 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2019*x**2*(y*z)**243+cf2319*x**2*(y*z)**147+cf2619*x**2*(y*z)** 51)
      L0x20015e6c*x**2*y**0*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2119*x**2*(y*z)** 19+cf2419*x**2*(y*z)**211+cf2719*x**2*(y*z)**115)
      L0x20015e78*x**2*y**1*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2219*x**2*(y*z)** 83+cf2519*x**2*(y*z)**275+cf2819*x**2*(y*z)**179)
      L0x20015e84*x**2*y**2*z**19 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2020*x**2*(y*z)**180+cf2320*x**2*(y*z)** 84+cf2620*x**2*(y*z)**276)
      L0x20015ed8*x**2*y**0*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2120*x**2*(y*z)**244+cf2420*x**2*(y*z)**148+cf2720*x**2*(y*z)** 52)
      L0x20015ee4*x**2*y**1*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2220*x**2*(y*z)** 20+cf2520*x**2*(y*z)**212+cf2820*x**2*(y*z)**116)
      L0x20015ef0*x**2*y**2*z**20 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2021*x**2*(y*z)**117+cf2321*x**2*(y*z)** 21+cf2621*x**2*(y*z)**213)
      L0x20015f44*x**2*y**0*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2121*x**2*(y*z)**181+cf2421*x**2*(y*z)** 85+cf2721*x**2*(y*z)**277)
      L0x20015f50*x**2*y**1*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2221*x**2*(y*z)**245+cf2521*x**2*(y*z)**149+cf2821*x**2*(y*z)** 53)
      L0x20015f5c*x**2*y**2*z**21 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2022*x**2*(y*z)** 54+cf2322*x**2*(y*z)**246+cf2622*x**2*(y*z)**150)
      L0x20015fb0*x**2*y**0*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2122*x**2*(y*z)**118+cf2422*x**2*(y*z)** 22+cf2722*x**2*(y*z)**214)
      L0x20015fbc*x**2*y**1*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2222*x**2*(y*z)**182+cf2522*x**2*(y*z)** 86+cf2822*x**2*(y*z)**278)
      L0x20015fc8*x**2*y**2*z**22 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2023*x**2*(y*z)**279+cf2323*x**2*(y*z)**183+cf2623*x**2*(y*z)** 87)
      L0x2001601c*x**2*y**0*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2123*x**2*(y*z)** 55+cf2423*x**2*(y*z)**247+cf2723*x**2*(y*z)**151)
      L0x20016028*x**2*y**1*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2223*x**2*(y*z)**119+cf2523*x**2*(y*z)** 23+cf2823*x**2*(y*z)**215)
      L0x20016034*x**2*y**2*z**23 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2024*x**2*(y*z)**216+cf2324*x**2*(y*z)**120+cf2624*x**2*(y*z)** 24)
      L0x20016088*x**2*y**0*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2124*x**2*(y*z)**280+cf2424*x**2*(y*z)**184+cf2724*x**2*(y*z)** 88)
      L0x20016094*x**2*y**1*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2224*x**2*(y*z)** 56+cf2524*x**2*(y*z)**248+cf2824*x**2*(y*z)**152)
      L0x200160a0*x**2*y**2*z**24 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2025*x**2*(y*z)**153+cf2325*x**2*(y*z)** 57+cf2625*x**2*(y*z)**249)
      L0x200160f4*x**2*y**0*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2125*x**2*(y*z)**217+cf2425*x**2*(y*z)**121+cf2725*x**2*(y*z)** 25)
      L0x20016100*x**2*y**1*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2225*x**2*(y*z)**281+cf2525*x**2*(y*z)**185+cf2825*x**2*(y*z)** 89)
      L0x2001610c*x**2*y**2*z**25 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2026*x**2*(y*z)** 90+cf2326*x**2*(y*z)**282+cf2626*x**2*(y*z)**186)
      L0x20016160*x**2*y**0*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2126*x**2*(y*z)**154+cf2426*x**2*(y*z)** 58+cf2726*x**2*(y*z)**250)
      L0x2001616c*x**2*y**1*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2226*x**2*(y*z)**218+cf2526*x**2*(y*z)**122+cf2826*x**2*(y*z)** 26)
      L0x20016178*x**2*y**2*z**26 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2027*x**2*(y*z)** 27+cf2327*x**2*(y*z)**219+cf2627*x**2*(y*z)**123)
      L0x200161cc*x**2*y**0*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2127*x**2*(y*z)** 91+cf2427*x**2*(y*z)**283+cf2727*x**2*(y*z)**187)
      L0x200161d8*x**2*y**1*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2227*x**2*(y*z)**155+cf2527*x**2*(y*z)** 59+cf2827*x**2*(y*z)**251)
      L0x200161e4*x**2*y**2*z**27 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2028*x**2*(y*z)**252+cf2328*x**2*(y*z)**156+cf2628*x**2*(y*z)** 60)
      L0x20016238*x**2*y**0*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2128*x**2*(y*z)** 28+cf2428*x**2*(y*z)**220+cf2728*x**2*(y*z)**124)
      L0x20016244*x**2*y**1*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2228*x**2*(y*z)** 92+cf2528*x**2*(y*z)**284+cf2828*x**2*(y*z)**188)
      L0x20016250*x**2*y**2*z**28 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2029*x**2*(y*z)**189+cf2329*x**2*(y*z)** 93+cf2629*x**2*(y*z)**285)
      L0x200162a4*x**2*y**0*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2129*x**2*(y*z)**253+cf2429*x**2*(y*z)**157+cf2729*x**2*(y*z)** 61)
      L0x200162b0*x**2*y**1*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2229*x**2*(y*z)** 29+cf2529*x**2*(y*z)**221+cf2829*x**2*(y*z)**125)
      L0x200162bc*x**2*y**2*z**29 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2030*x**2*(y*z)**126+cf2330*x**2*(y*z)** 30+cf2630*x**2*(y*z)**222)
      L0x20016310*x**2*y**0*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2130*x**2*(y*z)**190+cf2430*x**2*(y*z)** 94+cf2730*x**2*(y*z)**286)
      L0x2001631c*x**2*y**1*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2230*x**2*(y*z)**254+cf2530*x**2*(y*z)**158+cf2830*x**2*(y*z)** 62)
      L0x20016328*x**2*y**2*z**30 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2031*x**2*(y*z)** 63+cf2331*x**2*(y*z)**255+cf2631*x**2*(y*z)**159)
      L0x2001637c*x**2*y**0*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2131*x**2*(y*z)**127+cf2431*x**2*(y*z)** 31+cf2731*x**2*(y*z)**223)
      L0x20016388*x**2*y**1*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
eqmod (cf2231*x**2*(y*z)**191+cf2531*x**2*(y*z)** 95+cf2831*x**2*(y*z)**287)
      L0x20016394*x**2*y**2*z**31 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
] prove with [ all cuts ];



(**************** CUT 307, - *****************)

ecut true;



(******************** input polynomials ********************)

ghost F@sint32, CF0@sint32, CF1@sint32, CF2@sint32 : and [
F**2 = 
f0000*x**0*(y*z)**  0+f1000*x**1*(y*z)**  0+f2000*x**2*(y*z)**  0+
f0101*x**0*(y*z)**  1+f1101*x**1*(y*z)**  1+f2101*x**2*(y*z)**  1+
f0202*x**0*(y*z)**  2+f1202*x**1*(y*z)**  2+f2202*x**2*(y*z)**  2+
f0303*x**0*(y*z)**  3+f1303*x**1*(y*z)**  3+f2303*x**2*(y*z)**  3+
f0404*x**0*(y*z)**  4+f1404*x**1*(y*z)**  4+f2404*x**2*(y*z)**  4+
f0505*x**0*(y*z)**  5+f1505*x**1*(y*z)**  5+f2505*x**2*(y*z)**  5+
f0606*x**0*(y*z)**  6+f1606*x**1*(y*z)**  6+f2606*x**2*(y*z)**  6+
f0707*x**0*(y*z)**  7+f1707*x**1*(y*z)**  7+f2707*x**2*(y*z)**  7+
f0808*x**0*(y*z)**  8+f1808*x**1*(y*z)**  8+f2808*x**2*(y*z)**  8+
f0009*x**0*(y*z)**  9+f1009*x**1*(y*z)**  9+f2009*x**2*(y*z)**  9+
f0110*x**0*(y*z)** 10+f1110*x**1*(y*z)** 10+f2110*x**2*(y*z)** 10+
f0211*x**0*(y*z)** 11+f1211*x**1*(y*z)** 11+f2211*x**2*(y*z)** 11+
f0312*x**0*(y*z)** 12+f1312*x**1*(y*z)** 12+f2312*x**2*(y*z)** 12+
f0413*x**0*(y*z)** 13+f1413*x**1*(y*z)** 13+f2413*x**2*(y*z)** 13+
f0514*x**0*(y*z)** 14+f1514*x**1*(y*z)** 14+f2514*x**2*(y*z)** 14+
f0615*x**0*(y*z)** 15+f1615*x**1*(y*z)** 15+f2615*x**2*(y*z)** 15+
f0716*x**0*(y*z)** 16+f1716*x**1*(y*z)** 16+f2716*x**2*(y*z)** 16+
f0817*x**0*(y*z)** 17+f1817*x**1*(y*z)** 17+f2817*x**2*(y*z)** 17+
f0018*x**0*(y*z)** 18+f1018*x**1*(y*z)** 18+f2018*x**2*(y*z)** 18+
f0119*x**0*(y*z)** 19+f1119*x**1*(y*z)** 19+f2119*x**2*(y*z)** 19+
f0220*x**0*(y*z)** 20+f1220*x**1*(y*z)** 20+f2220*x**2*(y*z)** 20+
f0321*x**0*(y*z)** 21+f1321*x**1*(y*z)** 21+f2321*x**2*(y*z)** 21+
f0422*x**0*(y*z)** 22+f1422*x**1*(y*z)** 22+f2422*x**2*(y*z)** 22+
f0523*x**0*(y*z)** 23+f1523*x**1*(y*z)** 23+f2523*x**2*(y*z)** 23+
f0624*x**0*(y*z)** 24+f1624*x**1*(y*z)** 24+f2624*x**2*(y*z)** 24+
f0725*x**0*(y*z)** 25+f1725*x**1*(y*z)** 25+f2725*x**2*(y*z)** 25+
f0826*x**0*(y*z)** 26+f1826*x**1*(y*z)** 26+f2826*x**2*(y*z)** 26+
f0027*x**0*(y*z)** 27+f1027*x**1*(y*z)** 27+f2027*x**2*(y*z)** 27+
f0128*x**0*(y*z)** 28+f1128*x**1*(y*z)** 28+f2128*x**2*(y*z)** 28+
f0229*x**0*(y*z)** 29+f1229*x**1*(y*z)** 29+f2229*x**2*(y*z)** 29+
f0330*x**0*(y*z)** 30+f1330*x**1*(y*z)** 30+f2330*x**2*(y*z)** 30+
f0431*x**0*(y*z)** 31+f1431*x**1*(y*z)** 31+f2431*x**2*(y*z)** 31+
f0500*x**0*(y*z)** 32+f1500*x**1*(y*z)** 32+f2500*x**2*(y*z)** 32+
f0601*x**0*(y*z)** 33+f1601*x**1*(y*z)** 33+f2601*x**2*(y*z)** 33+
f0702*x**0*(y*z)** 34+f1702*x**1*(y*z)** 34+f2702*x**2*(y*z)** 34+
f0803*x**0*(y*z)** 35+f1803*x**1*(y*z)** 35+f2803*x**2*(y*z)** 35+
f0004*x**0*(y*z)** 36+f1004*x**1*(y*z)** 36+f2004*x**2*(y*z)** 36+
f0105*x**0*(y*z)** 37+f1105*x**1*(y*z)** 37+f2105*x**2*(y*z)** 37+
f0206*x**0*(y*z)** 38+f1206*x**1*(y*z)** 38+f2206*x**2*(y*z)** 38+
f0307*x**0*(y*z)** 39+f1307*x**1*(y*z)** 39+f2307*x**2*(y*z)** 39+
f0408*x**0*(y*z)** 40+f1408*x**1*(y*z)** 40+f2408*x**2*(y*z)** 40+
f0509*x**0*(y*z)** 41+f1509*x**1*(y*z)** 41+f2509*x**2*(y*z)** 41+
f0610*x**0*(y*z)** 42+f1610*x**1*(y*z)** 42+f2610*x**2*(y*z)** 42+
f0711*x**0*(y*z)** 43+f1711*x**1*(y*z)** 43+f2711*x**2*(y*z)** 43+
f0812*x**0*(y*z)** 44+f1812*x**1*(y*z)** 44+f2812*x**2*(y*z)** 44+
f0013*x**0*(y*z)** 45+f1013*x**1*(y*z)** 45+f2013*x**2*(y*z)** 45+
f0114*x**0*(y*z)** 46+f1114*x**1*(y*z)** 46+f2114*x**2*(y*z)** 46+
f0215*x**0*(y*z)** 47+f1215*x**1*(y*z)** 47+f2215*x**2*(y*z)** 47+
f0316*x**0*(y*z)** 48+f1316*x**1*(y*z)** 48+f2316*x**2*(y*z)** 48+
f0417*x**0*(y*z)** 49+f1417*x**1*(y*z)** 49+f2417*x**2*(y*z)** 49+
f0518*x**0*(y*z)** 50+f1518*x**1*(y*z)** 50+f2518*x**2*(y*z)** 50+
f0619*x**0*(y*z)** 51+f1619*x**1*(y*z)** 51+f2619*x**2*(y*z)** 51+
f0720*x**0*(y*z)** 52+f1720*x**1*(y*z)** 52+f2720*x**2*(y*z)** 52+
f0821*x**0*(y*z)** 53+f1821*x**1*(y*z)** 53+f2821*x**2*(y*z)** 53+
f0022*x**0*(y*z)** 54+f1022*x**1*(y*z)** 54+f2022*x**2*(y*z)** 54+
f0123*x**0*(y*z)** 55+f1123*x**1*(y*z)** 55+f2123*x**2*(y*z)** 55+
f0224*x**0*(y*z)** 56+f1224*x**1*(y*z)** 56+f2224*x**2*(y*z)** 56+
f0325*x**0*(y*z)** 57+f1325*x**1*(y*z)** 57+f2325*x**2*(y*z)** 57+
f0426*x**0*(y*z)** 58+f1426*x**1*(y*z)** 58+f2426*x**2*(y*z)** 58+
f0527*x**0*(y*z)** 59+f1527*x**1*(y*z)** 59+f2527*x**2*(y*z)** 59+
f0628*x**0*(y*z)** 60+f1628*x**1*(y*z)** 60+f2628*x**2*(y*z)** 60+
f0729*x**0*(y*z)** 61+f1729*x**1*(y*z)** 61+f2729*x**2*(y*z)** 61+
f0830*x**0*(y*z)** 62+f1830*x**1*(y*z)** 62+f2830*x**2*(y*z)** 62+
f0031*x**0*(y*z)** 63+f1031*x**1*(y*z)** 63+f2031*x**2*(y*z)** 63+
f0100*x**0*(y*z)** 64+f1100*x**1*(y*z)** 64+f2100*x**2*(y*z)** 64+
f0201*x**0*(y*z)** 65+f1201*x**1*(y*z)** 65+f2201*x**2*(y*z)** 65+
f0302*x**0*(y*z)** 66+f1302*x**1*(y*z)** 66+f2302*x**2*(y*z)** 66+
f0403*x**0*(y*z)** 67+f1403*x**1*(y*z)** 67+f2403*x**2*(y*z)** 67+
f0504*x**0*(y*z)** 68+f1504*x**1*(y*z)** 68+f2504*x**2*(y*z)** 68+
f0605*x**0*(y*z)** 69+f1605*x**1*(y*z)** 69+f2605*x**2*(y*z)** 69+
f0706*x**0*(y*z)** 70+f1706*x**1*(y*z)** 70+f2706*x**2*(y*z)** 70+
f0807*x**0*(y*z)** 71+f1807*x**1*(y*z)** 71+f2807*x**2*(y*z)** 71+
f0008*x**0*(y*z)** 72+f1008*x**1*(y*z)** 72+f2008*x**2*(y*z)** 72+
f0109*x**0*(y*z)** 73+f1109*x**1*(y*z)** 73+f2109*x**2*(y*z)** 73+
f0210*x**0*(y*z)** 74+f1210*x**1*(y*z)** 74+f2210*x**2*(y*z)** 74+
f0311*x**0*(y*z)** 75+f1311*x**1*(y*z)** 75+f2311*x**2*(y*z)** 75+
f0412*x**0*(y*z)** 76+f1412*x**1*(y*z)** 76+f2412*x**2*(y*z)** 76+
f0513*x**0*(y*z)** 77+f1513*x**1*(y*z)** 77+f2513*x**2*(y*z)** 77+
f0614*x**0*(y*z)** 78+f1614*x**1*(y*z)** 78+f2614*x**2*(y*z)** 78+
f0715*x**0*(y*z)** 79+f1715*x**1*(y*z)** 79+f2715*x**2*(y*z)** 79+
f0816*x**0*(y*z)** 80+f1816*x**1*(y*z)** 80+f2816*x**2*(y*z)** 80+
f0017*x**0*(y*z)** 81+f1017*x**1*(y*z)** 81+f2017*x**2*(y*z)** 81+
f0118*x**0*(y*z)** 82+f1118*x**1*(y*z)** 82+f2118*x**2*(y*z)** 82+
f0219*x**0*(y*z)** 83+f1219*x**1*(y*z)** 83+f2219*x**2*(y*z)** 83+
f0320*x**0*(y*z)** 84+f1320*x**1*(y*z)** 84+f2320*x**2*(y*z)** 84+
f0421*x**0*(y*z)** 85+f1421*x**1*(y*z)** 85+f2421*x**2*(y*z)** 85+
f0522*x**0*(y*z)** 86+f1522*x**1*(y*z)** 86+f2522*x**2*(y*z)** 86+
f0623*x**0*(y*z)** 87+f1623*x**1*(y*z)** 87+f2623*x**2*(y*z)** 87+
f0724*x**0*(y*z)** 88+f1724*x**1*(y*z)** 88+f2724*x**2*(y*z)** 88+
f0825*x**0*(y*z)** 89+f1825*x**1*(y*z)** 89+f2825*x**2*(y*z)** 89+
f0026*x**0*(y*z)** 90+f1026*x**1*(y*z)** 90+f2026*x**2*(y*z)** 90+
f0127*x**0*(y*z)** 91+f1127*x**1*(y*z)** 91+f2127*x**2*(y*z)** 91+
f0228*x**0*(y*z)** 92+f1228*x**1*(y*z)** 92+f2228*x**2*(y*z)** 92+
f0329*x**0*(y*z)** 93+f1329*x**1*(y*z)** 93+f2329*x**2*(y*z)** 93+
f0430*x**0*(y*z)** 94+f1430*x**1*(y*z)** 94+f2430*x**2*(y*z)** 94+
f0531*x**0*(y*z)** 95+f1531*x**1*(y*z)** 95+f2531*x**2*(y*z)** 95+
f0600*x**0*(y*z)** 96+f1600*x**1*(y*z)** 96+f2600*x**2*(y*z)** 96+
f0701*x**0*(y*z)** 97+f1701*x**1*(y*z)** 97+f2701*x**2*(y*z)** 97+
f0802*x**0*(y*z)** 98+f1802*x**1*(y*z)** 98+f2802*x**2*(y*z)** 98+
f0003*x**0*(y*z)** 99+f1003*x**1*(y*z)** 99+f2003*x**2*(y*z)** 99+
f0104*x**0*(y*z)**100+f1104*x**1*(y*z)**100+f2104*x**2*(y*z)**100+
f0205*x**0*(y*z)**101+f1205*x**1*(y*z)**101+f2205*x**2*(y*z)**101+
f0306*x**0*(y*z)**102+f1306*x**1*(y*z)**102+f2306*x**2*(y*z)**102+
f0407*x**0*(y*z)**103+f1407*x**1*(y*z)**103+f2407*x**2*(y*z)**103+
f0508*x**0*(y*z)**104+f1508*x**1*(y*z)**104+f2508*x**2*(y*z)**104+
f0609*x**0*(y*z)**105+f1609*x**1*(y*z)**105+f2609*x**2*(y*z)**105+
f0710*x**0*(y*z)**106+f1710*x**1*(y*z)**106+f2710*x**2*(y*z)**106+
f0811*x**0*(y*z)**107+f1811*x**1*(y*z)**107+f2811*x**2*(y*z)**107+
f0012*x**0*(y*z)**108+f1012*x**1*(y*z)**108+f2012*x**2*(y*z)**108+
f0113*x**0*(y*z)**109+f1113*x**1*(y*z)**109+f2113*x**2*(y*z)**109+
f0214*x**0*(y*z)**110+f1214*x**1*(y*z)**110+f2214*x**2*(y*z)**110+
f0315*x**0*(y*z)**111+f1315*x**1*(y*z)**111+f2315*x**2*(y*z)**111+
f0416*x**0*(y*z)**112+f1416*x**1*(y*z)**112+f2416*x**2*(y*z)**112+
f0517*x**0*(y*z)**113+f1517*x**1*(y*z)**113+f2517*x**2*(y*z)**113+
f0618*x**0*(y*z)**114+f1618*x**1*(y*z)**114+f2618*x**2*(y*z)**114+
f0719*x**0*(y*z)**115+f1719*x**1*(y*z)**115+f2719*x**2*(y*z)**115+
f0820*x**0*(y*z)**116+f1820*x**1*(y*z)**116+f2820*x**2*(y*z)**116+
f0021*x**0*(y*z)**117+f1021*x**1*(y*z)**117+f2021*x**2*(y*z)**117+
f0122*x**0*(y*z)**118+f1122*x**1*(y*z)**118+f2122*x**2*(y*z)**118+
f0223*x**0*(y*z)**119+f1223*x**1*(y*z)**119+f2223*x**2*(y*z)**119+
f0324*x**0*(y*z)**120+f1324*x**1*(y*z)**120+f2324*x**2*(y*z)**120+
f0425*x**0*(y*z)**121+f1425*x**1*(y*z)**121+f2425*x**2*(y*z)**121+
f0526*x**0*(y*z)**122+f1526*x**1*(y*z)**122+f2526*x**2*(y*z)**122+
f0627*x**0*(y*z)**123+f1627*x**1*(y*z)**123+f2627*x**2*(y*z)**123+
f0728*x**0*(y*z)**124+f1728*x**1*(y*z)**124+f2728*x**2*(y*z)**124+
f0829*x**0*(y*z)**125+f1829*x**1*(y*z)**125+f2829*x**2*(y*z)**125+
f0030*x**0*(y*z)**126+f1030*x**1*(y*z)**126+f2030*x**2*(y*z)**126+
f0131*x**0*(y*z)**127+f1131*x**1*(y*z)**127+f2131*x**2*(y*z)**127+
f0200*x**0*(y*z)**128+f1200*x**1*(y*z)**128+f2200*x**2*(y*z)**128+
f0301*x**0*(y*z)**129+f1301*x**1*(y*z)**129+f2301*x**2*(y*z)**129+
f0402*x**0*(y*z)**130+f1402*x**1*(y*z)**130+f2402*x**2*(y*z)**130+
f0503*x**0*(y*z)**131+f1503*x**1*(y*z)**131+f2503*x**2*(y*z)**131+
f0604*x**0*(y*z)**132+f1604*x**1*(y*z)**132+f2604*x**2*(y*z)**132+
f0705*x**0*(y*z)**133+f1705*x**1*(y*z)**133+f2705*x**2*(y*z)**133+
f0806*x**0*(y*z)**134+f1806*x**1*(y*z)**134+f2806*x**2*(y*z)**134+
f0007*x**0*(y*z)**135+f1007*x**1*(y*z)**135+f2007*x**2*(y*z)**135+
f0108*x**0*(y*z)**136+f1108*x**1*(y*z)**136+f2108*x**2*(y*z)**136+
f0209*x**0*(y*z)**137+f1209*x**1*(y*z)**137+f2209*x**2*(y*z)**137+
f0310*x**0*(y*z)**138+f1310*x**1*(y*z)**138+f2310*x**2*(y*z)**138+
f0411*x**0*(y*z)**139+f1411*x**1*(y*z)**139+f2411*x**2*(y*z)**139+
f0512*x**0*(y*z)**140+f1512*x**1*(y*z)**140+f2512*x**2*(y*z)**140+
f0613*x**0*(y*z)**141+f1613*x**1*(y*z)**141+f2613*x**2*(y*z)**141+
f0714*x**0*(y*z)**142+f1714*x**1*(y*z)**142+f2714*x**2*(y*z)**142+
f0815*x**0*(y*z)**143+f1815*x**1*(y*z)**143+f2815*x**2*(y*z)**143+
f0016*x**0*(y*z)**144+f1016*x**1*(y*z)**144+f2016*x**2*(y*z)**144+
f0117*x**0*(y*z)**145+f1117*x**1*(y*z)**145+f2117*x**2*(y*z)**145+
f0218*x**0*(y*z)**146+f1218*x**1*(y*z)**146+f2218*x**2*(y*z)**146+
f0319*x**0*(y*z)**147+f1319*x**1*(y*z)**147+f2319*x**2*(y*z)**147+
f0420*x**0*(y*z)**148+f1420*x**1*(y*z)**148+f2420*x**2*(y*z)**148+
f0521*x**0*(y*z)**149+f1521*x**1*(y*z)**149+f2521*x**2*(y*z)**149+
f0622*x**0*(y*z)**150+f1622*x**1*(y*z)**150+f2622*x**2*(y*z)**150+
f0723*x**0*(y*z)**151+f1723*x**1*(y*z)**151+f2723*x**2*(y*z)**151+
f0824*x**0*(y*z)**152+f1824*x**1*(y*z)**152+f2824*x**2*(y*z)**152+
f0025*x**0*(y*z)**153+f1025*x**1*(y*z)**153+f2025*x**2*(y*z)**153+
f0126*x**0*(y*z)**154+f1126*x**1*(y*z)**154+f2126*x**2*(y*z)**154+
f0227*x**0*(y*z)**155+f1227*x**1*(y*z)**155+f2227*x**2*(y*z)**155+
f0328*x**0*(y*z)**156+f1328*x**1*(y*z)**156+f2328*x**2*(y*z)**156+
f0429*x**0*(y*z)**157+f1429*x**1*(y*z)**157+f2429*x**2*(y*z)**157+
f0530*x**0*(y*z)**158+f1530*x**1*(y*z)**158+f2530*x**2*(y*z)**158+
f0631*x**0*(y*z)**159+f1631*x**1*(y*z)**159+f2631*x**2*(y*z)**159+
f0700*x**0*(y*z)**160+f1700*x**1*(y*z)**160+f2700*x**2*(y*z)**160+
f0801*x**0*(y*z)**161+f1801*x**1*(y*z)**161+f2801*x**2*(y*z)**161+
f0002*x**0*(y*z)**162+f1002*x**1*(y*z)**162+f2002*x**2*(y*z)**162+
f0103*x**0*(y*z)**163+f1103*x**1*(y*z)**163+f2103*x**2*(y*z)**163+
f0204*x**0*(y*z)**164+f1204*x**1*(y*z)**164+f2204*x**2*(y*z)**164+
f0305*x**0*(y*z)**165+f1305*x**1*(y*z)**165+f2305*x**2*(y*z)**165+
f0406*x**0*(y*z)**166+f1406*x**1*(y*z)**166+f2406*x**2*(y*z)**166+
f0507*x**0*(y*z)**167+f1507*x**1*(y*z)**167+f2507*x**2*(y*z)**167+
f0608*x**0*(y*z)**168+f1608*x**1*(y*z)**168+f2608*x**2*(y*z)**168+
f0709*x**0*(y*z)**169+f1709*x**1*(y*z)**169+f2709*x**2*(y*z)**169+
f0810*x**0*(y*z)**170+f1810*x**1*(y*z)**170+f2810*x**2*(y*z)**170+
f0011*x**0*(y*z)**171+f1011*x**1*(y*z)**171+f2011*x**2*(y*z)**171+
f0112*x**0*(y*z)**172+f1112*x**1*(y*z)**172+f2112*x**2*(y*z)**172+
f0213*x**0*(y*z)**173+f1213*x**1*(y*z)**173+f2213*x**2*(y*z)**173+
f0314*x**0*(y*z)**174+f1314*x**1*(y*z)**174+f2314*x**2*(y*z)**174+
f0415*x**0*(y*z)**175+f1415*x**1*(y*z)**175+f2415*x**2*(y*z)**175+
f0516*x**0*(y*z)**176+f1516*x**1*(y*z)**176+f2516*x**2*(y*z)**176+
f0617*x**0*(y*z)**177+f1617*x**1*(y*z)**177+f2617*x**2*(y*z)**177+
f0718*x**0*(y*z)**178+f1718*x**1*(y*z)**178+f2718*x**2*(y*z)**178+
f0819*x**0*(y*z)**179+f1819*x**1*(y*z)**179+f2819*x**2*(y*z)**179+
f0020*x**0*(y*z)**180+f1020*x**1*(y*z)**180+f2020*x**2*(y*z)**180+
f0121*x**0*(y*z)**181+f1121*x**1*(y*z)**181+f2121*x**2*(y*z)**181+
f0222*x**0*(y*z)**182+f1222*x**1*(y*z)**182+f2222*x**2*(y*z)**182+
f0323*x**0*(y*z)**183+f1323*x**1*(y*z)**183+f2323*x**2*(y*z)**183+
f0424*x**0*(y*z)**184+f1424*x**1*(y*z)**184+f2424*x**2*(y*z)**184+
f0525*x**0*(y*z)**185+f1525*x**1*(y*z)**185+f2525*x**2*(y*z)**185+
f0626*x**0*(y*z)**186+f1626*x**1*(y*z)**186+f2626*x**2*(y*z)**186+
f0727*x**0*(y*z)**187+f1727*x**1*(y*z)**187+f2727*x**2*(y*z)**187+
f0828*x**0*(y*z)**188+f1828*x**1*(y*z)**188+f2828*x**2*(y*z)**188+
f0029*x**0*(y*z)**189+f1029*x**1*(y*z)**189+f2029*x**2*(y*z)**189+
f0130*x**0*(y*z)**190+f1130*x**1*(y*z)**190+f2130*x**2*(y*z)**190+
f0231*x**0*(y*z)**191+f1231*x**1*(y*z)**191+f2231*x**2*(y*z)**191+
f0300*x**0*(y*z)**192+f1300*x**1*(y*z)**192+f2300*x**2*(y*z)**192+
f0401*x**0*(y*z)**193+f1401*x**1*(y*z)**193+f2401*x**2*(y*z)**193+
f0502*x**0*(y*z)**194+f1502*x**1*(y*z)**194+f2502*x**2*(y*z)**194+
f0603*x**0*(y*z)**195+f1603*x**1*(y*z)**195+f2603*x**2*(y*z)**195+
f0704*x**0*(y*z)**196+f1704*x**1*(y*z)**196+f2704*x**2*(y*z)**196+
f0805*x**0*(y*z)**197+f1805*x**1*(y*z)**197+f2805*x**2*(y*z)**197+
f0006*x**0*(y*z)**198+f1006*x**1*(y*z)**198+f2006*x**2*(y*z)**198+
f0107*x**0*(y*z)**199+f1107*x**1*(y*z)**199+f2107*x**2*(y*z)**199+
f0208*x**0*(y*z)**200+f1208*x**1*(y*z)**200+f2208*x**2*(y*z)**200+
f0309*x**0*(y*z)**201+f1309*x**1*(y*z)**201+f2309*x**2*(y*z)**201+
f0410*x**0*(y*z)**202+f1410*x**1*(y*z)**202+f2410*x**2*(y*z)**202+
f0511*x**0*(y*z)**203+f1511*x**1*(y*z)**203+f2511*x**2*(y*z)**203+
f0612*x**0*(y*z)**204+f1612*x**1*(y*z)**204+f2612*x**2*(y*z)**204+
f0713*x**0*(y*z)**205+f1713*x**1*(y*z)**205+f2713*x**2*(y*z)**205+
f0814*x**0*(y*z)**206+f1814*x**1*(y*z)**206+f2814*x**2*(y*z)**206+
f0015*x**0*(y*z)**207+f1015*x**1*(y*z)**207+f2015*x**2*(y*z)**207+
f0116*x**0*(y*z)**208+f1116*x**1*(y*z)**208+f2116*x**2*(y*z)**208+
f0217*x**0*(y*z)**209+f1217*x**1*(y*z)**209+f2217*x**2*(y*z)**209+
f0318*x**0*(y*z)**210+f1318*x**1*(y*z)**210+f2318*x**2*(y*z)**210+
f0419*x**0*(y*z)**211+f1419*x**1*(y*z)**211+f2419*x**2*(y*z)**211+
f0520*x**0*(y*z)**212+f1520*x**1*(y*z)**212+f2520*x**2*(y*z)**212+
f0621*x**0*(y*z)**213+f1621*x**1*(y*z)**213+f2621*x**2*(y*z)**213+
f0722*x**0*(y*z)**214+f1722*x**1*(y*z)**214+f2722*x**2*(y*z)**214+
f0823*x**0*(y*z)**215+f1823*x**1*(y*z)**215+f2823*x**2*(y*z)**215+
f0024*x**0*(y*z)**216+f1024*x**1*(y*z)**216+f2024*x**2*(y*z)**216+
f0125*x**0*(y*z)**217+f1125*x**1*(y*z)**217+f2125*x**2*(y*z)**217+
f0226*x**0*(y*z)**218+f1226*x**1*(y*z)**218+f2226*x**2*(y*z)**218+
f0327*x**0*(y*z)**219+f1327*x**1*(y*z)**219+f2327*x**2*(y*z)**219+
f0428*x**0*(y*z)**220+f1428*x**1*(y*z)**220+f2428*x**2*(y*z)**220+
f0529*x**0*(y*z)**221+f1529*x**1*(y*z)**221+f2529*x**2*(y*z)**221+
f0630*x**0*(y*z)**222+f1630*x**1*(y*z)**222+f2630*x**2*(y*z)**222+
f0731*x**0*(y*z)**223+f1731*x**1*(y*z)**223+f2731*x**2*(y*z)**223+
f0800*x**0*(y*z)**224+f1800*x**1*(y*z)**224+f2800*x**2*(y*z)**224+
f0001*x**0*(y*z)**225+f1001*x**1*(y*z)**225+f2001*x**2*(y*z)**225+
f0102*x**0*(y*z)**226+f1102*x**1*(y*z)**226+f2102*x**2*(y*z)**226+
f0203*x**0*(y*z)**227+f1203*x**1*(y*z)**227+f2203*x**2*(y*z)**227+
f0304*x**0*(y*z)**228+f1304*x**1*(y*z)**228+f2304*x**2*(y*z)**228+
f0405*x**0*(y*z)**229+f1405*x**1*(y*z)**229+f2405*x**2*(y*z)**229+
f0506*x**0*(y*z)**230+f1506*x**1*(y*z)**230+f2506*x**2*(y*z)**230+
f0607*x**0*(y*z)**231+f1607*x**1*(y*z)**231+f2607*x**2*(y*z)**231+
f0708*x**0*(y*z)**232+f1708*x**1*(y*z)**232+f2708*x**2*(y*z)**232+
f0809*x**0*(y*z)**233+f1809*x**1*(y*z)**233+f2809*x**2*(y*z)**233+
f0010*x**0*(y*z)**234+f1010*x**1*(y*z)**234+f2010*x**2*(y*z)**234+
f0111*x**0*(y*z)**235+f1111*x**1*(y*z)**235+f2111*x**2*(y*z)**235+
f0212*x**0*(y*z)**236+f1212*x**1*(y*z)**236+f2212*x**2*(y*z)**236+
f0313*x**0*(y*z)**237+f1313*x**1*(y*z)**237+f2313*x**2*(y*z)**237+
f0414*x**0*(y*z)**238+f1414*x**1*(y*z)**238+f2414*x**2*(y*z)**238+
f0515*x**0*(y*z)**239+f1515*x**1*(y*z)**239+f2515*x**2*(y*z)**239+
f0616*x**0*(y*z)**240+f1616*x**1*(y*z)**240+f2616*x**2*(y*z)**240+
f0717*x**0*(y*z)**241+f1717*x**1*(y*z)**241+f2717*x**2*(y*z)**241+
f0818*x**0*(y*z)**242+f1818*x**1*(y*z)**242+f2818*x**2*(y*z)**242+
f0019*x**0*(y*z)**243+f1019*x**1*(y*z)**243+f2019*x**2*(y*z)**243+
f0120*x**0*(y*z)**244+f1120*x**1*(y*z)**244+f2120*x**2*(y*z)**244+
f0221*x**0*(y*z)**245+f1221*x**1*(y*z)**245+f2221*x**2*(y*z)**245+
f0322*x**0*(y*z)**246+f1322*x**1*(y*z)**246+f2322*x**2*(y*z)**246+
f0423*x**0*(y*z)**247+f1423*x**1*(y*z)**247+f2423*x**2*(y*z)**247+
f0524*x**0*(y*z)**248+f1524*x**1*(y*z)**248+f2524*x**2*(y*z)**248+
f0625*x**0*(y*z)**249+f1625*x**1*(y*z)**249+f2625*x**2*(y*z)**249+
f0726*x**0*(y*z)**250+f1726*x**1*(y*z)**250+f2726*x**2*(y*z)**250+
f0827*x**0*(y*z)**251+f1827*x**1*(y*z)**251+f2827*x**2*(y*z)**251+
f0028*x**0*(y*z)**252+f1028*x**1*(y*z)**252+f2028*x**2*(y*z)**252+
f0129*x**0*(y*z)**253+f1129*x**1*(y*z)**253+f2129*x**2*(y*z)**253+
f0230*x**0*(y*z)**254+f1230*x**1*(y*z)**254+f2230*x**2*(y*z)**254+
f0331*x**0*(y*z)**255+f1331*x**1*(y*z)**255+f2331*x**2*(y*z)**255+
f0400*x**0*(y*z)**256+f1400*x**1*(y*z)**256+f2400*x**2*(y*z)**256+
f0501*x**0*(y*z)**257+f1501*x**1*(y*z)**257+f2501*x**2*(y*z)**257+
f0602*x**0*(y*z)**258+f1602*x**1*(y*z)**258+f2602*x**2*(y*z)**258+
f0703*x**0*(y*z)**259+f1703*x**1*(y*z)**259+f2703*x**2*(y*z)**259+
f0804*x**0*(y*z)**260+f1804*x**1*(y*z)**260+f2804*x**2*(y*z)**260+
f0005*x**0*(y*z)**261+f1005*x**1*(y*z)**261+f2005*x**2*(y*z)**261+
f0106*x**0*(y*z)**262+f1106*x**1*(y*z)**262+f2106*x**2*(y*z)**262+
f0207*x**0*(y*z)**263+f1207*x**1*(y*z)**263+f2207*x**2*(y*z)**263+
f0308*x**0*(y*z)**264+f1308*x**1*(y*z)**264+f2308*x**2*(y*z)**264+
f0409*x**0*(y*z)**265+f1409*x**1*(y*z)**265+f2409*x**2*(y*z)**265+
f0510*x**0*(y*z)**266+f1510*x**1*(y*z)**266+f2510*x**2*(y*z)**266+
f0611*x**0*(y*z)**267+f1611*x**1*(y*z)**267+f2611*x**2*(y*z)**267+
f0712*x**0*(y*z)**268+f1712*x**1*(y*z)**268+f2712*x**2*(y*z)**268+
f0813*x**0*(y*z)**269+f1813*x**1*(y*z)**269+f2813*x**2*(y*z)**269+
f0014*x**0*(y*z)**270+f1014*x**1*(y*z)**270+f2014*x**2*(y*z)**270+
f0115*x**0*(y*z)**271+f1115*x**1*(y*z)**271+f2115*x**2*(y*z)**271+
f0216*x**0*(y*z)**272+f1216*x**1*(y*z)**272+f2216*x**2*(y*z)**272+
f0317*x**0*(y*z)**273+f1317*x**1*(y*z)**273+f2317*x**2*(y*z)**273+
f0418*x**0*(y*z)**274+f1418*x**1*(y*z)**274+f2418*x**2*(y*z)**274+
f0519*x**0*(y*z)**275+f1519*x**1*(y*z)**275+f2519*x**2*(y*z)**275+
f0620*x**0*(y*z)**276+f1620*x**1*(y*z)**276+f2620*x**2*(y*z)**276+
f0721*x**0*(y*z)**277+f1721*x**1*(y*z)**277+f2721*x**2*(y*z)**277+
f0822*x**0*(y*z)**278+f1822*x**1*(y*z)**278+f2822*x**2*(y*z)**278+
f0023*x**0*(y*z)**279+f1023*x**1*(y*z)**279+f2023*x**2*(y*z)**279+
f0124*x**0*(y*z)**280+f1124*x**1*(y*z)**280+f2124*x**2*(y*z)**280+
f0225*x**0*(y*z)**281+f1225*x**1*(y*z)**281+f2225*x**2*(y*z)**281+
f0326*x**0*(y*z)**282+f1326*x**1*(y*z)**282+f2326*x**2*(y*z)**282+
f0427*x**0*(y*z)**283+f1427*x**1*(y*z)**283+f2427*x**2*(y*z)**283+
f0528*x**0*(y*z)**284+f1528*x**1*(y*z)**284+f2528*x**2*(y*z)**284+
f0629*x**0*(y*z)**285+f1629*x**1*(y*z)**285+f2629*x**2*(y*z)**285+
f0730*x**0*(y*z)**286+f1730*x**1*(y*z)**286+f2730*x**2*(y*z)**286+
f0831*x**0*(y*z)**287+f1831*x**1*(y*z)**287+f2831*x**2*(y*z)**287,
CF0**2 =
cf0000*x**0*(y*z)**  0+cf0101*x**0*(y*z)**  1+cf0202*x**0*(y*z)**  2+
cf0303*x**0*(y*z)**  3+cf0404*x**0*(y*z)**  4+cf0505*x**0*(y*z)**  5+
cf0606*x**0*(y*z)**  6+cf0707*x**0*(y*z)**  7+cf0808*x**0*(y*z)**  8+
cf0009*x**0*(y*z)**  9+cf0110*x**0*(y*z)** 10+cf0211*x**0*(y*z)** 11+
cf0312*x**0*(y*z)** 12+cf0413*x**0*(y*z)** 13+cf0514*x**0*(y*z)** 14+
cf0615*x**0*(y*z)** 15+cf0716*x**0*(y*z)** 16+cf0817*x**0*(y*z)** 17+
cf0018*x**0*(y*z)** 18+cf0119*x**0*(y*z)** 19+cf0220*x**0*(y*z)** 20+
cf0321*x**0*(y*z)** 21+cf0422*x**0*(y*z)** 22+cf0523*x**0*(y*z)** 23+
cf0624*x**0*(y*z)** 24+cf0725*x**0*(y*z)** 25+cf0826*x**0*(y*z)** 26+
cf0027*x**0*(y*z)** 27+cf0128*x**0*(y*z)** 28+cf0229*x**0*(y*z)** 29+
cf0330*x**0*(y*z)** 30+cf0431*x**0*(y*z)** 31+cf0500*x**0*(y*z)** 32+
cf0601*x**0*(y*z)** 33+cf0702*x**0*(y*z)** 34+cf0803*x**0*(y*z)** 35+
cf0004*x**0*(y*z)** 36+cf0105*x**0*(y*z)** 37+cf0206*x**0*(y*z)** 38+
cf0307*x**0*(y*z)** 39+cf0408*x**0*(y*z)** 40+cf0509*x**0*(y*z)** 41+
cf0610*x**0*(y*z)** 42+cf0711*x**0*(y*z)** 43+cf0812*x**0*(y*z)** 44+
cf0013*x**0*(y*z)** 45+cf0114*x**0*(y*z)** 46+cf0215*x**0*(y*z)** 47+
cf0316*x**0*(y*z)** 48+cf0417*x**0*(y*z)** 49+cf0518*x**0*(y*z)** 50+
cf0619*x**0*(y*z)** 51+cf0720*x**0*(y*z)** 52+cf0821*x**0*(y*z)** 53+
cf0022*x**0*(y*z)** 54+cf0123*x**0*(y*z)** 55+cf0224*x**0*(y*z)** 56+
cf0325*x**0*(y*z)** 57+cf0426*x**0*(y*z)** 58+cf0527*x**0*(y*z)** 59+
cf0628*x**0*(y*z)** 60+cf0729*x**0*(y*z)** 61+cf0830*x**0*(y*z)** 62+
cf0031*x**0*(y*z)** 63+cf0100*x**0*(y*z)** 64+cf0201*x**0*(y*z)** 65+
cf0302*x**0*(y*z)** 66+cf0403*x**0*(y*z)** 67+cf0504*x**0*(y*z)** 68+
cf0605*x**0*(y*z)** 69+cf0706*x**0*(y*z)** 70+cf0807*x**0*(y*z)** 71+
cf0008*x**0*(y*z)** 72+cf0109*x**0*(y*z)** 73+cf0210*x**0*(y*z)** 74+
cf0311*x**0*(y*z)** 75+cf0412*x**0*(y*z)** 76+cf0513*x**0*(y*z)** 77+
cf0614*x**0*(y*z)** 78+cf0715*x**0*(y*z)** 79+cf0816*x**0*(y*z)** 80+
cf0017*x**0*(y*z)** 81+cf0118*x**0*(y*z)** 82+cf0219*x**0*(y*z)** 83+
cf0320*x**0*(y*z)** 84+cf0421*x**0*(y*z)** 85+cf0522*x**0*(y*z)** 86+
cf0623*x**0*(y*z)** 87+cf0724*x**0*(y*z)** 88+cf0825*x**0*(y*z)** 89+
cf0026*x**0*(y*z)** 90+cf0127*x**0*(y*z)** 91+cf0228*x**0*(y*z)** 92+
cf0329*x**0*(y*z)** 93+cf0430*x**0*(y*z)** 94+cf0531*x**0*(y*z)** 95+
cf0600*x**0*(y*z)** 96+cf0701*x**0*(y*z)** 97+cf0802*x**0*(y*z)** 98+
cf0003*x**0*(y*z)** 99+cf0104*x**0*(y*z)**100+cf0205*x**0*(y*z)**101+
cf0306*x**0*(y*z)**102+cf0407*x**0*(y*z)**103+cf0508*x**0*(y*z)**104+
cf0609*x**0*(y*z)**105+cf0710*x**0*(y*z)**106+cf0811*x**0*(y*z)**107+
cf0012*x**0*(y*z)**108+cf0113*x**0*(y*z)**109+cf0214*x**0*(y*z)**110+
cf0315*x**0*(y*z)**111+cf0416*x**0*(y*z)**112+cf0517*x**0*(y*z)**113+
cf0618*x**0*(y*z)**114+cf0719*x**0*(y*z)**115+cf0820*x**0*(y*z)**116+
cf0021*x**0*(y*z)**117+cf0122*x**0*(y*z)**118+cf0223*x**0*(y*z)**119+
cf0324*x**0*(y*z)**120+cf0425*x**0*(y*z)**121+cf0526*x**0*(y*z)**122+
cf0627*x**0*(y*z)**123+cf0728*x**0*(y*z)**124+cf0829*x**0*(y*z)**125+
cf0030*x**0*(y*z)**126+cf0131*x**0*(y*z)**127+cf0200*x**0*(y*z)**128+
cf0301*x**0*(y*z)**129+cf0402*x**0*(y*z)**130+cf0503*x**0*(y*z)**131+
cf0604*x**0*(y*z)**132+cf0705*x**0*(y*z)**133+cf0806*x**0*(y*z)**134+
cf0007*x**0*(y*z)**135+cf0108*x**0*(y*z)**136+cf0209*x**0*(y*z)**137+
cf0310*x**0*(y*z)**138+cf0411*x**0*(y*z)**139+cf0512*x**0*(y*z)**140+
cf0613*x**0*(y*z)**141+cf0714*x**0*(y*z)**142+cf0815*x**0*(y*z)**143+
cf0016*x**0*(y*z)**144+cf0117*x**0*(y*z)**145+cf0218*x**0*(y*z)**146+
cf0319*x**0*(y*z)**147+cf0420*x**0*(y*z)**148+cf0521*x**0*(y*z)**149+
cf0622*x**0*(y*z)**150+cf0723*x**0*(y*z)**151+cf0824*x**0*(y*z)**152+
cf0025*x**0*(y*z)**153+cf0126*x**0*(y*z)**154+cf0227*x**0*(y*z)**155+
cf0328*x**0*(y*z)**156+cf0429*x**0*(y*z)**157+cf0530*x**0*(y*z)**158+
cf0631*x**0*(y*z)**159+cf0700*x**0*(y*z)**160+cf0801*x**0*(y*z)**161+
cf0002*x**0*(y*z)**162+cf0103*x**0*(y*z)**163+cf0204*x**0*(y*z)**164+
cf0305*x**0*(y*z)**165+cf0406*x**0*(y*z)**166+cf0507*x**0*(y*z)**167+
cf0608*x**0*(y*z)**168+cf0709*x**0*(y*z)**169+cf0810*x**0*(y*z)**170+
cf0011*x**0*(y*z)**171+cf0112*x**0*(y*z)**172+cf0213*x**0*(y*z)**173+
cf0314*x**0*(y*z)**174+cf0415*x**0*(y*z)**175+cf0516*x**0*(y*z)**176+
cf0617*x**0*(y*z)**177+cf0718*x**0*(y*z)**178+cf0819*x**0*(y*z)**179+
cf0020*x**0*(y*z)**180+cf0121*x**0*(y*z)**181+cf0222*x**0*(y*z)**182+
cf0323*x**0*(y*z)**183+cf0424*x**0*(y*z)**184+cf0525*x**0*(y*z)**185+
cf0626*x**0*(y*z)**186+cf0727*x**0*(y*z)**187+cf0828*x**0*(y*z)**188+
cf0029*x**0*(y*z)**189+cf0130*x**0*(y*z)**190+cf0231*x**0*(y*z)**191+
cf0300*x**0*(y*z)**192+cf0401*x**0*(y*z)**193+cf0502*x**0*(y*z)**194+
cf0603*x**0*(y*z)**195+cf0704*x**0*(y*z)**196+cf0805*x**0*(y*z)**197+
cf0006*x**0*(y*z)**198+cf0107*x**0*(y*z)**199+cf0208*x**0*(y*z)**200+
cf0309*x**0*(y*z)**201+cf0410*x**0*(y*z)**202+cf0511*x**0*(y*z)**203+
cf0612*x**0*(y*z)**204+cf0713*x**0*(y*z)**205+cf0814*x**0*(y*z)**206+
cf0015*x**0*(y*z)**207+cf0116*x**0*(y*z)**208+cf0217*x**0*(y*z)**209+
cf0318*x**0*(y*z)**210+cf0419*x**0*(y*z)**211+cf0520*x**0*(y*z)**212+
cf0621*x**0*(y*z)**213+cf0722*x**0*(y*z)**214+cf0823*x**0*(y*z)**215+
cf0024*x**0*(y*z)**216+cf0125*x**0*(y*z)**217+cf0226*x**0*(y*z)**218+
cf0327*x**0*(y*z)**219+cf0428*x**0*(y*z)**220+cf0529*x**0*(y*z)**221+
cf0630*x**0*(y*z)**222+cf0731*x**0*(y*z)**223+cf0800*x**0*(y*z)**224+
cf0001*x**0*(y*z)**225+cf0102*x**0*(y*z)**226+cf0203*x**0*(y*z)**227+
cf0304*x**0*(y*z)**228+cf0405*x**0*(y*z)**229+cf0506*x**0*(y*z)**230+
cf0607*x**0*(y*z)**231+cf0708*x**0*(y*z)**232+cf0809*x**0*(y*z)**233+
cf0010*x**0*(y*z)**234+cf0111*x**0*(y*z)**235+cf0212*x**0*(y*z)**236+
cf0313*x**0*(y*z)**237+cf0414*x**0*(y*z)**238+cf0515*x**0*(y*z)**239+
cf0616*x**0*(y*z)**240+cf0717*x**0*(y*z)**241+cf0818*x**0*(y*z)**242+
cf0019*x**0*(y*z)**243+cf0120*x**0*(y*z)**244+cf0221*x**0*(y*z)**245+
cf0322*x**0*(y*z)**246+cf0423*x**0*(y*z)**247+cf0524*x**0*(y*z)**248+
cf0625*x**0*(y*z)**249+cf0726*x**0*(y*z)**250+cf0827*x**0*(y*z)**251+
cf0028*x**0*(y*z)**252+cf0129*x**0*(y*z)**253+cf0230*x**0*(y*z)**254+
cf0331*x**0*(y*z)**255+cf0400*x**0*(y*z)**256+cf0501*x**0*(y*z)**257+
cf0602*x**0*(y*z)**258+cf0703*x**0*(y*z)**259+cf0804*x**0*(y*z)**260+
cf0005*x**0*(y*z)**261+cf0106*x**0*(y*z)**262+cf0207*x**0*(y*z)**263+
cf0308*x**0*(y*z)**264+cf0409*x**0*(y*z)**265+cf0510*x**0*(y*z)**266+
cf0611*x**0*(y*z)**267+cf0712*x**0*(y*z)**268+cf0813*x**0*(y*z)**269+
cf0014*x**0*(y*z)**270+cf0115*x**0*(y*z)**271+cf0216*x**0*(y*z)**272+
cf0317*x**0*(y*z)**273+cf0418*x**0*(y*z)**274+cf0519*x**0*(y*z)**275+
cf0620*x**0*(y*z)**276+cf0721*x**0*(y*z)**277+cf0822*x**0*(y*z)**278+
cf0023*x**0*(y*z)**279+cf0124*x**0*(y*z)**280+cf0225*x**0*(y*z)**281+
cf0326*x**0*(y*z)**282+cf0427*x**0*(y*z)**283+cf0528*x**0*(y*z)**284+
cf0629*x**0*(y*z)**285+cf0730*x**0*(y*z)**286+cf0831*x**0*(y*z)**287,
CF1**2 =
cf1000*x**1*(y*z)**  0+cf1101*x**1*(y*z)**  1+cf1202*x**1*(y*z)**  2+
cf1303*x**1*(y*z)**  3+cf1404*x**1*(y*z)**  4+cf1505*x**1*(y*z)**  5+
cf1606*x**1*(y*z)**  6+cf1707*x**1*(y*z)**  7+cf1808*x**1*(y*z)**  8+
cf1009*x**1*(y*z)**  9+cf1110*x**1*(y*z)** 10+cf1211*x**1*(y*z)** 11+
cf1312*x**1*(y*z)** 12+cf1413*x**1*(y*z)** 13+cf1514*x**1*(y*z)** 14+
cf1615*x**1*(y*z)** 15+cf1716*x**1*(y*z)** 16+cf1817*x**1*(y*z)** 17+
cf1018*x**1*(y*z)** 18+cf1119*x**1*(y*z)** 19+cf1220*x**1*(y*z)** 20+
cf1321*x**1*(y*z)** 21+cf1422*x**1*(y*z)** 22+cf1523*x**1*(y*z)** 23+
cf1624*x**1*(y*z)** 24+cf1725*x**1*(y*z)** 25+cf1826*x**1*(y*z)** 26+
cf1027*x**1*(y*z)** 27+cf1128*x**1*(y*z)** 28+cf1229*x**1*(y*z)** 29+
cf1330*x**1*(y*z)** 30+cf1431*x**1*(y*z)** 31+cf1500*x**1*(y*z)** 32+
cf1601*x**1*(y*z)** 33+cf1702*x**1*(y*z)** 34+cf1803*x**1*(y*z)** 35+
cf1004*x**1*(y*z)** 36+cf1105*x**1*(y*z)** 37+cf1206*x**1*(y*z)** 38+
cf1307*x**1*(y*z)** 39+cf1408*x**1*(y*z)** 40+cf1509*x**1*(y*z)** 41+
cf1610*x**1*(y*z)** 42+cf1711*x**1*(y*z)** 43+cf1812*x**1*(y*z)** 44+
cf1013*x**1*(y*z)** 45+cf1114*x**1*(y*z)** 46+cf1215*x**1*(y*z)** 47+
cf1316*x**1*(y*z)** 48+cf1417*x**1*(y*z)** 49+cf1518*x**1*(y*z)** 50+
cf1619*x**1*(y*z)** 51+cf1720*x**1*(y*z)** 52+cf1821*x**1*(y*z)** 53+
cf1022*x**1*(y*z)** 54+cf1123*x**1*(y*z)** 55+cf1224*x**1*(y*z)** 56+
cf1325*x**1*(y*z)** 57+cf1426*x**1*(y*z)** 58+cf1527*x**1*(y*z)** 59+
cf1628*x**1*(y*z)** 60+cf1729*x**1*(y*z)** 61+cf1830*x**1*(y*z)** 62+
cf1031*x**1*(y*z)** 63+cf1100*x**1*(y*z)** 64+cf1201*x**1*(y*z)** 65+
cf1302*x**1*(y*z)** 66+cf1403*x**1*(y*z)** 67+cf1504*x**1*(y*z)** 68+
cf1605*x**1*(y*z)** 69+cf1706*x**1*(y*z)** 70+cf1807*x**1*(y*z)** 71+
cf1008*x**1*(y*z)** 72+cf1109*x**1*(y*z)** 73+cf1210*x**1*(y*z)** 74+
cf1311*x**1*(y*z)** 75+cf1412*x**1*(y*z)** 76+cf1513*x**1*(y*z)** 77+
cf1614*x**1*(y*z)** 78+cf1715*x**1*(y*z)** 79+cf1816*x**1*(y*z)** 80+
cf1017*x**1*(y*z)** 81+cf1118*x**1*(y*z)** 82+cf1219*x**1*(y*z)** 83+
cf1320*x**1*(y*z)** 84+cf1421*x**1*(y*z)** 85+cf1522*x**1*(y*z)** 86+
cf1623*x**1*(y*z)** 87+cf1724*x**1*(y*z)** 88+cf1825*x**1*(y*z)** 89+
cf1026*x**1*(y*z)** 90+cf1127*x**1*(y*z)** 91+cf1228*x**1*(y*z)** 92+
cf1329*x**1*(y*z)** 93+cf1430*x**1*(y*z)** 94+cf1531*x**1*(y*z)** 95+
cf1600*x**1*(y*z)** 96+cf1701*x**1*(y*z)** 97+cf1802*x**1*(y*z)** 98+
cf1003*x**1*(y*z)** 99+cf1104*x**1*(y*z)**100+cf1205*x**1*(y*z)**101+
cf1306*x**1*(y*z)**102+cf1407*x**1*(y*z)**103+cf1508*x**1*(y*z)**104+
cf1609*x**1*(y*z)**105+cf1710*x**1*(y*z)**106+cf1811*x**1*(y*z)**107+
cf1012*x**1*(y*z)**108+cf1113*x**1*(y*z)**109+cf1214*x**1*(y*z)**110+
cf1315*x**1*(y*z)**111+cf1416*x**1*(y*z)**112+cf1517*x**1*(y*z)**113+
cf1618*x**1*(y*z)**114+cf1719*x**1*(y*z)**115+cf1820*x**1*(y*z)**116+
cf1021*x**1*(y*z)**117+cf1122*x**1*(y*z)**118+cf1223*x**1*(y*z)**119+
cf1324*x**1*(y*z)**120+cf1425*x**1*(y*z)**121+cf1526*x**1*(y*z)**122+
cf1627*x**1*(y*z)**123+cf1728*x**1*(y*z)**124+cf1829*x**1*(y*z)**125+
cf1030*x**1*(y*z)**126+cf1131*x**1*(y*z)**127+cf1200*x**1*(y*z)**128+
cf1301*x**1*(y*z)**129+cf1402*x**1*(y*z)**130+cf1503*x**1*(y*z)**131+
cf1604*x**1*(y*z)**132+cf1705*x**1*(y*z)**133+cf1806*x**1*(y*z)**134+
cf1007*x**1*(y*z)**135+cf1108*x**1*(y*z)**136+cf1209*x**1*(y*z)**137+
cf1310*x**1*(y*z)**138+cf1411*x**1*(y*z)**139+cf1512*x**1*(y*z)**140+
cf1613*x**1*(y*z)**141+cf1714*x**1*(y*z)**142+cf1815*x**1*(y*z)**143+
cf1016*x**1*(y*z)**144+cf1117*x**1*(y*z)**145+cf1218*x**1*(y*z)**146+
cf1319*x**1*(y*z)**147+cf1420*x**1*(y*z)**148+cf1521*x**1*(y*z)**149+
cf1622*x**1*(y*z)**150+cf1723*x**1*(y*z)**151+cf1824*x**1*(y*z)**152+
cf1025*x**1*(y*z)**153+cf1126*x**1*(y*z)**154+cf1227*x**1*(y*z)**155+
cf1328*x**1*(y*z)**156+cf1429*x**1*(y*z)**157+cf1530*x**1*(y*z)**158+
cf1631*x**1*(y*z)**159+cf1700*x**1*(y*z)**160+cf1801*x**1*(y*z)**161+
cf1002*x**1*(y*z)**162+cf1103*x**1*(y*z)**163+cf1204*x**1*(y*z)**164+
cf1305*x**1*(y*z)**165+cf1406*x**1*(y*z)**166+cf1507*x**1*(y*z)**167+
cf1608*x**1*(y*z)**168+cf1709*x**1*(y*z)**169+cf1810*x**1*(y*z)**170+
cf1011*x**1*(y*z)**171+cf1112*x**1*(y*z)**172+cf1213*x**1*(y*z)**173+
cf1314*x**1*(y*z)**174+cf1415*x**1*(y*z)**175+cf1516*x**1*(y*z)**176+
cf1617*x**1*(y*z)**177+cf1718*x**1*(y*z)**178+cf1819*x**1*(y*z)**179+
cf1020*x**1*(y*z)**180+cf1121*x**1*(y*z)**181+cf1222*x**1*(y*z)**182+
cf1323*x**1*(y*z)**183+cf1424*x**1*(y*z)**184+cf1525*x**1*(y*z)**185+
cf1626*x**1*(y*z)**186+cf1727*x**1*(y*z)**187+cf1828*x**1*(y*z)**188+
cf1029*x**1*(y*z)**189+cf1130*x**1*(y*z)**190+cf1231*x**1*(y*z)**191+
cf1300*x**1*(y*z)**192+cf1401*x**1*(y*z)**193+cf1502*x**1*(y*z)**194+
cf1603*x**1*(y*z)**195+cf1704*x**1*(y*z)**196+cf1805*x**1*(y*z)**197+
cf1006*x**1*(y*z)**198+cf1107*x**1*(y*z)**199+cf1208*x**1*(y*z)**200+
cf1309*x**1*(y*z)**201+cf1410*x**1*(y*z)**202+cf1511*x**1*(y*z)**203+
cf1612*x**1*(y*z)**204+cf1713*x**1*(y*z)**205+cf1814*x**1*(y*z)**206+
cf1015*x**1*(y*z)**207+cf1116*x**1*(y*z)**208+cf1217*x**1*(y*z)**209+
cf1318*x**1*(y*z)**210+cf1419*x**1*(y*z)**211+cf1520*x**1*(y*z)**212+
cf1621*x**1*(y*z)**213+cf1722*x**1*(y*z)**214+cf1823*x**1*(y*z)**215+
cf1024*x**1*(y*z)**216+cf1125*x**1*(y*z)**217+cf1226*x**1*(y*z)**218+
cf1327*x**1*(y*z)**219+cf1428*x**1*(y*z)**220+cf1529*x**1*(y*z)**221+
cf1630*x**1*(y*z)**222+cf1731*x**1*(y*z)**223+cf1800*x**1*(y*z)**224+
cf1001*x**1*(y*z)**225+cf1102*x**1*(y*z)**226+cf1203*x**1*(y*z)**227+
cf1304*x**1*(y*z)**228+cf1405*x**1*(y*z)**229+cf1506*x**1*(y*z)**230+
cf1607*x**1*(y*z)**231+cf1708*x**1*(y*z)**232+cf1809*x**1*(y*z)**233+
cf1010*x**1*(y*z)**234+cf1111*x**1*(y*z)**235+cf1212*x**1*(y*z)**236+
cf1313*x**1*(y*z)**237+cf1414*x**1*(y*z)**238+cf1515*x**1*(y*z)**239+
cf1616*x**1*(y*z)**240+cf1717*x**1*(y*z)**241+cf1818*x**1*(y*z)**242+
cf1019*x**1*(y*z)**243+cf1120*x**1*(y*z)**244+cf1221*x**1*(y*z)**245+
cf1322*x**1*(y*z)**246+cf1423*x**1*(y*z)**247+cf1524*x**1*(y*z)**248+
cf1625*x**1*(y*z)**249+cf1726*x**1*(y*z)**250+cf1827*x**1*(y*z)**251+
cf1028*x**1*(y*z)**252+cf1129*x**1*(y*z)**253+cf1230*x**1*(y*z)**254+
cf1331*x**1*(y*z)**255+cf1400*x**1*(y*z)**256+cf1501*x**1*(y*z)**257+
cf1602*x**1*(y*z)**258+cf1703*x**1*(y*z)**259+cf1804*x**1*(y*z)**260+
cf1005*x**1*(y*z)**261+cf1106*x**1*(y*z)**262+cf1207*x**1*(y*z)**263+
cf1308*x**1*(y*z)**264+cf1409*x**1*(y*z)**265+cf1510*x**1*(y*z)**266+
cf1611*x**1*(y*z)**267+cf1712*x**1*(y*z)**268+cf1813*x**1*(y*z)**269+
cf1014*x**1*(y*z)**270+cf1115*x**1*(y*z)**271+cf1216*x**1*(y*z)**272+
cf1317*x**1*(y*z)**273+cf1418*x**1*(y*z)**274+cf1519*x**1*(y*z)**275+
cf1620*x**1*(y*z)**276+cf1721*x**1*(y*z)**277+cf1822*x**1*(y*z)**278+
cf1023*x**1*(y*z)**279+cf1124*x**1*(y*z)**280+cf1225*x**1*(y*z)**281+
cf1326*x**1*(y*z)**282+cf1427*x**1*(y*z)**283+cf1528*x**1*(y*z)**284+
cf1629*x**1*(y*z)**285+cf1730*x**1*(y*z)**286+cf1831*x**1*(y*z)**287,
CF2**2 =
cf2000*x**2*(y*z)**  0+cf2101*x**2*(y*z)**  1+cf2202*x**2*(y*z)**  2+
cf2303*x**2*(y*z)**  3+cf2404*x**2*(y*z)**  4+cf2505*x**2*(y*z)**  5+
cf2606*x**2*(y*z)**  6+cf2707*x**2*(y*z)**  7+cf2808*x**2*(y*z)**  8+
cf2009*x**2*(y*z)**  9+cf2110*x**2*(y*z)** 10+cf2211*x**2*(y*z)** 11+
cf2312*x**2*(y*z)** 12+cf2413*x**2*(y*z)** 13+cf2514*x**2*(y*z)** 14+
cf2615*x**2*(y*z)** 15+cf2716*x**2*(y*z)** 16+cf2817*x**2*(y*z)** 17+
cf2018*x**2*(y*z)** 18+cf2119*x**2*(y*z)** 19+cf2220*x**2*(y*z)** 20+
cf2321*x**2*(y*z)** 21+cf2422*x**2*(y*z)** 22+cf2523*x**2*(y*z)** 23+
cf2624*x**2*(y*z)** 24+cf2725*x**2*(y*z)** 25+cf2826*x**2*(y*z)** 26+
cf2027*x**2*(y*z)** 27+cf2128*x**2*(y*z)** 28+cf2229*x**2*(y*z)** 29+
cf2330*x**2*(y*z)** 30+cf2431*x**2*(y*z)** 31+cf2500*x**2*(y*z)** 32+
cf2601*x**2*(y*z)** 33+cf2702*x**2*(y*z)** 34+cf2803*x**2*(y*z)** 35+
cf2004*x**2*(y*z)** 36+cf2105*x**2*(y*z)** 37+cf2206*x**2*(y*z)** 38+
cf2307*x**2*(y*z)** 39+cf2408*x**2*(y*z)** 40+cf2509*x**2*(y*z)** 41+
cf2610*x**2*(y*z)** 42+cf2711*x**2*(y*z)** 43+cf2812*x**2*(y*z)** 44+
cf2013*x**2*(y*z)** 45+cf2114*x**2*(y*z)** 46+cf2215*x**2*(y*z)** 47+
cf2316*x**2*(y*z)** 48+cf2417*x**2*(y*z)** 49+cf2518*x**2*(y*z)** 50+
cf2619*x**2*(y*z)** 51+cf2720*x**2*(y*z)** 52+cf2821*x**2*(y*z)** 53+
cf2022*x**2*(y*z)** 54+cf2123*x**2*(y*z)** 55+cf2224*x**2*(y*z)** 56+
cf2325*x**2*(y*z)** 57+cf2426*x**2*(y*z)** 58+cf2527*x**2*(y*z)** 59+
cf2628*x**2*(y*z)** 60+cf2729*x**2*(y*z)** 61+cf2830*x**2*(y*z)** 62+
cf2031*x**2*(y*z)** 63+cf2100*x**2*(y*z)** 64+cf2201*x**2*(y*z)** 65+
cf2302*x**2*(y*z)** 66+cf2403*x**2*(y*z)** 67+cf2504*x**2*(y*z)** 68+
cf2605*x**2*(y*z)** 69+cf2706*x**2*(y*z)** 70+cf2807*x**2*(y*z)** 71+
cf2008*x**2*(y*z)** 72+cf2109*x**2*(y*z)** 73+cf2210*x**2*(y*z)** 74+
cf2311*x**2*(y*z)** 75+cf2412*x**2*(y*z)** 76+cf2513*x**2*(y*z)** 77+
cf2614*x**2*(y*z)** 78+cf2715*x**2*(y*z)** 79+cf2816*x**2*(y*z)** 80+
cf2017*x**2*(y*z)** 81+cf2118*x**2*(y*z)** 82+cf2219*x**2*(y*z)** 83+
cf2320*x**2*(y*z)** 84+cf2421*x**2*(y*z)** 85+cf2522*x**2*(y*z)** 86+
cf2623*x**2*(y*z)** 87+cf2724*x**2*(y*z)** 88+cf2825*x**2*(y*z)** 89+
cf2026*x**2*(y*z)** 90+cf2127*x**2*(y*z)** 91+cf2228*x**2*(y*z)** 92+
cf2329*x**2*(y*z)** 93+cf2430*x**2*(y*z)** 94+cf2531*x**2*(y*z)** 95+
cf2600*x**2*(y*z)** 96+cf2701*x**2*(y*z)** 97+cf2802*x**2*(y*z)** 98+
cf2003*x**2*(y*z)** 99+cf2104*x**2*(y*z)**100+cf2205*x**2*(y*z)**101+
cf2306*x**2*(y*z)**102+cf2407*x**2*(y*z)**103+cf2508*x**2*(y*z)**104+
cf2609*x**2*(y*z)**105+cf2710*x**2*(y*z)**106+cf2811*x**2*(y*z)**107+
cf2012*x**2*(y*z)**108+cf2113*x**2*(y*z)**109+cf2214*x**2*(y*z)**110+
cf2315*x**2*(y*z)**111+cf2416*x**2*(y*z)**112+cf2517*x**2*(y*z)**113+
cf2618*x**2*(y*z)**114+cf2719*x**2*(y*z)**115+cf2820*x**2*(y*z)**116+
cf2021*x**2*(y*z)**117+cf2122*x**2*(y*z)**118+cf2223*x**2*(y*z)**119+
cf2324*x**2*(y*z)**120+cf2425*x**2*(y*z)**121+cf2526*x**2*(y*z)**122+
cf2627*x**2*(y*z)**123+cf2728*x**2*(y*z)**124+cf2829*x**2*(y*z)**125+
cf2030*x**2*(y*z)**126+cf2131*x**2*(y*z)**127+cf2200*x**2*(y*z)**128+
cf2301*x**2*(y*z)**129+cf2402*x**2*(y*z)**130+cf2503*x**2*(y*z)**131+
cf2604*x**2*(y*z)**132+cf2705*x**2*(y*z)**133+cf2806*x**2*(y*z)**134+
cf2007*x**2*(y*z)**135+cf2108*x**2*(y*z)**136+cf2209*x**2*(y*z)**137+
cf2310*x**2*(y*z)**138+cf2411*x**2*(y*z)**139+cf2512*x**2*(y*z)**140+
cf2613*x**2*(y*z)**141+cf2714*x**2*(y*z)**142+cf2815*x**2*(y*z)**143+
cf2016*x**2*(y*z)**144+cf2117*x**2*(y*z)**145+cf2218*x**2*(y*z)**146+
cf2319*x**2*(y*z)**147+cf2420*x**2*(y*z)**148+cf2521*x**2*(y*z)**149+
cf2622*x**2*(y*z)**150+cf2723*x**2*(y*z)**151+cf2824*x**2*(y*z)**152+
cf2025*x**2*(y*z)**153+cf2126*x**2*(y*z)**154+cf2227*x**2*(y*z)**155+
cf2328*x**2*(y*z)**156+cf2429*x**2*(y*z)**157+cf2530*x**2*(y*z)**158+
cf2631*x**2*(y*z)**159+cf2700*x**2*(y*z)**160+cf2801*x**2*(y*z)**161+
cf2002*x**2*(y*z)**162+cf2103*x**2*(y*z)**163+cf2204*x**2*(y*z)**164+
cf2305*x**2*(y*z)**165+cf2406*x**2*(y*z)**166+cf2507*x**2*(y*z)**167+
cf2608*x**2*(y*z)**168+cf2709*x**2*(y*z)**169+cf2810*x**2*(y*z)**170+
cf2011*x**2*(y*z)**171+cf2112*x**2*(y*z)**172+cf2213*x**2*(y*z)**173+
cf2314*x**2*(y*z)**174+cf2415*x**2*(y*z)**175+cf2516*x**2*(y*z)**176+
cf2617*x**2*(y*z)**177+cf2718*x**2*(y*z)**178+cf2819*x**2*(y*z)**179+
cf2020*x**2*(y*z)**180+cf2121*x**2*(y*z)**181+cf2222*x**2*(y*z)**182+
cf2323*x**2*(y*z)**183+cf2424*x**2*(y*z)**184+cf2525*x**2*(y*z)**185+
cf2626*x**2*(y*z)**186+cf2727*x**2*(y*z)**187+cf2828*x**2*(y*z)**188+
cf2029*x**2*(y*z)**189+cf2130*x**2*(y*z)**190+cf2231*x**2*(y*z)**191+
cf2300*x**2*(y*z)**192+cf2401*x**2*(y*z)**193+cf2502*x**2*(y*z)**194+
cf2603*x**2*(y*z)**195+cf2704*x**2*(y*z)**196+cf2805*x**2*(y*z)**197+
cf2006*x**2*(y*z)**198+cf2107*x**2*(y*z)**199+cf2208*x**2*(y*z)**200+
cf2309*x**2*(y*z)**201+cf2410*x**2*(y*z)**202+cf2511*x**2*(y*z)**203+
cf2612*x**2*(y*z)**204+cf2713*x**2*(y*z)**205+cf2814*x**2*(y*z)**206+
cf2015*x**2*(y*z)**207+cf2116*x**2*(y*z)**208+cf2217*x**2*(y*z)**209+
cf2318*x**2*(y*z)**210+cf2419*x**2*(y*z)**211+cf2520*x**2*(y*z)**212+
cf2621*x**2*(y*z)**213+cf2722*x**2*(y*z)**214+cf2823*x**2*(y*z)**215+
cf2024*x**2*(y*z)**216+cf2125*x**2*(y*z)**217+cf2226*x**2*(y*z)**218+
cf2327*x**2*(y*z)**219+cf2428*x**2*(y*z)**220+cf2529*x**2*(y*z)**221+
cf2630*x**2*(y*z)**222+cf2731*x**2*(y*z)**223+cf2800*x**2*(y*z)**224+
cf2001*x**2*(y*z)**225+cf2102*x**2*(y*z)**226+cf2203*x**2*(y*z)**227+
cf2304*x**2*(y*z)**228+cf2405*x**2*(y*z)**229+cf2506*x**2*(y*z)**230+
cf2607*x**2*(y*z)**231+cf2708*x**2*(y*z)**232+cf2809*x**2*(y*z)**233+
cf2010*x**2*(y*z)**234+cf2111*x**2*(y*z)**235+cf2212*x**2*(y*z)**236+
cf2313*x**2*(y*z)**237+cf2414*x**2*(y*z)**238+cf2515*x**2*(y*z)**239+
cf2616*x**2*(y*z)**240+cf2717*x**2*(y*z)**241+cf2818*x**2*(y*z)**242+
cf2019*x**2*(y*z)**243+cf2120*x**2*(y*z)**244+cf2221*x**2*(y*z)**245+
cf2322*x**2*(y*z)**246+cf2423*x**2*(y*z)**247+cf2524*x**2*(y*z)**248+
cf2625*x**2*(y*z)**249+cf2726*x**2*(y*z)**250+cf2827*x**2*(y*z)**251+
cf2028*x**2*(y*z)**252+cf2129*x**2*(y*z)**253+cf2230*x**2*(y*z)**254+
cf2331*x**2*(y*z)**255+cf2400*x**2*(y*z)**256+cf2501*x**2*(y*z)**257+
cf2602*x**2*(y*z)**258+cf2703*x**2*(y*z)**259+cf2804*x**2*(y*z)**260+
cf2005*x**2*(y*z)**261+cf2106*x**2*(y*z)**262+cf2207*x**2*(y*z)**263+
cf2308*x**2*(y*z)**264+cf2409*x**2*(y*z)**265+cf2510*x**2*(y*z)**266+
cf2611*x**2*(y*z)**267+cf2712*x**2*(y*z)**268+cf2813*x**2*(y*z)**269+
cf2014*x**2*(y*z)**270+cf2115*x**2*(y*z)**271+cf2216*x**2*(y*z)**272+
cf2317*x**2*(y*z)**273+cf2418*x**2*(y*z)**274+cf2519*x**2*(y*z)**275+
cf2620*x**2*(y*z)**276+cf2721*x**2*(y*z)**277+cf2822*x**2*(y*z)**278+
cf2023*x**2*(y*z)**279+cf2124*x**2*(y*z)**280+cf2225*x**2*(y*z)**281+
cf2326*x**2*(y*z)**282+cf2427*x**2*(y*z)**283+cf2528*x**2*(y*z)**284+
cf2629*x**2*(y*z)**285+cf2730*x**2*(y*z)**286+cf2831*x**2*(y*z)**287
] && true;


(**************** CUT 308, - *****************)

ecut eqmod CF0**2+CF1**2+CF2**2 F**2 2048 prove with [ cuts [ 288 ] ];




(******************** output polynomial 0 ********************)

ghost
CG00@sint32,
cg00000@sint32, cg00100@sint32, cg00200@sint32, cg00001@sint32, cg00101@sint32,
cg00201@sint32, cg00002@sint32, cg00102@sint32, cg00202@sint32, cg00003@sint32,
cg00103@sint32, cg00203@sint32, cg00004@sint32, cg00104@sint32, cg00204@sint32,
cg00005@sint32, cg00105@sint32, cg00205@sint32, cg00006@sint32, cg00106@sint32,
cg00206@sint32, cg00007@sint32, cg00107@sint32, cg00207@sint32, cg00008@sint32,
cg00108@sint32, cg00208@sint32, cg00009@sint32, cg00109@sint32, cg00209@sint32,
cg00010@sint32, cg00110@sint32, cg00210@sint32, cg00011@sint32, cg00111@sint32,
cg00211@sint32, cg00012@sint32, cg00112@sint32, cg00212@sint32, cg00013@sint32,
cg00113@sint32, cg00213@sint32, cg00014@sint32, cg00114@sint32, cg00214@sint32,
cg00015@sint32, cg00115@sint32, cg00215@sint32, cg00016@sint32, cg00116@sint32,
cg00216@sint32, cg00017@sint32, cg00117@sint32, cg00217@sint32, cg00018@sint32,
cg00118@sint32, cg00218@sint32, cg00019@sint32, cg00119@sint32, cg00219@sint32,
cg00020@sint32, cg00120@sint32, cg00220@sint32, cg00021@sint32, cg00121@sint32,
cg00221@sint32, cg00022@sint32, cg00122@sint32, cg00222@sint32, cg00023@sint32,
cg00123@sint32, cg00223@sint32, cg00024@sint32, cg00124@sint32, cg00224@sint32,
cg00025@sint32, cg00125@sint32, cg00225@sint32, cg00026@sint32, cg00126@sint32,
cg00226@sint32, cg00027@sint32, cg00127@sint32, cg00227@sint32, cg00028@sint32,
cg00128@sint32, cg00228@sint32, cg00029@sint32, cg00129@sint32, cg00229@sint32,
cg00030@sint32, cg00130@sint32, cg00230@sint32, cg00031@sint32, cg00131@sint32,
cg00231@sint32,
CG01@sint32,
cg01000@sint32, cg01100@sint32, cg01200@sint32, cg01001@sint32, cg01101@sint32,
cg01201@sint32, cg01002@sint32, cg01102@sint32, cg01202@sint32, cg01003@sint32,
cg01103@sint32, cg01203@sint32, cg01004@sint32, cg01104@sint32, cg01204@sint32,
cg01005@sint32, cg01105@sint32, cg01205@sint32, cg01006@sint32, cg01106@sint32,
cg01206@sint32, cg01007@sint32, cg01107@sint32, cg01207@sint32, cg01008@sint32,
cg01108@sint32, cg01208@sint32, cg01009@sint32, cg01109@sint32, cg01209@sint32,
cg01010@sint32, cg01110@sint32, cg01210@sint32, cg01011@sint32, cg01111@sint32,
cg01211@sint32, cg01012@sint32, cg01112@sint32, cg01212@sint32, cg01013@sint32,
cg01113@sint32, cg01213@sint32, cg01014@sint32, cg01114@sint32, cg01214@sint32,
cg01015@sint32, cg01115@sint32, cg01215@sint32, cg01016@sint32, cg01116@sint32,
cg01216@sint32, cg01017@sint32, cg01117@sint32, cg01217@sint32, cg01018@sint32,
cg01118@sint32, cg01218@sint32, cg01019@sint32, cg01119@sint32, cg01219@sint32,
cg01020@sint32, cg01120@sint32, cg01220@sint32, cg01021@sint32, cg01121@sint32,
cg01221@sint32, cg01022@sint32, cg01122@sint32, cg01222@sint32, cg01023@sint32,
cg01123@sint32, cg01223@sint32, cg01024@sint32, cg01124@sint32, cg01224@sint32,
cg01025@sint32, cg01125@sint32, cg01225@sint32, cg01026@sint32, cg01126@sint32,
cg01226@sint32, cg01027@sint32, cg01127@sint32, cg01227@sint32, cg01028@sint32,
cg01128@sint32, cg01228@sint32, cg01029@sint32, cg01129@sint32, cg01229@sint32,
cg01030@sint32, cg01130@sint32, cg01230@sint32, cg01031@sint32, cg01131@sint32,
cg01231@sint32,
CG02@sint32,
cg02000@sint32, cg02100@sint32, cg02200@sint32, cg02001@sint32, cg02101@sint32,
cg02201@sint32, cg02002@sint32, cg02102@sint32, cg02202@sint32, cg02003@sint32,
cg02103@sint32, cg02203@sint32, cg02004@sint32, cg02104@sint32, cg02204@sint32,
cg02005@sint32, cg02105@sint32, cg02205@sint32, cg02006@sint32, cg02106@sint32,
cg02206@sint32, cg02007@sint32, cg02107@sint32, cg02207@sint32, cg02008@sint32,
cg02108@sint32, cg02208@sint32, cg02009@sint32, cg02109@sint32, cg02209@sint32,
cg02010@sint32, cg02110@sint32, cg02210@sint32, cg02011@sint32, cg02111@sint32,
cg02211@sint32, cg02012@sint32, cg02112@sint32, cg02212@sint32, cg02013@sint32,
cg02113@sint32, cg02213@sint32, cg02014@sint32, cg02114@sint32, cg02214@sint32,
cg02015@sint32, cg02115@sint32, cg02215@sint32, cg02016@sint32, cg02116@sint32,
cg02216@sint32, cg02017@sint32, cg02117@sint32, cg02217@sint32, cg02018@sint32,
cg02118@sint32, cg02218@sint32, cg02019@sint32, cg02119@sint32, cg02219@sint32,
cg02020@sint32, cg02120@sint32, cg02220@sint32, cg02021@sint32, cg02121@sint32,
cg02221@sint32, cg02022@sint32, cg02122@sint32, cg02222@sint32, cg02023@sint32,
cg02123@sint32, cg02223@sint32, cg02024@sint32, cg02124@sint32, cg02224@sint32,
cg02025@sint32, cg02125@sint32, cg02225@sint32, cg02026@sint32, cg02126@sint32,
cg02226@sint32, cg02027@sint32, cg02127@sint32, cg02227@sint32, cg02028@sint32,
cg02128@sint32, cg02228@sint32, cg02029@sint32, cg02129@sint32, cg02229@sint32,
cg02030@sint32, cg02130@sint32, cg02230@sint32, cg02031@sint32, cg02131@sint32,
cg02231@sint32 : and [
cg00000=L0x20014898,cg01000=L0x2001489c,cg02000=L0x200148a0,cg00100=L0x200148a4,
cg01100=L0x200148a8,cg02100=L0x200148ac,cg00200=L0x200148b0,cg01200=L0x200148b4,
cg02200=L0x200148b8,cg00001=L0x20014904,cg01001=L0x20014908,cg02001=L0x2001490c,
cg00101=L0x20014910,cg01101=L0x20014914,cg02101=L0x20014918,cg00201=L0x2001491c,
cg01201=L0x20014920,cg02201=L0x20014924,cg00002=L0x20014970,cg01002=L0x20014974,
cg02002=L0x20014978,cg00102=L0x2001497c,cg01102=L0x20014980,cg02102=L0x20014984,
cg00202=L0x20014988,cg01202=L0x2001498c,cg02202=L0x20014990,cg00003=L0x200149dc,
cg01003=L0x200149e0,cg02003=L0x200149e4,cg00103=L0x200149e8,cg01103=L0x200149ec,
cg02103=L0x200149f0,cg00203=L0x200149f4,cg01203=L0x200149f8,cg02203=L0x200149fc,
cg00004=L0x20014a48,cg01004=L0x20014a4c,cg02004=L0x20014a50,cg00104=L0x20014a54,
cg01104=L0x20014a58,cg02104=L0x20014a5c,cg00204=L0x20014a60,cg01204=L0x20014a64,
cg02204=L0x20014a68,cg00005=L0x20014ab4,cg01005=L0x20014ab8,cg02005=L0x20014abc,
cg00105=L0x20014ac0,cg01105=L0x20014ac4,cg02105=L0x20014ac8,cg00205=L0x20014acc,
cg01205=L0x20014ad0,cg02205=L0x20014ad4,cg00006=L0x20014b20,cg01006=L0x20014b24,
cg02006=L0x20014b28,cg00106=L0x20014b2c,cg01106=L0x20014b30,cg02106=L0x20014b34,
cg00206=L0x20014b38,cg01206=L0x20014b3c,cg02206=L0x20014b40,cg00007=L0x20014b8c,
cg01007=L0x20014b90,cg02007=L0x20014b94,cg00107=L0x20014b98,cg01107=L0x20014b9c,
cg02107=L0x20014ba0,cg00207=L0x20014ba4,cg01207=L0x20014ba8,cg02207=L0x20014bac,
cg00008=L0x20014bf8,cg01008=L0x20014bfc,cg02008=L0x20014c00,cg00108=L0x20014c04,
cg01108=L0x20014c08,cg02108=L0x20014c0c,cg00208=L0x20014c10,cg01208=L0x20014c14,
cg02208=L0x20014c18,cg00009=L0x20014c64,cg01009=L0x20014c68,cg02009=L0x20014c6c,
cg00109=L0x20014c70,cg01109=L0x20014c74,cg02109=L0x20014c78,cg00209=L0x20014c7c,
cg01209=L0x20014c80,cg02209=L0x20014c84,cg00010=L0x20014cd0,cg01010=L0x20014cd4,
cg02010=L0x20014cd8,cg00110=L0x20014cdc,cg01110=L0x20014ce0,cg02110=L0x20014ce4,
cg00210=L0x20014ce8,cg01210=L0x20014cec,cg02210=L0x20014cf0,cg00011=L0x20014d3c,
cg01011=L0x20014d40,cg02011=L0x20014d44,cg00111=L0x20014d48,cg01111=L0x20014d4c,
cg02111=L0x20014d50,cg00211=L0x20014d54,cg01211=L0x20014d58,cg02211=L0x20014d5c,
cg00012=L0x20014da8,cg01012=L0x20014dac,cg02012=L0x20014db0,cg00112=L0x20014db4,
cg01112=L0x20014db8,cg02112=L0x20014dbc,cg00212=L0x20014dc0,cg01212=L0x20014dc4,
cg02212=L0x20014dc8,cg00013=L0x20014e14,cg01013=L0x20014e18,cg02013=L0x20014e1c,
cg00113=L0x20014e20,cg01113=L0x20014e24,cg02113=L0x20014e28,cg00213=L0x20014e2c,
cg01213=L0x20014e30,cg02213=L0x20014e34,cg00014=L0x20014e80,cg01014=L0x20014e84,
cg02014=L0x20014e88,cg00114=L0x20014e8c,cg01114=L0x20014e90,cg02114=L0x20014e94,
cg00214=L0x20014e98,cg01214=L0x20014e9c,cg02214=L0x20014ea0,cg00015=L0x20014eec,
cg01015=L0x20014ef0,cg02015=L0x20014ef4,cg00115=L0x20014ef8,cg01115=L0x20014efc,
cg02115=L0x20014f00,cg00215=L0x20014f04,cg01215=L0x20014f08,cg02215=L0x20014f0c,
cg00016=L0x20014f58,cg01016=L0x20014f5c,cg02016=L0x20014f60,cg00116=L0x20014f64,
cg01116=L0x20014f68,cg02116=L0x20014f6c,cg00216=L0x20014f70,cg01216=L0x20014f74,
cg02216=L0x20014f78,cg00017=L0x20014fc4,cg01017=L0x20014fc8,cg02017=L0x20014fcc,
cg00117=L0x20014fd0,cg01117=L0x20014fd4,cg02117=L0x20014fd8,cg00217=L0x20014fdc,
cg01217=L0x20014fe0,cg02217=L0x20014fe4,cg00018=L0x20015030,cg01018=L0x20015034,
cg02018=L0x20015038,cg00118=L0x2001503c,cg01118=L0x20015040,cg02118=L0x20015044,
cg00218=L0x20015048,cg01218=L0x2001504c,cg02218=L0x20015050,cg00019=L0x2001509c,
cg01019=L0x200150a0,cg02019=L0x200150a4,cg00119=L0x200150a8,cg01119=L0x200150ac,
cg02119=L0x200150b0,cg00219=L0x200150b4,cg01219=L0x200150b8,cg02219=L0x200150bc,
cg00020=L0x20015108,cg01020=L0x2001510c,cg02020=L0x20015110,cg00120=L0x20015114,
cg01120=L0x20015118,cg02120=L0x2001511c,cg00220=L0x20015120,cg01220=L0x20015124,
cg02220=L0x20015128,cg00021=L0x20015174,cg01021=L0x20015178,cg02021=L0x2001517c,
cg00121=L0x20015180,cg01121=L0x20015184,cg02121=L0x20015188,cg00221=L0x2001518c,
cg01221=L0x20015190,cg02221=L0x20015194,cg00022=L0x200151e0,cg01022=L0x200151e4,
cg02022=L0x200151e8,cg00122=L0x200151ec,cg01122=L0x200151f0,cg02122=L0x200151f4,
cg00222=L0x200151f8,cg01222=L0x200151fc,cg02222=L0x20015200,cg00023=L0x2001524c,
cg01023=L0x20015250,cg02023=L0x20015254,cg00123=L0x20015258,cg01123=L0x2001525c,
cg02123=L0x20015260,cg00223=L0x20015264,cg01223=L0x20015268,cg02223=L0x2001526c,
cg00024=L0x200152b8,cg01024=L0x200152bc,cg02024=L0x200152c0,cg00124=L0x200152c4,
cg01124=L0x200152c8,cg02124=L0x200152cc,cg00224=L0x200152d0,cg01224=L0x200152d4,
cg02224=L0x200152d8,cg00025=L0x20015324,cg01025=L0x20015328,cg02025=L0x2001532c,
cg00125=L0x20015330,cg01125=L0x20015334,cg02125=L0x20015338,cg00225=L0x2001533c,
cg01225=L0x20015340,cg02225=L0x20015344,cg00026=L0x20015390,cg01026=L0x20015394,
cg02026=L0x20015398,cg00126=L0x2001539c,cg01126=L0x200153a0,cg02126=L0x200153a4,
cg00226=L0x200153a8,cg01226=L0x200153ac,cg02226=L0x200153b0,cg00027=L0x200153fc,
cg01027=L0x20015400,cg02027=L0x20015404,cg00127=L0x20015408,cg01127=L0x2001540c,
cg02127=L0x20015410,cg00227=L0x20015414,cg01227=L0x20015418,cg02227=L0x2001541c,
cg00028=L0x20015468,cg01028=L0x2001546c,cg02028=L0x20015470,cg00128=L0x20015474,
cg01128=L0x20015478,cg02128=L0x2001547c,cg00228=L0x20015480,cg01228=L0x20015484,
cg02228=L0x20015488,cg00029=L0x200154d4,cg01029=L0x200154d8,cg02029=L0x200154dc,
cg00129=L0x200154e0,cg01129=L0x200154e4,cg02129=L0x200154e8,cg00229=L0x200154ec,
cg01229=L0x200154f0,cg02229=L0x200154f4,cg00030=L0x20015540,cg01030=L0x20015544,
cg02030=L0x20015548,cg00130=L0x2001554c,cg01130=L0x20015550,cg02130=L0x20015554,
cg00230=L0x20015558,cg01230=L0x2001555c,cg02230=L0x20015560,cg00031=L0x200155ac,
cg01031=L0x200155b0,cg02031=L0x200155b4,cg00131=L0x200155b8,cg01131=L0x200155bc,
cg02131=L0x200155c0,cg00231=L0x200155c4,cg01231=L0x200155c8,cg02231=L0x200155cc,
CG00**2 = 
cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16+
cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17+
cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18+
cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19+
cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20+
cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21+
cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22+
cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23+
cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24+
cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25+
cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26+
cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27+
cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28+
cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29+
cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30+
cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31,
CG01**2 = 
cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16+
cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17+
cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18+
cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19+
cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20+
cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21+
cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22+
cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23+
cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24+
cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25+
cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26+
cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27+
cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28+
cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29+
cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30+
cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31,
CG02**2 = 
cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16+
cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17+
cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18+
cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19+
cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20+
cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21+
cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22+
cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23+
cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24+
cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25+
cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26+
cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27+
cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28+
cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29+
cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30+
cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31
] && true;


(**************** CUT 309, - *****************)

ecut true;

(**************** CUT 310, - *****************)

ecut eqmod CG00**2 CF0**2 [ 3365569, y**3 -       1, z**32 -       1 ]
prove with [ all ghosts, cuts [ 289 ] ];


(**************** CUT 311, - *****************)

ecut true;

(**************** CUT 312, - *****************)

ecut eqmod CG01**2 CF1**2 [ 3365569, y**3 -       1, z**32 -       1 ]
prove with [ all ghosts, cuts [ 290 ] ];


(**************** CUT 313, - *****************)

ecut true;

(**************** CUT 314, - *****************)

ecut eqmod CG02**2 CF2**2 [ 3365569, y**3 -       1, z**32 -       1 ]
prove with [ all ghosts, cuts [ 291 ] ];


(**************** CUT 315, - *****************)

ecut true;

(**************** CUT 316, - *****************)

ecut eqmod CG00**2+CG01**2+CG02**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -       1, z**32 -       1 ]
prove with [ cuts [ 310, 312, 314 ] ];




(******************** output polynomial 1 ********************)

ghost
CG10@sint32,
cg10000@sint32, cg10100@sint32, cg10200@sint32, cg10001@sint32, cg10101@sint32,
cg10201@sint32, cg10002@sint32, cg10102@sint32, cg10202@sint32, cg10003@sint32,
cg10103@sint32, cg10203@sint32, cg10004@sint32, cg10104@sint32, cg10204@sint32,
cg10005@sint32, cg10105@sint32, cg10205@sint32, cg10006@sint32, cg10106@sint32,
cg10206@sint32, cg10007@sint32, cg10107@sint32, cg10207@sint32, cg10008@sint32,
cg10108@sint32, cg10208@sint32, cg10009@sint32, cg10109@sint32, cg10209@sint32,
cg10010@sint32, cg10110@sint32, cg10210@sint32, cg10011@sint32, cg10111@sint32,
cg10211@sint32, cg10012@sint32, cg10112@sint32, cg10212@sint32, cg10013@sint32,
cg10113@sint32, cg10213@sint32, cg10014@sint32, cg10114@sint32, cg10214@sint32,
cg10015@sint32, cg10115@sint32, cg10215@sint32, cg10016@sint32, cg10116@sint32,
cg10216@sint32, cg10017@sint32, cg10117@sint32, cg10217@sint32, cg10018@sint32,
cg10118@sint32, cg10218@sint32, cg10019@sint32, cg10119@sint32, cg10219@sint32,
cg10020@sint32, cg10120@sint32, cg10220@sint32, cg10021@sint32, cg10121@sint32,
cg10221@sint32, cg10022@sint32, cg10122@sint32, cg10222@sint32, cg10023@sint32,
cg10123@sint32, cg10223@sint32, cg10024@sint32, cg10124@sint32, cg10224@sint32,
cg10025@sint32, cg10125@sint32, cg10225@sint32, cg10026@sint32, cg10126@sint32,
cg10226@sint32, cg10027@sint32, cg10127@sint32, cg10227@sint32, cg10028@sint32,
cg10128@sint32, cg10228@sint32, cg10029@sint32, cg10129@sint32, cg10229@sint32,
cg10030@sint32, cg10130@sint32, cg10230@sint32, cg10031@sint32, cg10131@sint32,
cg10231@sint32,
CG11@sint32,
cg11000@sint32, cg11100@sint32, cg11200@sint32, cg11001@sint32, cg11101@sint32,
cg11201@sint32, cg11002@sint32, cg11102@sint32, cg11202@sint32, cg11003@sint32,
cg11103@sint32, cg11203@sint32, cg11004@sint32, cg11104@sint32, cg11204@sint32,
cg11005@sint32, cg11105@sint32, cg11205@sint32, cg11006@sint32, cg11106@sint32,
cg11206@sint32, cg11007@sint32, cg11107@sint32, cg11207@sint32, cg11008@sint32,
cg11108@sint32, cg11208@sint32, cg11009@sint32, cg11109@sint32, cg11209@sint32,
cg11010@sint32, cg11110@sint32, cg11210@sint32, cg11011@sint32, cg11111@sint32,
cg11211@sint32, cg11012@sint32, cg11112@sint32, cg11212@sint32, cg11013@sint32,
cg11113@sint32, cg11213@sint32, cg11014@sint32, cg11114@sint32, cg11214@sint32,
cg11015@sint32, cg11115@sint32, cg11215@sint32, cg11016@sint32, cg11116@sint32,
cg11216@sint32, cg11017@sint32, cg11117@sint32, cg11217@sint32, cg11018@sint32,
cg11118@sint32, cg11218@sint32, cg11019@sint32, cg11119@sint32, cg11219@sint32,
cg11020@sint32, cg11120@sint32, cg11220@sint32, cg11021@sint32, cg11121@sint32,
cg11221@sint32, cg11022@sint32, cg11122@sint32, cg11222@sint32, cg11023@sint32,
cg11123@sint32, cg11223@sint32, cg11024@sint32, cg11124@sint32, cg11224@sint32,
cg11025@sint32, cg11125@sint32, cg11225@sint32, cg11026@sint32, cg11126@sint32,
cg11226@sint32, cg11027@sint32, cg11127@sint32, cg11227@sint32, cg11028@sint32,
cg11128@sint32, cg11228@sint32, cg11029@sint32, cg11129@sint32, cg11229@sint32,
cg11030@sint32, cg11130@sint32, cg11230@sint32, cg11031@sint32, cg11131@sint32,
cg11231@sint32,
CG12@sint32,
cg12000@sint32, cg12100@sint32, cg12200@sint32, cg12001@sint32, cg12101@sint32,
cg12201@sint32, cg12002@sint32, cg12102@sint32, cg12202@sint32, cg12003@sint32,
cg12103@sint32, cg12203@sint32, cg12004@sint32, cg12104@sint32, cg12204@sint32,
cg12005@sint32, cg12105@sint32, cg12205@sint32, cg12006@sint32, cg12106@sint32,
cg12206@sint32, cg12007@sint32, cg12107@sint32, cg12207@sint32, cg12008@sint32,
cg12108@sint32, cg12208@sint32, cg12009@sint32, cg12109@sint32, cg12209@sint32,
cg12010@sint32, cg12110@sint32, cg12210@sint32, cg12011@sint32, cg12111@sint32,
cg12211@sint32, cg12012@sint32, cg12112@sint32, cg12212@sint32, cg12013@sint32,
cg12113@sint32, cg12213@sint32, cg12014@sint32, cg12114@sint32, cg12214@sint32,
cg12015@sint32, cg12115@sint32, cg12215@sint32, cg12016@sint32, cg12116@sint32,
cg12216@sint32, cg12017@sint32, cg12117@sint32, cg12217@sint32, cg12018@sint32,
cg12118@sint32, cg12218@sint32, cg12019@sint32, cg12119@sint32, cg12219@sint32,
cg12020@sint32, cg12120@sint32, cg12220@sint32, cg12021@sint32, cg12121@sint32,
cg12221@sint32, cg12022@sint32, cg12122@sint32, cg12222@sint32, cg12023@sint32,
cg12123@sint32, cg12223@sint32, cg12024@sint32, cg12124@sint32, cg12224@sint32,
cg12025@sint32, cg12125@sint32, cg12225@sint32, cg12026@sint32, cg12126@sint32,
cg12226@sint32, cg12027@sint32, cg12127@sint32, cg12227@sint32, cg12028@sint32,
cg12128@sint32, cg12228@sint32, cg12029@sint32, cg12129@sint32, cg12229@sint32,
cg12030@sint32, cg12130@sint32, cg12230@sint32, cg12031@sint32, cg12131@sint32,
cg12231@sint32 : and [
cg10000=L0x200148bc,cg11000=L0x200148c0,cg12000=L0x200148c4,cg10100=L0x200148c8,
cg11100=L0x200148cc,cg12100=L0x200148d0,cg10200=L0x200148d4,cg11200=L0x200148d8,
cg12200=L0x200148dc,cg10001=L0x20014928,cg11001=L0x2001492c,cg12001=L0x20014930,
cg10101=L0x20014934,cg11101=L0x20014938,cg12101=L0x2001493c,cg10201=L0x20014940,
cg11201=L0x20014944,cg12201=L0x20014948,cg10002=L0x20014994,cg11002=L0x20014998,
cg12002=L0x2001499c,cg10102=L0x200149a0,cg11102=L0x200149a4,cg12102=L0x200149a8,
cg10202=L0x200149ac,cg11202=L0x200149b0,cg12202=L0x200149b4,cg10003=L0x20014a00,
cg11003=L0x20014a04,cg12003=L0x20014a08,cg10103=L0x20014a0c,cg11103=L0x20014a10,
cg12103=L0x20014a14,cg10203=L0x20014a18,cg11203=L0x20014a1c,cg12203=L0x20014a20,
cg10004=L0x20014a6c,cg11004=L0x20014a70,cg12004=L0x20014a74,cg10104=L0x20014a78,
cg11104=L0x20014a7c,cg12104=L0x20014a80,cg10204=L0x20014a84,cg11204=L0x20014a88,
cg12204=L0x20014a8c,cg10005=L0x20014ad8,cg11005=L0x20014adc,cg12005=L0x20014ae0,
cg10105=L0x20014ae4,cg11105=L0x20014ae8,cg12105=L0x20014aec,cg10205=L0x20014af0,
cg11205=L0x20014af4,cg12205=L0x20014af8,cg10006=L0x20014b44,cg11006=L0x20014b48,
cg12006=L0x20014b4c,cg10106=L0x20014b50,cg11106=L0x20014b54,cg12106=L0x20014b58,
cg10206=L0x20014b5c,cg11206=L0x20014b60,cg12206=L0x20014b64,cg10007=L0x20014bb0,
cg11007=L0x20014bb4,cg12007=L0x20014bb8,cg10107=L0x20014bbc,cg11107=L0x20014bc0,
cg12107=L0x20014bc4,cg10207=L0x20014bc8,cg11207=L0x20014bcc,cg12207=L0x20014bd0,
cg10008=L0x20014c1c,cg11008=L0x20014c20,cg12008=L0x20014c24,cg10108=L0x20014c28,
cg11108=L0x20014c2c,cg12108=L0x20014c30,cg10208=L0x20014c34,cg11208=L0x20014c38,
cg12208=L0x20014c3c,cg10009=L0x20014c88,cg11009=L0x20014c8c,cg12009=L0x20014c90,
cg10109=L0x20014c94,cg11109=L0x20014c98,cg12109=L0x20014c9c,cg10209=L0x20014ca0,
cg11209=L0x20014ca4,cg12209=L0x20014ca8,cg10010=L0x20014cf4,cg11010=L0x20014cf8,
cg12010=L0x20014cfc,cg10110=L0x20014d00,cg11110=L0x20014d04,cg12110=L0x20014d08,
cg10210=L0x20014d0c,cg11210=L0x20014d10,cg12210=L0x20014d14,cg10011=L0x20014d60,
cg11011=L0x20014d64,cg12011=L0x20014d68,cg10111=L0x20014d6c,cg11111=L0x20014d70,
cg12111=L0x20014d74,cg10211=L0x20014d78,cg11211=L0x20014d7c,cg12211=L0x20014d80,
cg10012=L0x20014dcc,cg11012=L0x20014dd0,cg12012=L0x20014dd4,cg10112=L0x20014dd8,
cg11112=L0x20014ddc,cg12112=L0x20014de0,cg10212=L0x20014de4,cg11212=L0x20014de8,
cg12212=L0x20014dec,cg10013=L0x20014e38,cg11013=L0x20014e3c,cg12013=L0x20014e40,
cg10113=L0x20014e44,cg11113=L0x20014e48,cg12113=L0x20014e4c,cg10213=L0x20014e50,
cg11213=L0x20014e54,cg12213=L0x20014e58,cg10014=L0x20014ea4,cg11014=L0x20014ea8,
cg12014=L0x20014eac,cg10114=L0x20014eb0,cg11114=L0x20014eb4,cg12114=L0x20014eb8,
cg10214=L0x20014ebc,cg11214=L0x20014ec0,cg12214=L0x20014ec4,cg10015=L0x20014f10,
cg11015=L0x20014f14,cg12015=L0x20014f18,cg10115=L0x20014f1c,cg11115=L0x20014f20,
cg12115=L0x20014f24,cg10215=L0x20014f28,cg11215=L0x20014f2c,cg12215=L0x20014f30,
cg10016=L0x20014f7c,cg11016=L0x20014f80,cg12016=L0x20014f84,cg10116=L0x20014f88,
cg11116=L0x20014f8c,cg12116=L0x20014f90,cg10216=L0x20014f94,cg11216=L0x20014f98,
cg12216=L0x20014f9c,cg10017=L0x20014fe8,cg11017=L0x20014fec,cg12017=L0x20014ff0,
cg10117=L0x20014ff4,cg11117=L0x20014ff8,cg12117=L0x20014ffc,cg10217=L0x20015000,
cg11217=L0x20015004,cg12217=L0x20015008,cg10018=L0x20015054,cg11018=L0x20015058,
cg12018=L0x2001505c,cg10118=L0x20015060,cg11118=L0x20015064,cg12118=L0x20015068,
cg10218=L0x2001506c,cg11218=L0x20015070,cg12218=L0x20015074,cg10019=L0x200150c0,
cg11019=L0x200150c4,cg12019=L0x200150c8,cg10119=L0x200150cc,cg11119=L0x200150d0,
cg12119=L0x200150d4,cg10219=L0x200150d8,cg11219=L0x200150dc,cg12219=L0x200150e0,
cg10020=L0x2001512c,cg11020=L0x20015130,cg12020=L0x20015134,cg10120=L0x20015138,
cg11120=L0x2001513c,cg12120=L0x20015140,cg10220=L0x20015144,cg11220=L0x20015148,
cg12220=L0x2001514c,cg10021=L0x20015198,cg11021=L0x2001519c,cg12021=L0x200151a0,
cg10121=L0x200151a4,cg11121=L0x200151a8,cg12121=L0x200151ac,cg10221=L0x200151b0,
cg11221=L0x200151b4,cg12221=L0x200151b8,cg10022=L0x20015204,cg11022=L0x20015208,
cg12022=L0x2001520c,cg10122=L0x20015210,cg11122=L0x20015214,cg12122=L0x20015218,
cg10222=L0x2001521c,cg11222=L0x20015220,cg12222=L0x20015224,cg10023=L0x20015270,
cg11023=L0x20015274,cg12023=L0x20015278,cg10123=L0x2001527c,cg11123=L0x20015280,
cg12123=L0x20015284,cg10223=L0x20015288,cg11223=L0x2001528c,cg12223=L0x20015290,
cg10024=L0x200152dc,cg11024=L0x200152e0,cg12024=L0x200152e4,cg10124=L0x200152e8,
cg11124=L0x200152ec,cg12124=L0x200152f0,cg10224=L0x200152f4,cg11224=L0x200152f8,
cg12224=L0x200152fc,cg10025=L0x20015348,cg11025=L0x2001534c,cg12025=L0x20015350,
cg10125=L0x20015354,cg11125=L0x20015358,cg12125=L0x2001535c,cg10225=L0x20015360,
cg11225=L0x20015364,cg12225=L0x20015368,cg10026=L0x200153b4,cg11026=L0x200153b8,
cg12026=L0x200153bc,cg10126=L0x200153c0,cg11126=L0x200153c4,cg12126=L0x200153c8,
cg10226=L0x200153cc,cg11226=L0x200153d0,cg12226=L0x200153d4,cg10027=L0x20015420,
cg11027=L0x20015424,cg12027=L0x20015428,cg10127=L0x2001542c,cg11127=L0x20015430,
cg12127=L0x20015434,cg10227=L0x20015438,cg11227=L0x2001543c,cg12227=L0x20015440,
cg10028=L0x2001548c,cg11028=L0x20015490,cg12028=L0x20015494,cg10128=L0x20015498,
cg11128=L0x2001549c,cg12128=L0x200154a0,cg10228=L0x200154a4,cg11228=L0x200154a8,
cg12228=L0x200154ac,cg10029=L0x200154f8,cg11029=L0x200154fc,cg12029=L0x20015500,
cg10129=L0x20015504,cg11129=L0x20015508,cg12129=L0x2001550c,cg10229=L0x20015510,
cg11229=L0x20015514,cg12229=L0x20015518,cg10030=L0x20015564,cg11030=L0x20015568,
cg12030=L0x2001556c,cg10130=L0x20015570,cg11130=L0x20015574,cg12130=L0x20015578,
cg10230=L0x2001557c,cg11230=L0x20015580,cg12230=L0x20015584,cg10031=L0x200155d0,
cg11031=L0x200155d4,cg12031=L0x200155d8,cg10131=L0x200155dc,cg11131=L0x200155e0,
cg12131=L0x200155e4,cg10231=L0x200155e8,cg11231=L0x200155ec,cg12231=L0x200155f0,
CG10**2 = 
cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16+
cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17+
cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18+
cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19+
cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20+
cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21+
cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22+
cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23+
cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24+
cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25+
cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26+
cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27+
cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28+
cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29+
cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30+
cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31,
CG11**2 = 
cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16+
cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17+
cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18+
cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19+
cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20+
cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21+
cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22+
cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23+
cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24+
cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25+
cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26+
cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27+
cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28+
cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29+
cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30+
cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31,
CG12**2 = 
cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16+
cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17+
cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18+
cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19+
cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20+
cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21+
cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22+
cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23+
cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24+
cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25+
cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26+
cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27+
cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28+
cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29+
cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30+
cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31
] && true;


(**************** CUT 317, - *****************)

ecut true;

(**************** CUT 318, - *****************)

ecut eqmod CG10**2 CF0**2 [ 3365569, y**3 -  452650, z**32 -       1 ]
prove with [ all ghosts, cuts [ 292 ] ];


(**************** CUT 319, - *****************)

ecut true;

(**************** CUT 320, - *****************)

ecut eqmod CG11**2 CF1**2 [ 3365569, y**3 -  452650, z**32 -       1 ]
prove with [ all ghosts, cuts [ 293 ] ];


(**************** CUT 321, - *****************)

ecut true;

(**************** CUT 322, - *****************)

ecut eqmod CG12**2 CF2**2 [ 3365569, y**3 -  452650, z**32 -       1 ]
prove with [ all ghosts, cuts [ 294 ] ];


(**************** CUT 323, - *****************)

ecut true;

(**************** CUT 324, - *****************)

ecut eqmod CG10**2+CG11**2+CG12**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -  452650, z**32 -       1 ]
prove with [ cuts [ 318, 320, 322 ] ];




(******************** output polynomial 2 ********************)

ghost
CG20@sint32,
cg20000@sint32, cg20100@sint32, cg20200@sint32, cg20001@sint32, cg20101@sint32,
cg20201@sint32, cg20002@sint32, cg20102@sint32, cg20202@sint32, cg20003@sint32,
cg20103@sint32, cg20203@sint32, cg20004@sint32, cg20104@sint32, cg20204@sint32,
cg20005@sint32, cg20105@sint32, cg20205@sint32, cg20006@sint32, cg20106@sint32,
cg20206@sint32, cg20007@sint32, cg20107@sint32, cg20207@sint32, cg20008@sint32,
cg20108@sint32, cg20208@sint32, cg20009@sint32, cg20109@sint32, cg20209@sint32,
cg20010@sint32, cg20110@sint32, cg20210@sint32, cg20011@sint32, cg20111@sint32,
cg20211@sint32, cg20012@sint32, cg20112@sint32, cg20212@sint32, cg20013@sint32,
cg20113@sint32, cg20213@sint32, cg20014@sint32, cg20114@sint32, cg20214@sint32,
cg20015@sint32, cg20115@sint32, cg20215@sint32, cg20016@sint32, cg20116@sint32,
cg20216@sint32, cg20017@sint32, cg20117@sint32, cg20217@sint32, cg20018@sint32,
cg20118@sint32, cg20218@sint32, cg20019@sint32, cg20119@sint32, cg20219@sint32,
cg20020@sint32, cg20120@sint32, cg20220@sint32, cg20021@sint32, cg20121@sint32,
cg20221@sint32, cg20022@sint32, cg20122@sint32, cg20222@sint32, cg20023@sint32,
cg20123@sint32, cg20223@sint32, cg20024@sint32, cg20124@sint32, cg20224@sint32,
cg20025@sint32, cg20125@sint32, cg20225@sint32, cg20026@sint32, cg20126@sint32,
cg20226@sint32, cg20027@sint32, cg20127@sint32, cg20227@sint32, cg20028@sint32,
cg20128@sint32, cg20228@sint32, cg20029@sint32, cg20129@sint32, cg20229@sint32,
cg20030@sint32, cg20130@sint32, cg20230@sint32, cg20031@sint32, cg20131@sint32,
cg20231@sint32,
CG21@sint32,
cg21000@sint32, cg21100@sint32, cg21200@sint32, cg21001@sint32, cg21101@sint32,
cg21201@sint32, cg21002@sint32, cg21102@sint32, cg21202@sint32, cg21003@sint32,
cg21103@sint32, cg21203@sint32, cg21004@sint32, cg21104@sint32, cg21204@sint32,
cg21005@sint32, cg21105@sint32, cg21205@sint32, cg21006@sint32, cg21106@sint32,
cg21206@sint32, cg21007@sint32, cg21107@sint32, cg21207@sint32, cg21008@sint32,
cg21108@sint32, cg21208@sint32, cg21009@sint32, cg21109@sint32, cg21209@sint32,
cg21010@sint32, cg21110@sint32, cg21210@sint32, cg21011@sint32, cg21111@sint32,
cg21211@sint32, cg21012@sint32, cg21112@sint32, cg21212@sint32, cg21013@sint32,
cg21113@sint32, cg21213@sint32, cg21014@sint32, cg21114@sint32, cg21214@sint32,
cg21015@sint32, cg21115@sint32, cg21215@sint32, cg21016@sint32, cg21116@sint32,
cg21216@sint32, cg21017@sint32, cg21117@sint32, cg21217@sint32, cg21018@sint32,
cg21118@sint32, cg21218@sint32, cg21019@sint32, cg21119@sint32, cg21219@sint32,
cg21020@sint32, cg21120@sint32, cg21220@sint32, cg21021@sint32, cg21121@sint32,
cg21221@sint32, cg21022@sint32, cg21122@sint32, cg21222@sint32, cg21023@sint32,
cg21123@sint32, cg21223@sint32, cg21024@sint32, cg21124@sint32, cg21224@sint32,
cg21025@sint32, cg21125@sint32, cg21225@sint32, cg21026@sint32, cg21126@sint32,
cg21226@sint32, cg21027@sint32, cg21127@sint32, cg21227@sint32, cg21028@sint32,
cg21128@sint32, cg21228@sint32, cg21029@sint32, cg21129@sint32, cg21229@sint32,
cg21030@sint32, cg21130@sint32, cg21230@sint32, cg21031@sint32, cg21131@sint32,
cg21231@sint32,
CG22@sint32,
cg22000@sint32, cg22100@sint32, cg22200@sint32, cg22001@sint32, cg22101@sint32,
cg22201@sint32, cg22002@sint32, cg22102@sint32, cg22202@sint32, cg22003@sint32,
cg22103@sint32, cg22203@sint32, cg22004@sint32, cg22104@sint32, cg22204@sint32,
cg22005@sint32, cg22105@sint32, cg22205@sint32, cg22006@sint32, cg22106@sint32,
cg22206@sint32, cg22007@sint32, cg22107@sint32, cg22207@sint32, cg22008@sint32,
cg22108@sint32, cg22208@sint32, cg22009@sint32, cg22109@sint32, cg22209@sint32,
cg22010@sint32, cg22110@sint32, cg22210@sint32, cg22011@sint32, cg22111@sint32,
cg22211@sint32, cg22012@sint32, cg22112@sint32, cg22212@sint32, cg22013@sint32,
cg22113@sint32, cg22213@sint32, cg22014@sint32, cg22114@sint32, cg22214@sint32,
cg22015@sint32, cg22115@sint32, cg22215@sint32, cg22016@sint32, cg22116@sint32,
cg22216@sint32, cg22017@sint32, cg22117@sint32, cg22217@sint32, cg22018@sint32,
cg22118@sint32, cg22218@sint32, cg22019@sint32, cg22119@sint32, cg22219@sint32,
cg22020@sint32, cg22120@sint32, cg22220@sint32, cg22021@sint32, cg22121@sint32,
cg22221@sint32, cg22022@sint32, cg22122@sint32, cg22222@sint32, cg22023@sint32,
cg22123@sint32, cg22223@sint32, cg22024@sint32, cg22124@sint32, cg22224@sint32,
cg22025@sint32, cg22125@sint32, cg22225@sint32, cg22026@sint32, cg22126@sint32,
cg22226@sint32, cg22027@sint32, cg22127@sint32, cg22227@sint32, cg22028@sint32,
cg22128@sint32, cg22228@sint32, cg22029@sint32, cg22129@sint32, cg22229@sint32,
cg22030@sint32, cg22130@sint32, cg22230@sint32, cg22031@sint32, cg22131@sint32,
cg22231@sint32 : and [
cg20000=L0x200148e0,cg21000=L0x200148e4,cg22000=L0x200148e8,cg20100=L0x200148ec,
cg21100=L0x200148f0,cg22100=L0x200148f4,cg20200=L0x200148f8,cg21200=L0x200148fc,
cg22200=L0x20014900,cg20001=L0x2001494c,cg21001=L0x20014950,cg22001=L0x20014954,
cg20101=L0x20014958,cg21101=L0x2001495c,cg22101=L0x20014960,cg20201=L0x20014964,
cg21201=L0x20014968,cg22201=L0x2001496c,cg20002=L0x200149b8,cg21002=L0x200149bc,
cg22002=L0x200149c0,cg20102=L0x200149c4,cg21102=L0x200149c8,cg22102=L0x200149cc,
cg20202=L0x200149d0,cg21202=L0x200149d4,cg22202=L0x200149d8,cg20003=L0x20014a24,
cg21003=L0x20014a28,cg22003=L0x20014a2c,cg20103=L0x20014a30,cg21103=L0x20014a34,
cg22103=L0x20014a38,cg20203=L0x20014a3c,cg21203=L0x20014a40,cg22203=L0x20014a44,
cg20004=L0x20014a90,cg21004=L0x20014a94,cg22004=L0x20014a98,cg20104=L0x20014a9c,
cg21104=L0x20014aa0,cg22104=L0x20014aa4,cg20204=L0x20014aa8,cg21204=L0x20014aac,
cg22204=L0x20014ab0,cg20005=L0x20014afc,cg21005=L0x20014b00,cg22005=L0x20014b04,
cg20105=L0x20014b08,cg21105=L0x20014b0c,cg22105=L0x20014b10,cg20205=L0x20014b14,
cg21205=L0x20014b18,cg22205=L0x20014b1c,cg20006=L0x20014b68,cg21006=L0x20014b6c,
cg22006=L0x20014b70,cg20106=L0x20014b74,cg21106=L0x20014b78,cg22106=L0x20014b7c,
cg20206=L0x20014b80,cg21206=L0x20014b84,cg22206=L0x20014b88,cg20007=L0x20014bd4,
cg21007=L0x20014bd8,cg22007=L0x20014bdc,cg20107=L0x20014be0,cg21107=L0x20014be4,
cg22107=L0x20014be8,cg20207=L0x20014bec,cg21207=L0x20014bf0,cg22207=L0x20014bf4,
cg20008=L0x20014c40,cg21008=L0x20014c44,cg22008=L0x20014c48,cg20108=L0x20014c4c,
cg21108=L0x20014c50,cg22108=L0x20014c54,cg20208=L0x20014c58,cg21208=L0x20014c5c,
cg22208=L0x20014c60,cg20009=L0x20014cac,cg21009=L0x20014cb0,cg22009=L0x20014cb4,
cg20109=L0x20014cb8,cg21109=L0x20014cbc,cg22109=L0x20014cc0,cg20209=L0x20014cc4,
cg21209=L0x20014cc8,cg22209=L0x20014ccc,cg20010=L0x20014d18,cg21010=L0x20014d1c,
cg22010=L0x20014d20,cg20110=L0x20014d24,cg21110=L0x20014d28,cg22110=L0x20014d2c,
cg20210=L0x20014d30,cg21210=L0x20014d34,cg22210=L0x20014d38,cg20011=L0x20014d84,
cg21011=L0x20014d88,cg22011=L0x20014d8c,cg20111=L0x20014d90,cg21111=L0x20014d94,
cg22111=L0x20014d98,cg20211=L0x20014d9c,cg21211=L0x20014da0,cg22211=L0x20014da4,
cg20012=L0x20014df0,cg21012=L0x20014df4,cg22012=L0x20014df8,cg20112=L0x20014dfc,
cg21112=L0x20014e00,cg22112=L0x20014e04,cg20212=L0x20014e08,cg21212=L0x20014e0c,
cg22212=L0x20014e10,cg20013=L0x20014e5c,cg21013=L0x20014e60,cg22013=L0x20014e64,
cg20113=L0x20014e68,cg21113=L0x20014e6c,cg22113=L0x20014e70,cg20213=L0x20014e74,
cg21213=L0x20014e78,cg22213=L0x20014e7c,cg20014=L0x20014ec8,cg21014=L0x20014ecc,
cg22014=L0x20014ed0,cg20114=L0x20014ed4,cg21114=L0x20014ed8,cg22114=L0x20014edc,
cg20214=L0x20014ee0,cg21214=L0x20014ee4,cg22214=L0x20014ee8,cg20015=L0x20014f34,
cg21015=L0x20014f38,cg22015=L0x20014f3c,cg20115=L0x20014f40,cg21115=L0x20014f44,
cg22115=L0x20014f48,cg20215=L0x20014f4c,cg21215=L0x20014f50,cg22215=L0x20014f54,
cg20016=L0x20014fa0,cg21016=L0x20014fa4,cg22016=L0x20014fa8,cg20116=L0x20014fac,
cg21116=L0x20014fb0,cg22116=L0x20014fb4,cg20216=L0x20014fb8,cg21216=L0x20014fbc,
cg22216=L0x20014fc0,cg20017=L0x2001500c,cg21017=L0x20015010,cg22017=L0x20015014,
cg20117=L0x20015018,cg21117=L0x2001501c,cg22117=L0x20015020,cg20217=L0x20015024,
cg21217=L0x20015028,cg22217=L0x2001502c,cg20018=L0x20015078,cg21018=L0x2001507c,
cg22018=L0x20015080,cg20118=L0x20015084,cg21118=L0x20015088,cg22118=L0x2001508c,
cg20218=L0x20015090,cg21218=L0x20015094,cg22218=L0x20015098,cg20019=L0x200150e4,
cg21019=L0x200150e8,cg22019=L0x200150ec,cg20119=L0x200150f0,cg21119=L0x200150f4,
cg22119=L0x200150f8,cg20219=L0x200150fc,cg21219=L0x20015100,cg22219=L0x20015104,
cg20020=L0x20015150,cg21020=L0x20015154,cg22020=L0x20015158,cg20120=L0x2001515c,
cg21120=L0x20015160,cg22120=L0x20015164,cg20220=L0x20015168,cg21220=L0x2001516c,
cg22220=L0x20015170,cg20021=L0x200151bc,cg21021=L0x200151c0,cg22021=L0x200151c4,
cg20121=L0x200151c8,cg21121=L0x200151cc,cg22121=L0x200151d0,cg20221=L0x200151d4,
cg21221=L0x200151d8,cg22221=L0x200151dc,cg20022=L0x20015228,cg21022=L0x2001522c,
cg22022=L0x20015230,cg20122=L0x20015234,cg21122=L0x20015238,cg22122=L0x2001523c,
cg20222=L0x20015240,cg21222=L0x20015244,cg22222=L0x20015248,cg20023=L0x20015294,
cg21023=L0x20015298,cg22023=L0x2001529c,cg20123=L0x200152a0,cg21123=L0x200152a4,
cg22123=L0x200152a8,cg20223=L0x200152ac,cg21223=L0x200152b0,cg22223=L0x200152b4,
cg20024=L0x20015300,cg21024=L0x20015304,cg22024=L0x20015308,cg20124=L0x2001530c,
cg21124=L0x20015310,cg22124=L0x20015314,cg20224=L0x20015318,cg21224=L0x2001531c,
cg22224=L0x20015320,cg20025=L0x2001536c,cg21025=L0x20015370,cg22025=L0x20015374,
cg20125=L0x20015378,cg21125=L0x2001537c,cg22125=L0x20015380,cg20225=L0x20015384,
cg21225=L0x20015388,cg22225=L0x2001538c,cg20026=L0x200153d8,cg21026=L0x200153dc,
cg22026=L0x200153e0,cg20126=L0x200153e4,cg21126=L0x200153e8,cg22126=L0x200153ec,
cg20226=L0x200153f0,cg21226=L0x200153f4,cg22226=L0x200153f8,cg20027=L0x20015444,
cg21027=L0x20015448,cg22027=L0x2001544c,cg20127=L0x20015450,cg21127=L0x20015454,
cg22127=L0x20015458,cg20227=L0x2001545c,cg21227=L0x20015460,cg22227=L0x20015464,
cg20028=L0x200154b0,cg21028=L0x200154b4,cg22028=L0x200154b8,cg20128=L0x200154bc,
cg21128=L0x200154c0,cg22128=L0x200154c4,cg20228=L0x200154c8,cg21228=L0x200154cc,
cg22228=L0x200154d0,cg20029=L0x2001551c,cg21029=L0x20015520,cg22029=L0x20015524,
cg20129=L0x20015528,cg21129=L0x2001552c,cg22129=L0x20015530,cg20229=L0x20015534,
cg21229=L0x20015538,cg22229=L0x2001553c,cg20030=L0x20015588,cg21030=L0x2001558c,
cg22030=L0x20015590,cg20130=L0x20015594,cg21130=L0x20015598,cg22130=L0x2001559c,
cg20230=L0x200155a0,cg21230=L0x200155a4,cg22230=L0x200155a8,cg20031=L0x200155f4,
cg21031=L0x200155f8,cg22031=L0x200155fc,cg20131=L0x20015600,cg21131=L0x20015604,
cg22131=L0x20015608,cg20231=L0x2001560c,cg21231=L0x20015610,cg22231=L0x20015614,
CG20**2 = 
cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16+
cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17+
cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18+
cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19+
cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20+
cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21+
cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22+
cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23+
cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24+
cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25+
cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26+
cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27+
cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28+
cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29+
cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30+
cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31,
CG21**2 = 
cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16+
cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17+
cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18+
cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19+
cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20+
cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21+
cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22+
cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23+
cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24+
cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25+
cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26+
cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27+
cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28+
cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29+
cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30+
cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31,
CG22**2 = 
cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16+
cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17+
cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18+
cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19+
cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20+
cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21+
cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22+
cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23+
cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24+
cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25+
cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26+
cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27+
cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28+
cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29+
cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30+
cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31
] && true;


(**************** CUT 325, - *****************)

ecut true;

(**************** CUT 326, - *****************)

ecut eqmod CG20**2 CF0**2 [ 3365569, y**3 - 2912918, z**32 -       1 ]
prove with [ all ghosts, cuts [ 295 ] ];


(**************** CUT 327, - *****************)

ecut true;

(**************** CUT 328, - *****************)

ecut eqmod CG21**2 CF1**2 [ 3365569, y**3 - 2912918, z**32 -       1 ]
prove with [ all ghosts, cuts [ 296 ] ];


(**************** CUT 329, - *****************)

ecut true;

(**************** CUT 330, - *****************)

ecut eqmod CG22**2 CF2**2 [ 3365569, y**3 - 2912918, z**32 -       1 ]
prove with [ all ghosts, cuts [ 297 ] ];


(**************** CUT 331, - *****************)

ecut true;

(**************** CUT 332, - *****************)

ecut eqmod CG20**2+CG21**2+CG22**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 - 2912918, z**32 -       1 ]
prove with [ cuts [ 326, 328, 330 ] ];




(******************** output polynomial 3 ********************)

ghost
CG30@sint32,
cg30000@sint32, cg30100@sint32, cg30200@sint32, cg30001@sint32, cg30101@sint32,
cg30201@sint32, cg30002@sint32, cg30102@sint32, cg30202@sint32, cg30003@sint32,
cg30103@sint32, cg30203@sint32, cg30004@sint32, cg30104@sint32, cg30204@sint32,
cg30005@sint32, cg30105@sint32, cg30205@sint32, cg30006@sint32, cg30106@sint32,
cg30206@sint32, cg30007@sint32, cg30107@sint32, cg30207@sint32, cg30008@sint32,
cg30108@sint32, cg30208@sint32, cg30009@sint32, cg30109@sint32, cg30209@sint32,
cg30010@sint32, cg30110@sint32, cg30210@sint32, cg30011@sint32, cg30111@sint32,
cg30211@sint32, cg30012@sint32, cg30112@sint32, cg30212@sint32, cg30013@sint32,
cg30113@sint32, cg30213@sint32, cg30014@sint32, cg30114@sint32, cg30214@sint32,
cg30015@sint32, cg30115@sint32, cg30215@sint32, cg30016@sint32, cg30116@sint32,
cg30216@sint32, cg30017@sint32, cg30117@sint32, cg30217@sint32, cg30018@sint32,
cg30118@sint32, cg30218@sint32, cg30019@sint32, cg30119@sint32, cg30219@sint32,
cg30020@sint32, cg30120@sint32, cg30220@sint32, cg30021@sint32, cg30121@sint32,
cg30221@sint32, cg30022@sint32, cg30122@sint32, cg30222@sint32, cg30023@sint32,
cg30123@sint32, cg30223@sint32, cg30024@sint32, cg30124@sint32, cg30224@sint32,
cg30025@sint32, cg30125@sint32, cg30225@sint32, cg30026@sint32, cg30126@sint32,
cg30226@sint32, cg30027@sint32, cg30127@sint32, cg30227@sint32, cg30028@sint32,
cg30128@sint32, cg30228@sint32, cg30029@sint32, cg30129@sint32, cg30229@sint32,
cg30030@sint32, cg30130@sint32, cg30230@sint32, cg30031@sint32, cg30131@sint32,
cg30231@sint32,
CG31@sint32,
cg31000@sint32, cg31100@sint32, cg31200@sint32, cg31001@sint32, cg31101@sint32,
cg31201@sint32, cg31002@sint32, cg31102@sint32, cg31202@sint32, cg31003@sint32,
cg31103@sint32, cg31203@sint32, cg31004@sint32, cg31104@sint32, cg31204@sint32,
cg31005@sint32, cg31105@sint32, cg31205@sint32, cg31006@sint32, cg31106@sint32,
cg31206@sint32, cg31007@sint32, cg31107@sint32, cg31207@sint32, cg31008@sint32,
cg31108@sint32, cg31208@sint32, cg31009@sint32, cg31109@sint32, cg31209@sint32,
cg31010@sint32, cg31110@sint32, cg31210@sint32, cg31011@sint32, cg31111@sint32,
cg31211@sint32, cg31012@sint32, cg31112@sint32, cg31212@sint32, cg31013@sint32,
cg31113@sint32, cg31213@sint32, cg31014@sint32, cg31114@sint32, cg31214@sint32,
cg31015@sint32, cg31115@sint32, cg31215@sint32, cg31016@sint32, cg31116@sint32,
cg31216@sint32, cg31017@sint32, cg31117@sint32, cg31217@sint32, cg31018@sint32,
cg31118@sint32, cg31218@sint32, cg31019@sint32, cg31119@sint32, cg31219@sint32,
cg31020@sint32, cg31120@sint32, cg31220@sint32, cg31021@sint32, cg31121@sint32,
cg31221@sint32, cg31022@sint32, cg31122@sint32, cg31222@sint32, cg31023@sint32,
cg31123@sint32, cg31223@sint32, cg31024@sint32, cg31124@sint32, cg31224@sint32,
cg31025@sint32, cg31125@sint32, cg31225@sint32, cg31026@sint32, cg31126@sint32,
cg31226@sint32, cg31027@sint32, cg31127@sint32, cg31227@sint32, cg31028@sint32,
cg31128@sint32, cg31228@sint32, cg31029@sint32, cg31129@sint32, cg31229@sint32,
cg31030@sint32, cg31130@sint32, cg31230@sint32, cg31031@sint32, cg31131@sint32,
cg31231@sint32,
CG32@sint32,
cg32000@sint32, cg32100@sint32, cg32200@sint32, cg32001@sint32, cg32101@sint32,
cg32201@sint32, cg32002@sint32, cg32102@sint32, cg32202@sint32, cg32003@sint32,
cg32103@sint32, cg32203@sint32, cg32004@sint32, cg32104@sint32, cg32204@sint32,
cg32005@sint32, cg32105@sint32, cg32205@sint32, cg32006@sint32, cg32106@sint32,
cg32206@sint32, cg32007@sint32, cg32107@sint32, cg32207@sint32, cg32008@sint32,
cg32108@sint32, cg32208@sint32, cg32009@sint32, cg32109@sint32, cg32209@sint32,
cg32010@sint32, cg32110@sint32, cg32210@sint32, cg32011@sint32, cg32111@sint32,
cg32211@sint32, cg32012@sint32, cg32112@sint32, cg32212@sint32, cg32013@sint32,
cg32113@sint32, cg32213@sint32, cg32014@sint32, cg32114@sint32, cg32214@sint32,
cg32015@sint32, cg32115@sint32, cg32215@sint32, cg32016@sint32, cg32116@sint32,
cg32216@sint32, cg32017@sint32, cg32117@sint32, cg32217@sint32, cg32018@sint32,
cg32118@sint32, cg32218@sint32, cg32019@sint32, cg32119@sint32, cg32219@sint32,
cg32020@sint32, cg32120@sint32, cg32220@sint32, cg32021@sint32, cg32121@sint32,
cg32221@sint32, cg32022@sint32, cg32122@sint32, cg32222@sint32, cg32023@sint32,
cg32123@sint32, cg32223@sint32, cg32024@sint32, cg32124@sint32, cg32224@sint32,
cg32025@sint32, cg32125@sint32, cg32225@sint32, cg32026@sint32, cg32126@sint32,
cg32226@sint32, cg32027@sint32, cg32127@sint32, cg32227@sint32, cg32028@sint32,
cg32128@sint32, cg32228@sint32, cg32029@sint32, cg32129@sint32, cg32229@sint32,
cg32030@sint32, cg32130@sint32, cg32230@sint32, cg32031@sint32, cg32131@sint32,
cg32231@sint32 : and [
cg30000=L0x20015618,cg31000=L0x2001561c,cg32000=L0x20015620,cg30100=L0x20015624,
cg31100=L0x20015628,cg32100=L0x2001562c,cg30200=L0x20015630,cg31200=L0x20015634,
cg32200=L0x20015638,cg30001=L0x20015684,cg31001=L0x20015688,cg32001=L0x2001568c,
cg30101=L0x20015690,cg31101=L0x20015694,cg32101=L0x20015698,cg30201=L0x2001569c,
cg31201=L0x200156a0,cg32201=L0x200156a4,cg30002=L0x200156f0,cg31002=L0x200156f4,
cg32002=L0x200156f8,cg30102=L0x200156fc,cg31102=L0x20015700,cg32102=L0x20015704,
cg30202=L0x20015708,cg31202=L0x2001570c,cg32202=L0x20015710,cg30003=L0x2001575c,
cg31003=L0x20015760,cg32003=L0x20015764,cg30103=L0x20015768,cg31103=L0x2001576c,
cg32103=L0x20015770,cg30203=L0x20015774,cg31203=L0x20015778,cg32203=L0x2001577c,
cg30004=L0x200157c8,cg31004=L0x200157cc,cg32004=L0x200157d0,cg30104=L0x200157d4,
cg31104=L0x200157d8,cg32104=L0x200157dc,cg30204=L0x200157e0,cg31204=L0x200157e4,
cg32204=L0x200157e8,cg30005=L0x20015834,cg31005=L0x20015838,cg32005=L0x2001583c,
cg30105=L0x20015840,cg31105=L0x20015844,cg32105=L0x20015848,cg30205=L0x2001584c,
cg31205=L0x20015850,cg32205=L0x20015854,cg30006=L0x200158a0,cg31006=L0x200158a4,
cg32006=L0x200158a8,cg30106=L0x200158ac,cg31106=L0x200158b0,cg32106=L0x200158b4,
cg30206=L0x200158b8,cg31206=L0x200158bc,cg32206=L0x200158c0,cg30007=L0x2001590c,
cg31007=L0x20015910,cg32007=L0x20015914,cg30107=L0x20015918,cg31107=L0x2001591c,
cg32107=L0x20015920,cg30207=L0x20015924,cg31207=L0x20015928,cg32207=L0x2001592c,
cg30008=L0x20015978,cg31008=L0x2001597c,cg32008=L0x20015980,cg30108=L0x20015984,
cg31108=L0x20015988,cg32108=L0x2001598c,cg30208=L0x20015990,cg31208=L0x20015994,
cg32208=L0x20015998,cg30009=L0x200159e4,cg31009=L0x200159e8,cg32009=L0x200159ec,
cg30109=L0x200159f0,cg31109=L0x200159f4,cg32109=L0x200159f8,cg30209=L0x200159fc,
cg31209=L0x20015a00,cg32209=L0x20015a04,cg30010=L0x20015a50,cg31010=L0x20015a54,
cg32010=L0x20015a58,cg30110=L0x20015a5c,cg31110=L0x20015a60,cg32110=L0x20015a64,
cg30210=L0x20015a68,cg31210=L0x20015a6c,cg32210=L0x20015a70,cg30011=L0x20015abc,
cg31011=L0x20015ac0,cg32011=L0x20015ac4,cg30111=L0x20015ac8,cg31111=L0x20015acc,
cg32111=L0x20015ad0,cg30211=L0x20015ad4,cg31211=L0x20015ad8,cg32211=L0x20015adc,
cg30012=L0x20015b28,cg31012=L0x20015b2c,cg32012=L0x20015b30,cg30112=L0x20015b34,
cg31112=L0x20015b38,cg32112=L0x20015b3c,cg30212=L0x20015b40,cg31212=L0x20015b44,
cg32212=L0x20015b48,cg30013=L0x20015b94,cg31013=L0x20015b98,cg32013=L0x20015b9c,
cg30113=L0x20015ba0,cg31113=L0x20015ba4,cg32113=L0x20015ba8,cg30213=L0x20015bac,
cg31213=L0x20015bb0,cg32213=L0x20015bb4,cg30014=L0x20015c00,cg31014=L0x20015c04,
cg32014=L0x20015c08,cg30114=L0x20015c0c,cg31114=L0x20015c10,cg32114=L0x20015c14,
cg30214=L0x20015c18,cg31214=L0x20015c1c,cg32214=L0x20015c20,cg30015=L0x20015c6c,
cg31015=L0x20015c70,cg32015=L0x20015c74,cg30115=L0x20015c78,cg31115=L0x20015c7c,
cg32115=L0x20015c80,cg30215=L0x20015c84,cg31215=L0x20015c88,cg32215=L0x20015c8c,
cg30016=L0x20015cd8,cg31016=L0x20015cdc,cg32016=L0x20015ce0,cg30116=L0x20015ce4,
cg31116=L0x20015ce8,cg32116=L0x20015cec,cg30216=L0x20015cf0,cg31216=L0x20015cf4,
cg32216=L0x20015cf8,cg30017=L0x20015d44,cg31017=L0x20015d48,cg32017=L0x20015d4c,
cg30117=L0x20015d50,cg31117=L0x20015d54,cg32117=L0x20015d58,cg30217=L0x20015d5c,
cg31217=L0x20015d60,cg32217=L0x20015d64,cg30018=L0x20015db0,cg31018=L0x20015db4,
cg32018=L0x20015db8,cg30118=L0x20015dbc,cg31118=L0x20015dc0,cg32118=L0x20015dc4,
cg30218=L0x20015dc8,cg31218=L0x20015dcc,cg32218=L0x20015dd0,cg30019=L0x20015e1c,
cg31019=L0x20015e20,cg32019=L0x20015e24,cg30119=L0x20015e28,cg31119=L0x20015e2c,
cg32119=L0x20015e30,cg30219=L0x20015e34,cg31219=L0x20015e38,cg32219=L0x20015e3c,
cg30020=L0x20015e88,cg31020=L0x20015e8c,cg32020=L0x20015e90,cg30120=L0x20015e94,
cg31120=L0x20015e98,cg32120=L0x20015e9c,cg30220=L0x20015ea0,cg31220=L0x20015ea4,
cg32220=L0x20015ea8,cg30021=L0x20015ef4,cg31021=L0x20015ef8,cg32021=L0x20015efc,
cg30121=L0x20015f00,cg31121=L0x20015f04,cg32121=L0x20015f08,cg30221=L0x20015f0c,
cg31221=L0x20015f10,cg32221=L0x20015f14,cg30022=L0x20015f60,cg31022=L0x20015f64,
cg32022=L0x20015f68,cg30122=L0x20015f6c,cg31122=L0x20015f70,cg32122=L0x20015f74,
cg30222=L0x20015f78,cg31222=L0x20015f7c,cg32222=L0x20015f80,cg30023=L0x20015fcc,
cg31023=L0x20015fd0,cg32023=L0x20015fd4,cg30123=L0x20015fd8,cg31123=L0x20015fdc,
cg32123=L0x20015fe0,cg30223=L0x20015fe4,cg31223=L0x20015fe8,cg32223=L0x20015fec,
cg30024=L0x20016038,cg31024=L0x2001603c,cg32024=L0x20016040,cg30124=L0x20016044,
cg31124=L0x20016048,cg32124=L0x2001604c,cg30224=L0x20016050,cg31224=L0x20016054,
cg32224=L0x20016058,cg30025=L0x200160a4,cg31025=L0x200160a8,cg32025=L0x200160ac,
cg30125=L0x200160b0,cg31125=L0x200160b4,cg32125=L0x200160b8,cg30225=L0x200160bc,
cg31225=L0x200160c0,cg32225=L0x200160c4,cg30026=L0x20016110,cg31026=L0x20016114,
cg32026=L0x20016118,cg30126=L0x2001611c,cg31126=L0x20016120,cg32126=L0x20016124,
cg30226=L0x20016128,cg31226=L0x2001612c,cg32226=L0x20016130,cg30027=L0x2001617c,
cg31027=L0x20016180,cg32027=L0x20016184,cg30127=L0x20016188,cg31127=L0x2001618c,
cg32127=L0x20016190,cg30227=L0x20016194,cg31227=L0x20016198,cg32227=L0x2001619c,
cg30028=L0x200161e8,cg31028=L0x200161ec,cg32028=L0x200161f0,cg30128=L0x200161f4,
cg31128=L0x200161f8,cg32128=L0x200161fc,cg30228=L0x20016200,cg31228=L0x20016204,
cg32228=L0x20016208,cg30029=L0x20016254,cg31029=L0x20016258,cg32029=L0x2001625c,
cg30129=L0x20016260,cg31129=L0x20016264,cg32129=L0x20016268,cg30229=L0x2001626c,
cg31229=L0x20016270,cg32229=L0x20016274,cg30030=L0x200162c0,cg31030=L0x200162c4,
cg32030=L0x200162c8,cg30130=L0x200162cc,cg31130=L0x200162d0,cg32130=L0x200162d4,
cg30230=L0x200162d8,cg31230=L0x200162dc,cg32230=L0x200162e0,cg30031=L0x2001632c,
cg31031=L0x20016330,cg32031=L0x20016334,cg30131=L0x20016338,cg31131=L0x2001633c,
cg32131=L0x20016340,cg30231=L0x20016344,cg31231=L0x20016348,cg32231=L0x2001634c,
CG30**2 = 
cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16+
cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17+
cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18+
cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19+
cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20+
cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21+
cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22+
cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23+
cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24+
cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25+
cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26+
cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27+
cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28+
cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29+
cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30+
cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31,
CG31**2 = 
cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16+
cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17+
cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18+
cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19+
cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20+
cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21+
cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22+
cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23+
cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24+
cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25+
cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26+
cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27+
cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28+
cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29+
cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30+
cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31,
CG32**2 = 
cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16+
cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17+
cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18+
cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19+
cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20+
cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21+
cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22+
cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23+
cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24+
cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25+
cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26+
cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27+
cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28+
cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29+
cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30+
cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31
] && true;


(**************** CUT 333, - *****************)

ecut true;

(**************** CUT 334, - *****************)

ecut eqmod CG30**2 CF0**2 [ 3365569, y**3 -       1, z**32 - 3365568 ]
prove with [ all ghosts, cuts [ 298 ] ];


(**************** CUT 335, - *****************)

ecut true;

(**************** CUT 336, - *****************)

ecut eqmod CG31**2 CF1**2 [ 3365569, y**3 -       1, z**32 - 3365568 ]
prove with [ all ghosts, cuts [ 299 ] ];


(**************** CUT 337, - *****************)

ecut true;

(**************** CUT 338, - *****************)

ecut eqmod CG32**2 CF2**2 [ 3365569, y**3 -       1, z**32 - 3365568 ]
prove with [ all ghosts, cuts [ 300 ] ];


(**************** CUT 339, - *****************)

ecut true;

(**************** CUT 340, - *****************)

ecut eqmod CG30**2+CG31**2+CG32**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -       1, z**32 - 3365568 ]
prove with [ cuts [ 334, 336, 338 ] ];




(******************** output polynomial 4 ********************)

ghost
CG40@sint32,
cg40000@sint32, cg40100@sint32, cg40200@sint32, cg40001@sint32, cg40101@sint32,
cg40201@sint32, cg40002@sint32, cg40102@sint32, cg40202@sint32, cg40003@sint32,
cg40103@sint32, cg40203@sint32, cg40004@sint32, cg40104@sint32, cg40204@sint32,
cg40005@sint32, cg40105@sint32, cg40205@sint32, cg40006@sint32, cg40106@sint32,
cg40206@sint32, cg40007@sint32, cg40107@sint32, cg40207@sint32, cg40008@sint32,
cg40108@sint32, cg40208@sint32, cg40009@sint32, cg40109@sint32, cg40209@sint32,
cg40010@sint32, cg40110@sint32, cg40210@sint32, cg40011@sint32, cg40111@sint32,
cg40211@sint32, cg40012@sint32, cg40112@sint32, cg40212@sint32, cg40013@sint32,
cg40113@sint32, cg40213@sint32, cg40014@sint32, cg40114@sint32, cg40214@sint32,
cg40015@sint32, cg40115@sint32, cg40215@sint32, cg40016@sint32, cg40116@sint32,
cg40216@sint32, cg40017@sint32, cg40117@sint32, cg40217@sint32, cg40018@sint32,
cg40118@sint32, cg40218@sint32, cg40019@sint32, cg40119@sint32, cg40219@sint32,
cg40020@sint32, cg40120@sint32, cg40220@sint32, cg40021@sint32, cg40121@sint32,
cg40221@sint32, cg40022@sint32, cg40122@sint32, cg40222@sint32, cg40023@sint32,
cg40123@sint32, cg40223@sint32, cg40024@sint32, cg40124@sint32, cg40224@sint32,
cg40025@sint32, cg40125@sint32, cg40225@sint32, cg40026@sint32, cg40126@sint32,
cg40226@sint32, cg40027@sint32, cg40127@sint32, cg40227@sint32, cg40028@sint32,
cg40128@sint32, cg40228@sint32, cg40029@sint32, cg40129@sint32, cg40229@sint32,
cg40030@sint32, cg40130@sint32, cg40230@sint32, cg40031@sint32, cg40131@sint32,
cg40231@sint32,
CG41@sint32,
cg41000@sint32, cg41100@sint32, cg41200@sint32, cg41001@sint32, cg41101@sint32,
cg41201@sint32, cg41002@sint32, cg41102@sint32, cg41202@sint32, cg41003@sint32,
cg41103@sint32, cg41203@sint32, cg41004@sint32, cg41104@sint32, cg41204@sint32,
cg41005@sint32, cg41105@sint32, cg41205@sint32, cg41006@sint32, cg41106@sint32,
cg41206@sint32, cg41007@sint32, cg41107@sint32, cg41207@sint32, cg41008@sint32,
cg41108@sint32, cg41208@sint32, cg41009@sint32, cg41109@sint32, cg41209@sint32,
cg41010@sint32, cg41110@sint32, cg41210@sint32, cg41011@sint32, cg41111@sint32,
cg41211@sint32, cg41012@sint32, cg41112@sint32, cg41212@sint32, cg41013@sint32,
cg41113@sint32, cg41213@sint32, cg41014@sint32, cg41114@sint32, cg41214@sint32,
cg41015@sint32, cg41115@sint32, cg41215@sint32, cg41016@sint32, cg41116@sint32,
cg41216@sint32, cg41017@sint32, cg41117@sint32, cg41217@sint32, cg41018@sint32,
cg41118@sint32, cg41218@sint32, cg41019@sint32, cg41119@sint32, cg41219@sint32,
cg41020@sint32, cg41120@sint32, cg41220@sint32, cg41021@sint32, cg41121@sint32,
cg41221@sint32, cg41022@sint32, cg41122@sint32, cg41222@sint32, cg41023@sint32,
cg41123@sint32, cg41223@sint32, cg41024@sint32, cg41124@sint32, cg41224@sint32,
cg41025@sint32, cg41125@sint32, cg41225@sint32, cg41026@sint32, cg41126@sint32,
cg41226@sint32, cg41027@sint32, cg41127@sint32, cg41227@sint32, cg41028@sint32,
cg41128@sint32, cg41228@sint32, cg41029@sint32, cg41129@sint32, cg41229@sint32,
cg41030@sint32, cg41130@sint32, cg41230@sint32, cg41031@sint32, cg41131@sint32,
cg41231@sint32,
CG42@sint32,
cg42000@sint32, cg42100@sint32, cg42200@sint32, cg42001@sint32, cg42101@sint32,
cg42201@sint32, cg42002@sint32, cg42102@sint32, cg42202@sint32, cg42003@sint32,
cg42103@sint32, cg42203@sint32, cg42004@sint32, cg42104@sint32, cg42204@sint32,
cg42005@sint32, cg42105@sint32, cg42205@sint32, cg42006@sint32, cg42106@sint32,
cg42206@sint32, cg42007@sint32, cg42107@sint32, cg42207@sint32, cg42008@sint32,
cg42108@sint32, cg42208@sint32, cg42009@sint32, cg42109@sint32, cg42209@sint32,
cg42010@sint32, cg42110@sint32, cg42210@sint32, cg42011@sint32, cg42111@sint32,
cg42211@sint32, cg42012@sint32, cg42112@sint32, cg42212@sint32, cg42013@sint32,
cg42113@sint32, cg42213@sint32, cg42014@sint32, cg42114@sint32, cg42214@sint32,
cg42015@sint32, cg42115@sint32, cg42215@sint32, cg42016@sint32, cg42116@sint32,
cg42216@sint32, cg42017@sint32, cg42117@sint32, cg42217@sint32, cg42018@sint32,
cg42118@sint32, cg42218@sint32, cg42019@sint32, cg42119@sint32, cg42219@sint32,
cg42020@sint32, cg42120@sint32, cg42220@sint32, cg42021@sint32, cg42121@sint32,
cg42221@sint32, cg42022@sint32, cg42122@sint32, cg42222@sint32, cg42023@sint32,
cg42123@sint32, cg42223@sint32, cg42024@sint32, cg42124@sint32, cg42224@sint32,
cg42025@sint32, cg42125@sint32, cg42225@sint32, cg42026@sint32, cg42126@sint32,
cg42226@sint32, cg42027@sint32, cg42127@sint32, cg42227@sint32, cg42028@sint32,
cg42128@sint32, cg42228@sint32, cg42029@sint32, cg42129@sint32, cg42229@sint32,
cg42030@sint32, cg42130@sint32, cg42230@sint32, cg42031@sint32, cg42131@sint32,
cg42231@sint32 : and [
cg40000=L0x2001563c,cg41000=L0x20015640,cg42000=L0x20015644,cg40100=L0x20015648,
cg41100=L0x2001564c,cg42100=L0x20015650,cg40200=L0x20015654,cg41200=L0x20015658,
cg42200=L0x2001565c,cg40001=L0x200156a8,cg41001=L0x200156ac,cg42001=L0x200156b0,
cg40101=L0x200156b4,cg41101=L0x200156b8,cg42101=L0x200156bc,cg40201=L0x200156c0,
cg41201=L0x200156c4,cg42201=L0x200156c8,cg40002=L0x20015714,cg41002=L0x20015718,
cg42002=L0x2001571c,cg40102=L0x20015720,cg41102=L0x20015724,cg42102=L0x20015728,
cg40202=L0x2001572c,cg41202=L0x20015730,cg42202=L0x20015734,cg40003=L0x20015780,
cg41003=L0x20015784,cg42003=L0x20015788,cg40103=L0x2001578c,cg41103=L0x20015790,
cg42103=L0x20015794,cg40203=L0x20015798,cg41203=L0x2001579c,cg42203=L0x200157a0,
cg40004=L0x200157ec,cg41004=L0x200157f0,cg42004=L0x200157f4,cg40104=L0x200157f8,
cg41104=L0x200157fc,cg42104=L0x20015800,cg40204=L0x20015804,cg41204=L0x20015808,
cg42204=L0x2001580c,cg40005=L0x20015858,cg41005=L0x2001585c,cg42005=L0x20015860,
cg40105=L0x20015864,cg41105=L0x20015868,cg42105=L0x2001586c,cg40205=L0x20015870,
cg41205=L0x20015874,cg42205=L0x20015878,cg40006=L0x200158c4,cg41006=L0x200158c8,
cg42006=L0x200158cc,cg40106=L0x200158d0,cg41106=L0x200158d4,cg42106=L0x200158d8,
cg40206=L0x200158dc,cg41206=L0x200158e0,cg42206=L0x200158e4,cg40007=L0x20015930,
cg41007=L0x20015934,cg42007=L0x20015938,cg40107=L0x2001593c,cg41107=L0x20015940,
cg42107=L0x20015944,cg40207=L0x20015948,cg41207=L0x2001594c,cg42207=L0x20015950,
cg40008=L0x2001599c,cg41008=L0x200159a0,cg42008=L0x200159a4,cg40108=L0x200159a8,
cg41108=L0x200159ac,cg42108=L0x200159b0,cg40208=L0x200159b4,cg41208=L0x200159b8,
cg42208=L0x200159bc,cg40009=L0x20015a08,cg41009=L0x20015a0c,cg42009=L0x20015a10,
cg40109=L0x20015a14,cg41109=L0x20015a18,cg42109=L0x20015a1c,cg40209=L0x20015a20,
cg41209=L0x20015a24,cg42209=L0x20015a28,cg40010=L0x20015a74,cg41010=L0x20015a78,
cg42010=L0x20015a7c,cg40110=L0x20015a80,cg41110=L0x20015a84,cg42110=L0x20015a88,
cg40210=L0x20015a8c,cg41210=L0x20015a90,cg42210=L0x20015a94,cg40011=L0x20015ae0,
cg41011=L0x20015ae4,cg42011=L0x20015ae8,cg40111=L0x20015aec,cg41111=L0x20015af0,
cg42111=L0x20015af4,cg40211=L0x20015af8,cg41211=L0x20015afc,cg42211=L0x20015b00,
cg40012=L0x20015b4c,cg41012=L0x20015b50,cg42012=L0x20015b54,cg40112=L0x20015b58,
cg41112=L0x20015b5c,cg42112=L0x20015b60,cg40212=L0x20015b64,cg41212=L0x20015b68,
cg42212=L0x20015b6c,cg40013=L0x20015bb8,cg41013=L0x20015bbc,cg42013=L0x20015bc0,
cg40113=L0x20015bc4,cg41113=L0x20015bc8,cg42113=L0x20015bcc,cg40213=L0x20015bd0,
cg41213=L0x20015bd4,cg42213=L0x20015bd8,cg40014=L0x20015c24,cg41014=L0x20015c28,
cg42014=L0x20015c2c,cg40114=L0x20015c30,cg41114=L0x20015c34,cg42114=L0x20015c38,
cg40214=L0x20015c3c,cg41214=L0x20015c40,cg42214=L0x20015c44,cg40015=L0x20015c90,
cg41015=L0x20015c94,cg42015=L0x20015c98,cg40115=L0x20015c9c,cg41115=L0x20015ca0,
cg42115=L0x20015ca4,cg40215=L0x20015ca8,cg41215=L0x20015cac,cg42215=L0x20015cb0,
cg40016=L0x20015cfc,cg41016=L0x20015d00,cg42016=L0x20015d04,cg40116=L0x20015d08,
cg41116=L0x20015d0c,cg42116=L0x20015d10,cg40216=L0x20015d14,cg41216=L0x20015d18,
cg42216=L0x20015d1c,cg40017=L0x20015d68,cg41017=L0x20015d6c,cg42017=L0x20015d70,
cg40117=L0x20015d74,cg41117=L0x20015d78,cg42117=L0x20015d7c,cg40217=L0x20015d80,
cg41217=L0x20015d84,cg42217=L0x20015d88,cg40018=L0x20015dd4,cg41018=L0x20015dd8,
cg42018=L0x20015ddc,cg40118=L0x20015de0,cg41118=L0x20015de4,cg42118=L0x20015de8,
cg40218=L0x20015dec,cg41218=L0x20015df0,cg42218=L0x20015df4,cg40019=L0x20015e40,
cg41019=L0x20015e44,cg42019=L0x20015e48,cg40119=L0x20015e4c,cg41119=L0x20015e50,
cg42119=L0x20015e54,cg40219=L0x20015e58,cg41219=L0x20015e5c,cg42219=L0x20015e60,
cg40020=L0x20015eac,cg41020=L0x20015eb0,cg42020=L0x20015eb4,cg40120=L0x20015eb8,
cg41120=L0x20015ebc,cg42120=L0x20015ec0,cg40220=L0x20015ec4,cg41220=L0x20015ec8,
cg42220=L0x20015ecc,cg40021=L0x20015f18,cg41021=L0x20015f1c,cg42021=L0x20015f20,
cg40121=L0x20015f24,cg41121=L0x20015f28,cg42121=L0x20015f2c,cg40221=L0x20015f30,
cg41221=L0x20015f34,cg42221=L0x20015f38,cg40022=L0x20015f84,cg41022=L0x20015f88,
cg42022=L0x20015f8c,cg40122=L0x20015f90,cg41122=L0x20015f94,cg42122=L0x20015f98,
cg40222=L0x20015f9c,cg41222=L0x20015fa0,cg42222=L0x20015fa4,cg40023=L0x20015ff0,
cg41023=L0x20015ff4,cg42023=L0x20015ff8,cg40123=L0x20015ffc,cg41123=L0x20016000,
cg42123=L0x20016004,cg40223=L0x20016008,cg41223=L0x2001600c,cg42223=L0x20016010,
cg40024=L0x2001605c,cg41024=L0x20016060,cg42024=L0x20016064,cg40124=L0x20016068,
cg41124=L0x2001606c,cg42124=L0x20016070,cg40224=L0x20016074,cg41224=L0x20016078,
cg42224=L0x2001607c,cg40025=L0x200160c8,cg41025=L0x200160cc,cg42025=L0x200160d0,
cg40125=L0x200160d4,cg41125=L0x200160d8,cg42125=L0x200160dc,cg40225=L0x200160e0,
cg41225=L0x200160e4,cg42225=L0x200160e8,cg40026=L0x20016134,cg41026=L0x20016138,
cg42026=L0x2001613c,cg40126=L0x20016140,cg41126=L0x20016144,cg42126=L0x20016148,
cg40226=L0x2001614c,cg41226=L0x20016150,cg42226=L0x20016154,cg40027=L0x200161a0,
cg41027=L0x200161a4,cg42027=L0x200161a8,cg40127=L0x200161ac,cg41127=L0x200161b0,
cg42127=L0x200161b4,cg40227=L0x200161b8,cg41227=L0x200161bc,cg42227=L0x200161c0,
cg40028=L0x2001620c,cg41028=L0x20016210,cg42028=L0x20016214,cg40128=L0x20016218,
cg41128=L0x2001621c,cg42128=L0x20016220,cg40228=L0x20016224,cg41228=L0x20016228,
cg42228=L0x2001622c,cg40029=L0x20016278,cg41029=L0x2001627c,cg42029=L0x20016280,
cg40129=L0x20016284,cg41129=L0x20016288,cg42129=L0x2001628c,cg40229=L0x20016290,
cg41229=L0x20016294,cg42229=L0x20016298,cg40030=L0x200162e4,cg41030=L0x200162e8,
cg42030=L0x200162ec,cg40130=L0x200162f0,cg41130=L0x200162f4,cg42130=L0x200162f8,
cg40230=L0x200162fc,cg41230=L0x20016300,cg42230=L0x20016304,cg40031=L0x20016350,
cg41031=L0x20016354,cg42031=L0x20016358,cg40131=L0x2001635c,cg41131=L0x20016360,
cg42131=L0x20016364,cg40231=L0x20016368,cg41231=L0x2001636c,cg42231=L0x20016370,
CG40**2 = 
cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16+
cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17+
cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18+
cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19+
cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20+
cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21+
cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22+
cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23+
cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24+
cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25+
cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26+
cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27+
cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28+
cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29+
cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30+
cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31,
CG41**2 = 
cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16+
cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17+
cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18+
cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19+
cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20+
cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21+
cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22+
cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23+
cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24+
cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25+
cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26+
cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27+
cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28+
cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29+
cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30+
cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31,
CG42**2 = 
cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16+
cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17+
cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18+
cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19+
cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20+
cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21+
cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22+
cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23+
cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24+
cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25+
cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26+
cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27+
cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28+
cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29+
cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30+
cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31
] && true;


(**************** CUT 341, - *****************)

ecut true;

(**************** CUT 342, - *****************)

ecut eqmod CG40**2 CF0**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ]
prove with [ all ghosts, cuts [ 301 ] ];


(**************** CUT 343, - *****************)

ecut true;

(**************** CUT 344, - *****************)

ecut eqmod CG41**2 CF1**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ]
prove with [ all ghosts, cuts [ 302 ] ];


(**************** CUT 345, - *****************)

ecut true;

(**************** CUT 346, - *****************)

ecut eqmod CG42**2 CF2**2 [ 3365569, y**3 -  452650, z**32 - 3365568 ]
prove with [ all ghosts, cuts [ 303 ] ];


(**************** CUT 347, - *****************)

ecut true;

(**************** CUT 348, - *****************)

ecut eqmod CG40**2+CG41**2+CG42**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -  452650, z**32 - 3365568 ]
prove with [ cuts [ 342, 344, 346 ] ];




(******************** output polynomial 5 ********************)

ghost
CG50@sint32,
cg50000@sint32, cg50100@sint32, cg50200@sint32, cg50001@sint32, cg50101@sint32,
cg50201@sint32, cg50002@sint32, cg50102@sint32, cg50202@sint32, cg50003@sint32,
cg50103@sint32, cg50203@sint32, cg50004@sint32, cg50104@sint32, cg50204@sint32,
cg50005@sint32, cg50105@sint32, cg50205@sint32, cg50006@sint32, cg50106@sint32,
cg50206@sint32, cg50007@sint32, cg50107@sint32, cg50207@sint32, cg50008@sint32,
cg50108@sint32, cg50208@sint32, cg50009@sint32, cg50109@sint32, cg50209@sint32,
cg50010@sint32, cg50110@sint32, cg50210@sint32, cg50011@sint32, cg50111@sint32,
cg50211@sint32, cg50012@sint32, cg50112@sint32, cg50212@sint32, cg50013@sint32,
cg50113@sint32, cg50213@sint32, cg50014@sint32, cg50114@sint32, cg50214@sint32,
cg50015@sint32, cg50115@sint32, cg50215@sint32, cg50016@sint32, cg50116@sint32,
cg50216@sint32, cg50017@sint32, cg50117@sint32, cg50217@sint32, cg50018@sint32,
cg50118@sint32, cg50218@sint32, cg50019@sint32, cg50119@sint32, cg50219@sint32,
cg50020@sint32, cg50120@sint32, cg50220@sint32, cg50021@sint32, cg50121@sint32,
cg50221@sint32, cg50022@sint32, cg50122@sint32, cg50222@sint32, cg50023@sint32,
cg50123@sint32, cg50223@sint32, cg50024@sint32, cg50124@sint32, cg50224@sint32,
cg50025@sint32, cg50125@sint32, cg50225@sint32, cg50026@sint32, cg50126@sint32,
cg50226@sint32, cg50027@sint32, cg50127@sint32, cg50227@sint32, cg50028@sint32,
cg50128@sint32, cg50228@sint32, cg50029@sint32, cg50129@sint32, cg50229@sint32,
cg50030@sint32, cg50130@sint32, cg50230@sint32, cg50031@sint32, cg50131@sint32,
cg50231@sint32,
CG51@sint32,
cg51000@sint32, cg51100@sint32, cg51200@sint32, cg51001@sint32, cg51101@sint32,
cg51201@sint32, cg51002@sint32, cg51102@sint32, cg51202@sint32, cg51003@sint32,
cg51103@sint32, cg51203@sint32, cg51004@sint32, cg51104@sint32, cg51204@sint32,
cg51005@sint32, cg51105@sint32, cg51205@sint32, cg51006@sint32, cg51106@sint32,
cg51206@sint32, cg51007@sint32, cg51107@sint32, cg51207@sint32, cg51008@sint32,
cg51108@sint32, cg51208@sint32, cg51009@sint32, cg51109@sint32, cg51209@sint32,
cg51010@sint32, cg51110@sint32, cg51210@sint32, cg51011@sint32, cg51111@sint32,
cg51211@sint32, cg51012@sint32, cg51112@sint32, cg51212@sint32, cg51013@sint32,
cg51113@sint32, cg51213@sint32, cg51014@sint32, cg51114@sint32, cg51214@sint32,
cg51015@sint32, cg51115@sint32, cg51215@sint32, cg51016@sint32, cg51116@sint32,
cg51216@sint32, cg51017@sint32, cg51117@sint32, cg51217@sint32, cg51018@sint32,
cg51118@sint32, cg51218@sint32, cg51019@sint32, cg51119@sint32, cg51219@sint32,
cg51020@sint32, cg51120@sint32, cg51220@sint32, cg51021@sint32, cg51121@sint32,
cg51221@sint32, cg51022@sint32, cg51122@sint32, cg51222@sint32, cg51023@sint32,
cg51123@sint32, cg51223@sint32, cg51024@sint32, cg51124@sint32, cg51224@sint32,
cg51025@sint32, cg51125@sint32, cg51225@sint32, cg51026@sint32, cg51126@sint32,
cg51226@sint32, cg51027@sint32, cg51127@sint32, cg51227@sint32, cg51028@sint32,
cg51128@sint32, cg51228@sint32, cg51029@sint32, cg51129@sint32, cg51229@sint32,
cg51030@sint32, cg51130@sint32, cg51230@sint32, cg51031@sint32, cg51131@sint32,
cg51231@sint32,
CG52@sint32,
cg52000@sint32, cg52100@sint32, cg52200@sint32, cg52001@sint32, cg52101@sint32,
cg52201@sint32, cg52002@sint32, cg52102@sint32, cg52202@sint32, cg52003@sint32,
cg52103@sint32, cg52203@sint32, cg52004@sint32, cg52104@sint32, cg52204@sint32,
cg52005@sint32, cg52105@sint32, cg52205@sint32, cg52006@sint32, cg52106@sint32,
cg52206@sint32, cg52007@sint32, cg52107@sint32, cg52207@sint32, cg52008@sint32,
cg52108@sint32, cg52208@sint32, cg52009@sint32, cg52109@sint32, cg52209@sint32,
cg52010@sint32, cg52110@sint32, cg52210@sint32, cg52011@sint32, cg52111@sint32,
cg52211@sint32, cg52012@sint32, cg52112@sint32, cg52212@sint32, cg52013@sint32,
cg52113@sint32, cg52213@sint32, cg52014@sint32, cg52114@sint32, cg52214@sint32,
cg52015@sint32, cg52115@sint32, cg52215@sint32, cg52016@sint32, cg52116@sint32,
cg52216@sint32, cg52017@sint32, cg52117@sint32, cg52217@sint32, cg52018@sint32,
cg52118@sint32, cg52218@sint32, cg52019@sint32, cg52119@sint32, cg52219@sint32,
cg52020@sint32, cg52120@sint32, cg52220@sint32, cg52021@sint32, cg52121@sint32,
cg52221@sint32, cg52022@sint32, cg52122@sint32, cg52222@sint32, cg52023@sint32,
cg52123@sint32, cg52223@sint32, cg52024@sint32, cg52124@sint32, cg52224@sint32,
cg52025@sint32, cg52125@sint32, cg52225@sint32, cg52026@sint32, cg52126@sint32,
cg52226@sint32, cg52027@sint32, cg52127@sint32, cg52227@sint32, cg52028@sint32,
cg52128@sint32, cg52228@sint32, cg52029@sint32, cg52129@sint32, cg52229@sint32,
cg52030@sint32, cg52130@sint32, cg52230@sint32, cg52031@sint32, cg52131@sint32,
cg52231@sint32 : and [
cg50000=L0x20015660,cg51000=L0x20015664,cg52000=L0x20015668,cg50100=L0x2001566c,
cg51100=L0x20015670,cg52100=L0x20015674,cg50200=L0x20015678,cg51200=L0x2001567c,
cg52200=L0x20015680,cg50001=L0x200156cc,cg51001=L0x200156d0,cg52001=L0x200156d4,
cg50101=L0x200156d8,cg51101=L0x200156dc,cg52101=L0x200156e0,cg50201=L0x200156e4,
cg51201=L0x200156e8,cg52201=L0x200156ec,cg50002=L0x20015738,cg51002=L0x2001573c,
cg52002=L0x20015740,cg50102=L0x20015744,cg51102=L0x20015748,cg52102=L0x2001574c,
cg50202=L0x20015750,cg51202=L0x20015754,cg52202=L0x20015758,cg50003=L0x200157a4,
cg51003=L0x200157a8,cg52003=L0x200157ac,cg50103=L0x200157b0,cg51103=L0x200157b4,
cg52103=L0x200157b8,cg50203=L0x200157bc,cg51203=L0x200157c0,cg52203=L0x200157c4,
cg50004=L0x20015810,cg51004=L0x20015814,cg52004=L0x20015818,cg50104=L0x2001581c,
cg51104=L0x20015820,cg52104=L0x20015824,cg50204=L0x20015828,cg51204=L0x2001582c,
cg52204=L0x20015830,cg50005=L0x2001587c,cg51005=L0x20015880,cg52005=L0x20015884,
cg50105=L0x20015888,cg51105=L0x2001588c,cg52105=L0x20015890,cg50205=L0x20015894,
cg51205=L0x20015898,cg52205=L0x2001589c,cg50006=L0x200158e8,cg51006=L0x200158ec,
cg52006=L0x200158f0,cg50106=L0x200158f4,cg51106=L0x200158f8,cg52106=L0x200158fc,
cg50206=L0x20015900,cg51206=L0x20015904,cg52206=L0x20015908,cg50007=L0x20015954,
cg51007=L0x20015958,cg52007=L0x2001595c,cg50107=L0x20015960,cg51107=L0x20015964,
cg52107=L0x20015968,cg50207=L0x2001596c,cg51207=L0x20015970,cg52207=L0x20015974,
cg50008=L0x200159c0,cg51008=L0x200159c4,cg52008=L0x200159c8,cg50108=L0x200159cc,
cg51108=L0x200159d0,cg52108=L0x200159d4,cg50208=L0x200159d8,cg51208=L0x200159dc,
cg52208=L0x200159e0,cg50009=L0x20015a2c,cg51009=L0x20015a30,cg52009=L0x20015a34,
cg50109=L0x20015a38,cg51109=L0x20015a3c,cg52109=L0x20015a40,cg50209=L0x20015a44,
cg51209=L0x20015a48,cg52209=L0x20015a4c,cg50010=L0x20015a98,cg51010=L0x20015a9c,
cg52010=L0x20015aa0,cg50110=L0x20015aa4,cg51110=L0x20015aa8,cg52110=L0x20015aac,
cg50210=L0x20015ab0,cg51210=L0x20015ab4,cg52210=L0x20015ab8,cg50011=L0x20015b04,
cg51011=L0x20015b08,cg52011=L0x20015b0c,cg50111=L0x20015b10,cg51111=L0x20015b14,
cg52111=L0x20015b18,cg50211=L0x20015b1c,cg51211=L0x20015b20,cg52211=L0x20015b24,
cg50012=L0x20015b70,cg51012=L0x20015b74,cg52012=L0x20015b78,cg50112=L0x20015b7c,
cg51112=L0x20015b80,cg52112=L0x20015b84,cg50212=L0x20015b88,cg51212=L0x20015b8c,
cg52212=L0x20015b90,cg50013=L0x20015bdc,cg51013=L0x20015be0,cg52013=L0x20015be4,
cg50113=L0x20015be8,cg51113=L0x20015bec,cg52113=L0x20015bf0,cg50213=L0x20015bf4,
cg51213=L0x20015bf8,cg52213=L0x20015bfc,cg50014=L0x20015c48,cg51014=L0x20015c4c,
cg52014=L0x20015c50,cg50114=L0x20015c54,cg51114=L0x20015c58,cg52114=L0x20015c5c,
cg50214=L0x20015c60,cg51214=L0x20015c64,cg52214=L0x20015c68,cg50015=L0x20015cb4,
cg51015=L0x20015cb8,cg52015=L0x20015cbc,cg50115=L0x20015cc0,cg51115=L0x20015cc4,
cg52115=L0x20015cc8,cg50215=L0x20015ccc,cg51215=L0x20015cd0,cg52215=L0x20015cd4,
cg50016=L0x20015d20,cg51016=L0x20015d24,cg52016=L0x20015d28,cg50116=L0x20015d2c,
cg51116=L0x20015d30,cg52116=L0x20015d34,cg50216=L0x20015d38,cg51216=L0x20015d3c,
cg52216=L0x20015d40,cg50017=L0x20015d8c,cg51017=L0x20015d90,cg52017=L0x20015d94,
cg50117=L0x20015d98,cg51117=L0x20015d9c,cg52117=L0x20015da0,cg50217=L0x20015da4,
cg51217=L0x20015da8,cg52217=L0x20015dac,cg50018=L0x20015df8,cg51018=L0x20015dfc,
cg52018=L0x20015e00,cg50118=L0x20015e04,cg51118=L0x20015e08,cg52118=L0x20015e0c,
cg50218=L0x20015e10,cg51218=L0x20015e14,cg52218=L0x20015e18,cg50019=L0x20015e64,
cg51019=L0x20015e68,cg52019=L0x20015e6c,cg50119=L0x20015e70,cg51119=L0x20015e74,
cg52119=L0x20015e78,cg50219=L0x20015e7c,cg51219=L0x20015e80,cg52219=L0x20015e84,
cg50020=L0x20015ed0,cg51020=L0x20015ed4,cg52020=L0x20015ed8,cg50120=L0x20015edc,
cg51120=L0x20015ee0,cg52120=L0x20015ee4,cg50220=L0x20015ee8,cg51220=L0x20015eec,
cg52220=L0x20015ef0,cg50021=L0x20015f3c,cg51021=L0x20015f40,cg52021=L0x20015f44,
cg50121=L0x20015f48,cg51121=L0x20015f4c,cg52121=L0x20015f50,cg50221=L0x20015f54,
cg51221=L0x20015f58,cg52221=L0x20015f5c,cg50022=L0x20015fa8,cg51022=L0x20015fac,
cg52022=L0x20015fb0,cg50122=L0x20015fb4,cg51122=L0x20015fb8,cg52122=L0x20015fbc,
cg50222=L0x20015fc0,cg51222=L0x20015fc4,cg52222=L0x20015fc8,cg50023=L0x20016014,
cg51023=L0x20016018,cg52023=L0x2001601c,cg50123=L0x20016020,cg51123=L0x20016024,
cg52123=L0x20016028,cg50223=L0x2001602c,cg51223=L0x20016030,cg52223=L0x20016034,
cg50024=L0x20016080,cg51024=L0x20016084,cg52024=L0x20016088,cg50124=L0x2001608c,
cg51124=L0x20016090,cg52124=L0x20016094,cg50224=L0x20016098,cg51224=L0x2001609c,
cg52224=L0x200160a0,cg50025=L0x200160ec,cg51025=L0x200160f0,cg52025=L0x200160f4,
cg50125=L0x200160f8,cg51125=L0x200160fc,cg52125=L0x20016100,cg50225=L0x20016104,
cg51225=L0x20016108,cg52225=L0x2001610c,cg50026=L0x20016158,cg51026=L0x2001615c,
cg52026=L0x20016160,cg50126=L0x20016164,cg51126=L0x20016168,cg52126=L0x2001616c,
cg50226=L0x20016170,cg51226=L0x20016174,cg52226=L0x20016178,cg50027=L0x200161c4,
cg51027=L0x200161c8,cg52027=L0x200161cc,cg50127=L0x200161d0,cg51127=L0x200161d4,
cg52127=L0x200161d8,cg50227=L0x200161dc,cg51227=L0x200161e0,cg52227=L0x200161e4,
cg50028=L0x20016230,cg51028=L0x20016234,cg52028=L0x20016238,cg50128=L0x2001623c,
cg51128=L0x20016240,cg52128=L0x20016244,cg50228=L0x20016248,cg51228=L0x2001624c,
cg52228=L0x20016250,cg50029=L0x2001629c,cg51029=L0x200162a0,cg52029=L0x200162a4,
cg50129=L0x200162a8,cg51129=L0x200162ac,cg52129=L0x200162b0,cg50229=L0x200162b4,
cg51229=L0x200162b8,cg52229=L0x200162bc,cg50030=L0x20016308,cg51030=L0x2001630c,
cg52030=L0x20016310,cg50130=L0x20016314,cg51130=L0x20016318,cg52130=L0x2001631c,
cg50230=L0x20016320,cg51230=L0x20016324,cg52230=L0x20016328,cg50031=L0x20016374,
cg51031=L0x20016378,cg52031=L0x2001637c,cg50131=L0x20016380,cg51131=L0x20016384,
cg52131=L0x20016388,cg50231=L0x2001638c,cg51231=L0x20016390,cg52231=L0x20016394,
CG50**2 = 
cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16+
cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17+
cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18+
cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19+
cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20+
cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21+
cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22+
cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23+
cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24+
cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25+
cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26+
cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27+
cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28+
cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29+
cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30+
cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31,
CG51**2 = 
cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16+
cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17+
cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18+
cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19+
cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20+
cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21+
cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22+
cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23+
cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24+
cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25+
cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26+
cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27+
cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28+
cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29+
cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30+
cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31,
CG52**2 = 
cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16+
cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17+
cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18+
cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19+
cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20+
cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21+
cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22+
cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23+
cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24+
cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25+
cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26+
cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27+
cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28+
cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29+
cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30+
cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31
] && true;


(**************** CUT 349, - *****************)

ecut true;

(**************** CUT 350, - *****************)

ecut eqmod CG50**2 CF0**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
prove with [ all ghosts, cuts [ 304 ] ];


(**************** CUT 351, - *****************)

ecut true;

(**************** CUT 352, - *****************)

ecut eqmod CG51**2 CF1**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
prove with [ all ghosts, cuts [ 305 ] ];


(**************** CUT 353, - *****************)

ecut true;

(**************** CUT 354, - *****************)

ecut eqmod CG52**2 CF2**2 [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
prove with [ all ghosts, cuts [ 306 ] ];


(**************** CUT 355, - *****************)

ecut true;

(**************** CUT 356, - *****************)

ecut eqmod CG50**2+CG51**2+CG52**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
prove with [ cuts [ 350, 352, 354 ] ];




{
(******************** postcondition ********************)

and [
eqmod CF0**2+CF1**2+CF2**2 F**2 2048,
eqmod CG00**2+CG01**2+CG02**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -       1, z**32 -       1 ],
eqmod CG10**2+CG11**2+CG12**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -  452650, z**32 -       1 ],
eqmod CG20**2+CG21**2+CG22**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 - 2912918, z**32 -       1 ],
eqmod CG30**2+CG31**2+CG32**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -       1, z**32 - 3365568 ],
eqmod CG40**2+CG41**2+CG42**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -  452650, z**32 - 3365568 ],
eqmod CG50**2+CG51**2+CG52**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 - 2912918, z**32 - 3365568 ]
] prove with [ cuts [ 308, 316, 324, 332, 340, 348, 356 ] ] && 
and [
(-3367617)@32<=sL0x20014898,L0x20014898<=s3367617@32,
(-3367617)@32<=sL0x200148bc,L0x200148bc<=s3367617@32,
(-3367617)@32<=sL0x200148e0,L0x200148e0<=s3367617@32,
(-3367617)@32<=sL0x20015618,L0x20015618<=s3367617@32,
(-3367617)@32<=sL0x2001563c,L0x2001563c<=s3367617@32,
(-3367617)@32<=sL0x20015660,L0x20015660<=s3367617@32
,
(-3367617)@32<=sL0x20014904,L0x20014904<=s3367617@32,
(-3367617)@32<=sL0x20014928,L0x20014928<=s3367617@32,
(-3367617)@32<=sL0x2001494c,L0x2001494c<=s3367617@32,
(-3367617)@32<=sL0x20015684,L0x20015684<=s3367617@32,
(-3367617)@32<=sL0x200156a8,L0x200156a8<=s3367617@32,
(-3367617)@32<=sL0x200156cc,L0x200156cc<=s3367617@32
,
(-3367617)@32<=sL0x20014970,L0x20014970<=s3367617@32,
(-3367617)@32<=sL0x20014994,L0x20014994<=s3367617@32,
(-3367617)@32<=sL0x200149b8,L0x200149b8<=s3367617@32,
(-3367617)@32<=sL0x200156f0,L0x200156f0<=s3367617@32,
(-3367617)@32<=sL0x20015714,L0x20015714<=s3367617@32,
(-3367617)@32<=sL0x20015738,L0x20015738<=s3367617@32
,
(-3367617)@32<=sL0x200149dc,L0x200149dc<=s3367617@32,
(-3367617)@32<=sL0x20014a00,L0x20014a00<=s3367617@32,
(-3367617)@32<=sL0x20014a24,L0x20014a24<=s3367617@32,
(-3367617)@32<=sL0x2001575c,L0x2001575c<=s3367617@32,
(-3367617)@32<=sL0x20015780,L0x20015780<=s3367617@32,
(-3367617)@32<=sL0x200157a4,L0x200157a4<=s3367617@32
,
(-3367617)@32<=sL0x20014a48,L0x20014a48<=s3367617@32,
(-3367617)@32<=sL0x20014a6c,L0x20014a6c<=s3367617@32,
(-3367617)@32<=sL0x20014a90,L0x20014a90<=s3367617@32,
(-3367617)@32<=sL0x200157c8,L0x200157c8<=s3367617@32,
(-3367617)@32<=sL0x200157ec,L0x200157ec<=s3367617@32,
(-3367617)@32<=sL0x20015810,L0x20015810<=s3367617@32
,
(-3367617)@32<=sL0x20014ab4,L0x20014ab4<=s3367617@32,
(-3367617)@32<=sL0x20014ad8,L0x20014ad8<=s3367617@32,
(-3367617)@32<=sL0x20014afc,L0x20014afc<=s3367617@32,
(-3367617)@32<=sL0x20015834,L0x20015834<=s3367617@32,
(-3367617)@32<=sL0x20015858,L0x20015858<=s3367617@32,
(-3367617)@32<=sL0x2001587c,L0x2001587c<=s3367617@32
,
(-3367617)@32<=sL0x20014b20,L0x20014b20<=s3367617@32,
(-3367617)@32<=sL0x20014b44,L0x20014b44<=s3367617@32,
(-3367617)@32<=sL0x20014b68,L0x20014b68<=s3367617@32,
(-3367617)@32<=sL0x200158a0,L0x200158a0<=s3367617@32,
(-3367617)@32<=sL0x200158c4,L0x200158c4<=s3367617@32,
(-3367617)@32<=sL0x200158e8,L0x200158e8<=s3367617@32
,
(-3367617)@32<=sL0x20014b8c,L0x20014b8c<=s3367617@32,
(-3367617)@32<=sL0x20014bb0,L0x20014bb0<=s3367617@32,
(-3367617)@32<=sL0x20014bd4,L0x20014bd4<=s3367617@32,
(-3367617)@32<=sL0x2001590c,L0x2001590c<=s3367617@32,
(-3367617)@32<=sL0x20015930,L0x20015930<=s3367617@32,
(-3367617)@32<=sL0x20015954,L0x20015954<=s3367617@32
,
(-3367617)@32<=sL0x20014bf8,L0x20014bf8<=s3367617@32,
(-3367617)@32<=sL0x20014c1c,L0x20014c1c<=s3367617@32,
(-3367617)@32<=sL0x20014c40,L0x20014c40<=s3367617@32,
(-3367617)@32<=sL0x20015978,L0x20015978<=s3367617@32,
(-3367617)@32<=sL0x2001599c,L0x2001599c<=s3367617@32,
(-3367617)@32<=sL0x200159c0,L0x200159c0<=s3367617@32
,
(-3367617)@32<=sL0x20014c64,L0x20014c64<=s3367617@32,
(-3367617)@32<=sL0x20014c88,L0x20014c88<=s3367617@32,
(-3367617)@32<=sL0x20014cac,L0x20014cac<=s3367617@32,
(-3367617)@32<=sL0x200159e4,L0x200159e4<=s3367617@32,
(-3367617)@32<=sL0x20015a08,L0x20015a08<=s3367617@32,
(-3367617)@32<=sL0x20015a2c,L0x20015a2c<=s3367617@32
,
(-3367617)@32<=sL0x20014cd0,L0x20014cd0<=s3367617@32,
(-3367617)@32<=sL0x20014cf4,L0x20014cf4<=s3367617@32,
(-3367617)@32<=sL0x20014d18,L0x20014d18<=s3367617@32,
(-3367617)@32<=sL0x20015a50,L0x20015a50<=s3367617@32,
(-3367617)@32<=sL0x20015a74,L0x20015a74<=s3367617@32,
(-3367617)@32<=sL0x20015a98,L0x20015a98<=s3367617@32
,
(-3367617)@32<=sL0x20014d3c,L0x20014d3c<=s3367617@32,
(-3367617)@32<=sL0x20014d60,L0x20014d60<=s3367617@32,
(-3367617)@32<=sL0x20014d84,L0x20014d84<=s3367617@32,
(-3367617)@32<=sL0x20015abc,L0x20015abc<=s3367617@32,
(-3367617)@32<=sL0x20015ae0,L0x20015ae0<=s3367617@32,
(-3367617)@32<=sL0x20015b04,L0x20015b04<=s3367617@32
,
(-3367617)@32<=sL0x20014da8,L0x20014da8<=s3367617@32,
(-3367617)@32<=sL0x20014dcc,L0x20014dcc<=s3367617@32,
(-3367617)@32<=sL0x20014df0,L0x20014df0<=s3367617@32,
(-3367617)@32<=sL0x20015b28,L0x20015b28<=s3367617@32,
(-3367617)@32<=sL0x20015b4c,L0x20015b4c<=s3367617@32,
(-3367617)@32<=sL0x20015b70,L0x20015b70<=s3367617@32
,
(-3367617)@32<=sL0x20014e14,L0x20014e14<=s3367617@32,
(-3367617)@32<=sL0x20014e38,L0x20014e38<=s3367617@32,
(-3367617)@32<=sL0x20014e5c,L0x20014e5c<=s3367617@32,
(-3367617)@32<=sL0x20015b94,L0x20015b94<=s3367617@32,
(-3367617)@32<=sL0x20015bb8,L0x20015bb8<=s3367617@32,
(-3367617)@32<=sL0x20015bdc,L0x20015bdc<=s3367617@32
,
(-3367617)@32<=sL0x20014e80,L0x20014e80<=s3367617@32,
(-3367617)@32<=sL0x20014ea4,L0x20014ea4<=s3367617@32,
(-3367617)@32<=sL0x20014ec8,L0x20014ec8<=s3367617@32,
(-3367617)@32<=sL0x20015c00,L0x20015c00<=s3367617@32,
(-3367617)@32<=sL0x20015c24,L0x20015c24<=s3367617@32,
(-3367617)@32<=sL0x20015c48,L0x20015c48<=s3367617@32
,
(-3367617)@32<=sL0x20014eec,L0x20014eec<=s3367617@32,
(-3367617)@32<=sL0x20014f10,L0x20014f10<=s3367617@32,
(-3367617)@32<=sL0x20014f34,L0x20014f34<=s3367617@32,
(-3367617)@32<=sL0x20015c6c,L0x20015c6c<=s3367617@32,
(-3367617)@32<=sL0x20015c90,L0x20015c90<=s3367617@32,
(-3367617)@32<=sL0x20015cb4,L0x20015cb4<=s3367617@32
,
(-3367617)@32<=sL0x20014f58,L0x20014f58<=s3367617@32,
(-3367617)@32<=sL0x20014f7c,L0x20014f7c<=s3367617@32,
(-3367617)@32<=sL0x20014fa0,L0x20014fa0<=s3367617@32,
(-3367617)@32<=sL0x20015cd8,L0x20015cd8<=s3367617@32,
(-3367617)@32<=sL0x20015cfc,L0x20015cfc<=s3367617@32,
(-3367617)@32<=sL0x20015d20,L0x20015d20<=s3367617@32
,
(-3367617)@32<=sL0x20014fc4,L0x20014fc4<=s3367617@32,
(-3367617)@32<=sL0x20014fe8,L0x20014fe8<=s3367617@32,
(-3367617)@32<=sL0x2001500c,L0x2001500c<=s3367617@32,
(-3367617)@32<=sL0x20015d44,L0x20015d44<=s3367617@32,
(-3367617)@32<=sL0x20015d68,L0x20015d68<=s3367617@32,
(-3367617)@32<=sL0x20015d8c,L0x20015d8c<=s3367617@32
,
(-3367617)@32<=sL0x20015030,L0x20015030<=s3367617@32,
(-3367617)@32<=sL0x20015054,L0x20015054<=s3367617@32,
(-3367617)@32<=sL0x20015078,L0x20015078<=s3367617@32,
(-3367617)@32<=sL0x20015db0,L0x20015db0<=s3367617@32,
(-3367617)@32<=sL0x20015dd4,L0x20015dd4<=s3367617@32,
(-3367617)@32<=sL0x20015df8,L0x20015df8<=s3367617@32
,
(-3367617)@32<=sL0x2001509c,L0x2001509c<=s3367617@32,
(-3367617)@32<=sL0x200150c0,L0x200150c0<=s3367617@32,
(-3367617)@32<=sL0x200150e4,L0x200150e4<=s3367617@32,
(-3367617)@32<=sL0x20015e1c,L0x20015e1c<=s3367617@32,
(-3367617)@32<=sL0x20015e40,L0x20015e40<=s3367617@32,
(-3367617)@32<=sL0x20015e64,L0x20015e64<=s3367617@32
,
(-3367617)@32<=sL0x20015108,L0x20015108<=s3367617@32,
(-3367617)@32<=sL0x2001512c,L0x2001512c<=s3367617@32,
(-3367617)@32<=sL0x20015150,L0x20015150<=s3367617@32,
(-3367617)@32<=sL0x20015e88,L0x20015e88<=s3367617@32,
(-3367617)@32<=sL0x20015eac,L0x20015eac<=s3367617@32,
(-3367617)@32<=sL0x20015ed0,L0x20015ed0<=s3367617@32
,
(-3367617)@32<=sL0x20015174,L0x20015174<=s3367617@32,
(-3367617)@32<=sL0x20015198,L0x20015198<=s3367617@32,
(-3367617)@32<=sL0x200151bc,L0x200151bc<=s3367617@32,
(-3367617)@32<=sL0x20015ef4,L0x20015ef4<=s3367617@32,
(-3367617)@32<=sL0x20015f18,L0x20015f18<=s3367617@32,
(-3367617)@32<=sL0x20015f3c,L0x20015f3c<=s3367617@32
,
(-3367617)@32<=sL0x200151e0,L0x200151e0<=s3367617@32,
(-3367617)@32<=sL0x20015204,L0x20015204<=s3367617@32,
(-3367617)@32<=sL0x20015228,L0x20015228<=s3367617@32,
(-3367617)@32<=sL0x20015f60,L0x20015f60<=s3367617@32,
(-3367617)@32<=sL0x20015f84,L0x20015f84<=s3367617@32,
(-3367617)@32<=sL0x20015fa8,L0x20015fa8<=s3367617@32
,
(-3367617)@32<=sL0x2001524c,L0x2001524c<=s3367617@32,
(-3367617)@32<=sL0x20015270,L0x20015270<=s3367617@32,
(-3367617)@32<=sL0x20015294,L0x20015294<=s3367617@32,
(-3367617)@32<=sL0x20015fcc,L0x20015fcc<=s3367617@32,
(-3367617)@32<=sL0x20015ff0,L0x20015ff0<=s3367617@32,
(-3367617)@32<=sL0x20016014,L0x20016014<=s3367617@32
,
(-3367617)@32<=sL0x200152b8,L0x200152b8<=s3367617@32,
(-3367617)@32<=sL0x200152dc,L0x200152dc<=s3367617@32,
(-3367617)@32<=sL0x20015300,L0x20015300<=s3367617@32,
(-3367617)@32<=sL0x20016038,L0x20016038<=s3367617@32,
(-3367617)@32<=sL0x2001605c,L0x2001605c<=s3367617@32,
(-3367617)@32<=sL0x20016080,L0x20016080<=s3367617@32
,
(-3367617)@32<=sL0x20015324,L0x20015324<=s3367617@32,
(-3367617)@32<=sL0x20015348,L0x20015348<=s3367617@32,
(-3367617)@32<=sL0x2001536c,L0x2001536c<=s3367617@32,
(-3367617)@32<=sL0x200160a4,L0x200160a4<=s3367617@32,
(-3367617)@32<=sL0x200160c8,L0x200160c8<=s3367617@32,
(-3367617)@32<=sL0x200160ec,L0x200160ec<=s3367617@32
,
(-3367617)@32<=sL0x20015390,L0x20015390<=s3367617@32,
(-3367617)@32<=sL0x200153b4,L0x200153b4<=s3367617@32,
(-3367617)@32<=sL0x200153d8,L0x200153d8<=s3367617@32,
(-3367617)@32<=sL0x20016110,L0x20016110<=s3367617@32,
(-3367617)@32<=sL0x20016134,L0x20016134<=s3367617@32,
(-3367617)@32<=sL0x20016158,L0x20016158<=s3367617@32
,
(-3367617)@32<=sL0x200153fc,L0x200153fc<=s3367617@32,
(-3367617)@32<=sL0x20015420,L0x20015420<=s3367617@32,
(-3367617)@32<=sL0x20015444,L0x20015444<=s3367617@32,
(-3367617)@32<=sL0x2001617c,L0x2001617c<=s3367617@32,
(-3367617)@32<=sL0x200161a0,L0x200161a0<=s3367617@32,
(-3367617)@32<=sL0x200161c4,L0x200161c4<=s3367617@32
,
(-3367617)@32<=sL0x20015468,L0x20015468<=s3367617@32,
(-3367617)@32<=sL0x2001548c,L0x2001548c<=s3367617@32,
(-3367617)@32<=sL0x200154b0,L0x200154b0<=s3367617@32,
(-3367617)@32<=sL0x200161e8,L0x200161e8<=s3367617@32,
(-3367617)@32<=sL0x2001620c,L0x2001620c<=s3367617@32,
(-3367617)@32<=sL0x20016230,L0x20016230<=s3367617@32
,
(-3367617)@32<=sL0x200154d4,L0x200154d4<=s3367617@32,
(-3367617)@32<=sL0x200154f8,L0x200154f8<=s3367617@32,
(-3367617)@32<=sL0x2001551c,L0x2001551c<=s3367617@32,
(-3367617)@32<=sL0x20016254,L0x20016254<=s3367617@32,
(-3367617)@32<=sL0x20016278,L0x20016278<=s3367617@32,
(-3367617)@32<=sL0x2001629c,L0x2001629c<=s3367617@32
,
(-3367617)@32<=sL0x20015540,L0x20015540<=s3367617@32,
(-3367617)@32<=sL0x20015564,L0x20015564<=s3367617@32,
(-3367617)@32<=sL0x20015588,L0x20015588<=s3367617@32,
(-3367617)@32<=sL0x200162c0,L0x200162c0<=s3367617@32,
(-3367617)@32<=sL0x200162e4,L0x200162e4<=s3367617@32,
(-3367617)@32<=sL0x20016308,L0x20016308<=s3367617@32
,
(-3367617)@32<=sL0x200155ac,L0x200155ac<=s3367617@32,
(-3367617)@32<=sL0x200155d0,L0x200155d0<=s3367617@32,
(-3367617)@32<=sL0x200155f4,L0x200155f4<=s3367617@32,
(-3367617)@32<=sL0x2001632c,L0x2001632c<=s3367617@32,
(-3367617)@32<=sL0x20016350,L0x20016350<=s3367617@32,
(-3367617)@32<=sL0x20016374,L0x20016374<=s3367617@32
,
(-3367617)@32<=sL0x2001489c,L0x2001489c<=s3367617@32,
(-3367617)@32<=sL0x200148c0,L0x200148c0<=s3367617@32,
(-3367617)@32<=sL0x200148e4,L0x200148e4<=s3367617@32,
(-3367617)@32<=sL0x2001561c,L0x2001561c<=s3367617@32,
(-3367617)@32<=sL0x20015640,L0x20015640<=s3367617@32,
(-3367617)@32<=sL0x20015664,L0x20015664<=s3367617@32
,
(-3367617)@32<=sL0x20014908,L0x20014908<=s3367617@32,
(-3367617)@32<=sL0x2001492c,L0x2001492c<=s3367617@32,
(-3367617)@32<=sL0x20014950,L0x20014950<=s3367617@32,
(-3367617)@32<=sL0x20015688,L0x20015688<=s3367617@32,
(-3367617)@32<=sL0x200156ac,L0x200156ac<=s3367617@32,
(-3367617)@32<=sL0x200156d0,L0x200156d0<=s3367617@32
,
(-3367617)@32<=sL0x20014974,L0x20014974<=s3367617@32,
(-3367617)@32<=sL0x20014998,L0x20014998<=s3367617@32,
(-3367617)@32<=sL0x200149bc,L0x200149bc<=s3367617@32,
(-3367617)@32<=sL0x200156f4,L0x200156f4<=s3367617@32,
(-3367617)@32<=sL0x20015718,L0x20015718<=s3367617@32,
(-3367617)@32<=sL0x2001573c,L0x2001573c<=s3367617@32
,
(-3367617)@32<=sL0x200149e0,L0x200149e0<=s3367617@32,
(-3367617)@32<=sL0x20014a04,L0x20014a04<=s3367617@32,
(-3367617)@32<=sL0x20014a28,L0x20014a28<=s3367617@32,
(-3367617)@32<=sL0x20015760,L0x20015760<=s3367617@32,
(-3367617)@32<=sL0x20015784,L0x20015784<=s3367617@32,
(-3367617)@32<=sL0x200157a8,L0x200157a8<=s3367617@32
,
(-3367617)@32<=sL0x20014a4c,L0x20014a4c<=s3367617@32,
(-3367617)@32<=sL0x20014a70,L0x20014a70<=s3367617@32,
(-3367617)@32<=sL0x20014a94,L0x20014a94<=s3367617@32,
(-3367617)@32<=sL0x200157cc,L0x200157cc<=s3367617@32,
(-3367617)@32<=sL0x200157f0,L0x200157f0<=s3367617@32,
(-3367617)@32<=sL0x20015814,L0x20015814<=s3367617@32
,
(-3367617)@32<=sL0x20014ab8,L0x20014ab8<=s3367617@32,
(-3367617)@32<=sL0x20014adc,L0x20014adc<=s3367617@32,
(-3367617)@32<=sL0x20014b00,L0x20014b00<=s3367617@32,
(-3367617)@32<=sL0x20015838,L0x20015838<=s3367617@32,
(-3367617)@32<=sL0x2001585c,L0x2001585c<=s3367617@32,
(-3367617)@32<=sL0x20015880,L0x20015880<=s3367617@32
,
(-3367617)@32<=sL0x20014b24,L0x20014b24<=s3367617@32,
(-3367617)@32<=sL0x20014b48,L0x20014b48<=s3367617@32,
(-3367617)@32<=sL0x20014b6c,L0x20014b6c<=s3367617@32,
(-3367617)@32<=sL0x200158a4,L0x200158a4<=s3367617@32,
(-3367617)@32<=sL0x200158c8,L0x200158c8<=s3367617@32,
(-3367617)@32<=sL0x200158ec,L0x200158ec<=s3367617@32
,
(-3367617)@32<=sL0x20014b90,L0x20014b90<=s3367617@32,
(-3367617)@32<=sL0x20014bb4,L0x20014bb4<=s3367617@32,
(-3367617)@32<=sL0x20014bd8,L0x20014bd8<=s3367617@32,
(-3367617)@32<=sL0x20015910,L0x20015910<=s3367617@32,
(-3367617)@32<=sL0x20015934,L0x20015934<=s3367617@32,
(-3367617)@32<=sL0x20015958,L0x20015958<=s3367617@32
,
(-3367617)@32<=sL0x20014bfc,L0x20014bfc<=s3367617@32,
(-3367617)@32<=sL0x20014c20,L0x20014c20<=s3367617@32,
(-3367617)@32<=sL0x20014c44,L0x20014c44<=s3367617@32,
(-3367617)@32<=sL0x2001597c,L0x2001597c<=s3367617@32,
(-3367617)@32<=sL0x200159a0,L0x200159a0<=s3367617@32,
(-3367617)@32<=sL0x200159c4,L0x200159c4<=s3367617@32
,
(-3367617)@32<=sL0x20014c68,L0x20014c68<=s3367617@32,
(-3367617)@32<=sL0x20014c8c,L0x20014c8c<=s3367617@32,
(-3367617)@32<=sL0x20014cb0,L0x20014cb0<=s3367617@32,
(-3367617)@32<=sL0x200159e8,L0x200159e8<=s3367617@32,
(-3367617)@32<=sL0x20015a0c,L0x20015a0c<=s3367617@32,
(-3367617)@32<=sL0x20015a30,L0x20015a30<=s3367617@32
,
(-3367617)@32<=sL0x20014cd4,L0x20014cd4<=s3367617@32,
(-3367617)@32<=sL0x20014cf8,L0x20014cf8<=s3367617@32,
(-3367617)@32<=sL0x20014d1c,L0x20014d1c<=s3367617@32,
(-3367617)@32<=sL0x20015a54,L0x20015a54<=s3367617@32,
(-3367617)@32<=sL0x20015a78,L0x20015a78<=s3367617@32,
(-3367617)@32<=sL0x20015a9c,L0x20015a9c<=s3367617@32
,
(-3367617)@32<=sL0x20014d40,L0x20014d40<=s3367617@32,
(-3367617)@32<=sL0x20014d64,L0x20014d64<=s3367617@32,
(-3367617)@32<=sL0x20014d88,L0x20014d88<=s3367617@32,
(-3367617)@32<=sL0x20015ac0,L0x20015ac0<=s3367617@32,
(-3367617)@32<=sL0x20015ae4,L0x20015ae4<=s3367617@32,
(-3367617)@32<=sL0x20015b08,L0x20015b08<=s3367617@32
,
(-3367617)@32<=sL0x20014dac,L0x20014dac<=s3367617@32,
(-3367617)@32<=sL0x20014dd0,L0x20014dd0<=s3367617@32,
(-3367617)@32<=sL0x20014df4,L0x20014df4<=s3367617@32,
(-3367617)@32<=sL0x20015b2c,L0x20015b2c<=s3367617@32,
(-3367617)@32<=sL0x20015b50,L0x20015b50<=s3367617@32,
(-3367617)@32<=sL0x20015b74,L0x20015b74<=s3367617@32
,
(-3367617)@32<=sL0x20014e18,L0x20014e18<=s3367617@32,
(-3367617)@32<=sL0x20014e3c,L0x20014e3c<=s3367617@32,
(-3367617)@32<=sL0x20014e60,L0x20014e60<=s3367617@32,
(-3367617)@32<=sL0x20015b98,L0x20015b98<=s3367617@32,
(-3367617)@32<=sL0x20015bbc,L0x20015bbc<=s3367617@32,
(-3367617)@32<=sL0x20015be0,L0x20015be0<=s3367617@32
,
(-3367617)@32<=sL0x20014e84,L0x20014e84<=s3367617@32,
(-3367617)@32<=sL0x20014ea8,L0x20014ea8<=s3367617@32,
(-3367617)@32<=sL0x20014ecc,L0x20014ecc<=s3367617@32,
(-3367617)@32<=sL0x20015c04,L0x20015c04<=s3367617@32,
(-3367617)@32<=sL0x20015c28,L0x20015c28<=s3367617@32,
(-3367617)@32<=sL0x20015c4c,L0x20015c4c<=s3367617@32
,
(-3367617)@32<=sL0x20014ef0,L0x20014ef0<=s3367617@32,
(-3367617)@32<=sL0x20014f14,L0x20014f14<=s3367617@32,
(-3367617)@32<=sL0x20014f38,L0x20014f38<=s3367617@32,
(-3367617)@32<=sL0x20015c70,L0x20015c70<=s3367617@32,
(-3367617)@32<=sL0x20015c94,L0x20015c94<=s3367617@32,
(-3367617)@32<=sL0x20015cb8,L0x20015cb8<=s3367617@32
,
(-3367617)@32<=sL0x20014f5c,L0x20014f5c<=s3367617@32,
(-3367617)@32<=sL0x20014f80,L0x20014f80<=s3367617@32,
(-3367617)@32<=sL0x20014fa4,L0x20014fa4<=s3367617@32,
(-3367617)@32<=sL0x20015cdc,L0x20015cdc<=s3367617@32,
(-3367617)@32<=sL0x20015d00,L0x20015d00<=s3367617@32,
(-3367617)@32<=sL0x20015d24,L0x20015d24<=s3367617@32
,
(-3367617)@32<=sL0x20014fc8,L0x20014fc8<=s3367617@32,
(-3367617)@32<=sL0x20014fec,L0x20014fec<=s3367617@32,
(-3367617)@32<=sL0x20015010,L0x20015010<=s3367617@32,
(-3367617)@32<=sL0x20015d48,L0x20015d48<=s3367617@32,
(-3367617)@32<=sL0x20015d6c,L0x20015d6c<=s3367617@32,
(-3367617)@32<=sL0x20015d90,L0x20015d90<=s3367617@32
,
(-3367617)@32<=sL0x20015034,L0x20015034<=s3367617@32,
(-3367617)@32<=sL0x20015058,L0x20015058<=s3367617@32,
(-3367617)@32<=sL0x2001507c,L0x2001507c<=s3367617@32,
(-3367617)@32<=sL0x20015db4,L0x20015db4<=s3367617@32,
(-3367617)@32<=sL0x20015dd8,L0x20015dd8<=s3367617@32,
(-3367617)@32<=sL0x20015dfc,L0x20015dfc<=s3367617@32
,
(-3367617)@32<=sL0x200150a0,L0x200150a0<=s3367617@32,
(-3367617)@32<=sL0x200150c4,L0x200150c4<=s3367617@32,
(-3367617)@32<=sL0x200150e8,L0x200150e8<=s3367617@32,
(-3367617)@32<=sL0x20015e20,L0x20015e20<=s3367617@32,
(-3367617)@32<=sL0x20015e44,L0x20015e44<=s3367617@32,
(-3367617)@32<=sL0x20015e68,L0x20015e68<=s3367617@32
,
(-3367617)@32<=sL0x2001510c,L0x2001510c<=s3367617@32,
(-3367617)@32<=sL0x20015130,L0x20015130<=s3367617@32,
(-3367617)@32<=sL0x20015154,L0x20015154<=s3367617@32,
(-3367617)@32<=sL0x20015e8c,L0x20015e8c<=s3367617@32,
(-3367617)@32<=sL0x20015eb0,L0x20015eb0<=s3367617@32,
(-3367617)@32<=sL0x20015ed4,L0x20015ed4<=s3367617@32
,
(-3367617)@32<=sL0x20015178,L0x20015178<=s3367617@32,
(-3367617)@32<=sL0x2001519c,L0x2001519c<=s3367617@32,
(-3367617)@32<=sL0x200151c0,L0x200151c0<=s3367617@32,
(-3367617)@32<=sL0x20015ef8,L0x20015ef8<=s3367617@32,
(-3367617)@32<=sL0x20015f1c,L0x20015f1c<=s3367617@32,
(-3367617)@32<=sL0x20015f40,L0x20015f40<=s3367617@32
,
(-3367617)@32<=sL0x200151e4,L0x200151e4<=s3367617@32,
(-3367617)@32<=sL0x20015208,L0x20015208<=s3367617@32,
(-3367617)@32<=sL0x2001522c,L0x2001522c<=s3367617@32,
(-3367617)@32<=sL0x20015f64,L0x20015f64<=s3367617@32,
(-3367617)@32<=sL0x20015f88,L0x20015f88<=s3367617@32,
(-3367617)@32<=sL0x20015fac,L0x20015fac<=s3367617@32
,
(-3367617)@32<=sL0x20015250,L0x20015250<=s3367617@32,
(-3367617)@32<=sL0x20015274,L0x20015274<=s3367617@32,
(-3367617)@32<=sL0x20015298,L0x20015298<=s3367617@32,
(-3367617)@32<=sL0x20015fd0,L0x20015fd0<=s3367617@32,
(-3367617)@32<=sL0x20015ff4,L0x20015ff4<=s3367617@32,
(-3367617)@32<=sL0x20016018,L0x20016018<=s3367617@32
,
(-3367617)@32<=sL0x200152bc,L0x200152bc<=s3367617@32,
(-3367617)@32<=sL0x200152e0,L0x200152e0<=s3367617@32,
(-3367617)@32<=sL0x20015304,L0x20015304<=s3367617@32,
(-3367617)@32<=sL0x2001603c,L0x2001603c<=s3367617@32,
(-3367617)@32<=sL0x20016060,L0x20016060<=s3367617@32,
(-3367617)@32<=sL0x20016084,L0x20016084<=s3367617@32
,
(-3367617)@32<=sL0x20015328,L0x20015328<=s3367617@32,
(-3367617)@32<=sL0x2001534c,L0x2001534c<=s3367617@32,
(-3367617)@32<=sL0x20015370,L0x20015370<=s3367617@32,
(-3367617)@32<=sL0x200160a8,L0x200160a8<=s3367617@32,
(-3367617)@32<=sL0x200160cc,L0x200160cc<=s3367617@32,
(-3367617)@32<=sL0x200160f0,L0x200160f0<=s3367617@32
,
(-3367617)@32<=sL0x20015394,L0x20015394<=s3367617@32,
(-3367617)@32<=sL0x200153b8,L0x200153b8<=s3367617@32,
(-3367617)@32<=sL0x200153dc,L0x200153dc<=s3367617@32,
(-3367617)@32<=sL0x20016114,L0x20016114<=s3367617@32,
(-3367617)@32<=sL0x20016138,L0x20016138<=s3367617@32,
(-3367617)@32<=sL0x2001615c,L0x2001615c<=s3367617@32
,
(-3367617)@32<=sL0x20015400,L0x20015400<=s3367617@32,
(-3367617)@32<=sL0x20015424,L0x20015424<=s3367617@32,
(-3367617)@32<=sL0x20015448,L0x20015448<=s3367617@32,
(-3367617)@32<=sL0x20016180,L0x20016180<=s3367617@32,
(-3367617)@32<=sL0x200161a4,L0x200161a4<=s3367617@32,
(-3367617)@32<=sL0x200161c8,L0x200161c8<=s3367617@32
,
(-3367617)@32<=sL0x2001546c,L0x2001546c<=s3367617@32,
(-3367617)@32<=sL0x20015490,L0x20015490<=s3367617@32,
(-3367617)@32<=sL0x200154b4,L0x200154b4<=s3367617@32,
(-3367617)@32<=sL0x200161ec,L0x200161ec<=s3367617@32,
(-3367617)@32<=sL0x20016210,L0x20016210<=s3367617@32,
(-3367617)@32<=sL0x20016234,L0x20016234<=s3367617@32
,
(-3367617)@32<=sL0x200154d8,L0x200154d8<=s3367617@32,
(-3367617)@32<=sL0x200154fc,L0x200154fc<=s3367617@32,
(-3367617)@32<=sL0x20015520,L0x20015520<=s3367617@32,
(-3367617)@32<=sL0x20016258,L0x20016258<=s3367617@32,
(-3367617)@32<=sL0x2001627c,L0x2001627c<=s3367617@32,
(-3367617)@32<=sL0x200162a0,L0x200162a0<=s3367617@32
,
(-3367617)@32<=sL0x20015544,L0x20015544<=s3367617@32,
(-3367617)@32<=sL0x20015568,L0x20015568<=s3367617@32,
(-3367617)@32<=sL0x2001558c,L0x2001558c<=s3367617@32,
(-3367617)@32<=sL0x200162c4,L0x200162c4<=s3367617@32,
(-3367617)@32<=sL0x200162e8,L0x200162e8<=s3367617@32,
(-3367617)@32<=sL0x2001630c,L0x2001630c<=s3367617@32
,
(-3367617)@32<=sL0x200155b0,L0x200155b0<=s3367617@32,
(-3367617)@32<=sL0x200155d4,L0x200155d4<=s3367617@32,
(-3367617)@32<=sL0x200155f8,L0x200155f8<=s3367617@32,
(-3367617)@32<=sL0x20016330,L0x20016330<=s3367617@32,
(-3367617)@32<=sL0x20016354,L0x20016354<=s3367617@32,
(-3367617)@32<=sL0x20016378,L0x20016378<=s3367617@32
,
(-3367617)@32<=sL0x200148a0,L0x200148a0<=s3367617@32,
(-3367617)@32<=sL0x200148c4,L0x200148c4<=s3367617@32,
(-3367617)@32<=sL0x200148e8,L0x200148e8<=s3367617@32,
(-3367617)@32<=sL0x20015620,L0x20015620<=s3367617@32,
(-3367617)@32<=sL0x20015644,L0x20015644<=s3367617@32,
(-3367617)@32<=sL0x20015668,L0x20015668<=s3367617@32
,
(-3367617)@32<=sL0x2001490c,L0x2001490c<=s3367617@32,
(-3367617)@32<=sL0x20014930,L0x20014930<=s3367617@32,
(-3367617)@32<=sL0x20014954,L0x20014954<=s3367617@32,
(-3367617)@32<=sL0x2001568c,L0x2001568c<=s3367617@32,
(-3367617)@32<=sL0x200156b0,L0x200156b0<=s3367617@32,
(-3367617)@32<=sL0x200156d4,L0x200156d4<=s3367617@32
,
(-3367617)@32<=sL0x20014978,L0x20014978<=s3367617@32,
(-3367617)@32<=sL0x2001499c,L0x2001499c<=s3367617@32,
(-3367617)@32<=sL0x200149c0,L0x200149c0<=s3367617@32,
(-3367617)@32<=sL0x200156f8,L0x200156f8<=s3367617@32,
(-3367617)@32<=sL0x2001571c,L0x2001571c<=s3367617@32,
(-3367617)@32<=sL0x20015740,L0x20015740<=s3367617@32
,
(-3367617)@32<=sL0x200149e4,L0x200149e4<=s3367617@32,
(-3367617)@32<=sL0x20014a08,L0x20014a08<=s3367617@32,
(-3367617)@32<=sL0x20014a2c,L0x20014a2c<=s3367617@32,
(-3367617)@32<=sL0x20015764,L0x20015764<=s3367617@32,
(-3367617)@32<=sL0x20015788,L0x20015788<=s3367617@32,
(-3367617)@32<=sL0x200157ac,L0x200157ac<=s3367617@32
,
(-3367617)@32<=sL0x20014a50,L0x20014a50<=s3367617@32,
(-3367617)@32<=sL0x20014a74,L0x20014a74<=s3367617@32,
(-3367617)@32<=sL0x20014a98,L0x20014a98<=s3367617@32,
(-3367617)@32<=sL0x200157d0,L0x200157d0<=s3367617@32,
(-3367617)@32<=sL0x200157f4,L0x200157f4<=s3367617@32,
(-3367617)@32<=sL0x20015818,L0x20015818<=s3367617@32
,
(-3367617)@32<=sL0x20014abc,L0x20014abc<=s3367617@32,
(-3367617)@32<=sL0x20014ae0,L0x20014ae0<=s3367617@32,
(-3367617)@32<=sL0x20014b04,L0x20014b04<=s3367617@32,
(-3367617)@32<=sL0x2001583c,L0x2001583c<=s3367617@32,
(-3367617)@32<=sL0x20015860,L0x20015860<=s3367617@32,
(-3367617)@32<=sL0x20015884,L0x20015884<=s3367617@32
,
(-3367617)@32<=sL0x20014b28,L0x20014b28<=s3367617@32,
(-3367617)@32<=sL0x20014b4c,L0x20014b4c<=s3367617@32,
(-3367617)@32<=sL0x20014b70,L0x20014b70<=s3367617@32,
(-3367617)@32<=sL0x200158a8,L0x200158a8<=s3367617@32,
(-3367617)@32<=sL0x200158cc,L0x200158cc<=s3367617@32,
(-3367617)@32<=sL0x200158f0,L0x200158f0<=s3367617@32
,
(-3367617)@32<=sL0x20014b94,L0x20014b94<=s3367617@32,
(-3367617)@32<=sL0x20014bb8,L0x20014bb8<=s3367617@32,
(-3367617)@32<=sL0x20014bdc,L0x20014bdc<=s3367617@32,
(-3367617)@32<=sL0x20015914,L0x20015914<=s3367617@32,
(-3367617)@32<=sL0x20015938,L0x20015938<=s3367617@32,
(-3367617)@32<=sL0x2001595c,L0x2001595c<=s3367617@32
,
(-3367617)@32<=sL0x20014c00,L0x20014c00<=s3367617@32,
(-3367617)@32<=sL0x20014c24,L0x20014c24<=s3367617@32,
(-3367617)@32<=sL0x20014c48,L0x20014c48<=s3367617@32,
(-3367617)@32<=sL0x20015980,L0x20015980<=s3367617@32,
(-3367617)@32<=sL0x200159a4,L0x200159a4<=s3367617@32,
(-3367617)@32<=sL0x200159c8,L0x200159c8<=s3367617@32
,
(-3367617)@32<=sL0x20014c6c,L0x20014c6c<=s3367617@32,
(-3367617)@32<=sL0x20014c90,L0x20014c90<=s3367617@32,
(-3367617)@32<=sL0x20014cb4,L0x20014cb4<=s3367617@32,
(-3367617)@32<=sL0x200159ec,L0x200159ec<=s3367617@32,
(-3367617)@32<=sL0x20015a10,L0x20015a10<=s3367617@32,
(-3367617)@32<=sL0x20015a34,L0x20015a34<=s3367617@32
,
(-3367617)@32<=sL0x20014cd8,L0x20014cd8<=s3367617@32,
(-3367617)@32<=sL0x20014cfc,L0x20014cfc<=s3367617@32,
(-3367617)@32<=sL0x20014d20,L0x20014d20<=s3367617@32,
(-3367617)@32<=sL0x20015a58,L0x20015a58<=s3367617@32,
(-3367617)@32<=sL0x20015a7c,L0x20015a7c<=s3367617@32,
(-3367617)@32<=sL0x20015aa0,L0x20015aa0<=s3367617@32
,
(-3367617)@32<=sL0x20014d44,L0x20014d44<=s3367617@32,
(-3367617)@32<=sL0x20014d68,L0x20014d68<=s3367617@32,
(-3367617)@32<=sL0x20014d8c,L0x20014d8c<=s3367617@32,
(-3367617)@32<=sL0x20015ac4,L0x20015ac4<=s3367617@32,
(-3367617)@32<=sL0x20015ae8,L0x20015ae8<=s3367617@32,
(-3367617)@32<=sL0x20015b0c,L0x20015b0c<=s3367617@32
,
(-3367617)@32<=sL0x20014db0,L0x20014db0<=s3367617@32,
(-3367617)@32<=sL0x20014dd4,L0x20014dd4<=s3367617@32,
(-3367617)@32<=sL0x20014df8,L0x20014df8<=s3367617@32,
(-3367617)@32<=sL0x20015b30,L0x20015b30<=s3367617@32,
(-3367617)@32<=sL0x20015b54,L0x20015b54<=s3367617@32,
(-3367617)@32<=sL0x20015b78,L0x20015b78<=s3367617@32
,
(-3367617)@32<=sL0x20014e1c,L0x20014e1c<=s3367617@32,
(-3367617)@32<=sL0x20014e40,L0x20014e40<=s3367617@32,
(-3367617)@32<=sL0x20014e64,L0x20014e64<=s3367617@32,
(-3367617)@32<=sL0x20015b9c,L0x20015b9c<=s3367617@32,
(-3367617)@32<=sL0x20015bc0,L0x20015bc0<=s3367617@32,
(-3367617)@32<=sL0x20015be4,L0x20015be4<=s3367617@32
,
(-3367617)@32<=sL0x20014e88,L0x20014e88<=s3367617@32,
(-3367617)@32<=sL0x20014eac,L0x20014eac<=s3367617@32,
(-3367617)@32<=sL0x20014ed0,L0x20014ed0<=s3367617@32,
(-3367617)@32<=sL0x20015c08,L0x20015c08<=s3367617@32,
(-3367617)@32<=sL0x20015c2c,L0x20015c2c<=s3367617@32,
(-3367617)@32<=sL0x20015c50,L0x20015c50<=s3367617@32
,
(-3367617)@32<=sL0x20014ef4,L0x20014ef4<=s3367617@32,
(-3367617)@32<=sL0x20014f18,L0x20014f18<=s3367617@32,
(-3367617)@32<=sL0x20014f3c,L0x20014f3c<=s3367617@32,
(-3367617)@32<=sL0x20015c74,L0x20015c74<=s3367617@32,
(-3367617)@32<=sL0x20015c98,L0x20015c98<=s3367617@32,
(-3367617)@32<=sL0x20015cbc,L0x20015cbc<=s3367617@32
,
(-3367617)@32<=sL0x20014f60,L0x20014f60<=s3367617@32,
(-3367617)@32<=sL0x20014f84,L0x20014f84<=s3367617@32,
(-3367617)@32<=sL0x20014fa8,L0x20014fa8<=s3367617@32,
(-3367617)@32<=sL0x20015ce0,L0x20015ce0<=s3367617@32,
(-3367617)@32<=sL0x20015d04,L0x20015d04<=s3367617@32,
(-3367617)@32<=sL0x20015d28,L0x20015d28<=s3367617@32
,
(-3367617)@32<=sL0x20014fcc,L0x20014fcc<=s3367617@32,
(-3367617)@32<=sL0x20014ff0,L0x20014ff0<=s3367617@32,
(-3367617)@32<=sL0x20015014,L0x20015014<=s3367617@32,
(-3367617)@32<=sL0x20015d4c,L0x20015d4c<=s3367617@32,
(-3367617)@32<=sL0x20015d70,L0x20015d70<=s3367617@32,
(-3367617)@32<=sL0x20015d94,L0x20015d94<=s3367617@32
,
(-3367617)@32<=sL0x20015038,L0x20015038<=s3367617@32,
(-3367617)@32<=sL0x2001505c,L0x2001505c<=s3367617@32,
(-3367617)@32<=sL0x20015080,L0x20015080<=s3367617@32,
(-3367617)@32<=sL0x20015db8,L0x20015db8<=s3367617@32,
(-3367617)@32<=sL0x20015ddc,L0x20015ddc<=s3367617@32,
(-3367617)@32<=sL0x20015e00,L0x20015e00<=s3367617@32
,
(-3367617)@32<=sL0x200150a4,L0x200150a4<=s3367617@32,
(-3367617)@32<=sL0x200150c8,L0x200150c8<=s3367617@32,
(-3367617)@32<=sL0x200150ec,L0x200150ec<=s3367617@32,
(-3367617)@32<=sL0x20015e24,L0x20015e24<=s3367617@32,
(-3367617)@32<=sL0x20015e48,L0x20015e48<=s3367617@32,
(-3367617)@32<=sL0x20015e6c,L0x20015e6c<=s3367617@32
,
(-3367617)@32<=sL0x20015110,L0x20015110<=s3367617@32,
(-3367617)@32<=sL0x20015134,L0x20015134<=s3367617@32,
(-3367617)@32<=sL0x20015158,L0x20015158<=s3367617@32,
(-3367617)@32<=sL0x20015e90,L0x20015e90<=s3367617@32,
(-3367617)@32<=sL0x20015eb4,L0x20015eb4<=s3367617@32,
(-3367617)@32<=sL0x20015ed8,L0x20015ed8<=s3367617@32
,
(-3367617)@32<=sL0x2001517c,L0x2001517c<=s3367617@32,
(-3367617)@32<=sL0x200151a0,L0x200151a0<=s3367617@32,
(-3367617)@32<=sL0x200151c4,L0x200151c4<=s3367617@32,
(-3367617)@32<=sL0x20015efc,L0x20015efc<=s3367617@32,
(-3367617)@32<=sL0x20015f20,L0x20015f20<=s3367617@32,
(-3367617)@32<=sL0x20015f44,L0x20015f44<=s3367617@32
,
(-3367617)@32<=sL0x200151e8,L0x200151e8<=s3367617@32,
(-3367617)@32<=sL0x2001520c,L0x2001520c<=s3367617@32,
(-3367617)@32<=sL0x20015230,L0x20015230<=s3367617@32,
(-3367617)@32<=sL0x20015f68,L0x20015f68<=s3367617@32,
(-3367617)@32<=sL0x20015f8c,L0x20015f8c<=s3367617@32,
(-3367617)@32<=sL0x20015fb0,L0x20015fb0<=s3367617@32
,
(-3367617)@32<=sL0x20015254,L0x20015254<=s3367617@32,
(-3367617)@32<=sL0x20015278,L0x20015278<=s3367617@32,
(-3367617)@32<=sL0x2001529c,L0x2001529c<=s3367617@32,
(-3367617)@32<=sL0x20015fd4,L0x20015fd4<=s3367617@32,
(-3367617)@32<=sL0x20015ff8,L0x20015ff8<=s3367617@32,
(-3367617)@32<=sL0x2001601c,L0x2001601c<=s3367617@32
,
(-3367617)@32<=sL0x200152c0,L0x200152c0<=s3367617@32,
(-3367617)@32<=sL0x200152e4,L0x200152e4<=s3367617@32,
(-3367617)@32<=sL0x20015308,L0x20015308<=s3367617@32,
(-3367617)@32<=sL0x20016040,L0x20016040<=s3367617@32,
(-3367617)@32<=sL0x20016064,L0x20016064<=s3367617@32,
(-3367617)@32<=sL0x20016088,L0x20016088<=s3367617@32
,
(-3367617)@32<=sL0x2001532c,L0x2001532c<=s3367617@32,
(-3367617)@32<=sL0x20015350,L0x20015350<=s3367617@32,
(-3367617)@32<=sL0x20015374,L0x20015374<=s3367617@32,
(-3367617)@32<=sL0x200160ac,L0x200160ac<=s3367617@32,
(-3367617)@32<=sL0x200160d0,L0x200160d0<=s3367617@32,
(-3367617)@32<=sL0x200160f4,L0x200160f4<=s3367617@32
,
(-3367617)@32<=sL0x20015398,L0x20015398<=s3367617@32,
(-3367617)@32<=sL0x200153bc,L0x200153bc<=s3367617@32,
(-3367617)@32<=sL0x200153e0,L0x200153e0<=s3367617@32,
(-3367617)@32<=sL0x20016118,L0x20016118<=s3367617@32,
(-3367617)@32<=sL0x2001613c,L0x2001613c<=s3367617@32,
(-3367617)@32<=sL0x20016160,L0x20016160<=s3367617@32
,
(-3367617)@32<=sL0x20015404,L0x20015404<=s3367617@32,
(-3367617)@32<=sL0x20015428,L0x20015428<=s3367617@32,
(-3367617)@32<=sL0x2001544c,L0x2001544c<=s3367617@32,
(-3367617)@32<=sL0x20016184,L0x20016184<=s3367617@32,
(-3367617)@32<=sL0x200161a8,L0x200161a8<=s3367617@32,
(-3367617)@32<=sL0x200161cc,L0x200161cc<=s3367617@32
,
(-3367617)@32<=sL0x20015470,L0x20015470<=s3367617@32,
(-3367617)@32<=sL0x20015494,L0x20015494<=s3367617@32,
(-3367617)@32<=sL0x200154b8,L0x200154b8<=s3367617@32,
(-3367617)@32<=sL0x200161f0,L0x200161f0<=s3367617@32,
(-3367617)@32<=sL0x20016214,L0x20016214<=s3367617@32,
(-3367617)@32<=sL0x20016238,L0x20016238<=s3367617@32
,
(-3367617)@32<=sL0x200154dc,L0x200154dc<=s3367617@32,
(-3367617)@32<=sL0x20015500,L0x20015500<=s3367617@32,
(-3367617)@32<=sL0x20015524,L0x20015524<=s3367617@32,
(-3367617)@32<=sL0x2001625c,L0x2001625c<=s3367617@32,
(-3367617)@32<=sL0x20016280,L0x20016280<=s3367617@32,
(-3367617)@32<=sL0x200162a4,L0x200162a4<=s3367617@32
,
(-3367617)@32<=sL0x20015548,L0x20015548<=s3367617@32,
(-3367617)@32<=sL0x2001556c,L0x2001556c<=s3367617@32,
(-3367617)@32<=sL0x20015590,L0x20015590<=s3367617@32,
(-3367617)@32<=sL0x200162c8,L0x200162c8<=s3367617@32,
(-3367617)@32<=sL0x200162ec,L0x200162ec<=s3367617@32,
(-3367617)@32<=sL0x20016310,L0x20016310<=s3367617@32
,
(-3367617)@32<=sL0x200155b4,L0x200155b4<=s3367617@32,
(-3367617)@32<=sL0x200155d8,L0x200155d8<=s3367617@32,
(-3367617)@32<=sL0x200155fc,L0x200155fc<=s3367617@32,
(-3367617)@32<=sL0x20016334,L0x20016334<=s3367617@32,
(-3367617)@32<=sL0x20016358,L0x20016358<=s3367617@32,
(-3367617)@32<=sL0x2001637c,L0x2001637c<=s3367617@32
,
(-3367617)@32<=sL0x200148a4,L0x200148a4<=s3367617@32,
(-3367617)@32<=sL0x200148c8,L0x200148c8<=s3367617@32,
(-3367617)@32<=sL0x200148ec,L0x200148ec<=s3367617@32,
(-3367617)@32<=sL0x20015624,L0x20015624<=s3367617@32,
(-3367617)@32<=sL0x20015648,L0x20015648<=s3367617@32,
(-3367617)@32<=sL0x2001566c,L0x2001566c<=s3367617@32
,
(-3367617)@32<=sL0x20014910,L0x20014910<=s3367617@32,
(-3367617)@32<=sL0x20014934,L0x20014934<=s3367617@32,
(-3367617)@32<=sL0x20014958,L0x20014958<=s3367617@32,
(-3367617)@32<=sL0x20015690,L0x20015690<=s3367617@32,
(-3367617)@32<=sL0x200156b4,L0x200156b4<=s3367617@32,
(-3367617)@32<=sL0x200156d8,L0x200156d8<=s3367617@32
,
(-3367617)@32<=sL0x2001497c,L0x2001497c<=s3367617@32,
(-3367617)@32<=sL0x200149a0,L0x200149a0<=s3367617@32,
(-3367617)@32<=sL0x200149c4,L0x200149c4<=s3367617@32,
(-3367617)@32<=sL0x200156fc,L0x200156fc<=s3367617@32,
(-3367617)@32<=sL0x20015720,L0x20015720<=s3367617@32,
(-3367617)@32<=sL0x20015744,L0x20015744<=s3367617@32
,
(-3367617)@32<=sL0x200149e8,L0x200149e8<=s3367617@32,
(-3367617)@32<=sL0x20014a0c,L0x20014a0c<=s3367617@32,
(-3367617)@32<=sL0x20014a30,L0x20014a30<=s3367617@32,
(-3367617)@32<=sL0x20015768,L0x20015768<=s3367617@32,
(-3367617)@32<=sL0x2001578c,L0x2001578c<=s3367617@32,
(-3367617)@32<=sL0x200157b0,L0x200157b0<=s3367617@32
,
(-3367617)@32<=sL0x20014a54,L0x20014a54<=s3367617@32,
(-3367617)@32<=sL0x20014a78,L0x20014a78<=s3367617@32,
(-3367617)@32<=sL0x20014a9c,L0x20014a9c<=s3367617@32,
(-3367617)@32<=sL0x200157d4,L0x200157d4<=s3367617@32,
(-3367617)@32<=sL0x200157f8,L0x200157f8<=s3367617@32,
(-3367617)@32<=sL0x2001581c,L0x2001581c<=s3367617@32
,
(-3367617)@32<=sL0x20014ac0,L0x20014ac0<=s3367617@32,
(-3367617)@32<=sL0x20014ae4,L0x20014ae4<=s3367617@32,
(-3367617)@32<=sL0x20014b08,L0x20014b08<=s3367617@32,
(-3367617)@32<=sL0x20015840,L0x20015840<=s3367617@32,
(-3367617)@32<=sL0x20015864,L0x20015864<=s3367617@32,
(-3367617)@32<=sL0x20015888,L0x20015888<=s3367617@32
,
(-3367617)@32<=sL0x20014b2c,L0x20014b2c<=s3367617@32,
(-3367617)@32<=sL0x20014b50,L0x20014b50<=s3367617@32,
(-3367617)@32<=sL0x20014b74,L0x20014b74<=s3367617@32,
(-3367617)@32<=sL0x200158ac,L0x200158ac<=s3367617@32,
(-3367617)@32<=sL0x200158d0,L0x200158d0<=s3367617@32,
(-3367617)@32<=sL0x200158f4,L0x200158f4<=s3367617@32
,
(-3367617)@32<=sL0x20014b98,L0x20014b98<=s3367617@32,
(-3367617)@32<=sL0x20014bbc,L0x20014bbc<=s3367617@32,
(-3367617)@32<=sL0x20014be0,L0x20014be0<=s3367617@32,
(-3367617)@32<=sL0x20015918,L0x20015918<=s3367617@32,
(-3367617)@32<=sL0x2001593c,L0x2001593c<=s3367617@32,
(-3367617)@32<=sL0x20015960,L0x20015960<=s3367617@32
,
(-3367617)@32<=sL0x20014c04,L0x20014c04<=s3367617@32,
(-3367617)@32<=sL0x20014c28,L0x20014c28<=s3367617@32,
(-3367617)@32<=sL0x20014c4c,L0x20014c4c<=s3367617@32,
(-3367617)@32<=sL0x20015984,L0x20015984<=s3367617@32,
(-3367617)@32<=sL0x200159a8,L0x200159a8<=s3367617@32,
(-3367617)@32<=sL0x200159cc,L0x200159cc<=s3367617@32
,
(-3367617)@32<=sL0x20014c70,L0x20014c70<=s3367617@32,
(-3367617)@32<=sL0x20014c94,L0x20014c94<=s3367617@32,
(-3367617)@32<=sL0x20014cb8,L0x20014cb8<=s3367617@32,
(-3367617)@32<=sL0x200159f0,L0x200159f0<=s3367617@32,
(-3367617)@32<=sL0x20015a14,L0x20015a14<=s3367617@32,
(-3367617)@32<=sL0x20015a38,L0x20015a38<=s3367617@32
,
(-3367617)@32<=sL0x20014cdc,L0x20014cdc<=s3367617@32,
(-3367617)@32<=sL0x20014d00,L0x20014d00<=s3367617@32,
(-3367617)@32<=sL0x20014d24,L0x20014d24<=s3367617@32,
(-3367617)@32<=sL0x20015a5c,L0x20015a5c<=s3367617@32,
(-3367617)@32<=sL0x20015a80,L0x20015a80<=s3367617@32,
(-3367617)@32<=sL0x20015aa4,L0x20015aa4<=s3367617@32
,
(-3367617)@32<=sL0x20014d48,L0x20014d48<=s3367617@32,
(-3367617)@32<=sL0x20014d6c,L0x20014d6c<=s3367617@32,
(-3367617)@32<=sL0x20014d90,L0x20014d90<=s3367617@32,
(-3367617)@32<=sL0x20015ac8,L0x20015ac8<=s3367617@32,
(-3367617)@32<=sL0x20015aec,L0x20015aec<=s3367617@32,
(-3367617)@32<=sL0x20015b10,L0x20015b10<=s3367617@32
,
(-3367617)@32<=sL0x20014db4,L0x20014db4<=s3367617@32,
(-3367617)@32<=sL0x20014dd8,L0x20014dd8<=s3367617@32,
(-3367617)@32<=sL0x20014dfc,L0x20014dfc<=s3367617@32,
(-3367617)@32<=sL0x20015b34,L0x20015b34<=s3367617@32,
(-3367617)@32<=sL0x20015b58,L0x20015b58<=s3367617@32,
(-3367617)@32<=sL0x20015b7c,L0x20015b7c<=s3367617@32
,
(-3367617)@32<=sL0x20014e20,L0x20014e20<=s3367617@32,
(-3367617)@32<=sL0x20014e44,L0x20014e44<=s3367617@32,
(-3367617)@32<=sL0x20014e68,L0x20014e68<=s3367617@32,
(-3367617)@32<=sL0x20015ba0,L0x20015ba0<=s3367617@32,
(-3367617)@32<=sL0x20015bc4,L0x20015bc4<=s3367617@32,
(-3367617)@32<=sL0x20015be8,L0x20015be8<=s3367617@32
,
(-3367617)@32<=sL0x20014e8c,L0x20014e8c<=s3367617@32,
(-3367617)@32<=sL0x20014eb0,L0x20014eb0<=s3367617@32,
(-3367617)@32<=sL0x20014ed4,L0x20014ed4<=s3367617@32,
(-3367617)@32<=sL0x20015c0c,L0x20015c0c<=s3367617@32,
(-3367617)@32<=sL0x20015c30,L0x20015c30<=s3367617@32,
(-3367617)@32<=sL0x20015c54,L0x20015c54<=s3367617@32
,
(-3367617)@32<=sL0x20014ef8,L0x20014ef8<=s3367617@32,
(-3367617)@32<=sL0x20014f1c,L0x20014f1c<=s3367617@32,
(-3367617)@32<=sL0x20014f40,L0x20014f40<=s3367617@32,
(-3367617)@32<=sL0x20015c78,L0x20015c78<=s3367617@32,
(-3367617)@32<=sL0x20015c9c,L0x20015c9c<=s3367617@32,
(-3367617)@32<=sL0x20015cc0,L0x20015cc0<=s3367617@32
,
(-3367617)@32<=sL0x20014f64,L0x20014f64<=s3367617@32,
(-3367617)@32<=sL0x20014f88,L0x20014f88<=s3367617@32,
(-3367617)@32<=sL0x20014fac,L0x20014fac<=s3367617@32,
(-3367617)@32<=sL0x20015ce4,L0x20015ce4<=s3367617@32,
(-3367617)@32<=sL0x20015d08,L0x20015d08<=s3367617@32,
(-3367617)@32<=sL0x20015d2c,L0x20015d2c<=s3367617@32
,
(-3367617)@32<=sL0x20014fd0,L0x20014fd0<=s3367617@32,
(-3367617)@32<=sL0x20014ff4,L0x20014ff4<=s3367617@32,
(-3367617)@32<=sL0x20015018,L0x20015018<=s3367617@32,
(-3367617)@32<=sL0x20015d50,L0x20015d50<=s3367617@32,
(-3367617)@32<=sL0x20015d74,L0x20015d74<=s3367617@32,
(-3367617)@32<=sL0x20015d98,L0x20015d98<=s3367617@32
,
(-3367617)@32<=sL0x2001503c,L0x2001503c<=s3367617@32,
(-3367617)@32<=sL0x20015060,L0x20015060<=s3367617@32,
(-3367617)@32<=sL0x20015084,L0x20015084<=s3367617@32,
(-3367617)@32<=sL0x20015dbc,L0x20015dbc<=s3367617@32,
(-3367617)@32<=sL0x20015de0,L0x20015de0<=s3367617@32,
(-3367617)@32<=sL0x20015e04,L0x20015e04<=s3367617@32
,
(-3367617)@32<=sL0x200150a8,L0x200150a8<=s3367617@32,
(-3367617)@32<=sL0x200150cc,L0x200150cc<=s3367617@32,
(-3367617)@32<=sL0x200150f0,L0x200150f0<=s3367617@32,
(-3367617)@32<=sL0x20015e28,L0x20015e28<=s3367617@32,
(-3367617)@32<=sL0x20015e4c,L0x20015e4c<=s3367617@32,
(-3367617)@32<=sL0x20015e70,L0x20015e70<=s3367617@32
,
(-3367617)@32<=sL0x20015114,L0x20015114<=s3367617@32,
(-3367617)@32<=sL0x20015138,L0x20015138<=s3367617@32,
(-3367617)@32<=sL0x2001515c,L0x2001515c<=s3367617@32,
(-3367617)@32<=sL0x20015e94,L0x20015e94<=s3367617@32,
(-3367617)@32<=sL0x20015eb8,L0x20015eb8<=s3367617@32,
(-3367617)@32<=sL0x20015edc,L0x20015edc<=s3367617@32
,
(-3367617)@32<=sL0x20015180,L0x20015180<=s3367617@32,
(-3367617)@32<=sL0x200151a4,L0x200151a4<=s3367617@32,
(-3367617)@32<=sL0x200151c8,L0x200151c8<=s3367617@32,
(-3367617)@32<=sL0x20015f00,L0x20015f00<=s3367617@32,
(-3367617)@32<=sL0x20015f24,L0x20015f24<=s3367617@32,
(-3367617)@32<=sL0x20015f48,L0x20015f48<=s3367617@32
,
(-3367617)@32<=sL0x200151ec,L0x200151ec<=s3367617@32,
(-3367617)@32<=sL0x20015210,L0x20015210<=s3367617@32,
(-3367617)@32<=sL0x20015234,L0x20015234<=s3367617@32,
(-3367617)@32<=sL0x20015f6c,L0x20015f6c<=s3367617@32,
(-3367617)@32<=sL0x20015f90,L0x20015f90<=s3367617@32,
(-3367617)@32<=sL0x20015fb4,L0x20015fb4<=s3367617@32
,
(-3367617)@32<=sL0x20015258,L0x20015258<=s3367617@32,
(-3367617)@32<=sL0x2001527c,L0x2001527c<=s3367617@32,
(-3367617)@32<=sL0x200152a0,L0x200152a0<=s3367617@32,
(-3367617)@32<=sL0x20015fd8,L0x20015fd8<=s3367617@32,
(-3367617)@32<=sL0x20015ffc,L0x20015ffc<=s3367617@32,
(-3367617)@32<=sL0x20016020,L0x20016020<=s3367617@32
,
(-3367617)@32<=sL0x200152c4,L0x200152c4<=s3367617@32,
(-3367617)@32<=sL0x200152e8,L0x200152e8<=s3367617@32,
(-3367617)@32<=sL0x2001530c,L0x2001530c<=s3367617@32,
(-3367617)@32<=sL0x20016044,L0x20016044<=s3367617@32,
(-3367617)@32<=sL0x20016068,L0x20016068<=s3367617@32,
(-3367617)@32<=sL0x2001608c,L0x2001608c<=s3367617@32
,
(-3367617)@32<=sL0x20015330,L0x20015330<=s3367617@32,
(-3367617)@32<=sL0x20015354,L0x20015354<=s3367617@32,
(-3367617)@32<=sL0x20015378,L0x20015378<=s3367617@32,
(-3367617)@32<=sL0x200160b0,L0x200160b0<=s3367617@32,
(-3367617)@32<=sL0x200160d4,L0x200160d4<=s3367617@32,
(-3367617)@32<=sL0x200160f8,L0x200160f8<=s3367617@32
,
(-3367617)@32<=sL0x2001539c,L0x2001539c<=s3367617@32,
(-3367617)@32<=sL0x200153c0,L0x200153c0<=s3367617@32,
(-3367617)@32<=sL0x200153e4,L0x200153e4<=s3367617@32,
(-3367617)@32<=sL0x2001611c,L0x2001611c<=s3367617@32,
(-3367617)@32<=sL0x20016140,L0x20016140<=s3367617@32,
(-3367617)@32<=sL0x20016164,L0x20016164<=s3367617@32
,
(-3367617)@32<=sL0x20015408,L0x20015408<=s3367617@32,
(-3367617)@32<=sL0x2001542c,L0x2001542c<=s3367617@32,
(-3367617)@32<=sL0x20015450,L0x20015450<=s3367617@32,
(-3367617)@32<=sL0x20016188,L0x20016188<=s3367617@32,
(-3367617)@32<=sL0x200161ac,L0x200161ac<=s3367617@32,
(-3367617)@32<=sL0x200161d0,L0x200161d0<=s3367617@32
,
(-3367617)@32<=sL0x20015474,L0x20015474<=s3367617@32,
(-3367617)@32<=sL0x20015498,L0x20015498<=s3367617@32,
(-3367617)@32<=sL0x200154bc,L0x200154bc<=s3367617@32,
(-3367617)@32<=sL0x200161f4,L0x200161f4<=s3367617@32,
(-3367617)@32<=sL0x20016218,L0x20016218<=s3367617@32,
(-3367617)@32<=sL0x2001623c,L0x2001623c<=s3367617@32
,
(-3367617)@32<=sL0x200154e0,L0x200154e0<=s3367617@32,
(-3367617)@32<=sL0x20015504,L0x20015504<=s3367617@32,
(-3367617)@32<=sL0x20015528,L0x20015528<=s3367617@32,
(-3367617)@32<=sL0x20016260,L0x20016260<=s3367617@32,
(-3367617)@32<=sL0x20016284,L0x20016284<=s3367617@32,
(-3367617)@32<=sL0x200162a8,L0x200162a8<=s3367617@32
,
(-3367617)@32<=sL0x2001554c,L0x2001554c<=s3367617@32,
(-3367617)@32<=sL0x20015570,L0x20015570<=s3367617@32,
(-3367617)@32<=sL0x20015594,L0x20015594<=s3367617@32,
(-3367617)@32<=sL0x200162cc,L0x200162cc<=s3367617@32,
(-3367617)@32<=sL0x200162f0,L0x200162f0<=s3367617@32,
(-3367617)@32<=sL0x20016314,L0x20016314<=s3367617@32
,
(-3367617)@32<=sL0x200155b8,L0x200155b8<=s3367617@32,
(-3367617)@32<=sL0x200155dc,L0x200155dc<=s3367617@32,
(-3367617)@32<=sL0x20015600,L0x20015600<=s3367617@32,
(-3367617)@32<=sL0x20016338,L0x20016338<=s3367617@32,
(-3367617)@32<=sL0x2001635c,L0x2001635c<=s3367617@32,
(-3367617)@32<=sL0x20016380,L0x20016380<=s3367617@32
,
(-3367617)@32<=sL0x200148a8,L0x200148a8<=s3367617@32,
(-3367617)@32<=sL0x200148cc,L0x200148cc<=s3367617@32,
(-3367617)@32<=sL0x200148f0,L0x200148f0<=s3367617@32,
(-3367617)@32<=sL0x20015628,L0x20015628<=s3367617@32,
(-3367617)@32<=sL0x2001564c,L0x2001564c<=s3367617@32,
(-3367617)@32<=sL0x20015670,L0x20015670<=s3367617@32
,
(-3367617)@32<=sL0x20014914,L0x20014914<=s3367617@32,
(-3367617)@32<=sL0x20014938,L0x20014938<=s3367617@32,
(-3367617)@32<=sL0x2001495c,L0x2001495c<=s3367617@32,
(-3367617)@32<=sL0x20015694,L0x20015694<=s3367617@32,
(-3367617)@32<=sL0x200156b8,L0x200156b8<=s3367617@32,
(-3367617)@32<=sL0x200156dc,L0x200156dc<=s3367617@32
,
(-3367617)@32<=sL0x20014980,L0x20014980<=s3367617@32,
(-3367617)@32<=sL0x200149a4,L0x200149a4<=s3367617@32,
(-3367617)@32<=sL0x200149c8,L0x200149c8<=s3367617@32,
(-3367617)@32<=sL0x20015700,L0x20015700<=s3367617@32,
(-3367617)@32<=sL0x20015724,L0x20015724<=s3367617@32,
(-3367617)@32<=sL0x20015748,L0x20015748<=s3367617@32
,
(-3367617)@32<=sL0x200149ec,L0x200149ec<=s3367617@32,
(-3367617)@32<=sL0x20014a10,L0x20014a10<=s3367617@32,
(-3367617)@32<=sL0x20014a34,L0x20014a34<=s3367617@32,
(-3367617)@32<=sL0x2001576c,L0x2001576c<=s3367617@32,
(-3367617)@32<=sL0x20015790,L0x20015790<=s3367617@32,
(-3367617)@32<=sL0x200157b4,L0x200157b4<=s3367617@32
,
(-3367617)@32<=sL0x20014a58,L0x20014a58<=s3367617@32,
(-3367617)@32<=sL0x20014a7c,L0x20014a7c<=s3367617@32,
(-3367617)@32<=sL0x20014aa0,L0x20014aa0<=s3367617@32,
(-3367617)@32<=sL0x200157d8,L0x200157d8<=s3367617@32,
(-3367617)@32<=sL0x200157fc,L0x200157fc<=s3367617@32,
(-3367617)@32<=sL0x20015820,L0x20015820<=s3367617@32
,
(-3367617)@32<=sL0x20014ac4,L0x20014ac4<=s3367617@32,
(-3367617)@32<=sL0x20014ae8,L0x20014ae8<=s3367617@32,
(-3367617)@32<=sL0x20014b0c,L0x20014b0c<=s3367617@32,
(-3367617)@32<=sL0x20015844,L0x20015844<=s3367617@32,
(-3367617)@32<=sL0x20015868,L0x20015868<=s3367617@32,
(-3367617)@32<=sL0x2001588c,L0x2001588c<=s3367617@32
,
(-3367617)@32<=sL0x20014b30,L0x20014b30<=s3367617@32,
(-3367617)@32<=sL0x20014b54,L0x20014b54<=s3367617@32,
(-3367617)@32<=sL0x20014b78,L0x20014b78<=s3367617@32,
(-3367617)@32<=sL0x200158b0,L0x200158b0<=s3367617@32,
(-3367617)@32<=sL0x200158d4,L0x200158d4<=s3367617@32,
(-3367617)@32<=sL0x200158f8,L0x200158f8<=s3367617@32
,
(-3367617)@32<=sL0x20014b9c,L0x20014b9c<=s3367617@32,
(-3367617)@32<=sL0x20014bc0,L0x20014bc0<=s3367617@32,
(-3367617)@32<=sL0x20014be4,L0x20014be4<=s3367617@32,
(-3367617)@32<=sL0x2001591c,L0x2001591c<=s3367617@32,
(-3367617)@32<=sL0x20015940,L0x20015940<=s3367617@32,
(-3367617)@32<=sL0x20015964,L0x20015964<=s3367617@32
,
(-3367617)@32<=sL0x20014c08,L0x20014c08<=s3367617@32,
(-3367617)@32<=sL0x20014c2c,L0x20014c2c<=s3367617@32,
(-3367617)@32<=sL0x20014c50,L0x20014c50<=s3367617@32,
(-3367617)@32<=sL0x20015988,L0x20015988<=s3367617@32,
(-3367617)@32<=sL0x200159ac,L0x200159ac<=s3367617@32,
(-3367617)@32<=sL0x200159d0,L0x200159d0<=s3367617@32
,
(-3367617)@32<=sL0x20014c74,L0x20014c74<=s3367617@32,
(-3367617)@32<=sL0x20014c98,L0x20014c98<=s3367617@32,
(-3367617)@32<=sL0x20014cbc,L0x20014cbc<=s3367617@32,
(-3367617)@32<=sL0x200159f4,L0x200159f4<=s3367617@32,
(-3367617)@32<=sL0x20015a18,L0x20015a18<=s3367617@32,
(-3367617)@32<=sL0x20015a3c,L0x20015a3c<=s3367617@32
,
(-3367617)@32<=sL0x20014ce0,L0x20014ce0<=s3367617@32,
(-3367617)@32<=sL0x20014d04,L0x20014d04<=s3367617@32,
(-3367617)@32<=sL0x20014d28,L0x20014d28<=s3367617@32,
(-3367617)@32<=sL0x20015a60,L0x20015a60<=s3367617@32,
(-3367617)@32<=sL0x20015a84,L0x20015a84<=s3367617@32,
(-3367617)@32<=sL0x20015aa8,L0x20015aa8<=s3367617@32
,
(-3367617)@32<=sL0x20014d4c,L0x20014d4c<=s3367617@32,
(-3367617)@32<=sL0x20014d70,L0x20014d70<=s3367617@32,
(-3367617)@32<=sL0x20014d94,L0x20014d94<=s3367617@32,
(-3367617)@32<=sL0x20015acc,L0x20015acc<=s3367617@32,
(-3367617)@32<=sL0x20015af0,L0x20015af0<=s3367617@32,
(-3367617)@32<=sL0x20015b14,L0x20015b14<=s3367617@32
,
(-3367617)@32<=sL0x20014db8,L0x20014db8<=s3367617@32,
(-3367617)@32<=sL0x20014ddc,L0x20014ddc<=s3367617@32,
(-3367617)@32<=sL0x20014e00,L0x20014e00<=s3367617@32,
(-3367617)@32<=sL0x20015b38,L0x20015b38<=s3367617@32,
(-3367617)@32<=sL0x20015b5c,L0x20015b5c<=s3367617@32,
(-3367617)@32<=sL0x20015b80,L0x20015b80<=s3367617@32
,
(-3367617)@32<=sL0x20014e24,L0x20014e24<=s3367617@32,
(-3367617)@32<=sL0x20014e48,L0x20014e48<=s3367617@32,
(-3367617)@32<=sL0x20014e6c,L0x20014e6c<=s3367617@32,
(-3367617)@32<=sL0x20015ba4,L0x20015ba4<=s3367617@32,
(-3367617)@32<=sL0x20015bc8,L0x20015bc8<=s3367617@32,
(-3367617)@32<=sL0x20015bec,L0x20015bec<=s3367617@32
,
(-3367617)@32<=sL0x20014e90,L0x20014e90<=s3367617@32,
(-3367617)@32<=sL0x20014eb4,L0x20014eb4<=s3367617@32,
(-3367617)@32<=sL0x20014ed8,L0x20014ed8<=s3367617@32,
(-3367617)@32<=sL0x20015c10,L0x20015c10<=s3367617@32,
(-3367617)@32<=sL0x20015c34,L0x20015c34<=s3367617@32,
(-3367617)@32<=sL0x20015c58,L0x20015c58<=s3367617@32
,
(-3367617)@32<=sL0x20014efc,L0x20014efc<=s3367617@32,
(-3367617)@32<=sL0x20014f20,L0x20014f20<=s3367617@32,
(-3367617)@32<=sL0x20014f44,L0x20014f44<=s3367617@32,
(-3367617)@32<=sL0x20015c7c,L0x20015c7c<=s3367617@32,
(-3367617)@32<=sL0x20015ca0,L0x20015ca0<=s3367617@32,
(-3367617)@32<=sL0x20015cc4,L0x20015cc4<=s3367617@32
,
(-3367617)@32<=sL0x20014f68,L0x20014f68<=s3367617@32,
(-3367617)@32<=sL0x20014f8c,L0x20014f8c<=s3367617@32,
(-3367617)@32<=sL0x20014fb0,L0x20014fb0<=s3367617@32,
(-3367617)@32<=sL0x20015ce8,L0x20015ce8<=s3367617@32,
(-3367617)@32<=sL0x20015d0c,L0x20015d0c<=s3367617@32,
(-3367617)@32<=sL0x20015d30,L0x20015d30<=s3367617@32
,
(-3367617)@32<=sL0x20014fd4,L0x20014fd4<=s3367617@32,
(-3367617)@32<=sL0x20014ff8,L0x20014ff8<=s3367617@32,
(-3367617)@32<=sL0x2001501c,L0x2001501c<=s3367617@32,
(-3367617)@32<=sL0x20015d54,L0x20015d54<=s3367617@32,
(-3367617)@32<=sL0x20015d78,L0x20015d78<=s3367617@32,
(-3367617)@32<=sL0x20015d9c,L0x20015d9c<=s3367617@32
,
(-3367617)@32<=sL0x20015040,L0x20015040<=s3367617@32,
(-3367617)@32<=sL0x20015064,L0x20015064<=s3367617@32,
(-3367617)@32<=sL0x20015088,L0x20015088<=s3367617@32,
(-3367617)@32<=sL0x20015dc0,L0x20015dc0<=s3367617@32,
(-3367617)@32<=sL0x20015de4,L0x20015de4<=s3367617@32,
(-3367617)@32<=sL0x20015e08,L0x20015e08<=s3367617@32
,
(-3367617)@32<=sL0x200150ac,L0x200150ac<=s3367617@32,
(-3367617)@32<=sL0x200150d0,L0x200150d0<=s3367617@32,
(-3367617)@32<=sL0x200150f4,L0x200150f4<=s3367617@32,
(-3367617)@32<=sL0x20015e2c,L0x20015e2c<=s3367617@32,
(-3367617)@32<=sL0x20015e50,L0x20015e50<=s3367617@32,
(-3367617)@32<=sL0x20015e74,L0x20015e74<=s3367617@32
,
(-3367617)@32<=sL0x20015118,L0x20015118<=s3367617@32,
(-3367617)@32<=sL0x2001513c,L0x2001513c<=s3367617@32,
(-3367617)@32<=sL0x20015160,L0x20015160<=s3367617@32,
(-3367617)@32<=sL0x20015e98,L0x20015e98<=s3367617@32,
(-3367617)@32<=sL0x20015ebc,L0x20015ebc<=s3367617@32,
(-3367617)@32<=sL0x20015ee0,L0x20015ee0<=s3367617@32
,
(-3367617)@32<=sL0x20015184,L0x20015184<=s3367617@32,
(-3367617)@32<=sL0x200151a8,L0x200151a8<=s3367617@32,
(-3367617)@32<=sL0x200151cc,L0x200151cc<=s3367617@32,
(-3367617)@32<=sL0x20015f04,L0x20015f04<=s3367617@32,
(-3367617)@32<=sL0x20015f28,L0x20015f28<=s3367617@32,
(-3367617)@32<=sL0x20015f4c,L0x20015f4c<=s3367617@32
,
(-3367617)@32<=sL0x200151f0,L0x200151f0<=s3367617@32,
(-3367617)@32<=sL0x20015214,L0x20015214<=s3367617@32,
(-3367617)@32<=sL0x20015238,L0x20015238<=s3367617@32,
(-3367617)@32<=sL0x20015f70,L0x20015f70<=s3367617@32,
(-3367617)@32<=sL0x20015f94,L0x20015f94<=s3367617@32,
(-3367617)@32<=sL0x20015fb8,L0x20015fb8<=s3367617@32
,
(-3367617)@32<=sL0x2001525c,L0x2001525c<=s3367617@32,
(-3367617)@32<=sL0x20015280,L0x20015280<=s3367617@32,
(-3367617)@32<=sL0x200152a4,L0x200152a4<=s3367617@32,
(-3367617)@32<=sL0x20015fdc,L0x20015fdc<=s3367617@32,
(-3367617)@32<=sL0x20016000,L0x20016000<=s3367617@32,
(-3367617)@32<=sL0x20016024,L0x20016024<=s3367617@32
,
(-3367617)@32<=sL0x200152c8,L0x200152c8<=s3367617@32,
(-3367617)@32<=sL0x200152ec,L0x200152ec<=s3367617@32,
(-3367617)@32<=sL0x20015310,L0x20015310<=s3367617@32,
(-3367617)@32<=sL0x20016048,L0x20016048<=s3367617@32,
(-3367617)@32<=sL0x2001606c,L0x2001606c<=s3367617@32,
(-3367617)@32<=sL0x20016090,L0x20016090<=s3367617@32
,
(-3367617)@32<=sL0x20015334,L0x20015334<=s3367617@32,
(-3367617)@32<=sL0x20015358,L0x20015358<=s3367617@32,
(-3367617)@32<=sL0x2001537c,L0x2001537c<=s3367617@32,
(-3367617)@32<=sL0x200160b4,L0x200160b4<=s3367617@32,
(-3367617)@32<=sL0x200160d8,L0x200160d8<=s3367617@32,
(-3367617)@32<=sL0x200160fc,L0x200160fc<=s3367617@32
,
(-3367617)@32<=sL0x200153a0,L0x200153a0<=s3367617@32,
(-3367617)@32<=sL0x200153c4,L0x200153c4<=s3367617@32,
(-3367617)@32<=sL0x200153e8,L0x200153e8<=s3367617@32,
(-3367617)@32<=sL0x20016120,L0x20016120<=s3367617@32,
(-3367617)@32<=sL0x20016144,L0x20016144<=s3367617@32,
(-3367617)@32<=sL0x20016168,L0x20016168<=s3367617@32
,
(-3367617)@32<=sL0x2001540c,L0x2001540c<=s3367617@32,
(-3367617)@32<=sL0x20015430,L0x20015430<=s3367617@32,
(-3367617)@32<=sL0x20015454,L0x20015454<=s3367617@32,
(-3367617)@32<=sL0x2001618c,L0x2001618c<=s3367617@32,
(-3367617)@32<=sL0x200161b0,L0x200161b0<=s3367617@32,
(-3367617)@32<=sL0x200161d4,L0x200161d4<=s3367617@32
,
(-3367617)@32<=sL0x20015478,L0x20015478<=s3367617@32,
(-3367617)@32<=sL0x2001549c,L0x2001549c<=s3367617@32,
(-3367617)@32<=sL0x200154c0,L0x200154c0<=s3367617@32,
(-3367617)@32<=sL0x200161f8,L0x200161f8<=s3367617@32,
(-3367617)@32<=sL0x2001621c,L0x2001621c<=s3367617@32,
(-3367617)@32<=sL0x20016240,L0x20016240<=s3367617@32
,
(-3367617)@32<=sL0x200154e4,L0x200154e4<=s3367617@32,
(-3367617)@32<=sL0x20015508,L0x20015508<=s3367617@32,
(-3367617)@32<=sL0x2001552c,L0x2001552c<=s3367617@32,
(-3367617)@32<=sL0x20016264,L0x20016264<=s3367617@32,
(-3367617)@32<=sL0x20016288,L0x20016288<=s3367617@32,
(-3367617)@32<=sL0x200162ac,L0x200162ac<=s3367617@32
,
(-3367617)@32<=sL0x20015550,L0x20015550<=s3367617@32,
(-3367617)@32<=sL0x20015574,L0x20015574<=s3367617@32,
(-3367617)@32<=sL0x20015598,L0x20015598<=s3367617@32,
(-3367617)@32<=sL0x200162d0,L0x200162d0<=s3367617@32,
(-3367617)@32<=sL0x200162f4,L0x200162f4<=s3367617@32,
(-3367617)@32<=sL0x20016318,L0x20016318<=s3367617@32
,
(-3367617)@32<=sL0x200155bc,L0x200155bc<=s3367617@32,
(-3367617)@32<=sL0x200155e0,L0x200155e0<=s3367617@32,
(-3367617)@32<=sL0x20015604,L0x20015604<=s3367617@32,
(-3367617)@32<=sL0x2001633c,L0x2001633c<=s3367617@32,
(-3367617)@32<=sL0x20016360,L0x20016360<=s3367617@32,
(-3367617)@32<=sL0x20016384,L0x20016384<=s3367617@32
,
(-3367617)@32<=sL0x200148ac,L0x200148ac<=s3367617@32,
(-3367617)@32<=sL0x200148d0,L0x200148d0<=s3367617@32,
(-3367617)@32<=sL0x200148f4,L0x200148f4<=s3367617@32,
(-3367617)@32<=sL0x2001562c,L0x2001562c<=s3367617@32,
(-3367617)@32<=sL0x20015650,L0x20015650<=s3367617@32,
(-3367617)@32<=sL0x20015674,L0x20015674<=s3367617@32
,
(-3367617)@32<=sL0x20014918,L0x20014918<=s3367617@32,
(-3367617)@32<=sL0x2001493c,L0x2001493c<=s3367617@32,
(-3367617)@32<=sL0x20014960,L0x20014960<=s3367617@32,
(-3367617)@32<=sL0x20015698,L0x20015698<=s3367617@32,
(-3367617)@32<=sL0x200156bc,L0x200156bc<=s3367617@32,
(-3367617)@32<=sL0x200156e0,L0x200156e0<=s3367617@32
,
(-3367617)@32<=sL0x20014984,L0x20014984<=s3367617@32,
(-3367617)@32<=sL0x200149a8,L0x200149a8<=s3367617@32,
(-3367617)@32<=sL0x200149cc,L0x200149cc<=s3367617@32,
(-3367617)@32<=sL0x20015704,L0x20015704<=s3367617@32,
(-3367617)@32<=sL0x20015728,L0x20015728<=s3367617@32,
(-3367617)@32<=sL0x2001574c,L0x2001574c<=s3367617@32
,
(-3367617)@32<=sL0x200149f0,L0x200149f0<=s3367617@32,
(-3367617)@32<=sL0x20014a14,L0x20014a14<=s3367617@32,
(-3367617)@32<=sL0x20014a38,L0x20014a38<=s3367617@32,
(-3367617)@32<=sL0x20015770,L0x20015770<=s3367617@32,
(-3367617)@32<=sL0x20015794,L0x20015794<=s3367617@32,
(-3367617)@32<=sL0x200157b8,L0x200157b8<=s3367617@32
,
(-3367617)@32<=sL0x20014a5c,L0x20014a5c<=s3367617@32,
(-3367617)@32<=sL0x20014a80,L0x20014a80<=s3367617@32,
(-3367617)@32<=sL0x20014aa4,L0x20014aa4<=s3367617@32,
(-3367617)@32<=sL0x200157dc,L0x200157dc<=s3367617@32,
(-3367617)@32<=sL0x20015800,L0x20015800<=s3367617@32,
(-3367617)@32<=sL0x20015824,L0x20015824<=s3367617@32
,
(-3367617)@32<=sL0x20014ac8,L0x20014ac8<=s3367617@32,
(-3367617)@32<=sL0x20014aec,L0x20014aec<=s3367617@32,
(-3367617)@32<=sL0x20014b10,L0x20014b10<=s3367617@32,
(-3367617)@32<=sL0x20015848,L0x20015848<=s3367617@32,
(-3367617)@32<=sL0x2001586c,L0x2001586c<=s3367617@32,
(-3367617)@32<=sL0x20015890,L0x20015890<=s3367617@32
,
(-3367617)@32<=sL0x20014b34,L0x20014b34<=s3367617@32,
(-3367617)@32<=sL0x20014b58,L0x20014b58<=s3367617@32,
(-3367617)@32<=sL0x20014b7c,L0x20014b7c<=s3367617@32,
(-3367617)@32<=sL0x200158b4,L0x200158b4<=s3367617@32,
(-3367617)@32<=sL0x200158d8,L0x200158d8<=s3367617@32,
(-3367617)@32<=sL0x200158fc,L0x200158fc<=s3367617@32
,
(-3367617)@32<=sL0x20014ba0,L0x20014ba0<=s3367617@32,
(-3367617)@32<=sL0x20014bc4,L0x20014bc4<=s3367617@32,
(-3367617)@32<=sL0x20014be8,L0x20014be8<=s3367617@32,
(-3367617)@32<=sL0x20015920,L0x20015920<=s3367617@32,
(-3367617)@32<=sL0x20015944,L0x20015944<=s3367617@32,
(-3367617)@32<=sL0x20015968,L0x20015968<=s3367617@32
,
(-3367617)@32<=sL0x20014c0c,L0x20014c0c<=s3367617@32,
(-3367617)@32<=sL0x20014c30,L0x20014c30<=s3367617@32,
(-3367617)@32<=sL0x20014c54,L0x20014c54<=s3367617@32,
(-3367617)@32<=sL0x2001598c,L0x2001598c<=s3367617@32,
(-3367617)@32<=sL0x200159b0,L0x200159b0<=s3367617@32,
(-3367617)@32<=sL0x200159d4,L0x200159d4<=s3367617@32
,
(-3367617)@32<=sL0x20014c78,L0x20014c78<=s3367617@32,
(-3367617)@32<=sL0x20014c9c,L0x20014c9c<=s3367617@32,
(-3367617)@32<=sL0x20014cc0,L0x20014cc0<=s3367617@32,
(-3367617)@32<=sL0x200159f8,L0x200159f8<=s3367617@32,
(-3367617)@32<=sL0x20015a1c,L0x20015a1c<=s3367617@32,
(-3367617)@32<=sL0x20015a40,L0x20015a40<=s3367617@32
,
(-3367617)@32<=sL0x20014ce4,L0x20014ce4<=s3367617@32,
(-3367617)@32<=sL0x20014d08,L0x20014d08<=s3367617@32,
(-3367617)@32<=sL0x20014d2c,L0x20014d2c<=s3367617@32,
(-3367617)@32<=sL0x20015a64,L0x20015a64<=s3367617@32,
(-3367617)@32<=sL0x20015a88,L0x20015a88<=s3367617@32,
(-3367617)@32<=sL0x20015aac,L0x20015aac<=s3367617@32
,
(-3367617)@32<=sL0x20014d50,L0x20014d50<=s3367617@32,
(-3367617)@32<=sL0x20014d74,L0x20014d74<=s3367617@32,
(-3367617)@32<=sL0x20014d98,L0x20014d98<=s3367617@32,
(-3367617)@32<=sL0x20015ad0,L0x20015ad0<=s3367617@32,
(-3367617)@32<=sL0x20015af4,L0x20015af4<=s3367617@32,
(-3367617)@32<=sL0x20015b18,L0x20015b18<=s3367617@32
,
(-3367617)@32<=sL0x20014dbc,L0x20014dbc<=s3367617@32,
(-3367617)@32<=sL0x20014de0,L0x20014de0<=s3367617@32,
(-3367617)@32<=sL0x20014e04,L0x20014e04<=s3367617@32,
(-3367617)@32<=sL0x20015b3c,L0x20015b3c<=s3367617@32,
(-3367617)@32<=sL0x20015b60,L0x20015b60<=s3367617@32,
(-3367617)@32<=sL0x20015b84,L0x20015b84<=s3367617@32
,
(-3367617)@32<=sL0x20014e28,L0x20014e28<=s3367617@32,
(-3367617)@32<=sL0x20014e4c,L0x20014e4c<=s3367617@32,
(-3367617)@32<=sL0x20014e70,L0x20014e70<=s3367617@32,
(-3367617)@32<=sL0x20015ba8,L0x20015ba8<=s3367617@32,
(-3367617)@32<=sL0x20015bcc,L0x20015bcc<=s3367617@32,
(-3367617)@32<=sL0x20015bf0,L0x20015bf0<=s3367617@32
,
(-3367617)@32<=sL0x20014e94,L0x20014e94<=s3367617@32,
(-3367617)@32<=sL0x20014eb8,L0x20014eb8<=s3367617@32,
(-3367617)@32<=sL0x20014edc,L0x20014edc<=s3367617@32,
(-3367617)@32<=sL0x20015c14,L0x20015c14<=s3367617@32,
(-3367617)@32<=sL0x20015c38,L0x20015c38<=s3367617@32,
(-3367617)@32<=sL0x20015c5c,L0x20015c5c<=s3367617@32
,
(-3367617)@32<=sL0x20014f00,L0x20014f00<=s3367617@32,
(-3367617)@32<=sL0x20014f24,L0x20014f24<=s3367617@32,
(-3367617)@32<=sL0x20014f48,L0x20014f48<=s3367617@32,
(-3367617)@32<=sL0x20015c80,L0x20015c80<=s3367617@32,
(-3367617)@32<=sL0x20015ca4,L0x20015ca4<=s3367617@32,
(-3367617)@32<=sL0x20015cc8,L0x20015cc8<=s3367617@32
,
(-3367617)@32<=sL0x20014f6c,L0x20014f6c<=s3367617@32,
(-3367617)@32<=sL0x20014f90,L0x20014f90<=s3367617@32,
(-3367617)@32<=sL0x20014fb4,L0x20014fb4<=s3367617@32,
(-3367617)@32<=sL0x20015cec,L0x20015cec<=s3367617@32,
(-3367617)@32<=sL0x20015d10,L0x20015d10<=s3367617@32,
(-3367617)@32<=sL0x20015d34,L0x20015d34<=s3367617@32
,
(-3367617)@32<=sL0x20014fd8,L0x20014fd8<=s3367617@32,
(-3367617)@32<=sL0x20014ffc,L0x20014ffc<=s3367617@32,
(-3367617)@32<=sL0x20015020,L0x20015020<=s3367617@32,
(-3367617)@32<=sL0x20015d58,L0x20015d58<=s3367617@32,
(-3367617)@32<=sL0x20015d7c,L0x20015d7c<=s3367617@32,
(-3367617)@32<=sL0x20015da0,L0x20015da0<=s3367617@32
,
(-3367617)@32<=sL0x20015044,L0x20015044<=s3367617@32,
(-3367617)@32<=sL0x20015068,L0x20015068<=s3367617@32,
(-3367617)@32<=sL0x2001508c,L0x2001508c<=s3367617@32,
(-3367617)@32<=sL0x20015dc4,L0x20015dc4<=s3367617@32,
(-3367617)@32<=sL0x20015de8,L0x20015de8<=s3367617@32,
(-3367617)@32<=sL0x20015e0c,L0x20015e0c<=s3367617@32
,
(-3367617)@32<=sL0x200150b0,L0x200150b0<=s3367617@32,
(-3367617)@32<=sL0x200150d4,L0x200150d4<=s3367617@32,
(-3367617)@32<=sL0x200150f8,L0x200150f8<=s3367617@32,
(-3367617)@32<=sL0x20015e30,L0x20015e30<=s3367617@32,
(-3367617)@32<=sL0x20015e54,L0x20015e54<=s3367617@32,
(-3367617)@32<=sL0x20015e78,L0x20015e78<=s3367617@32
,
(-3367617)@32<=sL0x2001511c,L0x2001511c<=s3367617@32,
(-3367617)@32<=sL0x20015140,L0x20015140<=s3367617@32,
(-3367617)@32<=sL0x20015164,L0x20015164<=s3367617@32,
(-3367617)@32<=sL0x20015e9c,L0x20015e9c<=s3367617@32,
(-3367617)@32<=sL0x20015ec0,L0x20015ec0<=s3367617@32,
(-3367617)@32<=sL0x20015ee4,L0x20015ee4<=s3367617@32
,
(-3367617)@32<=sL0x20015188,L0x20015188<=s3367617@32,
(-3367617)@32<=sL0x200151ac,L0x200151ac<=s3367617@32,
(-3367617)@32<=sL0x200151d0,L0x200151d0<=s3367617@32,
(-3367617)@32<=sL0x20015f08,L0x20015f08<=s3367617@32,
(-3367617)@32<=sL0x20015f2c,L0x20015f2c<=s3367617@32,
(-3367617)@32<=sL0x20015f50,L0x20015f50<=s3367617@32
,
(-3367617)@32<=sL0x200151f4,L0x200151f4<=s3367617@32,
(-3367617)@32<=sL0x20015218,L0x20015218<=s3367617@32,
(-3367617)@32<=sL0x2001523c,L0x2001523c<=s3367617@32,
(-3367617)@32<=sL0x20015f74,L0x20015f74<=s3367617@32,
(-3367617)@32<=sL0x20015f98,L0x20015f98<=s3367617@32,
(-3367617)@32<=sL0x20015fbc,L0x20015fbc<=s3367617@32
,
(-3367617)@32<=sL0x20015260,L0x20015260<=s3367617@32,
(-3367617)@32<=sL0x20015284,L0x20015284<=s3367617@32,
(-3367617)@32<=sL0x200152a8,L0x200152a8<=s3367617@32,
(-3367617)@32<=sL0x20015fe0,L0x20015fe0<=s3367617@32,
(-3367617)@32<=sL0x20016004,L0x20016004<=s3367617@32,
(-3367617)@32<=sL0x20016028,L0x20016028<=s3367617@32
,
(-3367617)@32<=sL0x200152cc,L0x200152cc<=s3367617@32,
(-3367617)@32<=sL0x200152f0,L0x200152f0<=s3367617@32,
(-3367617)@32<=sL0x20015314,L0x20015314<=s3367617@32,
(-3367617)@32<=sL0x2001604c,L0x2001604c<=s3367617@32,
(-3367617)@32<=sL0x20016070,L0x20016070<=s3367617@32,
(-3367617)@32<=sL0x20016094,L0x20016094<=s3367617@32
,
(-3367617)@32<=sL0x20015338,L0x20015338<=s3367617@32,
(-3367617)@32<=sL0x2001535c,L0x2001535c<=s3367617@32,
(-3367617)@32<=sL0x20015380,L0x20015380<=s3367617@32,
(-3367617)@32<=sL0x200160b8,L0x200160b8<=s3367617@32,
(-3367617)@32<=sL0x200160dc,L0x200160dc<=s3367617@32,
(-3367617)@32<=sL0x20016100,L0x20016100<=s3367617@32
,
(-3367617)@32<=sL0x200153a4,L0x200153a4<=s3367617@32,
(-3367617)@32<=sL0x200153c8,L0x200153c8<=s3367617@32,
(-3367617)@32<=sL0x200153ec,L0x200153ec<=s3367617@32,
(-3367617)@32<=sL0x20016124,L0x20016124<=s3367617@32,
(-3367617)@32<=sL0x20016148,L0x20016148<=s3367617@32,
(-3367617)@32<=sL0x2001616c,L0x2001616c<=s3367617@32
,
(-3367617)@32<=sL0x20015410,L0x20015410<=s3367617@32,
(-3367617)@32<=sL0x20015434,L0x20015434<=s3367617@32,
(-3367617)@32<=sL0x20015458,L0x20015458<=s3367617@32,
(-3367617)@32<=sL0x20016190,L0x20016190<=s3367617@32,
(-3367617)@32<=sL0x200161b4,L0x200161b4<=s3367617@32,
(-3367617)@32<=sL0x200161d8,L0x200161d8<=s3367617@32
,
(-3367617)@32<=sL0x2001547c,L0x2001547c<=s3367617@32,
(-3367617)@32<=sL0x200154a0,L0x200154a0<=s3367617@32,
(-3367617)@32<=sL0x200154c4,L0x200154c4<=s3367617@32,
(-3367617)@32<=sL0x200161fc,L0x200161fc<=s3367617@32,
(-3367617)@32<=sL0x20016220,L0x20016220<=s3367617@32,
(-3367617)@32<=sL0x20016244,L0x20016244<=s3367617@32
,
(-3367617)@32<=sL0x200154e8,L0x200154e8<=s3367617@32,
(-3367617)@32<=sL0x2001550c,L0x2001550c<=s3367617@32,
(-3367617)@32<=sL0x20015530,L0x20015530<=s3367617@32,
(-3367617)@32<=sL0x20016268,L0x20016268<=s3367617@32,
(-3367617)@32<=sL0x2001628c,L0x2001628c<=s3367617@32,
(-3367617)@32<=sL0x200162b0,L0x200162b0<=s3367617@32
,
(-3367617)@32<=sL0x20015554,L0x20015554<=s3367617@32,
(-3367617)@32<=sL0x20015578,L0x20015578<=s3367617@32,
(-3367617)@32<=sL0x2001559c,L0x2001559c<=s3367617@32,
(-3367617)@32<=sL0x200162d4,L0x200162d4<=s3367617@32,
(-3367617)@32<=sL0x200162f8,L0x200162f8<=s3367617@32,
(-3367617)@32<=sL0x2001631c,L0x2001631c<=s3367617@32
,
(-3367617)@32<=sL0x200155c0,L0x200155c0<=s3367617@32,
(-3367617)@32<=sL0x200155e4,L0x200155e4<=s3367617@32,
(-3367617)@32<=sL0x20015608,L0x20015608<=s3367617@32,
(-3367617)@32<=sL0x20016340,L0x20016340<=s3367617@32,
(-3367617)@32<=sL0x20016364,L0x20016364<=s3367617@32,
(-3367617)@32<=sL0x20016388,L0x20016388<=s3367617@32
,
(-3367617)@32<=sL0x200148b0,L0x200148b0<=s3367617@32,
(-3367617)@32<=sL0x200148d4,L0x200148d4<=s3367617@32,
(-3367617)@32<=sL0x200148f8,L0x200148f8<=s3367617@32,
(-3367617)@32<=sL0x20015630,L0x20015630<=s3367617@32,
(-3367617)@32<=sL0x20015654,L0x20015654<=s3367617@32,
(-3367617)@32<=sL0x20015678,L0x20015678<=s3367617@32
,
(-3367617)@32<=sL0x2001491c,L0x2001491c<=s3367617@32,
(-3367617)@32<=sL0x20014940,L0x20014940<=s3367617@32,
(-3367617)@32<=sL0x20014964,L0x20014964<=s3367617@32,
(-3367617)@32<=sL0x2001569c,L0x2001569c<=s3367617@32,
(-3367617)@32<=sL0x200156c0,L0x200156c0<=s3367617@32,
(-3367617)@32<=sL0x200156e4,L0x200156e4<=s3367617@32
,
(-3367617)@32<=sL0x20014988,L0x20014988<=s3367617@32,
(-3367617)@32<=sL0x200149ac,L0x200149ac<=s3367617@32,
(-3367617)@32<=sL0x200149d0,L0x200149d0<=s3367617@32,
(-3367617)@32<=sL0x20015708,L0x20015708<=s3367617@32,
(-3367617)@32<=sL0x2001572c,L0x2001572c<=s3367617@32,
(-3367617)@32<=sL0x20015750,L0x20015750<=s3367617@32
,
(-3367617)@32<=sL0x200149f4,L0x200149f4<=s3367617@32,
(-3367617)@32<=sL0x20014a18,L0x20014a18<=s3367617@32,
(-3367617)@32<=sL0x20014a3c,L0x20014a3c<=s3367617@32,
(-3367617)@32<=sL0x20015774,L0x20015774<=s3367617@32,
(-3367617)@32<=sL0x20015798,L0x20015798<=s3367617@32,
(-3367617)@32<=sL0x200157bc,L0x200157bc<=s3367617@32
,
(-3367617)@32<=sL0x20014a60,L0x20014a60<=s3367617@32,
(-3367617)@32<=sL0x20014a84,L0x20014a84<=s3367617@32,
(-3367617)@32<=sL0x20014aa8,L0x20014aa8<=s3367617@32,
(-3367617)@32<=sL0x200157e0,L0x200157e0<=s3367617@32,
(-3367617)@32<=sL0x20015804,L0x20015804<=s3367617@32,
(-3367617)@32<=sL0x20015828,L0x20015828<=s3367617@32
,
(-3367617)@32<=sL0x20014acc,L0x20014acc<=s3367617@32,
(-3367617)@32<=sL0x20014af0,L0x20014af0<=s3367617@32,
(-3367617)@32<=sL0x20014b14,L0x20014b14<=s3367617@32,
(-3367617)@32<=sL0x2001584c,L0x2001584c<=s3367617@32,
(-3367617)@32<=sL0x20015870,L0x20015870<=s3367617@32,
(-3367617)@32<=sL0x20015894,L0x20015894<=s3367617@32
,
(-3367617)@32<=sL0x20014b38,L0x20014b38<=s3367617@32,
(-3367617)@32<=sL0x20014b5c,L0x20014b5c<=s3367617@32,
(-3367617)@32<=sL0x20014b80,L0x20014b80<=s3367617@32,
(-3367617)@32<=sL0x200158b8,L0x200158b8<=s3367617@32,
(-3367617)@32<=sL0x200158dc,L0x200158dc<=s3367617@32,
(-3367617)@32<=sL0x20015900,L0x20015900<=s3367617@32
,
(-3367617)@32<=sL0x20014ba4,L0x20014ba4<=s3367617@32,
(-3367617)@32<=sL0x20014bc8,L0x20014bc8<=s3367617@32,
(-3367617)@32<=sL0x20014bec,L0x20014bec<=s3367617@32,
(-3367617)@32<=sL0x20015924,L0x20015924<=s3367617@32,
(-3367617)@32<=sL0x20015948,L0x20015948<=s3367617@32,
(-3367617)@32<=sL0x2001596c,L0x2001596c<=s3367617@32
,
(-3367617)@32<=sL0x20014c10,L0x20014c10<=s3367617@32,
(-3367617)@32<=sL0x20014c34,L0x20014c34<=s3367617@32,
(-3367617)@32<=sL0x20014c58,L0x20014c58<=s3367617@32,
(-3367617)@32<=sL0x20015990,L0x20015990<=s3367617@32,
(-3367617)@32<=sL0x200159b4,L0x200159b4<=s3367617@32,
(-3367617)@32<=sL0x200159d8,L0x200159d8<=s3367617@32
,
(-3367617)@32<=sL0x20014c7c,L0x20014c7c<=s3367617@32,
(-3367617)@32<=sL0x20014ca0,L0x20014ca0<=s3367617@32,
(-3367617)@32<=sL0x20014cc4,L0x20014cc4<=s3367617@32,
(-3367617)@32<=sL0x200159fc,L0x200159fc<=s3367617@32,
(-3367617)@32<=sL0x20015a20,L0x20015a20<=s3367617@32,
(-3367617)@32<=sL0x20015a44,L0x20015a44<=s3367617@32
,
(-3367617)@32<=sL0x20014ce8,L0x20014ce8<=s3367617@32,
(-3367617)@32<=sL0x20014d0c,L0x20014d0c<=s3367617@32,
(-3367617)@32<=sL0x20014d30,L0x20014d30<=s3367617@32,
(-3367617)@32<=sL0x20015a68,L0x20015a68<=s3367617@32,
(-3367617)@32<=sL0x20015a8c,L0x20015a8c<=s3367617@32,
(-3367617)@32<=sL0x20015ab0,L0x20015ab0<=s3367617@32
,
(-3367617)@32<=sL0x20014d54,L0x20014d54<=s3367617@32,
(-3367617)@32<=sL0x20014d78,L0x20014d78<=s3367617@32,
(-3367617)@32<=sL0x20014d9c,L0x20014d9c<=s3367617@32,
(-3367617)@32<=sL0x20015ad4,L0x20015ad4<=s3367617@32,
(-3367617)@32<=sL0x20015af8,L0x20015af8<=s3367617@32,
(-3367617)@32<=sL0x20015b1c,L0x20015b1c<=s3367617@32
,
(-3367617)@32<=sL0x20014dc0,L0x20014dc0<=s3367617@32,
(-3367617)@32<=sL0x20014de4,L0x20014de4<=s3367617@32,
(-3367617)@32<=sL0x20014e08,L0x20014e08<=s3367617@32,
(-3367617)@32<=sL0x20015b40,L0x20015b40<=s3367617@32,
(-3367617)@32<=sL0x20015b64,L0x20015b64<=s3367617@32,
(-3367617)@32<=sL0x20015b88,L0x20015b88<=s3367617@32
,
(-3367617)@32<=sL0x20014e2c,L0x20014e2c<=s3367617@32,
(-3367617)@32<=sL0x20014e50,L0x20014e50<=s3367617@32,
(-3367617)@32<=sL0x20014e74,L0x20014e74<=s3367617@32,
(-3367617)@32<=sL0x20015bac,L0x20015bac<=s3367617@32,
(-3367617)@32<=sL0x20015bd0,L0x20015bd0<=s3367617@32,
(-3367617)@32<=sL0x20015bf4,L0x20015bf4<=s3367617@32
,
(-3367617)@32<=sL0x20014e98,L0x20014e98<=s3367617@32,
(-3367617)@32<=sL0x20014ebc,L0x20014ebc<=s3367617@32,
(-3367617)@32<=sL0x20014ee0,L0x20014ee0<=s3367617@32,
(-3367617)@32<=sL0x20015c18,L0x20015c18<=s3367617@32,
(-3367617)@32<=sL0x20015c3c,L0x20015c3c<=s3367617@32,
(-3367617)@32<=sL0x20015c60,L0x20015c60<=s3367617@32
,
(-3367617)@32<=sL0x20014f04,L0x20014f04<=s3367617@32,
(-3367617)@32<=sL0x20014f28,L0x20014f28<=s3367617@32,
(-3367617)@32<=sL0x20014f4c,L0x20014f4c<=s3367617@32,
(-3367617)@32<=sL0x20015c84,L0x20015c84<=s3367617@32,
(-3367617)@32<=sL0x20015ca8,L0x20015ca8<=s3367617@32,
(-3367617)@32<=sL0x20015ccc,L0x20015ccc<=s3367617@32
,
(-3367617)@32<=sL0x20014f70,L0x20014f70<=s3367617@32,
(-3367617)@32<=sL0x20014f94,L0x20014f94<=s3367617@32,
(-3367617)@32<=sL0x20014fb8,L0x20014fb8<=s3367617@32,
(-3367617)@32<=sL0x20015cf0,L0x20015cf0<=s3367617@32,
(-3367617)@32<=sL0x20015d14,L0x20015d14<=s3367617@32,
(-3367617)@32<=sL0x20015d38,L0x20015d38<=s3367617@32
,
(-3367617)@32<=sL0x20014fdc,L0x20014fdc<=s3367617@32,
(-3367617)@32<=sL0x20015000,L0x20015000<=s3367617@32,
(-3367617)@32<=sL0x20015024,L0x20015024<=s3367617@32,
(-3367617)@32<=sL0x20015d5c,L0x20015d5c<=s3367617@32,
(-3367617)@32<=sL0x20015d80,L0x20015d80<=s3367617@32,
(-3367617)@32<=sL0x20015da4,L0x20015da4<=s3367617@32
,
(-3367617)@32<=sL0x20015048,L0x20015048<=s3367617@32,
(-3367617)@32<=sL0x2001506c,L0x2001506c<=s3367617@32,
(-3367617)@32<=sL0x20015090,L0x20015090<=s3367617@32,
(-3367617)@32<=sL0x20015dc8,L0x20015dc8<=s3367617@32,
(-3367617)@32<=sL0x20015dec,L0x20015dec<=s3367617@32,
(-3367617)@32<=sL0x20015e10,L0x20015e10<=s3367617@32
,
(-3367617)@32<=sL0x200150b4,L0x200150b4<=s3367617@32,
(-3367617)@32<=sL0x200150d8,L0x200150d8<=s3367617@32,
(-3367617)@32<=sL0x200150fc,L0x200150fc<=s3367617@32,
(-3367617)@32<=sL0x20015e34,L0x20015e34<=s3367617@32,
(-3367617)@32<=sL0x20015e58,L0x20015e58<=s3367617@32,
(-3367617)@32<=sL0x20015e7c,L0x20015e7c<=s3367617@32
,
(-3367617)@32<=sL0x20015120,L0x20015120<=s3367617@32,
(-3367617)@32<=sL0x20015144,L0x20015144<=s3367617@32,
(-3367617)@32<=sL0x20015168,L0x20015168<=s3367617@32,
(-3367617)@32<=sL0x20015ea0,L0x20015ea0<=s3367617@32,
(-3367617)@32<=sL0x20015ec4,L0x20015ec4<=s3367617@32,
(-3367617)@32<=sL0x20015ee8,L0x20015ee8<=s3367617@32
,
(-3367617)@32<=sL0x2001518c,L0x2001518c<=s3367617@32,
(-3367617)@32<=sL0x200151b0,L0x200151b0<=s3367617@32,
(-3367617)@32<=sL0x200151d4,L0x200151d4<=s3367617@32,
(-3367617)@32<=sL0x20015f0c,L0x20015f0c<=s3367617@32,
(-3367617)@32<=sL0x20015f30,L0x20015f30<=s3367617@32,
(-3367617)@32<=sL0x20015f54,L0x20015f54<=s3367617@32
,
(-3367617)@32<=sL0x200151f8,L0x200151f8<=s3367617@32,
(-3367617)@32<=sL0x2001521c,L0x2001521c<=s3367617@32,
(-3367617)@32<=sL0x20015240,L0x20015240<=s3367617@32,
(-3367617)@32<=sL0x20015f78,L0x20015f78<=s3367617@32,
(-3367617)@32<=sL0x20015f9c,L0x20015f9c<=s3367617@32,
(-3367617)@32<=sL0x20015fc0,L0x20015fc0<=s3367617@32
,
(-3367617)@32<=sL0x20015264,L0x20015264<=s3367617@32,
(-3367617)@32<=sL0x20015288,L0x20015288<=s3367617@32,
(-3367617)@32<=sL0x200152ac,L0x200152ac<=s3367617@32,
(-3367617)@32<=sL0x20015fe4,L0x20015fe4<=s3367617@32,
(-3367617)@32<=sL0x20016008,L0x20016008<=s3367617@32,
(-3367617)@32<=sL0x2001602c,L0x2001602c<=s3367617@32
,
(-3367617)@32<=sL0x200152d0,L0x200152d0<=s3367617@32,
(-3367617)@32<=sL0x200152f4,L0x200152f4<=s3367617@32,
(-3367617)@32<=sL0x20015318,L0x20015318<=s3367617@32,
(-3367617)@32<=sL0x20016050,L0x20016050<=s3367617@32,
(-3367617)@32<=sL0x20016074,L0x20016074<=s3367617@32,
(-3367617)@32<=sL0x20016098,L0x20016098<=s3367617@32
,
(-3367617)@32<=sL0x2001533c,L0x2001533c<=s3367617@32,
(-3367617)@32<=sL0x20015360,L0x20015360<=s3367617@32,
(-3367617)@32<=sL0x20015384,L0x20015384<=s3367617@32,
(-3367617)@32<=sL0x200160bc,L0x200160bc<=s3367617@32,
(-3367617)@32<=sL0x200160e0,L0x200160e0<=s3367617@32,
(-3367617)@32<=sL0x20016104,L0x20016104<=s3367617@32
,
(-3367617)@32<=sL0x200153a8,L0x200153a8<=s3367617@32,
(-3367617)@32<=sL0x200153cc,L0x200153cc<=s3367617@32,
(-3367617)@32<=sL0x200153f0,L0x200153f0<=s3367617@32,
(-3367617)@32<=sL0x20016128,L0x20016128<=s3367617@32,
(-3367617)@32<=sL0x2001614c,L0x2001614c<=s3367617@32,
(-3367617)@32<=sL0x20016170,L0x20016170<=s3367617@32
,
(-3367617)@32<=sL0x20015414,L0x20015414<=s3367617@32,
(-3367617)@32<=sL0x20015438,L0x20015438<=s3367617@32,
(-3367617)@32<=sL0x2001545c,L0x2001545c<=s3367617@32,
(-3367617)@32<=sL0x20016194,L0x20016194<=s3367617@32,
(-3367617)@32<=sL0x200161b8,L0x200161b8<=s3367617@32,
(-3367617)@32<=sL0x200161dc,L0x200161dc<=s3367617@32
,
(-3367617)@32<=sL0x20015480,L0x20015480<=s3367617@32,
(-3367617)@32<=sL0x200154a4,L0x200154a4<=s3367617@32,
(-3367617)@32<=sL0x200154c8,L0x200154c8<=s3367617@32,
(-3367617)@32<=sL0x20016200,L0x20016200<=s3367617@32,
(-3367617)@32<=sL0x20016224,L0x20016224<=s3367617@32,
(-3367617)@32<=sL0x20016248,L0x20016248<=s3367617@32
,
(-3367617)@32<=sL0x200154ec,L0x200154ec<=s3367617@32,
(-3367617)@32<=sL0x20015510,L0x20015510<=s3367617@32,
(-3367617)@32<=sL0x20015534,L0x20015534<=s3367617@32,
(-3367617)@32<=sL0x2001626c,L0x2001626c<=s3367617@32,
(-3367617)@32<=sL0x20016290,L0x20016290<=s3367617@32,
(-3367617)@32<=sL0x200162b4,L0x200162b4<=s3367617@32
,
(-3367617)@32<=sL0x20015558,L0x20015558<=s3367617@32,
(-3367617)@32<=sL0x2001557c,L0x2001557c<=s3367617@32,
(-3367617)@32<=sL0x200155a0,L0x200155a0<=s3367617@32,
(-3367617)@32<=sL0x200162d8,L0x200162d8<=s3367617@32,
(-3367617)@32<=sL0x200162fc,L0x200162fc<=s3367617@32,
(-3367617)@32<=sL0x20016320,L0x20016320<=s3367617@32
,
(-3367617)@32<=sL0x200155c4,L0x200155c4<=s3367617@32,
(-3367617)@32<=sL0x200155e8,L0x200155e8<=s3367617@32,
(-3367617)@32<=sL0x2001560c,L0x2001560c<=s3367617@32,
(-3367617)@32<=sL0x20016344,L0x20016344<=s3367617@32,
(-3367617)@32<=sL0x20016368,L0x20016368<=s3367617@32,
(-3367617)@32<=sL0x2001638c,L0x2001638c<=s3367617@32
,
(-3367617)@32<=sL0x200148b4,L0x200148b4<=s3367617@32,
(-3367617)@32<=sL0x200148d8,L0x200148d8<=s3367617@32,
(-3367617)@32<=sL0x200148fc,L0x200148fc<=s3367617@32,
(-3367617)@32<=sL0x20015634,L0x20015634<=s3367617@32,
(-3367617)@32<=sL0x20015658,L0x20015658<=s3367617@32,
(-3367617)@32<=sL0x2001567c,L0x2001567c<=s3367617@32
,
(-3367617)@32<=sL0x20014920,L0x20014920<=s3367617@32,
(-3367617)@32<=sL0x20014944,L0x20014944<=s3367617@32,
(-3367617)@32<=sL0x20014968,L0x20014968<=s3367617@32,
(-3367617)@32<=sL0x200156a0,L0x200156a0<=s3367617@32,
(-3367617)@32<=sL0x200156c4,L0x200156c4<=s3367617@32,
(-3367617)@32<=sL0x200156e8,L0x200156e8<=s3367617@32
,
(-3367617)@32<=sL0x2001498c,L0x2001498c<=s3367617@32,
(-3367617)@32<=sL0x200149b0,L0x200149b0<=s3367617@32,
(-3367617)@32<=sL0x200149d4,L0x200149d4<=s3367617@32,
(-3367617)@32<=sL0x2001570c,L0x2001570c<=s3367617@32,
(-3367617)@32<=sL0x20015730,L0x20015730<=s3367617@32,
(-3367617)@32<=sL0x20015754,L0x20015754<=s3367617@32
,
(-3367617)@32<=sL0x200149f8,L0x200149f8<=s3367617@32,
(-3367617)@32<=sL0x20014a1c,L0x20014a1c<=s3367617@32,
(-3367617)@32<=sL0x20014a40,L0x20014a40<=s3367617@32,
(-3367617)@32<=sL0x20015778,L0x20015778<=s3367617@32,
(-3367617)@32<=sL0x2001579c,L0x2001579c<=s3367617@32,
(-3367617)@32<=sL0x200157c0,L0x200157c0<=s3367617@32
,
(-3367617)@32<=sL0x20014a64,L0x20014a64<=s3367617@32,
(-3367617)@32<=sL0x20014a88,L0x20014a88<=s3367617@32,
(-3367617)@32<=sL0x20014aac,L0x20014aac<=s3367617@32,
(-3367617)@32<=sL0x200157e4,L0x200157e4<=s3367617@32,
(-3367617)@32<=sL0x20015808,L0x20015808<=s3367617@32,
(-3367617)@32<=sL0x2001582c,L0x2001582c<=s3367617@32
,
(-3367617)@32<=sL0x20014ad0,L0x20014ad0<=s3367617@32,
(-3367617)@32<=sL0x20014af4,L0x20014af4<=s3367617@32,
(-3367617)@32<=sL0x20014b18,L0x20014b18<=s3367617@32,
(-3367617)@32<=sL0x20015850,L0x20015850<=s3367617@32,
(-3367617)@32<=sL0x20015874,L0x20015874<=s3367617@32,
(-3367617)@32<=sL0x20015898,L0x20015898<=s3367617@32
,
(-3367617)@32<=sL0x20014b3c,L0x20014b3c<=s3367617@32,
(-3367617)@32<=sL0x20014b60,L0x20014b60<=s3367617@32,
(-3367617)@32<=sL0x20014b84,L0x20014b84<=s3367617@32,
(-3367617)@32<=sL0x200158bc,L0x200158bc<=s3367617@32,
(-3367617)@32<=sL0x200158e0,L0x200158e0<=s3367617@32,
(-3367617)@32<=sL0x20015904,L0x20015904<=s3367617@32
,
(-3367617)@32<=sL0x20014ba8,L0x20014ba8<=s3367617@32,
(-3367617)@32<=sL0x20014bcc,L0x20014bcc<=s3367617@32,
(-3367617)@32<=sL0x20014bf0,L0x20014bf0<=s3367617@32,
(-3367617)@32<=sL0x20015928,L0x20015928<=s3367617@32,
(-3367617)@32<=sL0x2001594c,L0x2001594c<=s3367617@32,
(-3367617)@32<=sL0x20015970,L0x20015970<=s3367617@32
,
(-3367617)@32<=sL0x20014c14,L0x20014c14<=s3367617@32,
(-3367617)@32<=sL0x20014c38,L0x20014c38<=s3367617@32,
(-3367617)@32<=sL0x20014c5c,L0x20014c5c<=s3367617@32,
(-3367617)@32<=sL0x20015994,L0x20015994<=s3367617@32,
(-3367617)@32<=sL0x200159b8,L0x200159b8<=s3367617@32,
(-3367617)@32<=sL0x200159dc,L0x200159dc<=s3367617@32
,
(-3367617)@32<=sL0x20014c80,L0x20014c80<=s3367617@32,
(-3367617)@32<=sL0x20014ca4,L0x20014ca4<=s3367617@32,
(-3367617)@32<=sL0x20014cc8,L0x20014cc8<=s3367617@32,
(-3367617)@32<=sL0x20015a00,L0x20015a00<=s3367617@32,
(-3367617)@32<=sL0x20015a24,L0x20015a24<=s3367617@32,
(-3367617)@32<=sL0x20015a48,L0x20015a48<=s3367617@32
,
(-3367617)@32<=sL0x20014cec,L0x20014cec<=s3367617@32,
(-3367617)@32<=sL0x20014d10,L0x20014d10<=s3367617@32,
(-3367617)@32<=sL0x20014d34,L0x20014d34<=s3367617@32,
(-3367617)@32<=sL0x20015a6c,L0x20015a6c<=s3367617@32,
(-3367617)@32<=sL0x20015a90,L0x20015a90<=s3367617@32,
(-3367617)@32<=sL0x20015ab4,L0x20015ab4<=s3367617@32
,
(-3367617)@32<=sL0x20014d58,L0x20014d58<=s3367617@32,
(-3367617)@32<=sL0x20014d7c,L0x20014d7c<=s3367617@32,
(-3367617)@32<=sL0x20014da0,L0x20014da0<=s3367617@32,
(-3367617)@32<=sL0x20015ad8,L0x20015ad8<=s3367617@32,
(-3367617)@32<=sL0x20015afc,L0x20015afc<=s3367617@32,
(-3367617)@32<=sL0x20015b20,L0x20015b20<=s3367617@32
,
(-3367617)@32<=sL0x20014dc4,L0x20014dc4<=s3367617@32,
(-3367617)@32<=sL0x20014de8,L0x20014de8<=s3367617@32,
(-3367617)@32<=sL0x20014e0c,L0x20014e0c<=s3367617@32,
(-3367617)@32<=sL0x20015b44,L0x20015b44<=s3367617@32,
(-3367617)@32<=sL0x20015b68,L0x20015b68<=s3367617@32,
(-3367617)@32<=sL0x20015b8c,L0x20015b8c<=s3367617@32
,
(-3367617)@32<=sL0x20014e30,L0x20014e30<=s3367617@32,
(-3367617)@32<=sL0x20014e54,L0x20014e54<=s3367617@32,
(-3367617)@32<=sL0x20014e78,L0x20014e78<=s3367617@32,
(-3367617)@32<=sL0x20015bb0,L0x20015bb0<=s3367617@32,
(-3367617)@32<=sL0x20015bd4,L0x20015bd4<=s3367617@32,
(-3367617)@32<=sL0x20015bf8,L0x20015bf8<=s3367617@32
,
(-3367617)@32<=sL0x20014e9c,L0x20014e9c<=s3367617@32,
(-3367617)@32<=sL0x20014ec0,L0x20014ec0<=s3367617@32,
(-3367617)@32<=sL0x20014ee4,L0x20014ee4<=s3367617@32,
(-3367617)@32<=sL0x20015c1c,L0x20015c1c<=s3367617@32,
(-3367617)@32<=sL0x20015c40,L0x20015c40<=s3367617@32,
(-3367617)@32<=sL0x20015c64,L0x20015c64<=s3367617@32
,
(-3367617)@32<=sL0x20014f08,L0x20014f08<=s3367617@32,
(-3367617)@32<=sL0x20014f2c,L0x20014f2c<=s3367617@32,
(-3367617)@32<=sL0x20014f50,L0x20014f50<=s3367617@32,
(-3367617)@32<=sL0x20015c88,L0x20015c88<=s3367617@32,
(-3367617)@32<=sL0x20015cac,L0x20015cac<=s3367617@32,
(-3367617)@32<=sL0x20015cd0,L0x20015cd0<=s3367617@32
,
(-3367617)@32<=sL0x20014f74,L0x20014f74<=s3367617@32,
(-3367617)@32<=sL0x20014f98,L0x20014f98<=s3367617@32,
(-3367617)@32<=sL0x20014fbc,L0x20014fbc<=s3367617@32,
(-3367617)@32<=sL0x20015cf4,L0x20015cf4<=s3367617@32,
(-3367617)@32<=sL0x20015d18,L0x20015d18<=s3367617@32,
(-3367617)@32<=sL0x20015d3c,L0x20015d3c<=s3367617@32
,
(-3367617)@32<=sL0x20014fe0,L0x20014fe0<=s3367617@32,
(-3367617)@32<=sL0x20015004,L0x20015004<=s3367617@32,
(-3367617)@32<=sL0x20015028,L0x20015028<=s3367617@32,
(-3367617)@32<=sL0x20015d60,L0x20015d60<=s3367617@32,
(-3367617)@32<=sL0x20015d84,L0x20015d84<=s3367617@32,
(-3367617)@32<=sL0x20015da8,L0x20015da8<=s3367617@32
,
(-3367617)@32<=sL0x2001504c,L0x2001504c<=s3367617@32,
(-3367617)@32<=sL0x20015070,L0x20015070<=s3367617@32,
(-3367617)@32<=sL0x20015094,L0x20015094<=s3367617@32,
(-3367617)@32<=sL0x20015dcc,L0x20015dcc<=s3367617@32,
(-3367617)@32<=sL0x20015df0,L0x20015df0<=s3367617@32,
(-3367617)@32<=sL0x20015e14,L0x20015e14<=s3367617@32
,
(-3367617)@32<=sL0x200150b8,L0x200150b8<=s3367617@32,
(-3367617)@32<=sL0x200150dc,L0x200150dc<=s3367617@32,
(-3367617)@32<=sL0x20015100,L0x20015100<=s3367617@32,
(-3367617)@32<=sL0x20015e38,L0x20015e38<=s3367617@32,
(-3367617)@32<=sL0x20015e5c,L0x20015e5c<=s3367617@32,
(-3367617)@32<=sL0x20015e80,L0x20015e80<=s3367617@32
,
(-3367617)@32<=sL0x20015124,L0x20015124<=s3367617@32,
(-3367617)@32<=sL0x20015148,L0x20015148<=s3367617@32,
(-3367617)@32<=sL0x2001516c,L0x2001516c<=s3367617@32,
(-3367617)@32<=sL0x20015ea4,L0x20015ea4<=s3367617@32,
(-3367617)@32<=sL0x20015ec8,L0x20015ec8<=s3367617@32,
(-3367617)@32<=sL0x20015eec,L0x20015eec<=s3367617@32
,
(-3367617)@32<=sL0x20015190,L0x20015190<=s3367617@32,
(-3367617)@32<=sL0x200151b4,L0x200151b4<=s3367617@32,
(-3367617)@32<=sL0x200151d8,L0x200151d8<=s3367617@32,
(-3367617)@32<=sL0x20015f10,L0x20015f10<=s3367617@32,
(-3367617)@32<=sL0x20015f34,L0x20015f34<=s3367617@32,
(-3367617)@32<=sL0x20015f58,L0x20015f58<=s3367617@32
,
(-3367617)@32<=sL0x200151fc,L0x200151fc<=s3367617@32,
(-3367617)@32<=sL0x20015220,L0x20015220<=s3367617@32,
(-3367617)@32<=sL0x20015244,L0x20015244<=s3367617@32,
(-3367617)@32<=sL0x20015f7c,L0x20015f7c<=s3367617@32,
(-3367617)@32<=sL0x20015fa0,L0x20015fa0<=s3367617@32,
(-3367617)@32<=sL0x20015fc4,L0x20015fc4<=s3367617@32
,
(-3367617)@32<=sL0x20015268,L0x20015268<=s3367617@32,
(-3367617)@32<=sL0x2001528c,L0x2001528c<=s3367617@32,
(-3367617)@32<=sL0x200152b0,L0x200152b0<=s3367617@32,
(-3367617)@32<=sL0x20015fe8,L0x20015fe8<=s3367617@32,
(-3367617)@32<=sL0x2001600c,L0x2001600c<=s3367617@32,
(-3367617)@32<=sL0x20016030,L0x20016030<=s3367617@32
,
(-3367617)@32<=sL0x200152d4,L0x200152d4<=s3367617@32,
(-3367617)@32<=sL0x200152f8,L0x200152f8<=s3367617@32,
(-3367617)@32<=sL0x2001531c,L0x2001531c<=s3367617@32,
(-3367617)@32<=sL0x20016054,L0x20016054<=s3367617@32,
(-3367617)@32<=sL0x20016078,L0x20016078<=s3367617@32,
(-3367617)@32<=sL0x2001609c,L0x2001609c<=s3367617@32
,
(-3367617)@32<=sL0x20015340,L0x20015340<=s3367617@32,
(-3367617)@32<=sL0x20015364,L0x20015364<=s3367617@32,
(-3367617)@32<=sL0x20015388,L0x20015388<=s3367617@32,
(-3367617)@32<=sL0x200160c0,L0x200160c0<=s3367617@32,
(-3367617)@32<=sL0x200160e4,L0x200160e4<=s3367617@32,
(-3367617)@32<=sL0x20016108,L0x20016108<=s3367617@32
,
(-3367617)@32<=sL0x200153ac,L0x200153ac<=s3367617@32,
(-3367617)@32<=sL0x200153d0,L0x200153d0<=s3367617@32,
(-3367617)@32<=sL0x200153f4,L0x200153f4<=s3367617@32,
(-3367617)@32<=sL0x2001612c,L0x2001612c<=s3367617@32,
(-3367617)@32<=sL0x20016150,L0x20016150<=s3367617@32,
(-3367617)@32<=sL0x20016174,L0x20016174<=s3367617@32
,
(-3367617)@32<=sL0x20015418,L0x20015418<=s3367617@32,
(-3367617)@32<=sL0x2001543c,L0x2001543c<=s3367617@32,
(-3367617)@32<=sL0x20015460,L0x20015460<=s3367617@32,
(-3367617)@32<=sL0x20016198,L0x20016198<=s3367617@32,
(-3367617)@32<=sL0x200161bc,L0x200161bc<=s3367617@32,
(-3367617)@32<=sL0x200161e0,L0x200161e0<=s3367617@32
,
(-3367617)@32<=sL0x20015484,L0x20015484<=s3367617@32,
(-3367617)@32<=sL0x200154a8,L0x200154a8<=s3367617@32,
(-3367617)@32<=sL0x200154cc,L0x200154cc<=s3367617@32,
(-3367617)@32<=sL0x20016204,L0x20016204<=s3367617@32,
(-3367617)@32<=sL0x20016228,L0x20016228<=s3367617@32,
(-3367617)@32<=sL0x2001624c,L0x2001624c<=s3367617@32
,
(-3367617)@32<=sL0x200154f0,L0x200154f0<=s3367617@32,
(-3367617)@32<=sL0x20015514,L0x20015514<=s3367617@32,
(-3367617)@32<=sL0x20015538,L0x20015538<=s3367617@32,
(-3367617)@32<=sL0x20016270,L0x20016270<=s3367617@32,
(-3367617)@32<=sL0x20016294,L0x20016294<=s3367617@32,
(-3367617)@32<=sL0x200162b8,L0x200162b8<=s3367617@32
,
(-3367617)@32<=sL0x2001555c,L0x2001555c<=s3367617@32,
(-3367617)@32<=sL0x20015580,L0x20015580<=s3367617@32,
(-3367617)@32<=sL0x200155a4,L0x200155a4<=s3367617@32,
(-3367617)@32<=sL0x200162dc,L0x200162dc<=s3367617@32,
(-3367617)@32<=sL0x20016300,L0x20016300<=s3367617@32,
(-3367617)@32<=sL0x20016324,L0x20016324<=s3367617@32
,
(-3367617)@32<=sL0x200155c8,L0x200155c8<=s3367617@32,
(-3367617)@32<=sL0x200155ec,L0x200155ec<=s3367617@32,
(-3367617)@32<=sL0x20015610,L0x20015610<=s3367617@32,
(-3367617)@32<=sL0x20016348,L0x20016348<=s3367617@32,
(-3367617)@32<=sL0x2001636c,L0x2001636c<=s3367617@32,
(-3367617)@32<=sL0x20016390,L0x20016390<=s3367617@32
,
(-3367617)@32<=sL0x200148b8,L0x200148b8<=s3367617@32,
(-3367617)@32<=sL0x200148dc,L0x200148dc<=s3367617@32,
(-3367617)@32<=sL0x20014900,L0x20014900<=s3367617@32,
(-3367617)@32<=sL0x20015638,L0x20015638<=s3367617@32,
(-3367617)@32<=sL0x2001565c,L0x2001565c<=s3367617@32,
(-3367617)@32<=sL0x20015680,L0x20015680<=s3367617@32
,
(-3367617)@32<=sL0x20014924,L0x20014924<=s3367617@32,
(-3367617)@32<=sL0x20014948,L0x20014948<=s3367617@32,
(-3367617)@32<=sL0x2001496c,L0x2001496c<=s3367617@32,
(-3367617)@32<=sL0x200156a4,L0x200156a4<=s3367617@32,
(-3367617)@32<=sL0x200156c8,L0x200156c8<=s3367617@32,
(-3367617)@32<=sL0x200156ec,L0x200156ec<=s3367617@32
,
(-3367617)@32<=sL0x20014990,L0x20014990<=s3367617@32,
(-3367617)@32<=sL0x200149b4,L0x200149b4<=s3367617@32,
(-3367617)@32<=sL0x200149d8,L0x200149d8<=s3367617@32,
(-3367617)@32<=sL0x20015710,L0x20015710<=s3367617@32,
(-3367617)@32<=sL0x20015734,L0x20015734<=s3367617@32,
(-3367617)@32<=sL0x20015758,L0x20015758<=s3367617@32
,
(-3367617)@32<=sL0x200149fc,L0x200149fc<=s3367617@32,
(-3367617)@32<=sL0x20014a20,L0x20014a20<=s3367617@32,
(-3367617)@32<=sL0x20014a44,L0x20014a44<=s3367617@32,
(-3367617)@32<=sL0x2001577c,L0x2001577c<=s3367617@32,
(-3367617)@32<=sL0x200157a0,L0x200157a0<=s3367617@32,
(-3367617)@32<=sL0x200157c4,L0x200157c4<=s3367617@32
,
(-3367617)@32<=sL0x20014a68,L0x20014a68<=s3367617@32,
(-3367617)@32<=sL0x20014a8c,L0x20014a8c<=s3367617@32,
(-3367617)@32<=sL0x20014ab0,L0x20014ab0<=s3367617@32,
(-3367617)@32<=sL0x200157e8,L0x200157e8<=s3367617@32,
(-3367617)@32<=sL0x2001580c,L0x2001580c<=s3367617@32,
(-3367617)@32<=sL0x20015830,L0x20015830<=s3367617@32
,
(-3367617)@32<=sL0x20014ad4,L0x20014ad4<=s3367617@32,
(-3367617)@32<=sL0x20014af8,L0x20014af8<=s3367617@32,
(-3367617)@32<=sL0x20014b1c,L0x20014b1c<=s3367617@32,
(-3367617)@32<=sL0x20015854,L0x20015854<=s3367617@32,
(-3367617)@32<=sL0x20015878,L0x20015878<=s3367617@32,
(-3367617)@32<=sL0x2001589c,L0x2001589c<=s3367617@32
,
(-3367617)@32<=sL0x20014b40,L0x20014b40<=s3367617@32,
(-3367617)@32<=sL0x20014b64,L0x20014b64<=s3367617@32,
(-3367617)@32<=sL0x20014b88,L0x20014b88<=s3367617@32,
(-3367617)@32<=sL0x200158c0,L0x200158c0<=s3367617@32,
(-3367617)@32<=sL0x200158e4,L0x200158e4<=s3367617@32,
(-3367617)@32<=sL0x20015908,L0x20015908<=s3367617@32
,
(-3367617)@32<=sL0x20014bac,L0x20014bac<=s3367617@32,
(-3367617)@32<=sL0x20014bd0,L0x20014bd0<=s3367617@32,
(-3367617)@32<=sL0x20014bf4,L0x20014bf4<=s3367617@32,
(-3367617)@32<=sL0x2001592c,L0x2001592c<=s3367617@32,
(-3367617)@32<=sL0x20015950,L0x20015950<=s3367617@32,
(-3367617)@32<=sL0x20015974,L0x20015974<=s3367617@32
,
(-3367617)@32<=sL0x20014c18,L0x20014c18<=s3367617@32,
(-3367617)@32<=sL0x20014c3c,L0x20014c3c<=s3367617@32,
(-3367617)@32<=sL0x20014c60,L0x20014c60<=s3367617@32,
(-3367617)@32<=sL0x20015998,L0x20015998<=s3367617@32,
(-3367617)@32<=sL0x200159bc,L0x200159bc<=s3367617@32,
(-3367617)@32<=sL0x200159e0,L0x200159e0<=s3367617@32
,
(-3367617)@32<=sL0x20014c84,L0x20014c84<=s3367617@32,
(-3367617)@32<=sL0x20014ca8,L0x20014ca8<=s3367617@32,
(-3367617)@32<=sL0x20014ccc,L0x20014ccc<=s3367617@32,
(-3367617)@32<=sL0x20015a04,L0x20015a04<=s3367617@32,
(-3367617)@32<=sL0x20015a28,L0x20015a28<=s3367617@32,
(-3367617)@32<=sL0x20015a4c,L0x20015a4c<=s3367617@32
,
(-3367617)@32<=sL0x20014cf0,L0x20014cf0<=s3367617@32,
(-3367617)@32<=sL0x20014d14,L0x20014d14<=s3367617@32,
(-3367617)@32<=sL0x20014d38,L0x20014d38<=s3367617@32,
(-3367617)@32<=sL0x20015a70,L0x20015a70<=s3367617@32,
(-3367617)@32<=sL0x20015a94,L0x20015a94<=s3367617@32,
(-3367617)@32<=sL0x20015ab8,L0x20015ab8<=s3367617@32
,
(-3367617)@32<=sL0x20014d5c,L0x20014d5c<=s3367617@32,
(-3367617)@32<=sL0x20014d80,L0x20014d80<=s3367617@32,
(-3367617)@32<=sL0x20014da4,L0x20014da4<=s3367617@32,
(-3367617)@32<=sL0x20015adc,L0x20015adc<=s3367617@32,
(-3367617)@32<=sL0x20015b00,L0x20015b00<=s3367617@32,
(-3367617)@32<=sL0x20015b24,L0x20015b24<=s3367617@32
,
(-3367617)@32<=sL0x20014dc8,L0x20014dc8<=s3367617@32,
(-3367617)@32<=sL0x20014dec,L0x20014dec<=s3367617@32,
(-3367617)@32<=sL0x20014e10,L0x20014e10<=s3367617@32,
(-3367617)@32<=sL0x20015b48,L0x20015b48<=s3367617@32,
(-3367617)@32<=sL0x20015b6c,L0x20015b6c<=s3367617@32,
(-3367617)@32<=sL0x20015b90,L0x20015b90<=s3367617@32
,
(-3367617)@32<=sL0x20014e34,L0x20014e34<=s3367617@32,
(-3367617)@32<=sL0x20014e58,L0x20014e58<=s3367617@32,
(-3367617)@32<=sL0x20014e7c,L0x20014e7c<=s3367617@32,
(-3367617)@32<=sL0x20015bb4,L0x20015bb4<=s3367617@32,
(-3367617)@32<=sL0x20015bd8,L0x20015bd8<=s3367617@32,
(-3367617)@32<=sL0x20015bfc,L0x20015bfc<=s3367617@32
,
(-3367617)@32<=sL0x20014ea0,L0x20014ea0<=s3367617@32,
(-3367617)@32<=sL0x20014ec4,L0x20014ec4<=s3367617@32,
(-3367617)@32<=sL0x20014ee8,L0x20014ee8<=s3367617@32,
(-3367617)@32<=sL0x20015c20,L0x20015c20<=s3367617@32,
(-3367617)@32<=sL0x20015c44,L0x20015c44<=s3367617@32,
(-3367617)@32<=sL0x20015c68,L0x20015c68<=s3367617@32
,
(-3367617)@32<=sL0x20014f0c,L0x20014f0c<=s3367617@32,
(-3367617)@32<=sL0x20014f30,L0x20014f30<=s3367617@32,
(-3367617)@32<=sL0x20014f54,L0x20014f54<=s3367617@32,
(-3367617)@32<=sL0x20015c8c,L0x20015c8c<=s3367617@32,
(-3367617)@32<=sL0x20015cb0,L0x20015cb0<=s3367617@32,
(-3367617)@32<=sL0x20015cd4,L0x20015cd4<=s3367617@32
,
(-3367617)@32<=sL0x20014f78,L0x20014f78<=s3367617@32,
(-3367617)@32<=sL0x20014f9c,L0x20014f9c<=s3367617@32,
(-3367617)@32<=sL0x20014fc0,L0x20014fc0<=s3367617@32,
(-3367617)@32<=sL0x20015cf8,L0x20015cf8<=s3367617@32,
(-3367617)@32<=sL0x20015d1c,L0x20015d1c<=s3367617@32,
(-3367617)@32<=sL0x20015d40,L0x20015d40<=s3367617@32
,
(-3367617)@32<=sL0x20014fe4,L0x20014fe4<=s3367617@32,
(-3367617)@32<=sL0x20015008,L0x20015008<=s3367617@32,
(-3367617)@32<=sL0x2001502c,L0x2001502c<=s3367617@32,
(-3367617)@32<=sL0x20015d64,L0x20015d64<=s3367617@32,
(-3367617)@32<=sL0x20015d88,L0x20015d88<=s3367617@32,
(-3367617)@32<=sL0x20015dac,L0x20015dac<=s3367617@32
,
(-3367617)@32<=sL0x20015050,L0x20015050<=s3367617@32,
(-3367617)@32<=sL0x20015074,L0x20015074<=s3367617@32,
(-3367617)@32<=sL0x20015098,L0x20015098<=s3367617@32,
(-3367617)@32<=sL0x20015dd0,L0x20015dd0<=s3367617@32,
(-3367617)@32<=sL0x20015df4,L0x20015df4<=s3367617@32,
(-3367617)@32<=sL0x20015e18,L0x20015e18<=s3367617@32
,
(-3367617)@32<=sL0x200150bc,L0x200150bc<=s3367617@32,
(-3367617)@32<=sL0x200150e0,L0x200150e0<=s3367617@32,
(-3367617)@32<=sL0x20015104,L0x20015104<=s3367617@32,
(-3367617)@32<=sL0x20015e3c,L0x20015e3c<=s3367617@32,
(-3367617)@32<=sL0x20015e60,L0x20015e60<=s3367617@32,
(-3367617)@32<=sL0x20015e84,L0x20015e84<=s3367617@32
,
(-3367617)@32<=sL0x20015128,L0x20015128<=s3367617@32,
(-3367617)@32<=sL0x2001514c,L0x2001514c<=s3367617@32,
(-3367617)@32<=sL0x20015170,L0x20015170<=s3367617@32,
(-3367617)@32<=sL0x20015ea8,L0x20015ea8<=s3367617@32,
(-3367617)@32<=sL0x20015ecc,L0x20015ecc<=s3367617@32,
(-3367617)@32<=sL0x20015ef0,L0x20015ef0<=s3367617@32
,
(-3367617)@32<=sL0x20015194,L0x20015194<=s3367617@32,
(-3367617)@32<=sL0x200151b8,L0x200151b8<=s3367617@32,
(-3367617)@32<=sL0x200151dc,L0x200151dc<=s3367617@32,
(-3367617)@32<=sL0x20015f14,L0x20015f14<=s3367617@32,
(-3367617)@32<=sL0x20015f38,L0x20015f38<=s3367617@32,
(-3367617)@32<=sL0x20015f5c,L0x20015f5c<=s3367617@32
,
(-3367617)@32<=sL0x20015200,L0x20015200<=s3367617@32,
(-3367617)@32<=sL0x20015224,L0x20015224<=s3367617@32,
(-3367617)@32<=sL0x20015248,L0x20015248<=s3367617@32,
(-3367617)@32<=sL0x20015f80,L0x20015f80<=s3367617@32,
(-3367617)@32<=sL0x20015fa4,L0x20015fa4<=s3367617@32,
(-3367617)@32<=sL0x20015fc8,L0x20015fc8<=s3367617@32
,
(-3367617)@32<=sL0x2001526c,L0x2001526c<=s3367617@32,
(-3367617)@32<=sL0x20015290,L0x20015290<=s3367617@32,
(-3367617)@32<=sL0x200152b4,L0x200152b4<=s3367617@32,
(-3367617)@32<=sL0x20015fec,L0x20015fec<=s3367617@32,
(-3367617)@32<=sL0x20016010,L0x20016010<=s3367617@32,
(-3367617)@32<=sL0x20016034,L0x20016034<=s3367617@32
,
(-3367617)@32<=sL0x200152d8,L0x200152d8<=s3367617@32,
(-3367617)@32<=sL0x200152fc,L0x200152fc<=s3367617@32,
(-3367617)@32<=sL0x20015320,L0x20015320<=s3367617@32,
(-3367617)@32<=sL0x20016058,L0x20016058<=s3367617@32,
(-3367617)@32<=sL0x2001607c,L0x2001607c<=s3367617@32,
(-3367617)@32<=sL0x200160a0,L0x200160a0<=s3367617@32
,
(-3367617)@32<=sL0x20015344,L0x20015344<=s3367617@32,
(-3367617)@32<=sL0x20015368,L0x20015368<=s3367617@32,
(-3367617)@32<=sL0x2001538c,L0x2001538c<=s3367617@32,
(-3367617)@32<=sL0x200160c4,L0x200160c4<=s3367617@32,
(-3367617)@32<=sL0x200160e8,L0x200160e8<=s3367617@32,
(-3367617)@32<=sL0x2001610c,L0x2001610c<=s3367617@32
,
(-3367617)@32<=sL0x200153b0,L0x200153b0<=s3367617@32,
(-3367617)@32<=sL0x200153d4,L0x200153d4<=s3367617@32,
(-3367617)@32<=sL0x200153f8,L0x200153f8<=s3367617@32,
(-3367617)@32<=sL0x20016130,L0x20016130<=s3367617@32,
(-3367617)@32<=sL0x20016154,L0x20016154<=s3367617@32,
(-3367617)@32<=sL0x20016178,L0x20016178<=s3367617@32
,
(-3367617)@32<=sL0x2001541c,L0x2001541c<=s3367617@32,
(-3367617)@32<=sL0x20015440,L0x20015440<=s3367617@32,
(-3367617)@32<=sL0x20015464,L0x20015464<=s3367617@32,
(-3367617)@32<=sL0x2001619c,L0x2001619c<=s3367617@32,
(-3367617)@32<=sL0x200161c0,L0x200161c0<=s3367617@32,
(-3367617)@32<=sL0x200161e4,L0x200161e4<=s3367617@32
,
(-3367617)@32<=sL0x20015488,L0x20015488<=s3367617@32,
(-3367617)@32<=sL0x200154ac,L0x200154ac<=s3367617@32,
(-3367617)@32<=sL0x200154d0,L0x200154d0<=s3367617@32,
(-3367617)@32<=sL0x20016208,L0x20016208<=s3367617@32,
(-3367617)@32<=sL0x2001622c,L0x2001622c<=s3367617@32,
(-3367617)@32<=sL0x20016250,L0x20016250<=s3367617@32
,
(-3367617)@32<=sL0x200154f4,L0x200154f4<=s3367617@32,
(-3367617)@32<=sL0x20015518,L0x20015518<=s3367617@32,
(-3367617)@32<=sL0x2001553c,L0x2001553c<=s3367617@32,
(-3367617)@32<=sL0x20016274,L0x20016274<=s3367617@32,
(-3367617)@32<=sL0x20016298,L0x20016298<=s3367617@32,
(-3367617)@32<=sL0x200162bc,L0x200162bc<=s3367617@32
,
(-3367617)@32<=sL0x20015560,L0x20015560<=s3367617@32,
(-3367617)@32<=sL0x20015584,L0x20015584<=s3367617@32,
(-3367617)@32<=sL0x200155a8,L0x200155a8<=s3367617@32,
(-3367617)@32<=sL0x200162e0,L0x200162e0<=s3367617@32,
(-3367617)@32<=sL0x20016304,L0x20016304<=s3367617@32,
(-3367617)@32<=sL0x20016328,L0x20016328<=s3367617@32
,
(-3367617)@32<=sL0x200155cc,L0x200155cc<=s3367617@32,
(-3367617)@32<=sL0x200155f0,L0x200155f0<=s3367617@32,
(-3367617)@32<=sL0x20015614,L0x20015614<=s3367617@32,
(-3367617)@32<=sL0x2001634c,L0x2001634c<=s3367617@32,
(-3367617)@32<=sL0x20016370,L0x20016370<=s3367617@32,
(-3367617)@32<=sL0x20016394,L0x20016394<=s3367617@32
] prove with [ all cuts ]
}

