(* frege: -v -isafety -slicing -no_carry_constraint -jobs 42 -vo lex asm_3x2block_ntt.cl
Parsing Cryptoline file:                [OK]            0.399537 seconds
Checking well-formedness:               [OK]            0.616345 seconds
Transforming to SSA form:               [OK]            0.076222 seconds
Normalizing specification:              [OK]            0.001967 seconds
Rewriting assignments:                  [OK]            0.050140 seconds
Verifying program safety:               [OK]            4120.593797 seconds
Verifying range assertions:             [OK]            11.851406 seconds
Verifying range specification:          [OK]            2823.852756 seconds
Rewriting value-preserved casting:      [OK]            0.001951 seconds
Verifying algebraic assertions:         [OK]            295.549729 seconds
Verifying algebraic specification:      [OK]            323.909800 seconds
Verification result:                    [OK]            7576.939898 seconds
*)

(* quine: no post condition
  -v -isafety -jobs 42 -slicing -no_carry_constraint asm_3x2block_ntt.cl
Parsing Cryptoline file:                [OK]            0.300012 seconds
Checking well-formedness:               [OK]            0.411012 seconds
Transforming to SSA form:               [OK]            0.056335 seconds
Normalizing specification:              [OK]            0.001789 seconds
Rewriting assignments:                  [OK]            0.050455 seconds
Verifying program safety:               [OK]            4627.423362 seconds
Verifying range assertions:             [OK]            18.494155 seconds
Verifying range specification:          [OK]            2886.883892 seconds
Rewriting value-preserved casting:      [OK]            0.002481 seconds
Verifying algebraic assertions:         [OK]            445.434134 seconds
Verifying algebraic specification:      [OK]            476.422793 seconds
Verification result:                    [OK]            8455.509355 seconds
                                
*)

proc main (
sint32 z, sint32 y, sint32 x,
sint32 CF0, sint32 CF1, sint32 CF2,
sint32 CG00, sint32 CG01, sint32 CG02,
sint32 cg00000, sint32 cg00100, sint32 cg00200, sint32 cg00001, sint32 cg00101,
sint32 cg00201, sint32 cg00002, sint32 cg00102, sint32 cg00202, sint32 cg00003,
sint32 cg00103, sint32 cg00203, sint32 cg00004, sint32 cg00104, sint32 cg00204,
sint32 cg00005, sint32 cg00105, sint32 cg00205, sint32 cg00006, sint32 cg00106,
sint32 cg00206, sint32 cg00007, sint32 cg00107, sint32 cg00207, sint32 cg00008,
sint32 cg00108, sint32 cg00208, sint32 cg00009, sint32 cg00109, sint32 cg00209,
sint32 cg00010, sint32 cg00110, sint32 cg00210, sint32 cg00011, sint32 cg00111,
sint32 cg00211, sint32 cg00012, sint32 cg00112, sint32 cg00212, sint32 cg00013,
sint32 cg00113, sint32 cg00213, sint32 cg00014, sint32 cg00114, sint32 cg00214,
sint32 cg00015, sint32 cg00115, sint32 cg00215, sint32 cg00016, sint32 cg00116,
sint32 cg00216, sint32 cg00017, sint32 cg00117, sint32 cg00217, sint32 cg00018,
sint32 cg00118, sint32 cg00218, sint32 cg00019, sint32 cg00119, sint32 cg00219,
sint32 cg00020, sint32 cg00120, sint32 cg00220, sint32 cg00021, sint32 cg00121,
sint32 cg00221, sint32 cg00022, sint32 cg00122, sint32 cg00222, sint32 cg00023,
sint32 cg00123, sint32 cg00223, sint32 cg00024, sint32 cg00124, sint32 cg00224,
sint32 cg00025, sint32 cg00125, sint32 cg00225, sint32 cg00026, sint32 cg00126,
sint32 cg00226, sint32 cg00027, sint32 cg00127, sint32 cg00227, sint32 cg00028,
sint32 cg00128, sint32 cg00228, sint32 cg00029, sint32 cg00129, sint32 cg00229,
sint32 cg00030, sint32 cg00130, sint32 cg00230, sint32 cg00031, sint32 cg00131,
sint32 cg00231, sint32 cg01000, sint32 cg01100, sint32 cg01200, sint32 cg01001,
sint32 cg01101, sint32 cg01201, sint32 cg01002, sint32 cg01102, sint32 cg01202,
sint32 cg01003, sint32 cg01103, sint32 cg01203, sint32 cg01004, sint32 cg01104,
sint32 cg01204, sint32 cg01005, sint32 cg01105, sint32 cg01205, sint32 cg01006,
sint32 cg01106, sint32 cg01206, sint32 cg01007, sint32 cg01107, sint32 cg01207,
sint32 cg01008, sint32 cg01108, sint32 cg01208, sint32 cg01009, sint32 cg01109,
sint32 cg01209, sint32 cg01010, sint32 cg01110, sint32 cg01210, sint32 cg01011,
sint32 cg01111, sint32 cg01211, sint32 cg01012, sint32 cg01112, sint32 cg01212,
sint32 cg01013, sint32 cg01113, sint32 cg01213, sint32 cg01014, sint32 cg01114,
sint32 cg01214, sint32 cg01015, sint32 cg01115, sint32 cg01215, sint32 cg01016,
sint32 cg01116, sint32 cg01216, sint32 cg01017, sint32 cg01117, sint32 cg01217,
sint32 cg01018, sint32 cg01118, sint32 cg01218, sint32 cg01019, sint32 cg01119,
sint32 cg01219, sint32 cg01020, sint32 cg01120, sint32 cg01220, sint32 cg01021,
sint32 cg01121, sint32 cg01221, sint32 cg01022, sint32 cg01122, sint32 cg01222,
sint32 cg01023, sint32 cg01123, sint32 cg01223, sint32 cg01024, sint32 cg01124,
sint32 cg01224, sint32 cg01025, sint32 cg01125, sint32 cg01225, sint32 cg01026,
sint32 cg01126, sint32 cg01226, sint32 cg01027, sint32 cg01127, sint32 cg01227,
sint32 cg01028, sint32 cg01128, sint32 cg01228, sint32 cg01029, sint32 cg01129,
sint32 cg01229, sint32 cg01030, sint32 cg01130, sint32 cg01230, sint32 cg01031,
sint32 cg01131, sint32 cg01231, sint32 cg02000, sint32 cg02100, sint32 cg02200,
sint32 cg02001, sint32 cg02101, sint32 cg02201, sint32 cg02002, sint32 cg02102,
sint32 cg02202, sint32 cg02003, sint32 cg02103, sint32 cg02203, sint32 cg02004,
sint32 cg02104, sint32 cg02204, sint32 cg02005, sint32 cg02105, sint32 cg02205,
sint32 cg02006, sint32 cg02106, sint32 cg02206, sint32 cg02007, sint32 cg02107,
sint32 cg02207, sint32 cg02008, sint32 cg02108, sint32 cg02208, sint32 cg02009,
sint32 cg02109, sint32 cg02209, sint32 cg02010, sint32 cg02110, sint32 cg02210,
sint32 cg02011, sint32 cg02111, sint32 cg02211, sint32 cg02012, sint32 cg02112,
sint32 cg02212, sint32 cg02013, sint32 cg02113, sint32 cg02213, sint32 cg02014,
sint32 cg02114, sint32 cg02214, sint32 cg02015, sint32 cg02115, sint32 cg02215,
sint32 cg02016, sint32 cg02116, sint32 cg02216, sint32 cg02017, sint32 cg02117,
sint32 cg02217, sint32 cg02018, sint32 cg02118, sint32 cg02218, sint32 cg02019,
sint32 cg02119, sint32 cg02219, sint32 cg02020, sint32 cg02120, sint32 cg02220,
sint32 cg02021, sint32 cg02121, sint32 cg02221, sint32 cg02022, sint32 cg02122,
sint32 cg02222, sint32 cg02023, sint32 cg02123, sint32 cg02223, sint32 cg02024,
sint32 cg02124, sint32 cg02224, sint32 cg02025, sint32 cg02125, sint32 cg02225,
sint32 cg02026, sint32 cg02126, sint32 cg02226, sint32 cg02027, sint32 cg02127,
sint32 cg02227, sint32 cg02028, sint32 cg02128, sint32 cg02228, sint32 cg02029,
sint32 cg02129, sint32 cg02229, sint32 cg02030, sint32 cg02130, sint32 cg02230,
sint32 cg02031, sint32 cg02131, sint32 cg02231,
sint32 CG10, sint32 CG11, sint32 CG12,
sint32 cg10000, sint32 cg10100, sint32 cg10200, sint32 cg10001, sint32 cg10101,
sint32 cg10201, sint32 cg10002, sint32 cg10102, sint32 cg10202, sint32 cg10003,
sint32 cg10103, sint32 cg10203, sint32 cg10004, sint32 cg10104, sint32 cg10204,
sint32 cg10005, sint32 cg10105, sint32 cg10205, sint32 cg10006, sint32 cg10106,
sint32 cg10206, sint32 cg10007, sint32 cg10107, sint32 cg10207, sint32 cg10008,
sint32 cg10108, sint32 cg10208, sint32 cg10009, sint32 cg10109, sint32 cg10209,
sint32 cg10010, sint32 cg10110, sint32 cg10210, sint32 cg10011, sint32 cg10111,
sint32 cg10211, sint32 cg10012, sint32 cg10112, sint32 cg10212, sint32 cg10013,
sint32 cg10113, sint32 cg10213, sint32 cg10014, sint32 cg10114, sint32 cg10214,
sint32 cg10015, sint32 cg10115, sint32 cg10215, sint32 cg10016, sint32 cg10116,
sint32 cg10216, sint32 cg10017, sint32 cg10117, sint32 cg10217, sint32 cg10018,
sint32 cg10118, sint32 cg10218, sint32 cg10019, sint32 cg10119, sint32 cg10219,
sint32 cg10020, sint32 cg10120, sint32 cg10220, sint32 cg10021, sint32 cg10121,
sint32 cg10221, sint32 cg10022, sint32 cg10122, sint32 cg10222, sint32 cg10023,
sint32 cg10123, sint32 cg10223, sint32 cg10024, sint32 cg10124, sint32 cg10224,
sint32 cg10025, sint32 cg10125, sint32 cg10225, sint32 cg10026, sint32 cg10126,
sint32 cg10226, sint32 cg10027, sint32 cg10127, sint32 cg10227, sint32 cg10028,
sint32 cg10128, sint32 cg10228, sint32 cg10029, sint32 cg10129, sint32 cg10229,
sint32 cg10030, sint32 cg10130, sint32 cg10230, sint32 cg10031, sint32 cg10131,
sint32 cg10231, sint32 cg11000, sint32 cg11100, sint32 cg11200, sint32 cg11001,
sint32 cg11101, sint32 cg11201, sint32 cg11002, sint32 cg11102, sint32 cg11202,
sint32 cg11003, sint32 cg11103, sint32 cg11203, sint32 cg11004, sint32 cg11104,
sint32 cg11204, sint32 cg11005, sint32 cg11105, sint32 cg11205, sint32 cg11006,
sint32 cg11106, sint32 cg11206, sint32 cg11007, sint32 cg11107, sint32 cg11207,
sint32 cg11008, sint32 cg11108, sint32 cg11208, sint32 cg11009, sint32 cg11109,
sint32 cg11209, sint32 cg11010, sint32 cg11110, sint32 cg11210, sint32 cg11011,
sint32 cg11111, sint32 cg11211, sint32 cg11012, sint32 cg11112, sint32 cg11212,
sint32 cg11013, sint32 cg11113, sint32 cg11213, sint32 cg11014, sint32 cg11114,
sint32 cg11214, sint32 cg11015, sint32 cg11115, sint32 cg11215, sint32 cg11016,
sint32 cg11116, sint32 cg11216, sint32 cg11017, sint32 cg11117, sint32 cg11217,
sint32 cg11018, sint32 cg11118, sint32 cg11218, sint32 cg11019, sint32 cg11119,
sint32 cg11219, sint32 cg11020, sint32 cg11120, sint32 cg11220, sint32 cg11021,
sint32 cg11121, sint32 cg11221, sint32 cg11022, sint32 cg11122, sint32 cg11222,
sint32 cg11023, sint32 cg11123, sint32 cg11223, sint32 cg11024, sint32 cg11124,
sint32 cg11224, sint32 cg11025, sint32 cg11125, sint32 cg11225, sint32 cg11026,
sint32 cg11126, sint32 cg11226, sint32 cg11027, sint32 cg11127, sint32 cg11227,
sint32 cg11028, sint32 cg11128, sint32 cg11228, sint32 cg11029, sint32 cg11129,
sint32 cg11229, sint32 cg11030, sint32 cg11130, sint32 cg11230, sint32 cg11031,
sint32 cg11131, sint32 cg11231, sint32 cg12000, sint32 cg12100, sint32 cg12200,
sint32 cg12001, sint32 cg12101, sint32 cg12201, sint32 cg12002, sint32 cg12102,
sint32 cg12202, sint32 cg12003, sint32 cg12103, sint32 cg12203, sint32 cg12004,
sint32 cg12104, sint32 cg12204, sint32 cg12005, sint32 cg12105, sint32 cg12205,
sint32 cg12006, sint32 cg12106, sint32 cg12206, sint32 cg12007, sint32 cg12107,
sint32 cg12207, sint32 cg12008, sint32 cg12108, sint32 cg12208, sint32 cg12009,
sint32 cg12109, sint32 cg12209, sint32 cg12010, sint32 cg12110, sint32 cg12210,
sint32 cg12011, sint32 cg12111, sint32 cg12211, sint32 cg12012, sint32 cg12112,
sint32 cg12212, sint32 cg12013, sint32 cg12113, sint32 cg12213, sint32 cg12014,
sint32 cg12114, sint32 cg12214, sint32 cg12015, sint32 cg12115, sint32 cg12215,
sint32 cg12016, sint32 cg12116, sint32 cg12216, sint32 cg12017, sint32 cg12117,
sint32 cg12217, sint32 cg12018, sint32 cg12118, sint32 cg12218, sint32 cg12019,
sint32 cg12119, sint32 cg12219, sint32 cg12020, sint32 cg12120, sint32 cg12220,
sint32 cg12021, sint32 cg12121, sint32 cg12221, sint32 cg12022, sint32 cg12122,
sint32 cg12222, sint32 cg12023, sint32 cg12123, sint32 cg12223, sint32 cg12024,
sint32 cg12124, sint32 cg12224, sint32 cg12025, sint32 cg12125, sint32 cg12225,
sint32 cg12026, sint32 cg12126, sint32 cg12226, sint32 cg12027, sint32 cg12127,
sint32 cg12227, sint32 cg12028, sint32 cg12128, sint32 cg12228, sint32 cg12029,
sint32 cg12129, sint32 cg12229, sint32 cg12030, sint32 cg12130, sint32 cg12230,
sint32 cg12031, sint32 cg12131, sint32 cg12231,
sint32 CG20, sint32 CG21, sint32 CG22,
sint32 cg20000, sint32 cg20100, sint32 cg20200, sint32 cg20001, sint32 cg20101,
sint32 cg20201, sint32 cg20002, sint32 cg20102, sint32 cg20202, sint32 cg20003,
sint32 cg20103, sint32 cg20203, sint32 cg20004, sint32 cg20104, sint32 cg20204,
sint32 cg20005, sint32 cg20105, sint32 cg20205, sint32 cg20006, sint32 cg20106,
sint32 cg20206, sint32 cg20007, sint32 cg20107, sint32 cg20207, sint32 cg20008,
sint32 cg20108, sint32 cg20208, sint32 cg20009, sint32 cg20109, sint32 cg20209,
sint32 cg20010, sint32 cg20110, sint32 cg20210, sint32 cg20011, sint32 cg20111,
sint32 cg20211, sint32 cg20012, sint32 cg20112, sint32 cg20212, sint32 cg20013,
sint32 cg20113, sint32 cg20213, sint32 cg20014, sint32 cg20114, sint32 cg20214,
sint32 cg20015, sint32 cg20115, sint32 cg20215, sint32 cg20016, sint32 cg20116,
sint32 cg20216, sint32 cg20017, sint32 cg20117, sint32 cg20217, sint32 cg20018,
sint32 cg20118, sint32 cg20218, sint32 cg20019, sint32 cg20119, sint32 cg20219,
sint32 cg20020, sint32 cg20120, sint32 cg20220, sint32 cg20021, sint32 cg20121,
sint32 cg20221, sint32 cg20022, sint32 cg20122, sint32 cg20222, sint32 cg20023,
sint32 cg20123, sint32 cg20223, sint32 cg20024, sint32 cg20124, sint32 cg20224,
sint32 cg20025, sint32 cg20125, sint32 cg20225, sint32 cg20026, sint32 cg20126,
sint32 cg20226, sint32 cg20027, sint32 cg20127, sint32 cg20227, sint32 cg20028,
sint32 cg20128, sint32 cg20228, sint32 cg20029, sint32 cg20129, sint32 cg20229,
sint32 cg20030, sint32 cg20130, sint32 cg20230, sint32 cg20031, sint32 cg20131,
sint32 cg20231, sint32 cg21000, sint32 cg21100, sint32 cg21200, sint32 cg21001,
sint32 cg21101, sint32 cg21201, sint32 cg21002, sint32 cg21102, sint32 cg21202,
sint32 cg21003, sint32 cg21103, sint32 cg21203, sint32 cg21004, sint32 cg21104,
sint32 cg21204, sint32 cg21005, sint32 cg21105, sint32 cg21205, sint32 cg21006,
sint32 cg21106, sint32 cg21206, sint32 cg21007, sint32 cg21107, sint32 cg21207,
sint32 cg21008, sint32 cg21108, sint32 cg21208, sint32 cg21009, sint32 cg21109,
sint32 cg21209, sint32 cg21010, sint32 cg21110, sint32 cg21210, sint32 cg21011,
sint32 cg21111, sint32 cg21211, sint32 cg21012, sint32 cg21112, sint32 cg21212,
sint32 cg21013, sint32 cg21113, sint32 cg21213, sint32 cg21014, sint32 cg21114,
sint32 cg21214, sint32 cg21015, sint32 cg21115, sint32 cg21215, sint32 cg21016,
sint32 cg21116, sint32 cg21216, sint32 cg21017, sint32 cg21117, sint32 cg21217,
sint32 cg21018, sint32 cg21118, sint32 cg21218, sint32 cg21019, sint32 cg21119,
sint32 cg21219, sint32 cg21020, sint32 cg21120, sint32 cg21220, sint32 cg21021,
sint32 cg21121, sint32 cg21221, sint32 cg21022, sint32 cg21122, sint32 cg21222,
sint32 cg21023, sint32 cg21123, sint32 cg21223, sint32 cg21024, sint32 cg21124,
sint32 cg21224, sint32 cg21025, sint32 cg21125, sint32 cg21225, sint32 cg21026,
sint32 cg21126, sint32 cg21226, sint32 cg21027, sint32 cg21127, sint32 cg21227,
sint32 cg21028, sint32 cg21128, sint32 cg21228, sint32 cg21029, sint32 cg21129,
sint32 cg21229, sint32 cg21030, sint32 cg21130, sint32 cg21230, sint32 cg21031,
sint32 cg21131, sint32 cg21231, sint32 cg22000, sint32 cg22100, sint32 cg22200,
sint32 cg22001, sint32 cg22101, sint32 cg22201, sint32 cg22002, sint32 cg22102,
sint32 cg22202, sint32 cg22003, sint32 cg22103, sint32 cg22203, sint32 cg22004,
sint32 cg22104, sint32 cg22204, sint32 cg22005, sint32 cg22105, sint32 cg22205,
sint32 cg22006, sint32 cg22106, sint32 cg22206, sint32 cg22007, sint32 cg22107,
sint32 cg22207, sint32 cg22008, sint32 cg22108, sint32 cg22208, sint32 cg22009,
sint32 cg22109, sint32 cg22209, sint32 cg22010, sint32 cg22110, sint32 cg22210,
sint32 cg22011, sint32 cg22111, sint32 cg22211, sint32 cg22012, sint32 cg22112,
sint32 cg22212, sint32 cg22013, sint32 cg22113, sint32 cg22213, sint32 cg22014,
sint32 cg22114, sint32 cg22214, sint32 cg22015, sint32 cg22115, sint32 cg22215,
sint32 cg22016, sint32 cg22116, sint32 cg22216, sint32 cg22017, sint32 cg22117,
sint32 cg22217, sint32 cg22018, sint32 cg22118, sint32 cg22218, sint32 cg22019,
sint32 cg22119, sint32 cg22219, sint32 cg22020, sint32 cg22120, sint32 cg22220,
sint32 cg22021, sint32 cg22121, sint32 cg22221, sint32 cg22022, sint32 cg22122,
sint32 cg22222, sint32 cg22023, sint32 cg22123, sint32 cg22223, sint32 cg22024,
sint32 cg22124, sint32 cg22224, sint32 cg22025, sint32 cg22125, sint32 cg22225,
sint32 cg22026, sint32 cg22126, sint32 cg22226, sint32 cg22027, sint32 cg22127,
sint32 cg22227, sint32 cg22028, sint32 cg22128, sint32 cg22228, sint32 cg22029,
sint32 cg22129, sint32 cg22229, sint32 cg22030, sint32 cg22130, sint32 cg22230,
sint32 cg22031, sint32 cg22131, sint32 cg22231,
sint32 CG30, sint32 CG31, sint32 CG32,
sint32 cg30000, sint32 cg30100, sint32 cg30200, sint32 cg30001, sint32 cg30101,
sint32 cg30201, sint32 cg30002, sint32 cg30102, sint32 cg30202, sint32 cg30003,
sint32 cg30103, sint32 cg30203, sint32 cg30004, sint32 cg30104, sint32 cg30204,
sint32 cg30005, sint32 cg30105, sint32 cg30205, sint32 cg30006, sint32 cg30106,
sint32 cg30206, sint32 cg30007, sint32 cg30107, sint32 cg30207, sint32 cg30008,
sint32 cg30108, sint32 cg30208, sint32 cg30009, sint32 cg30109, sint32 cg30209,
sint32 cg30010, sint32 cg30110, sint32 cg30210, sint32 cg30011, sint32 cg30111,
sint32 cg30211, sint32 cg30012, sint32 cg30112, sint32 cg30212, sint32 cg30013,
sint32 cg30113, sint32 cg30213, sint32 cg30014, sint32 cg30114, sint32 cg30214,
sint32 cg30015, sint32 cg30115, sint32 cg30215, sint32 cg30016, sint32 cg30116,
sint32 cg30216, sint32 cg30017, sint32 cg30117, sint32 cg30217, sint32 cg30018,
sint32 cg30118, sint32 cg30218, sint32 cg30019, sint32 cg30119, sint32 cg30219,
sint32 cg30020, sint32 cg30120, sint32 cg30220, sint32 cg30021, sint32 cg30121,
sint32 cg30221, sint32 cg30022, sint32 cg30122, sint32 cg30222, sint32 cg30023,
sint32 cg30123, sint32 cg30223, sint32 cg30024, sint32 cg30124, sint32 cg30224,
sint32 cg30025, sint32 cg30125, sint32 cg30225, sint32 cg30026, sint32 cg30126,
sint32 cg30226, sint32 cg30027, sint32 cg30127, sint32 cg30227, sint32 cg30028,
sint32 cg30128, sint32 cg30228, sint32 cg30029, sint32 cg30129, sint32 cg30229,
sint32 cg30030, sint32 cg30130, sint32 cg30230, sint32 cg30031, sint32 cg30131,
sint32 cg30231, sint32 cg31000, sint32 cg31100, sint32 cg31200, sint32 cg31001,
sint32 cg31101, sint32 cg31201, sint32 cg31002, sint32 cg31102, sint32 cg31202,
sint32 cg31003, sint32 cg31103, sint32 cg31203, sint32 cg31004, sint32 cg31104,
sint32 cg31204, sint32 cg31005, sint32 cg31105, sint32 cg31205, sint32 cg31006,
sint32 cg31106, sint32 cg31206, sint32 cg31007, sint32 cg31107, sint32 cg31207,
sint32 cg31008, sint32 cg31108, sint32 cg31208, sint32 cg31009, sint32 cg31109,
sint32 cg31209, sint32 cg31010, sint32 cg31110, sint32 cg31210, sint32 cg31011,
sint32 cg31111, sint32 cg31211, sint32 cg31012, sint32 cg31112, sint32 cg31212,
sint32 cg31013, sint32 cg31113, sint32 cg31213, sint32 cg31014, sint32 cg31114,
sint32 cg31214, sint32 cg31015, sint32 cg31115, sint32 cg31215, sint32 cg31016,
sint32 cg31116, sint32 cg31216, sint32 cg31017, sint32 cg31117, sint32 cg31217,
sint32 cg31018, sint32 cg31118, sint32 cg31218, sint32 cg31019, sint32 cg31119,
sint32 cg31219, sint32 cg31020, sint32 cg31120, sint32 cg31220, sint32 cg31021,
sint32 cg31121, sint32 cg31221, sint32 cg31022, sint32 cg31122, sint32 cg31222,
sint32 cg31023, sint32 cg31123, sint32 cg31223, sint32 cg31024, sint32 cg31124,
sint32 cg31224, sint32 cg31025, sint32 cg31125, sint32 cg31225, sint32 cg31026,
sint32 cg31126, sint32 cg31226, sint32 cg31027, sint32 cg31127, sint32 cg31227,
sint32 cg31028, sint32 cg31128, sint32 cg31228, sint32 cg31029, sint32 cg31129,
sint32 cg31229, sint32 cg31030, sint32 cg31130, sint32 cg31230, sint32 cg31031,
sint32 cg31131, sint32 cg31231, sint32 cg32000, sint32 cg32100, sint32 cg32200,
sint32 cg32001, sint32 cg32101, sint32 cg32201, sint32 cg32002, sint32 cg32102,
sint32 cg32202, sint32 cg32003, sint32 cg32103, sint32 cg32203, sint32 cg32004,
sint32 cg32104, sint32 cg32204, sint32 cg32005, sint32 cg32105, sint32 cg32205,
sint32 cg32006, sint32 cg32106, sint32 cg32206, sint32 cg32007, sint32 cg32107,
sint32 cg32207, sint32 cg32008, sint32 cg32108, sint32 cg32208, sint32 cg32009,
sint32 cg32109, sint32 cg32209, sint32 cg32010, sint32 cg32110, sint32 cg32210,
sint32 cg32011, sint32 cg32111, sint32 cg32211, sint32 cg32012, sint32 cg32112,
sint32 cg32212, sint32 cg32013, sint32 cg32113, sint32 cg32213, sint32 cg32014,
sint32 cg32114, sint32 cg32214, sint32 cg32015, sint32 cg32115, sint32 cg32215,
sint32 cg32016, sint32 cg32116, sint32 cg32216, sint32 cg32017, sint32 cg32117,
sint32 cg32217, sint32 cg32018, sint32 cg32118, sint32 cg32218, sint32 cg32019,
sint32 cg32119, sint32 cg32219, sint32 cg32020, sint32 cg32120, sint32 cg32220,
sint32 cg32021, sint32 cg32121, sint32 cg32221, sint32 cg32022, sint32 cg32122,
sint32 cg32222, sint32 cg32023, sint32 cg32123, sint32 cg32223, sint32 cg32024,
sint32 cg32124, sint32 cg32224, sint32 cg32025, sint32 cg32125, sint32 cg32225,
sint32 cg32026, sint32 cg32126, sint32 cg32226, sint32 cg32027, sint32 cg32127,
sint32 cg32227, sint32 cg32028, sint32 cg32128, sint32 cg32228, sint32 cg32029,
sint32 cg32129, sint32 cg32229, sint32 cg32030, sint32 cg32130, sint32 cg32230,
sint32 cg32031, sint32 cg32131, sint32 cg32231,
sint32 CG40, sint32 CG41, sint32 CG42,
sint32 cg40000, sint32 cg40100, sint32 cg40200, sint32 cg40001, sint32 cg40101,
sint32 cg40201, sint32 cg40002, sint32 cg40102, sint32 cg40202, sint32 cg40003,
sint32 cg40103, sint32 cg40203, sint32 cg40004, sint32 cg40104, sint32 cg40204,
sint32 cg40005, sint32 cg40105, sint32 cg40205, sint32 cg40006, sint32 cg40106,
sint32 cg40206, sint32 cg40007, sint32 cg40107, sint32 cg40207, sint32 cg40008,
sint32 cg40108, sint32 cg40208, sint32 cg40009, sint32 cg40109, sint32 cg40209,
sint32 cg40010, sint32 cg40110, sint32 cg40210, sint32 cg40011, sint32 cg40111,
sint32 cg40211, sint32 cg40012, sint32 cg40112, sint32 cg40212, sint32 cg40013,
sint32 cg40113, sint32 cg40213, sint32 cg40014, sint32 cg40114, sint32 cg40214,
sint32 cg40015, sint32 cg40115, sint32 cg40215, sint32 cg40016, sint32 cg40116,
sint32 cg40216, sint32 cg40017, sint32 cg40117, sint32 cg40217, sint32 cg40018,
sint32 cg40118, sint32 cg40218, sint32 cg40019, sint32 cg40119, sint32 cg40219,
sint32 cg40020, sint32 cg40120, sint32 cg40220, sint32 cg40021, sint32 cg40121,
sint32 cg40221, sint32 cg40022, sint32 cg40122, sint32 cg40222, sint32 cg40023,
sint32 cg40123, sint32 cg40223, sint32 cg40024, sint32 cg40124, sint32 cg40224,
sint32 cg40025, sint32 cg40125, sint32 cg40225, sint32 cg40026, sint32 cg40126,
sint32 cg40226, sint32 cg40027, sint32 cg40127, sint32 cg40227, sint32 cg40028,
sint32 cg40128, sint32 cg40228, sint32 cg40029, sint32 cg40129, sint32 cg40229,
sint32 cg40030, sint32 cg40130, sint32 cg40230, sint32 cg40031, sint32 cg40131,
sint32 cg40231, sint32 cg41000, sint32 cg41100, sint32 cg41200, sint32 cg41001,
sint32 cg41101, sint32 cg41201, sint32 cg41002, sint32 cg41102, sint32 cg41202,
sint32 cg41003, sint32 cg41103, sint32 cg41203, sint32 cg41004, sint32 cg41104,
sint32 cg41204, sint32 cg41005, sint32 cg41105, sint32 cg41205, sint32 cg41006,
sint32 cg41106, sint32 cg41206, sint32 cg41007, sint32 cg41107, sint32 cg41207,
sint32 cg41008, sint32 cg41108, sint32 cg41208, sint32 cg41009, sint32 cg41109,
sint32 cg41209, sint32 cg41010, sint32 cg41110, sint32 cg41210, sint32 cg41011,
sint32 cg41111, sint32 cg41211, sint32 cg41012, sint32 cg41112, sint32 cg41212,
sint32 cg41013, sint32 cg41113, sint32 cg41213, sint32 cg41014, sint32 cg41114,
sint32 cg41214, sint32 cg41015, sint32 cg41115, sint32 cg41215, sint32 cg41016,
sint32 cg41116, sint32 cg41216, sint32 cg41017, sint32 cg41117, sint32 cg41217,
sint32 cg41018, sint32 cg41118, sint32 cg41218, sint32 cg41019, sint32 cg41119,
sint32 cg41219, sint32 cg41020, sint32 cg41120, sint32 cg41220, sint32 cg41021,
sint32 cg41121, sint32 cg41221, sint32 cg41022, sint32 cg41122, sint32 cg41222,
sint32 cg41023, sint32 cg41123, sint32 cg41223, sint32 cg41024, sint32 cg41124,
sint32 cg41224, sint32 cg41025, sint32 cg41125, sint32 cg41225, sint32 cg41026,
sint32 cg41126, sint32 cg41226, sint32 cg41027, sint32 cg41127, sint32 cg41227,
sint32 cg41028, sint32 cg41128, sint32 cg41228, sint32 cg41029, sint32 cg41129,
sint32 cg41229, sint32 cg41030, sint32 cg41130, sint32 cg41230, sint32 cg41031,
sint32 cg41131, sint32 cg41231, sint32 cg42000, sint32 cg42100, sint32 cg42200,
sint32 cg42001, sint32 cg42101, sint32 cg42201, sint32 cg42002, sint32 cg42102,
sint32 cg42202, sint32 cg42003, sint32 cg42103, sint32 cg42203, sint32 cg42004,
sint32 cg42104, sint32 cg42204, sint32 cg42005, sint32 cg42105, sint32 cg42205,
sint32 cg42006, sint32 cg42106, sint32 cg42206, sint32 cg42007, sint32 cg42107,
sint32 cg42207, sint32 cg42008, sint32 cg42108, sint32 cg42208, sint32 cg42009,
sint32 cg42109, sint32 cg42209, sint32 cg42010, sint32 cg42110, sint32 cg42210,
sint32 cg42011, sint32 cg42111, sint32 cg42211, sint32 cg42012, sint32 cg42112,
sint32 cg42212, sint32 cg42013, sint32 cg42113, sint32 cg42213, sint32 cg42014,
sint32 cg42114, sint32 cg42214, sint32 cg42015, sint32 cg42115, sint32 cg42215,
sint32 cg42016, sint32 cg42116, sint32 cg42216, sint32 cg42017, sint32 cg42117,
sint32 cg42217, sint32 cg42018, sint32 cg42118, sint32 cg42218, sint32 cg42019,
sint32 cg42119, sint32 cg42219, sint32 cg42020, sint32 cg42120, sint32 cg42220,
sint32 cg42021, sint32 cg42121, sint32 cg42221, sint32 cg42022, sint32 cg42122,
sint32 cg42222, sint32 cg42023, sint32 cg42123, sint32 cg42223, sint32 cg42024,
sint32 cg42124, sint32 cg42224, sint32 cg42025, sint32 cg42125, sint32 cg42225,
sint32 cg42026, sint32 cg42126, sint32 cg42226, sint32 cg42027, sint32 cg42127,
sint32 cg42227, sint32 cg42028, sint32 cg42128, sint32 cg42228, sint32 cg42029,
sint32 cg42129, sint32 cg42229, sint32 cg42030, sint32 cg42130, sint32 cg42230,
sint32 cg42031, sint32 cg42131, sint32 cg42231,
sint32 CG50, sint32 CG51, sint32 CG52,
sint32 cg50000, sint32 cg50100, sint32 cg50200, sint32 cg50001, sint32 cg50101,
sint32 cg50201, sint32 cg50002, sint32 cg50102, sint32 cg50202, sint32 cg50003,
sint32 cg50103, sint32 cg50203, sint32 cg50004, sint32 cg50104, sint32 cg50204,
sint32 cg50005, sint32 cg50105, sint32 cg50205, sint32 cg50006, sint32 cg50106,
sint32 cg50206, sint32 cg50007, sint32 cg50107, sint32 cg50207, sint32 cg50008,
sint32 cg50108, sint32 cg50208, sint32 cg50009, sint32 cg50109, sint32 cg50209,
sint32 cg50010, sint32 cg50110, sint32 cg50210, sint32 cg50011, sint32 cg50111,
sint32 cg50211, sint32 cg50012, sint32 cg50112, sint32 cg50212, sint32 cg50013,
sint32 cg50113, sint32 cg50213, sint32 cg50014, sint32 cg50114, sint32 cg50214,
sint32 cg50015, sint32 cg50115, sint32 cg50215, sint32 cg50016, sint32 cg50116,
sint32 cg50216, sint32 cg50017, sint32 cg50117, sint32 cg50217, sint32 cg50018,
sint32 cg50118, sint32 cg50218, sint32 cg50019, sint32 cg50119, sint32 cg50219,
sint32 cg50020, sint32 cg50120, sint32 cg50220, sint32 cg50021, sint32 cg50121,
sint32 cg50221, sint32 cg50022, sint32 cg50122, sint32 cg50222, sint32 cg50023,
sint32 cg50123, sint32 cg50223, sint32 cg50024, sint32 cg50124, sint32 cg50224,
sint32 cg50025, sint32 cg50125, sint32 cg50225, sint32 cg50026, sint32 cg50126,
sint32 cg50226, sint32 cg50027, sint32 cg50127, sint32 cg50227, sint32 cg50028,
sint32 cg50128, sint32 cg50228, sint32 cg50029, sint32 cg50129, sint32 cg50229,
sint32 cg50030, sint32 cg50130, sint32 cg50230, sint32 cg50031, sint32 cg50131,
sint32 cg50231, sint32 cg51000, sint32 cg51100, sint32 cg51200, sint32 cg51001,
sint32 cg51101, sint32 cg51201, sint32 cg51002, sint32 cg51102, sint32 cg51202,
sint32 cg51003, sint32 cg51103, sint32 cg51203, sint32 cg51004, sint32 cg51104,
sint32 cg51204, sint32 cg51005, sint32 cg51105, sint32 cg51205, sint32 cg51006,
sint32 cg51106, sint32 cg51206, sint32 cg51007, sint32 cg51107, sint32 cg51207,
sint32 cg51008, sint32 cg51108, sint32 cg51208, sint32 cg51009, sint32 cg51109,
sint32 cg51209, sint32 cg51010, sint32 cg51110, sint32 cg51210, sint32 cg51011,
sint32 cg51111, sint32 cg51211, sint32 cg51012, sint32 cg51112, sint32 cg51212,
sint32 cg51013, sint32 cg51113, sint32 cg51213, sint32 cg51014, sint32 cg51114,
sint32 cg51214, sint32 cg51015, sint32 cg51115, sint32 cg51215, sint32 cg51016,
sint32 cg51116, sint32 cg51216, sint32 cg51017, sint32 cg51117, sint32 cg51217,
sint32 cg51018, sint32 cg51118, sint32 cg51218, sint32 cg51019, sint32 cg51119,
sint32 cg51219, sint32 cg51020, sint32 cg51120, sint32 cg51220, sint32 cg51021,
sint32 cg51121, sint32 cg51221, sint32 cg51022, sint32 cg51122, sint32 cg51222,
sint32 cg51023, sint32 cg51123, sint32 cg51223, sint32 cg51024, sint32 cg51124,
sint32 cg51224, sint32 cg51025, sint32 cg51125, sint32 cg51225, sint32 cg51026,
sint32 cg51126, sint32 cg51226, sint32 cg51027, sint32 cg51127, sint32 cg51227,
sint32 cg51028, sint32 cg51128, sint32 cg51228, sint32 cg51029, sint32 cg51129,
sint32 cg51229, sint32 cg51030, sint32 cg51130, sint32 cg51230, sint32 cg51031,
sint32 cg51131, sint32 cg51231, sint32 cg52000, sint32 cg52100, sint32 cg52200,
sint32 cg52001, sint32 cg52101, sint32 cg52201, sint32 cg52002, sint32 cg52102,
sint32 cg52202, sint32 cg52003, sint32 cg52103, sint32 cg52203, sint32 cg52004,
sint32 cg52104, sint32 cg52204, sint32 cg52005, sint32 cg52105, sint32 cg52205,
sint32 cg52006, sint32 cg52106, sint32 cg52206, sint32 cg52007, sint32 cg52107,
sint32 cg52207, sint32 cg52008, sint32 cg52108, sint32 cg52208, sint32 cg52009,
sint32 cg52109, sint32 cg52209, sint32 cg52010, sint32 cg52110, sint32 cg52210,
sint32 cg52011, sint32 cg52111, sint32 cg52211, sint32 cg52012, sint32 cg52112,
sint32 cg52212, sint32 cg52013, sint32 cg52113, sint32 cg52213, sint32 cg52014,
sint32 cg52114, sint32 cg52214, sint32 cg52015, sint32 cg52115, sint32 cg52215,
sint32 cg52016, sint32 cg52116, sint32 cg52216, sint32 cg52017, sint32 cg52117,
sint32 cg52217, sint32 cg52018, sint32 cg52118, sint32 cg52218, sint32 cg52019,
sint32 cg52119, sint32 cg52219, sint32 cg52020, sint32 cg52120, sint32 cg52220,
sint32 cg52021, sint32 cg52121, sint32 cg52221, sint32 cg52022, sint32 cg52122,
sint32 cg52222, sint32 cg52023, sint32 cg52123, sint32 cg52223, sint32 cg52024,
sint32 cg52124, sint32 cg52224, sint32 cg52025, sint32 cg52125, sint32 cg52225,
sint32 cg52026, sint32 cg52126, sint32 cg52226, sint32 cg52027, sint32 cg52127,
sint32 cg52227, sint32 cg52028, sint32 cg52128, sint32 cg52228, sint32 cg52029,
sint32 cg52129, sint32 cg52229, sint32 cg52030, sint32 cg52130, sint32 cg52230,
sint32 cg52031, sint32 cg52131, sint32 cg52231
) =
{
(******************** precondition ********************)

and [
eqmod CG00**2+CG01**2+CG02**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -       1, z**32 -       1 ],
CG00**2 =
cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16+
cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17+
cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18+
cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19+
cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20+
cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21+
cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22+
cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23+
cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24+
cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25+
cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26+
cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27+
cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28+
cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29+
cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30+
cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31,
CG01**2 =
cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16+
cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17+
cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18+
cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19+
cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20+
cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21+
cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22+
cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23+
cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24+
cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25+
cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26+
cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27+
cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28+
cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29+
cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30+
cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31,
CG02**2 =
cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16+
cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17+
cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18+
cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19+
cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20+
cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21+
cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22+
cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23+
cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24+
cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25+
cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26+
cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27+
cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28+
cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29+
cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30+
cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31
,
eqmod CG10**2+CG11**2+CG12**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -  452650, z**32 -       1 ],
CG10**2 =
cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16+
cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17+
cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18+
cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19+
cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20+
cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21+
cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22+
cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23+
cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24+
cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25+
cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26+
cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27+
cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28+
cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29+
cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30+
cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31,
CG11**2 =
cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16+
cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17+
cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18+
cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19+
cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20+
cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21+
cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22+
cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23+
cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24+
cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25+
cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26+
cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27+
cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28+
cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29+
cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30+
cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31,
CG12**2 =
cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16+
cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17+
cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18+
cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19+
cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20+
cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21+
cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22+
cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23+
cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24+
cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25+
cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26+
cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27+
cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28+
cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29+
cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30+
cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31
,
eqmod CG20**2+CG21**2+CG22**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 - 2912918, z**32 -       1 ],
CG20**2 =
cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16+
cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17+
cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18+
cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19+
cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20+
cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21+
cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22+
cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23+
cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24+
cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25+
cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26+
cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27+
cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28+
cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29+
cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30+
cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31,
CG21**2 =
cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16+
cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17+
cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18+
cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19+
cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20+
cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21+
cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22+
cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23+
cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24+
cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25+
cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26+
cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27+
cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28+
cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29+
cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30+
cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31,
CG22**2 =
cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16+
cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17+
cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18+
cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19+
cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20+
cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21+
cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22+
cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23+
cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24+
cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25+
cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26+
cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27+
cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28+
cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29+
cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30+
cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31
,
eqmod CG30**2+CG31**2+CG32**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -       1, z**32 - 3365568 ],
CG30**2 =
cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16+
cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17+
cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18+
cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19+
cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20+
cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21+
cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22+
cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23+
cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24+
cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25+
cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26+
cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27+
cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28+
cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29+
cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30+
cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31,
CG31**2 =
cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16+
cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17+
cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18+
cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19+
cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20+
cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21+
cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22+
cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23+
cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24+
cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25+
cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26+
cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27+
cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28+
cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29+
cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30+
cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31,
CG32**2 =
cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16+
cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17+
cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18+
cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19+
cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20+
cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21+
cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22+
cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23+
cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24+
cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25+
cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26+
cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27+
cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28+
cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29+
cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30+
cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31
,
eqmod CG40**2+CG41**2+CG42**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -  452650, z**32 - 3365568 ],
CG40**2 =
cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16+
cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17+
cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18+
cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19+
cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20+
cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21+
cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22+
cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23+
cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24+
cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25+
cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26+
cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27+
cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28+
cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29+
cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30+
cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31,
CG41**2 =
cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16+
cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17+
cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18+
cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19+
cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20+
cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21+
cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22+
cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23+
cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24+
cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25+
cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26+
cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27+
cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28+
cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29+
cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30+
cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31,
CG42**2 =
cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16+
cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17+
cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18+
cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19+
cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20+
cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21+
cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22+
cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23+
cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24+
cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25+
cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26+
cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27+
cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28+
cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29+
cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30+
cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31
,
eqmod CG50**2+CG51**2+CG52**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
CG50**2 =
cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16+
cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17+
cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18+
cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19+
cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20+
cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21+
cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22+
cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23+
cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24+
cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25+
cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26+
cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27+
cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28+
cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29+
cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30+
cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31,
CG51**2 =
cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16+
cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17+
cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18+
cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19+
cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20+
cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21+
cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22+
cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23+
cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24+
cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25+
cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26+
cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27+
cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28+
cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29+
cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30+
cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31,
CG52**2 =
cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16+
cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17+
cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18+
cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19+
cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20+
cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21+
cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22+
cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23+
cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24+
cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25+
cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26+
cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27+
cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28+
cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29+
cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30+
cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31
] && and [
(-3367617)@32 <=s cg00000, cg00000 <=s 3367617@32,
(-3367617)@32 <=s cg00100, cg00100 <=s 3367617@32,
(-3367617)@32 <=s cg00200, cg00200 <=s 3367617@32,
(-3367617)@32 <=s cg00001, cg00001 <=s 3367617@32,
(-3367617)@32 <=s cg00101, cg00101 <=s 3367617@32,
(-3367617)@32 <=s cg00201, cg00201 <=s 3367617@32,
(-3367617)@32 <=s cg00002, cg00002 <=s 3367617@32,
(-3367617)@32 <=s cg00102, cg00102 <=s 3367617@32,
(-3367617)@32 <=s cg00202, cg00202 <=s 3367617@32,
(-3367617)@32 <=s cg00003, cg00003 <=s 3367617@32,
(-3367617)@32 <=s cg00103, cg00103 <=s 3367617@32,
(-3367617)@32 <=s cg00203, cg00203 <=s 3367617@32,
(-3367617)@32 <=s cg00004, cg00004 <=s 3367617@32,
(-3367617)@32 <=s cg00104, cg00104 <=s 3367617@32,
(-3367617)@32 <=s cg00204, cg00204 <=s 3367617@32,
(-3367617)@32 <=s cg00005, cg00005 <=s 3367617@32,
(-3367617)@32 <=s cg00105, cg00105 <=s 3367617@32,
(-3367617)@32 <=s cg00205, cg00205 <=s 3367617@32,
(-3367617)@32 <=s cg00006, cg00006 <=s 3367617@32,
(-3367617)@32 <=s cg00106, cg00106 <=s 3367617@32,
(-3367617)@32 <=s cg00206, cg00206 <=s 3367617@32,
(-3367617)@32 <=s cg00007, cg00007 <=s 3367617@32,
(-3367617)@32 <=s cg00107, cg00107 <=s 3367617@32,
(-3367617)@32 <=s cg00207, cg00207 <=s 3367617@32,
(-3367617)@32 <=s cg00008, cg00008 <=s 3367617@32,
(-3367617)@32 <=s cg00108, cg00108 <=s 3367617@32,
(-3367617)@32 <=s cg00208, cg00208 <=s 3367617@32,
(-3367617)@32 <=s cg00009, cg00009 <=s 3367617@32,
(-3367617)@32 <=s cg00109, cg00109 <=s 3367617@32,
(-3367617)@32 <=s cg00209, cg00209 <=s 3367617@32,
(-3367617)@32 <=s cg00010, cg00010 <=s 3367617@32,
(-3367617)@32 <=s cg00110, cg00110 <=s 3367617@32,
(-3367617)@32 <=s cg00210, cg00210 <=s 3367617@32,
(-3367617)@32 <=s cg00011, cg00011 <=s 3367617@32,
(-3367617)@32 <=s cg00111, cg00111 <=s 3367617@32,
(-3367617)@32 <=s cg00211, cg00211 <=s 3367617@32,
(-3367617)@32 <=s cg00012, cg00012 <=s 3367617@32,
(-3367617)@32 <=s cg00112, cg00112 <=s 3367617@32,
(-3367617)@32 <=s cg00212, cg00212 <=s 3367617@32,
(-3367617)@32 <=s cg00013, cg00013 <=s 3367617@32,
(-3367617)@32 <=s cg00113, cg00113 <=s 3367617@32,
(-3367617)@32 <=s cg00213, cg00213 <=s 3367617@32,
(-3367617)@32 <=s cg00014, cg00014 <=s 3367617@32,
(-3367617)@32 <=s cg00114, cg00114 <=s 3367617@32,
(-3367617)@32 <=s cg00214, cg00214 <=s 3367617@32,
(-3367617)@32 <=s cg00015, cg00015 <=s 3367617@32,
(-3367617)@32 <=s cg00115, cg00115 <=s 3367617@32,
(-3367617)@32 <=s cg00215, cg00215 <=s 3367617@32,
(-3367617)@32 <=s cg00016, cg00016 <=s 3367617@32,
(-3367617)@32 <=s cg00116, cg00116 <=s 3367617@32,
(-3367617)@32 <=s cg00216, cg00216 <=s 3367617@32,
(-3367617)@32 <=s cg00017, cg00017 <=s 3367617@32,
(-3367617)@32 <=s cg00117, cg00117 <=s 3367617@32,
(-3367617)@32 <=s cg00217, cg00217 <=s 3367617@32,
(-3367617)@32 <=s cg00018, cg00018 <=s 3367617@32,
(-3367617)@32 <=s cg00118, cg00118 <=s 3367617@32,
(-3367617)@32 <=s cg00218, cg00218 <=s 3367617@32,
(-3367617)@32 <=s cg00019, cg00019 <=s 3367617@32,
(-3367617)@32 <=s cg00119, cg00119 <=s 3367617@32,
(-3367617)@32 <=s cg00219, cg00219 <=s 3367617@32,
(-3367617)@32 <=s cg00020, cg00020 <=s 3367617@32,
(-3367617)@32 <=s cg00120, cg00120 <=s 3367617@32,
(-3367617)@32 <=s cg00220, cg00220 <=s 3367617@32,
(-3367617)@32 <=s cg00021, cg00021 <=s 3367617@32,
(-3367617)@32 <=s cg00121, cg00121 <=s 3367617@32,
(-3367617)@32 <=s cg00221, cg00221 <=s 3367617@32,
(-3367617)@32 <=s cg00022, cg00022 <=s 3367617@32,
(-3367617)@32 <=s cg00122, cg00122 <=s 3367617@32,
(-3367617)@32 <=s cg00222, cg00222 <=s 3367617@32,
(-3367617)@32 <=s cg00023, cg00023 <=s 3367617@32,
(-3367617)@32 <=s cg00123, cg00123 <=s 3367617@32,
(-3367617)@32 <=s cg00223, cg00223 <=s 3367617@32,
(-3367617)@32 <=s cg00024, cg00024 <=s 3367617@32,
(-3367617)@32 <=s cg00124, cg00124 <=s 3367617@32,
(-3367617)@32 <=s cg00224, cg00224 <=s 3367617@32,
(-3367617)@32 <=s cg00025, cg00025 <=s 3367617@32,
(-3367617)@32 <=s cg00125, cg00125 <=s 3367617@32,
(-3367617)@32 <=s cg00225, cg00225 <=s 3367617@32,
(-3367617)@32 <=s cg00026, cg00026 <=s 3367617@32,
(-3367617)@32 <=s cg00126, cg00126 <=s 3367617@32,
(-3367617)@32 <=s cg00226, cg00226 <=s 3367617@32,
(-3367617)@32 <=s cg00027, cg00027 <=s 3367617@32,
(-3367617)@32 <=s cg00127, cg00127 <=s 3367617@32,
(-3367617)@32 <=s cg00227, cg00227 <=s 3367617@32,
(-3367617)@32 <=s cg00028, cg00028 <=s 3367617@32,
(-3367617)@32 <=s cg00128, cg00128 <=s 3367617@32,
(-3367617)@32 <=s cg00228, cg00228 <=s 3367617@32,
(-3367617)@32 <=s cg00029, cg00029 <=s 3367617@32,
(-3367617)@32 <=s cg00129, cg00129 <=s 3367617@32,
(-3367617)@32 <=s cg00229, cg00229 <=s 3367617@32,
(-3367617)@32 <=s cg00030, cg00030 <=s 3367617@32,
(-3367617)@32 <=s cg00130, cg00130 <=s 3367617@32,
(-3367617)@32 <=s cg00230, cg00230 <=s 3367617@32,
(-3367617)@32 <=s cg00031, cg00031 <=s 3367617@32,
(-3367617)@32 <=s cg00131, cg00131 <=s 3367617@32,
(-3367617)@32 <=s cg00231, cg00231 <=s 3367617@32,
(-3367617)@32 <=s cg01000, cg01000 <=s 3367617@32,
(-3367617)@32 <=s cg01100, cg01100 <=s 3367617@32,
(-3367617)@32 <=s cg01200, cg01200 <=s 3367617@32,
(-3367617)@32 <=s cg01001, cg01001 <=s 3367617@32,
(-3367617)@32 <=s cg01101, cg01101 <=s 3367617@32,
(-3367617)@32 <=s cg01201, cg01201 <=s 3367617@32,
(-3367617)@32 <=s cg01002, cg01002 <=s 3367617@32,
(-3367617)@32 <=s cg01102, cg01102 <=s 3367617@32,
(-3367617)@32 <=s cg01202, cg01202 <=s 3367617@32,
(-3367617)@32 <=s cg01003, cg01003 <=s 3367617@32,
(-3367617)@32 <=s cg01103, cg01103 <=s 3367617@32,
(-3367617)@32 <=s cg01203, cg01203 <=s 3367617@32,
(-3367617)@32 <=s cg01004, cg01004 <=s 3367617@32,
(-3367617)@32 <=s cg01104, cg01104 <=s 3367617@32,
(-3367617)@32 <=s cg01204, cg01204 <=s 3367617@32,
(-3367617)@32 <=s cg01005, cg01005 <=s 3367617@32,
(-3367617)@32 <=s cg01105, cg01105 <=s 3367617@32,
(-3367617)@32 <=s cg01205, cg01205 <=s 3367617@32,
(-3367617)@32 <=s cg01006, cg01006 <=s 3367617@32,
(-3367617)@32 <=s cg01106, cg01106 <=s 3367617@32,
(-3367617)@32 <=s cg01206, cg01206 <=s 3367617@32,
(-3367617)@32 <=s cg01007, cg01007 <=s 3367617@32,
(-3367617)@32 <=s cg01107, cg01107 <=s 3367617@32,
(-3367617)@32 <=s cg01207, cg01207 <=s 3367617@32,
(-3367617)@32 <=s cg01008, cg01008 <=s 3367617@32,
(-3367617)@32 <=s cg01108, cg01108 <=s 3367617@32,
(-3367617)@32 <=s cg01208, cg01208 <=s 3367617@32,
(-3367617)@32 <=s cg01009, cg01009 <=s 3367617@32,
(-3367617)@32 <=s cg01109, cg01109 <=s 3367617@32,
(-3367617)@32 <=s cg01209, cg01209 <=s 3367617@32,
(-3367617)@32 <=s cg01010, cg01010 <=s 3367617@32,
(-3367617)@32 <=s cg01110, cg01110 <=s 3367617@32,
(-3367617)@32 <=s cg01210, cg01210 <=s 3367617@32,
(-3367617)@32 <=s cg01011, cg01011 <=s 3367617@32,
(-3367617)@32 <=s cg01111, cg01111 <=s 3367617@32,
(-3367617)@32 <=s cg01211, cg01211 <=s 3367617@32,
(-3367617)@32 <=s cg01012, cg01012 <=s 3367617@32,
(-3367617)@32 <=s cg01112, cg01112 <=s 3367617@32,
(-3367617)@32 <=s cg01212, cg01212 <=s 3367617@32,
(-3367617)@32 <=s cg01013, cg01013 <=s 3367617@32,
(-3367617)@32 <=s cg01113, cg01113 <=s 3367617@32,
(-3367617)@32 <=s cg01213, cg01213 <=s 3367617@32,
(-3367617)@32 <=s cg01014, cg01014 <=s 3367617@32,
(-3367617)@32 <=s cg01114, cg01114 <=s 3367617@32,
(-3367617)@32 <=s cg01214, cg01214 <=s 3367617@32,
(-3367617)@32 <=s cg01015, cg01015 <=s 3367617@32,
(-3367617)@32 <=s cg01115, cg01115 <=s 3367617@32,
(-3367617)@32 <=s cg01215, cg01215 <=s 3367617@32,
(-3367617)@32 <=s cg01016, cg01016 <=s 3367617@32,
(-3367617)@32 <=s cg01116, cg01116 <=s 3367617@32,
(-3367617)@32 <=s cg01216, cg01216 <=s 3367617@32,
(-3367617)@32 <=s cg01017, cg01017 <=s 3367617@32,
(-3367617)@32 <=s cg01117, cg01117 <=s 3367617@32,
(-3367617)@32 <=s cg01217, cg01217 <=s 3367617@32,
(-3367617)@32 <=s cg01018, cg01018 <=s 3367617@32,
(-3367617)@32 <=s cg01118, cg01118 <=s 3367617@32,
(-3367617)@32 <=s cg01218, cg01218 <=s 3367617@32,
(-3367617)@32 <=s cg01019, cg01019 <=s 3367617@32,
(-3367617)@32 <=s cg01119, cg01119 <=s 3367617@32,
(-3367617)@32 <=s cg01219, cg01219 <=s 3367617@32,
(-3367617)@32 <=s cg01020, cg01020 <=s 3367617@32,
(-3367617)@32 <=s cg01120, cg01120 <=s 3367617@32,
(-3367617)@32 <=s cg01220, cg01220 <=s 3367617@32,
(-3367617)@32 <=s cg01021, cg01021 <=s 3367617@32,
(-3367617)@32 <=s cg01121, cg01121 <=s 3367617@32,
(-3367617)@32 <=s cg01221, cg01221 <=s 3367617@32,
(-3367617)@32 <=s cg01022, cg01022 <=s 3367617@32,
(-3367617)@32 <=s cg01122, cg01122 <=s 3367617@32,
(-3367617)@32 <=s cg01222, cg01222 <=s 3367617@32,
(-3367617)@32 <=s cg01023, cg01023 <=s 3367617@32,
(-3367617)@32 <=s cg01123, cg01123 <=s 3367617@32,
(-3367617)@32 <=s cg01223, cg01223 <=s 3367617@32,
(-3367617)@32 <=s cg01024, cg01024 <=s 3367617@32,
(-3367617)@32 <=s cg01124, cg01124 <=s 3367617@32,
(-3367617)@32 <=s cg01224, cg01224 <=s 3367617@32,
(-3367617)@32 <=s cg01025, cg01025 <=s 3367617@32,
(-3367617)@32 <=s cg01125, cg01125 <=s 3367617@32,
(-3367617)@32 <=s cg01225, cg01225 <=s 3367617@32,
(-3367617)@32 <=s cg01026, cg01026 <=s 3367617@32,
(-3367617)@32 <=s cg01126, cg01126 <=s 3367617@32,
(-3367617)@32 <=s cg01226, cg01226 <=s 3367617@32,
(-3367617)@32 <=s cg01027, cg01027 <=s 3367617@32,
(-3367617)@32 <=s cg01127, cg01127 <=s 3367617@32,
(-3367617)@32 <=s cg01227, cg01227 <=s 3367617@32,
(-3367617)@32 <=s cg01028, cg01028 <=s 3367617@32,
(-3367617)@32 <=s cg01128, cg01128 <=s 3367617@32,
(-3367617)@32 <=s cg01228, cg01228 <=s 3367617@32,
(-3367617)@32 <=s cg01029, cg01029 <=s 3367617@32,
(-3367617)@32 <=s cg01129, cg01129 <=s 3367617@32,
(-3367617)@32 <=s cg01229, cg01229 <=s 3367617@32,
(-3367617)@32 <=s cg01030, cg01030 <=s 3367617@32,
(-3367617)@32 <=s cg01130, cg01130 <=s 3367617@32,
(-3367617)@32 <=s cg01230, cg01230 <=s 3367617@32,
(-3367617)@32 <=s cg01031, cg01031 <=s 3367617@32,
(-3367617)@32 <=s cg01131, cg01131 <=s 3367617@32,
(-3367617)@32 <=s cg01231, cg01231 <=s 3367617@32,
(-3367617)@32 <=s cg02000, cg02000 <=s 3367617@32,
(-3367617)@32 <=s cg02100, cg02100 <=s 3367617@32,
(-3367617)@32 <=s cg02200, cg02200 <=s 3367617@32,
(-3367617)@32 <=s cg02001, cg02001 <=s 3367617@32,
(-3367617)@32 <=s cg02101, cg02101 <=s 3367617@32,
(-3367617)@32 <=s cg02201, cg02201 <=s 3367617@32,
(-3367617)@32 <=s cg02002, cg02002 <=s 3367617@32,
(-3367617)@32 <=s cg02102, cg02102 <=s 3367617@32,
(-3367617)@32 <=s cg02202, cg02202 <=s 3367617@32,
(-3367617)@32 <=s cg02003, cg02003 <=s 3367617@32,
(-3367617)@32 <=s cg02103, cg02103 <=s 3367617@32,
(-3367617)@32 <=s cg02203, cg02203 <=s 3367617@32,
(-3367617)@32 <=s cg02004, cg02004 <=s 3367617@32,
(-3367617)@32 <=s cg02104, cg02104 <=s 3367617@32,
(-3367617)@32 <=s cg02204, cg02204 <=s 3367617@32,
(-3367617)@32 <=s cg02005, cg02005 <=s 3367617@32,
(-3367617)@32 <=s cg02105, cg02105 <=s 3367617@32,
(-3367617)@32 <=s cg02205, cg02205 <=s 3367617@32,
(-3367617)@32 <=s cg02006, cg02006 <=s 3367617@32,
(-3367617)@32 <=s cg02106, cg02106 <=s 3367617@32,
(-3367617)@32 <=s cg02206, cg02206 <=s 3367617@32,
(-3367617)@32 <=s cg02007, cg02007 <=s 3367617@32,
(-3367617)@32 <=s cg02107, cg02107 <=s 3367617@32,
(-3367617)@32 <=s cg02207, cg02207 <=s 3367617@32,
(-3367617)@32 <=s cg02008, cg02008 <=s 3367617@32,
(-3367617)@32 <=s cg02108, cg02108 <=s 3367617@32,
(-3367617)@32 <=s cg02208, cg02208 <=s 3367617@32,
(-3367617)@32 <=s cg02009, cg02009 <=s 3367617@32,
(-3367617)@32 <=s cg02109, cg02109 <=s 3367617@32,
(-3367617)@32 <=s cg02209, cg02209 <=s 3367617@32,
(-3367617)@32 <=s cg02010, cg02010 <=s 3367617@32,
(-3367617)@32 <=s cg02110, cg02110 <=s 3367617@32,
(-3367617)@32 <=s cg02210, cg02210 <=s 3367617@32,
(-3367617)@32 <=s cg02011, cg02011 <=s 3367617@32,
(-3367617)@32 <=s cg02111, cg02111 <=s 3367617@32,
(-3367617)@32 <=s cg02211, cg02211 <=s 3367617@32,
(-3367617)@32 <=s cg02012, cg02012 <=s 3367617@32,
(-3367617)@32 <=s cg02112, cg02112 <=s 3367617@32,
(-3367617)@32 <=s cg02212, cg02212 <=s 3367617@32,
(-3367617)@32 <=s cg02013, cg02013 <=s 3367617@32,
(-3367617)@32 <=s cg02113, cg02113 <=s 3367617@32,
(-3367617)@32 <=s cg02213, cg02213 <=s 3367617@32,
(-3367617)@32 <=s cg02014, cg02014 <=s 3367617@32,
(-3367617)@32 <=s cg02114, cg02114 <=s 3367617@32,
(-3367617)@32 <=s cg02214, cg02214 <=s 3367617@32,
(-3367617)@32 <=s cg02015, cg02015 <=s 3367617@32,
(-3367617)@32 <=s cg02115, cg02115 <=s 3367617@32,
(-3367617)@32 <=s cg02215, cg02215 <=s 3367617@32,
(-3367617)@32 <=s cg02016, cg02016 <=s 3367617@32,
(-3367617)@32 <=s cg02116, cg02116 <=s 3367617@32,
(-3367617)@32 <=s cg02216, cg02216 <=s 3367617@32,
(-3367617)@32 <=s cg02017, cg02017 <=s 3367617@32,
(-3367617)@32 <=s cg02117, cg02117 <=s 3367617@32,
(-3367617)@32 <=s cg02217, cg02217 <=s 3367617@32,
(-3367617)@32 <=s cg02018, cg02018 <=s 3367617@32,
(-3367617)@32 <=s cg02118, cg02118 <=s 3367617@32,
(-3367617)@32 <=s cg02218, cg02218 <=s 3367617@32,
(-3367617)@32 <=s cg02019, cg02019 <=s 3367617@32,
(-3367617)@32 <=s cg02119, cg02119 <=s 3367617@32,
(-3367617)@32 <=s cg02219, cg02219 <=s 3367617@32,
(-3367617)@32 <=s cg02020, cg02020 <=s 3367617@32,
(-3367617)@32 <=s cg02120, cg02120 <=s 3367617@32,
(-3367617)@32 <=s cg02220, cg02220 <=s 3367617@32,
(-3367617)@32 <=s cg02021, cg02021 <=s 3367617@32,
(-3367617)@32 <=s cg02121, cg02121 <=s 3367617@32,
(-3367617)@32 <=s cg02221, cg02221 <=s 3367617@32,
(-3367617)@32 <=s cg02022, cg02022 <=s 3367617@32,
(-3367617)@32 <=s cg02122, cg02122 <=s 3367617@32,
(-3367617)@32 <=s cg02222, cg02222 <=s 3367617@32,
(-3367617)@32 <=s cg02023, cg02023 <=s 3367617@32,
(-3367617)@32 <=s cg02123, cg02123 <=s 3367617@32,
(-3367617)@32 <=s cg02223, cg02223 <=s 3367617@32,
(-3367617)@32 <=s cg02024, cg02024 <=s 3367617@32,
(-3367617)@32 <=s cg02124, cg02124 <=s 3367617@32,
(-3367617)@32 <=s cg02224, cg02224 <=s 3367617@32,
(-3367617)@32 <=s cg02025, cg02025 <=s 3367617@32,
(-3367617)@32 <=s cg02125, cg02125 <=s 3367617@32,
(-3367617)@32 <=s cg02225, cg02225 <=s 3367617@32,
(-3367617)@32 <=s cg02026, cg02026 <=s 3367617@32,
(-3367617)@32 <=s cg02126, cg02126 <=s 3367617@32,
(-3367617)@32 <=s cg02226, cg02226 <=s 3367617@32,
(-3367617)@32 <=s cg02027, cg02027 <=s 3367617@32,
(-3367617)@32 <=s cg02127, cg02127 <=s 3367617@32,
(-3367617)@32 <=s cg02227, cg02227 <=s 3367617@32,
(-3367617)@32 <=s cg02028, cg02028 <=s 3367617@32,
(-3367617)@32 <=s cg02128, cg02128 <=s 3367617@32,
(-3367617)@32 <=s cg02228, cg02228 <=s 3367617@32,
(-3367617)@32 <=s cg02029, cg02029 <=s 3367617@32,
(-3367617)@32 <=s cg02129, cg02129 <=s 3367617@32,
(-3367617)@32 <=s cg02229, cg02229 <=s 3367617@32,
(-3367617)@32 <=s cg02030, cg02030 <=s 3367617@32,
(-3367617)@32 <=s cg02130, cg02130 <=s 3367617@32,
(-3367617)@32 <=s cg02230, cg02230 <=s 3367617@32,
(-3367617)@32 <=s cg02031, cg02031 <=s 3367617@32,
(-3367617)@32 <=s cg02131, cg02131 <=s 3367617@32,
(-3367617)@32 <=s cg02231, cg02231 <=s 3367617@32
,
(-3367617)@32 <=s cg10000, cg10000 <=s 3367617@32,
(-3367617)@32 <=s cg10100, cg10100 <=s 3367617@32,
(-3367617)@32 <=s cg10200, cg10200 <=s 3367617@32,
(-3367617)@32 <=s cg10001, cg10001 <=s 3367617@32,
(-3367617)@32 <=s cg10101, cg10101 <=s 3367617@32,
(-3367617)@32 <=s cg10201, cg10201 <=s 3367617@32,
(-3367617)@32 <=s cg10002, cg10002 <=s 3367617@32,
(-3367617)@32 <=s cg10102, cg10102 <=s 3367617@32,
(-3367617)@32 <=s cg10202, cg10202 <=s 3367617@32,
(-3367617)@32 <=s cg10003, cg10003 <=s 3367617@32,
(-3367617)@32 <=s cg10103, cg10103 <=s 3367617@32,
(-3367617)@32 <=s cg10203, cg10203 <=s 3367617@32,
(-3367617)@32 <=s cg10004, cg10004 <=s 3367617@32,
(-3367617)@32 <=s cg10104, cg10104 <=s 3367617@32,
(-3367617)@32 <=s cg10204, cg10204 <=s 3367617@32,
(-3367617)@32 <=s cg10005, cg10005 <=s 3367617@32,
(-3367617)@32 <=s cg10105, cg10105 <=s 3367617@32,
(-3367617)@32 <=s cg10205, cg10205 <=s 3367617@32,
(-3367617)@32 <=s cg10006, cg10006 <=s 3367617@32,
(-3367617)@32 <=s cg10106, cg10106 <=s 3367617@32,
(-3367617)@32 <=s cg10206, cg10206 <=s 3367617@32,
(-3367617)@32 <=s cg10007, cg10007 <=s 3367617@32,
(-3367617)@32 <=s cg10107, cg10107 <=s 3367617@32,
(-3367617)@32 <=s cg10207, cg10207 <=s 3367617@32,
(-3367617)@32 <=s cg10008, cg10008 <=s 3367617@32,
(-3367617)@32 <=s cg10108, cg10108 <=s 3367617@32,
(-3367617)@32 <=s cg10208, cg10208 <=s 3367617@32,
(-3367617)@32 <=s cg10009, cg10009 <=s 3367617@32,
(-3367617)@32 <=s cg10109, cg10109 <=s 3367617@32,
(-3367617)@32 <=s cg10209, cg10209 <=s 3367617@32,
(-3367617)@32 <=s cg10010, cg10010 <=s 3367617@32,
(-3367617)@32 <=s cg10110, cg10110 <=s 3367617@32,
(-3367617)@32 <=s cg10210, cg10210 <=s 3367617@32,
(-3367617)@32 <=s cg10011, cg10011 <=s 3367617@32,
(-3367617)@32 <=s cg10111, cg10111 <=s 3367617@32,
(-3367617)@32 <=s cg10211, cg10211 <=s 3367617@32,
(-3367617)@32 <=s cg10012, cg10012 <=s 3367617@32,
(-3367617)@32 <=s cg10112, cg10112 <=s 3367617@32,
(-3367617)@32 <=s cg10212, cg10212 <=s 3367617@32,
(-3367617)@32 <=s cg10013, cg10013 <=s 3367617@32,
(-3367617)@32 <=s cg10113, cg10113 <=s 3367617@32,
(-3367617)@32 <=s cg10213, cg10213 <=s 3367617@32,
(-3367617)@32 <=s cg10014, cg10014 <=s 3367617@32,
(-3367617)@32 <=s cg10114, cg10114 <=s 3367617@32,
(-3367617)@32 <=s cg10214, cg10214 <=s 3367617@32,
(-3367617)@32 <=s cg10015, cg10015 <=s 3367617@32,
(-3367617)@32 <=s cg10115, cg10115 <=s 3367617@32,
(-3367617)@32 <=s cg10215, cg10215 <=s 3367617@32,
(-3367617)@32 <=s cg10016, cg10016 <=s 3367617@32,
(-3367617)@32 <=s cg10116, cg10116 <=s 3367617@32,
(-3367617)@32 <=s cg10216, cg10216 <=s 3367617@32,
(-3367617)@32 <=s cg10017, cg10017 <=s 3367617@32,
(-3367617)@32 <=s cg10117, cg10117 <=s 3367617@32,
(-3367617)@32 <=s cg10217, cg10217 <=s 3367617@32,
(-3367617)@32 <=s cg10018, cg10018 <=s 3367617@32,
(-3367617)@32 <=s cg10118, cg10118 <=s 3367617@32,
(-3367617)@32 <=s cg10218, cg10218 <=s 3367617@32,
(-3367617)@32 <=s cg10019, cg10019 <=s 3367617@32,
(-3367617)@32 <=s cg10119, cg10119 <=s 3367617@32,
(-3367617)@32 <=s cg10219, cg10219 <=s 3367617@32,
(-3367617)@32 <=s cg10020, cg10020 <=s 3367617@32,
(-3367617)@32 <=s cg10120, cg10120 <=s 3367617@32,
(-3367617)@32 <=s cg10220, cg10220 <=s 3367617@32,
(-3367617)@32 <=s cg10021, cg10021 <=s 3367617@32,
(-3367617)@32 <=s cg10121, cg10121 <=s 3367617@32,
(-3367617)@32 <=s cg10221, cg10221 <=s 3367617@32,
(-3367617)@32 <=s cg10022, cg10022 <=s 3367617@32,
(-3367617)@32 <=s cg10122, cg10122 <=s 3367617@32,
(-3367617)@32 <=s cg10222, cg10222 <=s 3367617@32,
(-3367617)@32 <=s cg10023, cg10023 <=s 3367617@32,
(-3367617)@32 <=s cg10123, cg10123 <=s 3367617@32,
(-3367617)@32 <=s cg10223, cg10223 <=s 3367617@32,
(-3367617)@32 <=s cg10024, cg10024 <=s 3367617@32,
(-3367617)@32 <=s cg10124, cg10124 <=s 3367617@32,
(-3367617)@32 <=s cg10224, cg10224 <=s 3367617@32,
(-3367617)@32 <=s cg10025, cg10025 <=s 3367617@32,
(-3367617)@32 <=s cg10125, cg10125 <=s 3367617@32,
(-3367617)@32 <=s cg10225, cg10225 <=s 3367617@32,
(-3367617)@32 <=s cg10026, cg10026 <=s 3367617@32,
(-3367617)@32 <=s cg10126, cg10126 <=s 3367617@32,
(-3367617)@32 <=s cg10226, cg10226 <=s 3367617@32,
(-3367617)@32 <=s cg10027, cg10027 <=s 3367617@32,
(-3367617)@32 <=s cg10127, cg10127 <=s 3367617@32,
(-3367617)@32 <=s cg10227, cg10227 <=s 3367617@32,
(-3367617)@32 <=s cg10028, cg10028 <=s 3367617@32,
(-3367617)@32 <=s cg10128, cg10128 <=s 3367617@32,
(-3367617)@32 <=s cg10228, cg10228 <=s 3367617@32,
(-3367617)@32 <=s cg10029, cg10029 <=s 3367617@32,
(-3367617)@32 <=s cg10129, cg10129 <=s 3367617@32,
(-3367617)@32 <=s cg10229, cg10229 <=s 3367617@32,
(-3367617)@32 <=s cg10030, cg10030 <=s 3367617@32,
(-3367617)@32 <=s cg10130, cg10130 <=s 3367617@32,
(-3367617)@32 <=s cg10230, cg10230 <=s 3367617@32,
(-3367617)@32 <=s cg10031, cg10031 <=s 3367617@32,
(-3367617)@32 <=s cg10131, cg10131 <=s 3367617@32,
(-3367617)@32 <=s cg10231, cg10231 <=s 3367617@32,
(-3367617)@32 <=s cg11000, cg11000 <=s 3367617@32,
(-3367617)@32 <=s cg11100, cg11100 <=s 3367617@32,
(-3367617)@32 <=s cg11200, cg11200 <=s 3367617@32,
(-3367617)@32 <=s cg11001, cg11001 <=s 3367617@32,
(-3367617)@32 <=s cg11101, cg11101 <=s 3367617@32,
(-3367617)@32 <=s cg11201, cg11201 <=s 3367617@32,
(-3367617)@32 <=s cg11002, cg11002 <=s 3367617@32,
(-3367617)@32 <=s cg11102, cg11102 <=s 3367617@32,
(-3367617)@32 <=s cg11202, cg11202 <=s 3367617@32,
(-3367617)@32 <=s cg11003, cg11003 <=s 3367617@32,
(-3367617)@32 <=s cg11103, cg11103 <=s 3367617@32,
(-3367617)@32 <=s cg11203, cg11203 <=s 3367617@32,
(-3367617)@32 <=s cg11004, cg11004 <=s 3367617@32,
(-3367617)@32 <=s cg11104, cg11104 <=s 3367617@32,
(-3367617)@32 <=s cg11204, cg11204 <=s 3367617@32,
(-3367617)@32 <=s cg11005, cg11005 <=s 3367617@32,
(-3367617)@32 <=s cg11105, cg11105 <=s 3367617@32,
(-3367617)@32 <=s cg11205, cg11205 <=s 3367617@32,
(-3367617)@32 <=s cg11006, cg11006 <=s 3367617@32,
(-3367617)@32 <=s cg11106, cg11106 <=s 3367617@32,
(-3367617)@32 <=s cg11206, cg11206 <=s 3367617@32,
(-3367617)@32 <=s cg11007, cg11007 <=s 3367617@32,
(-3367617)@32 <=s cg11107, cg11107 <=s 3367617@32,
(-3367617)@32 <=s cg11207, cg11207 <=s 3367617@32,
(-3367617)@32 <=s cg11008, cg11008 <=s 3367617@32,
(-3367617)@32 <=s cg11108, cg11108 <=s 3367617@32,
(-3367617)@32 <=s cg11208, cg11208 <=s 3367617@32,
(-3367617)@32 <=s cg11009, cg11009 <=s 3367617@32,
(-3367617)@32 <=s cg11109, cg11109 <=s 3367617@32,
(-3367617)@32 <=s cg11209, cg11209 <=s 3367617@32,
(-3367617)@32 <=s cg11010, cg11010 <=s 3367617@32,
(-3367617)@32 <=s cg11110, cg11110 <=s 3367617@32,
(-3367617)@32 <=s cg11210, cg11210 <=s 3367617@32,
(-3367617)@32 <=s cg11011, cg11011 <=s 3367617@32,
(-3367617)@32 <=s cg11111, cg11111 <=s 3367617@32,
(-3367617)@32 <=s cg11211, cg11211 <=s 3367617@32,
(-3367617)@32 <=s cg11012, cg11012 <=s 3367617@32,
(-3367617)@32 <=s cg11112, cg11112 <=s 3367617@32,
(-3367617)@32 <=s cg11212, cg11212 <=s 3367617@32,
(-3367617)@32 <=s cg11013, cg11013 <=s 3367617@32,
(-3367617)@32 <=s cg11113, cg11113 <=s 3367617@32,
(-3367617)@32 <=s cg11213, cg11213 <=s 3367617@32,
(-3367617)@32 <=s cg11014, cg11014 <=s 3367617@32,
(-3367617)@32 <=s cg11114, cg11114 <=s 3367617@32,
(-3367617)@32 <=s cg11214, cg11214 <=s 3367617@32,
(-3367617)@32 <=s cg11015, cg11015 <=s 3367617@32,
(-3367617)@32 <=s cg11115, cg11115 <=s 3367617@32,
(-3367617)@32 <=s cg11215, cg11215 <=s 3367617@32,
(-3367617)@32 <=s cg11016, cg11016 <=s 3367617@32,
(-3367617)@32 <=s cg11116, cg11116 <=s 3367617@32,
(-3367617)@32 <=s cg11216, cg11216 <=s 3367617@32,
(-3367617)@32 <=s cg11017, cg11017 <=s 3367617@32,
(-3367617)@32 <=s cg11117, cg11117 <=s 3367617@32,
(-3367617)@32 <=s cg11217, cg11217 <=s 3367617@32,
(-3367617)@32 <=s cg11018, cg11018 <=s 3367617@32,
(-3367617)@32 <=s cg11118, cg11118 <=s 3367617@32,
(-3367617)@32 <=s cg11218, cg11218 <=s 3367617@32,
(-3367617)@32 <=s cg11019, cg11019 <=s 3367617@32,
(-3367617)@32 <=s cg11119, cg11119 <=s 3367617@32,
(-3367617)@32 <=s cg11219, cg11219 <=s 3367617@32,
(-3367617)@32 <=s cg11020, cg11020 <=s 3367617@32,
(-3367617)@32 <=s cg11120, cg11120 <=s 3367617@32,
(-3367617)@32 <=s cg11220, cg11220 <=s 3367617@32,
(-3367617)@32 <=s cg11021, cg11021 <=s 3367617@32,
(-3367617)@32 <=s cg11121, cg11121 <=s 3367617@32,
(-3367617)@32 <=s cg11221, cg11221 <=s 3367617@32,
(-3367617)@32 <=s cg11022, cg11022 <=s 3367617@32,
(-3367617)@32 <=s cg11122, cg11122 <=s 3367617@32,
(-3367617)@32 <=s cg11222, cg11222 <=s 3367617@32,
(-3367617)@32 <=s cg11023, cg11023 <=s 3367617@32,
(-3367617)@32 <=s cg11123, cg11123 <=s 3367617@32,
(-3367617)@32 <=s cg11223, cg11223 <=s 3367617@32,
(-3367617)@32 <=s cg11024, cg11024 <=s 3367617@32,
(-3367617)@32 <=s cg11124, cg11124 <=s 3367617@32,
(-3367617)@32 <=s cg11224, cg11224 <=s 3367617@32,
(-3367617)@32 <=s cg11025, cg11025 <=s 3367617@32,
(-3367617)@32 <=s cg11125, cg11125 <=s 3367617@32,
(-3367617)@32 <=s cg11225, cg11225 <=s 3367617@32,
(-3367617)@32 <=s cg11026, cg11026 <=s 3367617@32,
(-3367617)@32 <=s cg11126, cg11126 <=s 3367617@32,
(-3367617)@32 <=s cg11226, cg11226 <=s 3367617@32,
(-3367617)@32 <=s cg11027, cg11027 <=s 3367617@32,
(-3367617)@32 <=s cg11127, cg11127 <=s 3367617@32,
(-3367617)@32 <=s cg11227, cg11227 <=s 3367617@32,
(-3367617)@32 <=s cg11028, cg11028 <=s 3367617@32,
(-3367617)@32 <=s cg11128, cg11128 <=s 3367617@32,
(-3367617)@32 <=s cg11228, cg11228 <=s 3367617@32,
(-3367617)@32 <=s cg11029, cg11029 <=s 3367617@32,
(-3367617)@32 <=s cg11129, cg11129 <=s 3367617@32,
(-3367617)@32 <=s cg11229, cg11229 <=s 3367617@32,
(-3367617)@32 <=s cg11030, cg11030 <=s 3367617@32,
(-3367617)@32 <=s cg11130, cg11130 <=s 3367617@32,
(-3367617)@32 <=s cg11230, cg11230 <=s 3367617@32,
(-3367617)@32 <=s cg11031, cg11031 <=s 3367617@32,
(-3367617)@32 <=s cg11131, cg11131 <=s 3367617@32,
(-3367617)@32 <=s cg11231, cg11231 <=s 3367617@32,
(-3367617)@32 <=s cg12000, cg12000 <=s 3367617@32,
(-3367617)@32 <=s cg12100, cg12100 <=s 3367617@32,
(-3367617)@32 <=s cg12200, cg12200 <=s 3367617@32,
(-3367617)@32 <=s cg12001, cg12001 <=s 3367617@32,
(-3367617)@32 <=s cg12101, cg12101 <=s 3367617@32,
(-3367617)@32 <=s cg12201, cg12201 <=s 3367617@32,
(-3367617)@32 <=s cg12002, cg12002 <=s 3367617@32,
(-3367617)@32 <=s cg12102, cg12102 <=s 3367617@32,
(-3367617)@32 <=s cg12202, cg12202 <=s 3367617@32,
(-3367617)@32 <=s cg12003, cg12003 <=s 3367617@32,
(-3367617)@32 <=s cg12103, cg12103 <=s 3367617@32,
(-3367617)@32 <=s cg12203, cg12203 <=s 3367617@32,
(-3367617)@32 <=s cg12004, cg12004 <=s 3367617@32,
(-3367617)@32 <=s cg12104, cg12104 <=s 3367617@32,
(-3367617)@32 <=s cg12204, cg12204 <=s 3367617@32,
(-3367617)@32 <=s cg12005, cg12005 <=s 3367617@32,
(-3367617)@32 <=s cg12105, cg12105 <=s 3367617@32,
(-3367617)@32 <=s cg12205, cg12205 <=s 3367617@32,
(-3367617)@32 <=s cg12006, cg12006 <=s 3367617@32,
(-3367617)@32 <=s cg12106, cg12106 <=s 3367617@32,
(-3367617)@32 <=s cg12206, cg12206 <=s 3367617@32,
(-3367617)@32 <=s cg12007, cg12007 <=s 3367617@32,
(-3367617)@32 <=s cg12107, cg12107 <=s 3367617@32,
(-3367617)@32 <=s cg12207, cg12207 <=s 3367617@32,
(-3367617)@32 <=s cg12008, cg12008 <=s 3367617@32,
(-3367617)@32 <=s cg12108, cg12108 <=s 3367617@32,
(-3367617)@32 <=s cg12208, cg12208 <=s 3367617@32,
(-3367617)@32 <=s cg12009, cg12009 <=s 3367617@32,
(-3367617)@32 <=s cg12109, cg12109 <=s 3367617@32,
(-3367617)@32 <=s cg12209, cg12209 <=s 3367617@32,
(-3367617)@32 <=s cg12010, cg12010 <=s 3367617@32,
(-3367617)@32 <=s cg12110, cg12110 <=s 3367617@32,
(-3367617)@32 <=s cg12210, cg12210 <=s 3367617@32,
(-3367617)@32 <=s cg12011, cg12011 <=s 3367617@32,
(-3367617)@32 <=s cg12111, cg12111 <=s 3367617@32,
(-3367617)@32 <=s cg12211, cg12211 <=s 3367617@32,
(-3367617)@32 <=s cg12012, cg12012 <=s 3367617@32,
(-3367617)@32 <=s cg12112, cg12112 <=s 3367617@32,
(-3367617)@32 <=s cg12212, cg12212 <=s 3367617@32,
(-3367617)@32 <=s cg12013, cg12013 <=s 3367617@32,
(-3367617)@32 <=s cg12113, cg12113 <=s 3367617@32,
(-3367617)@32 <=s cg12213, cg12213 <=s 3367617@32,
(-3367617)@32 <=s cg12014, cg12014 <=s 3367617@32,
(-3367617)@32 <=s cg12114, cg12114 <=s 3367617@32,
(-3367617)@32 <=s cg12214, cg12214 <=s 3367617@32,
(-3367617)@32 <=s cg12015, cg12015 <=s 3367617@32,
(-3367617)@32 <=s cg12115, cg12115 <=s 3367617@32,
(-3367617)@32 <=s cg12215, cg12215 <=s 3367617@32,
(-3367617)@32 <=s cg12016, cg12016 <=s 3367617@32,
(-3367617)@32 <=s cg12116, cg12116 <=s 3367617@32,
(-3367617)@32 <=s cg12216, cg12216 <=s 3367617@32,
(-3367617)@32 <=s cg12017, cg12017 <=s 3367617@32,
(-3367617)@32 <=s cg12117, cg12117 <=s 3367617@32,
(-3367617)@32 <=s cg12217, cg12217 <=s 3367617@32,
(-3367617)@32 <=s cg12018, cg12018 <=s 3367617@32,
(-3367617)@32 <=s cg12118, cg12118 <=s 3367617@32,
(-3367617)@32 <=s cg12218, cg12218 <=s 3367617@32,
(-3367617)@32 <=s cg12019, cg12019 <=s 3367617@32,
(-3367617)@32 <=s cg12119, cg12119 <=s 3367617@32,
(-3367617)@32 <=s cg12219, cg12219 <=s 3367617@32,
(-3367617)@32 <=s cg12020, cg12020 <=s 3367617@32,
(-3367617)@32 <=s cg12120, cg12120 <=s 3367617@32,
(-3367617)@32 <=s cg12220, cg12220 <=s 3367617@32,
(-3367617)@32 <=s cg12021, cg12021 <=s 3367617@32,
(-3367617)@32 <=s cg12121, cg12121 <=s 3367617@32,
(-3367617)@32 <=s cg12221, cg12221 <=s 3367617@32,
(-3367617)@32 <=s cg12022, cg12022 <=s 3367617@32,
(-3367617)@32 <=s cg12122, cg12122 <=s 3367617@32,
(-3367617)@32 <=s cg12222, cg12222 <=s 3367617@32,
(-3367617)@32 <=s cg12023, cg12023 <=s 3367617@32,
(-3367617)@32 <=s cg12123, cg12123 <=s 3367617@32,
(-3367617)@32 <=s cg12223, cg12223 <=s 3367617@32,
(-3367617)@32 <=s cg12024, cg12024 <=s 3367617@32,
(-3367617)@32 <=s cg12124, cg12124 <=s 3367617@32,
(-3367617)@32 <=s cg12224, cg12224 <=s 3367617@32,
(-3367617)@32 <=s cg12025, cg12025 <=s 3367617@32,
(-3367617)@32 <=s cg12125, cg12125 <=s 3367617@32,
(-3367617)@32 <=s cg12225, cg12225 <=s 3367617@32,
(-3367617)@32 <=s cg12026, cg12026 <=s 3367617@32,
(-3367617)@32 <=s cg12126, cg12126 <=s 3367617@32,
(-3367617)@32 <=s cg12226, cg12226 <=s 3367617@32,
(-3367617)@32 <=s cg12027, cg12027 <=s 3367617@32,
(-3367617)@32 <=s cg12127, cg12127 <=s 3367617@32,
(-3367617)@32 <=s cg12227, cg12227 <=s 3367617@32,
(-3367617)@32 <=s cg12028, cg12028 <=s 3367617@32,
(-3367617)@32 <=s cg12128, cg12128 <=s 3367617@32,
(-3367617)@32 <=s cg12228, cg12228 <=s 3367617@32,
(-3367617)@32 <=s cg12029, cg12029 <=s 3367617@32,
(-3367617)@32 <=s cg12129, cg12129 <=s 3367617@32,
(-3367617)@32 <=s cg12229, cg12229 <=s 3367617@32,
(-3367617)@32 <=s cg12030, cg12030 <=s 3367617@32,
(-3367617)@32 <=s cg12130, cg12130 <=s 3367617@32,
(-3367617)@32 <=s cg12230, cg12230 <=s 3367617@32,
(-3367617)@32 <=s cg12031, cg12031 <=s 3367617@32,
(-3367617)@32 <=s cg12131, cg12131 <=s 3367617@32,
(-3367617)@32 <=s cg12231, cg12231 <=s 3367617@32
,
(-3367617)@32 <=s cg20000, cg20000 <=s 3367617@32,
(-3367617)@32 <=s cg20100, cg20100 <=s 3367617@32,
(-3367617)@32 <=s cg20200, cg20200 <=s 3367617@32,
(-3367617)@32 <=s cg20001, cg20001 <=s 3367617@32,
(-3367617)@32 <=s cg20101, cg20101 <=s 3367617@32,
(-3367617)@32 <=s cg20201, cg20201 <=s 3367617@32,
(-3367617)@32 <=s cg20002, cg20002 <=s 3367617@32,
(-3367617)@32 <=s cg20102, cg20102 <=s 3367617@32,
(-3367617)@32 <=s cg20202, cg20202 <=s 3367617@32,
(-3367617)@32 <=s cg20003, cg20003 <=s 3367617@32,
(-3367617)@32 <=s cg20103, cg20103 <=s 3367617@32,
(-3367617)@32 <=s cg20203, cg20203 <=s 3367617@32,
(-3367617)@32 <=s cg20004, cg20004 <=s 3367617@32,
(-3367617)@32 <=s cg20104, cg20104 <=s 3367617@32,
(-3367617)@32 <=s cg20204, cg20204 <=s 3367617@32,
(-3367617)@32 <=s cg20005, cg20005 <=s 3367617@32,
(-3367617)@32 <=s cg20105, cg20105 <=s 3367617@32,
(-3367617)@32 <=s cg20205, cg20205 <=s 3367617@32,
(-3367617)@32 <=s cg20006, cg20006 <=s 3367617@32,
(-3367617)@32 <=s cg20106, cg20106 <=s 3367617@32,
(-3367617)@32 <=s cg20206, cg20206 <=s 3367617@32,
(-3367617)@32 <=s cg20007, cg20007 <=s 3367617@32,
(-3367617)@32 <=s cg20107, cg20107 <=s 3367617@32,
(-3367617)@32 <=s cg20207, cg20207 <=s 3367617@32,
(-3367617)@32 <=s cg20008, cg20008 <=s 3367617@32,
(-3367617)@32 <=s cg20108, cg20108 <=s 3367617@32,
(-3367617)@32 <=s cg20208, cg20208 <=s 3367617@32,
(-3367617)@32 <=s cg20009, cg20009 <=s 3367617@32,
(-3367617)@32 <=s cg20109, cg20109 <=s 3367617@32,
(-3367617)@32 <=s cg20209, cg20209 <=s 3367617@32,
(-3367617)@32 <=s cg20010, cg20010 <=s 3367617@32,
(-3367617)@32 <=s cg20110, cg20110 <=s 3367617@32,
(-3367617)@32 <=s cg20210, cg20210 <=s 3367617@32,
(-3367617)@32 <=s cg20011, cg20011 <=s 3367617@32,
(-3367617)@32 <=s cg20111, cg20111 <=s 3367617@32,
(-3367617)@32 <=s cg20211, cg20211 <=s 3367617@32,
(-3367617)@32 <=s cg20012, cg20012 <=s 3367617@32,
(-3367617)@32 <=s cg20112, cg20112 <=s 3367617@32,
(-3367617)@32 <=s cg20212, cg20212 <=s 3367617@32,
(-3367617)@32 <=s cg20013, cg20013 <=s 3367617@32,
(-3367617)@32 <=s cg20113, cg20113 <=s 3367617@32,
(-3367617)@32 <=s cg20213, cg20213 <=s 3367617@32,
(-3367617)@32 <=s cg20014, cg20014 <=s 3367617@32,
(-3367617)@32 <=s cg20114, cg20114 <=s 3367617@32,
(-3367617)@32 <=s cg20214, cg20214 <=s 3367617@32,
(-3367617)@32 <=s cg20015, cg20015 <=s 3367617@32,
(-3367617)@32 <=s cg20115, cg20115 <=s 3367617@32,
(-3367617)@32 <=s cg20215, cg20215 <=s 3367617@32,
(-3367617)@32 <=s cg20016, cg20016 <=s 3367617@32,
(-3367617)@32 <=s cg20116, cg20116 <=s 3367617@32,
(-3367617)@32 <=s cg20216, cg20216 <=s 3367617@32,
(-3367617)@32 <=s cg20017, cg20017 <=s 3367617@32,
(-3367617)@32 <=s cg20117, cg20117 <=s 3367617@32,
(-3367617)@32 <=s cg20217, cg20217 <=s 3367617@32,
(-3367617)@32 <=s cg20018, cg20018 <=s 3367617@32,
(-3367617)@32 <=s cg20118, cg20118 <=s 3367617@32,
(-3367617)@32 <=s cg20218, cg20218 <=s 3367617@32,
(-3367617)@32 <=s cg20019, cg20019 <=s 3367617@32,
(-3367617)@32 <=s cg20119, cg20119 <=s 3367617@32,
(-3367617)@32 <=s cg20219, cg20219 <=s 3367617@32,
(-3367617)@32 <=s cg20020, cg20020 <=s 3367617@32,
(-3367617)@32 <=s cg20120, cg20120 <=s 3367617@32,
(-3367617)@32 <=s cg20220, cg20220 <=s 3367617@32,
(-3367617)@32 <=s cg20021, cg20021 <=s 3367617@32,
(-3367617)@32 <=s cg20121, cg20121 <=s 3367617@32,
(-3367617)@32 <=s cg20221, cg20221 <=s 3367617@32,
(-3367617)@32 <=s cg20022, cg20022 <=s 3367617@32,
(-3367617)@32 <=s cg20122, cg20122 <=s 3367617@32,
(-3367617)@32 <=s cg20222, cg20222 <=s 3367617@32,
(-3367617)@32 <=s cg20023, cg20023 <=s 3367617@32,
(-3367617)@32 <=s cg20123, cg20123 <=s 3367617@32,
(-3367617)@32 <=s cg20223, cg20223 <=s 3367617@32,
(-3367617)@32 <=s cg20024, cg20024 <=s 3367617@32,
(-3367617)@32 <=s cg20124, cg20124 <=s 3367617@32,
(-3367617)@32 <=s cg20224, cg20224 <=s 3367617@32,
(-3367617)@32 <=s cg20025, cg20025 <=s 3367617@32,
(-3367617)@32 <=s cg20125, cg20125 <=s 3367617@32,
(-3367617)@32 <=s cg20225, cg20225 <=s 3367617@32,
(-3367617)@32 <=s cg20026, cg20026 <=s 3367617@32,
(-3367617)@32 <=s cg20126, cg20126 <=s 3367617@32,
(-3367617)@32 <=s cg20226, cg20226 <=s 3367617@32,
(-3367617)@32 <=s cg20027, cg20027 <=s 3367617@32,
(-3367617)@32 <=s cg20127, cg20127 <=s 3367617@32,
(-3367617)@32 <=s cg20227, cg20227 <=s 3367617@32,
(-3367617)@32 <=s cg20028, cg20028 <=s 3367617@32,
(-3367617)@32 <=s cg20128, cg20128 <=s 3367617@32,
(-3367617)@32 <=s cg20228, cg20228 <=s 3367617@32,
(-3367617)@32 <=s cg20029, cg20029 <=s 3367617@32,
(-3367617)@32 <=s cg20129, cg20129 <=s 3367617@32,
(-3367617)@32 <=s cg20229, cg20229 <=s 3367617@32,
(-3367617)@32 <=s cg20030, cg20030 <=s 3367617@32,
(-3367617)@32 <=s cg20130, cg20130 <=s 3367617@32,
(-3367617)@32 <=s cg20230, cg20230 <=s 3367617@32,
(-3367617)@32 <=s cg20031, cg20031 <=s 3367617@32,
(-3367617)@32 <=s cg20131, cg20131 <=s 3367617@32,
(-3367617)@32 <=s cg20231, cg20231 <=s 3367617@32,
(-3367617)@32 <=s cg21000, cg21000 <=s 3367617@32,
(-3367617)@32 <=s cg21100, cg21100 <=s 3367617@32,
(-3367617)@32 <=s cg21200, cg21200 <=s 3367617@32,
(-3367617)@32 <=s cg21001, cg21001 <=s 3367617@32,
(-3367617)@32 <=s cg21101, cg21101 <=s 3367617@32,
(-3367617)@32 <=s cg21201, cg21201 <=s 3367617@32,
(-3367617)@32 <=s cg21002, cg21002 <=s 3367617@32,
(-3367617)@32 <=s cg21102, cg21102 <=s 3367617@32,
(-3367617)@32 <=s cg21202, cg21202 <=s 3367617@32,
(-3367617)@32 <=s cg21003, cg21003 <=s 3367617@32,
(-3367617)@32 <=s cg21103, cg21103 <=s 3367617@32,
(-3367617)@32 <=s cg21203, cg21203 <=s 3367617@32,
(-3367617)@32 <=s cg21004, cg21004 <=s 3367617@32,
(-3367617)@32 <=s cg21104, cg21104 <=s 3367617@32,
(-3367617)@32 <=s cg21204, cg21204 <=s 3367617@32,
(-3367617)@32 <=s cg21005, cg21005 <=s 3367617@32,
(-3367617)@32 <=s cg21105, cg21105 <=s 3367617@32,
(-3367617)@32 <=s cg21205, cg21205 <=s 3367617@32,
(-3367617)@32 <=s cg21006, cg21006 <=s 3367617@32,
(-3367617)@32 <=s cg21106, cg21106 <=s 3367617@32,
(-3367617)@32 <=s cg21206, cg21206 <=s 3367617@32,
(-3367617)@32 <=s cg21007, cg21007 <=s 3367617@32,
(-3367617)@32 <=s cg21107, cg21107 <=s 3367617@32,
(-3367617)@32 <=s cg21207, cg21207 <=s 3367617@32,
(-3367617)@32 <=s cg21008, cg21008 <=s 3367617@32,
(-3367617)@32 <=s cg21108, cg21108 <=s 3367617@32,
(-3367617)@32 <=s cg21208, cg21208 <=s 3367617@32,
(-3367617)@32 <=s cg21009, cg21009 <=s 3367617@32,
(-3367617)@32 <=s cg21109, cg21109 <=s 3367617@32,
(-3367617)@32 <=s cg21209, cg21209 <=s 3367617@32,
(-3367617)@32 <=s cg21010, cg21010 <=s 3367617@32,
(-3367617)@32 <=s cg21110, cg21110 <=s 3367617@32,
(-3367617)@32 <=s cg21210, cg21210 <=s 3367617@32,
(-3367617)@32 <=s cg21011, cg21011 <=s 3367617@32,
(-3367617)@32 <=s cg21111, cg21111 <=s 3367617@32,
(-3367617)@32 <=s cg21211, cg21211 <=s 3367617@32,
(-3367617)@32 <=s cg21012, cg21012 <=s 3367617@32,
(-3367617)@32 <=s cg21112, cg21112 <=s 3367617@32,
(-3367617)@32 <=s cg21212, cg21212 <=s 3367617@32,
(-3367617)@32 <=s cg21013, cg21013 <=s 3367617@32,
(-3367617)@32 <=s cg21113, cg21113 <=s 3367617@32,
(-3367617)@32 <=s cg21213, cg21213 <=s 3367617@32,
(-3367617)@32 <=s cg21014, cg21014 <=s 3367617@32,
(-3367617)@32 <=s cg21114, cg21114 <=s 3367617@32,
(-3367617)@32 <=s cg21214, cg21214 <=s 3367617@32,
(-3367617)@32 <=s cg21015, cg21015 <=s 3367617@32,
(-3367617)@32 <=s cg21115, cg21115 <=s 3367617@32,
(-3367617)@32 <=s cg21215, cg21215 <=s 3367617@32,
(-3367617)@32 <=s cg21016, cg21016 <=s 3367617@32,
(-3367617)@32 <=s cg21116, cg21116 <=s 3367617@32,
(-3367617)@32 <=s cg21216, cg21216 <=s 3367617@32,
(-3367617)@32 <=s cg21017, cg21017 <=s 3367617@32,
(-3367617)@32 <=s cg21117, cg21117 <=s 3367617@32,
(-3367617)@32 <=s cg21217, cg21217 <=s 3367617@32,
(-3367617)@32 <=s cg21018, cg21018 <=s 3367617@32,
(-3367617)@32 <=s cg21118, cg21118 <=s 3367617@32,
(-3367617)@32 <=s cg21218, cg21218 <=s 3367617@32,
(-3367617)@32 <=s cg21019, cg21019 <=s 3367617@32,
(-3367617)@32 <=s cg21119, cg21119 <=s 3367617@32,
(-3367617)@32 <=s cg21219, cg21219 <=s 3367617@32,
(-3367617)@32 <=s cg21020, cg21020 <=s 3367617@32,
(-3367617)@32 <=s cg21120, cg21120 <=s 3367617@32,
(-3367617)@32 <=s cg21220, cg21220 <=s 3367617@32,
(-3367617)@32 <=s cg21021, cg21021 <=s 3367617@32,
(-3367617)@32 <=s cg21121, cg21121 <=s 3367617@32,
(-3367617)@32 <=s cg21221, cg21221 <=s 3367617@32,
(-3367617)@32 <=s cg21022, cg21022 <=s 3367617@32,
(-3367617)@32 <=s cg21122, cg21122 <=s 3367617@32,
(-3367617)@32 <=s cg21222, cg21222 <=s 3367617@32,
(-3367617)@32 <=s cg21023, cg21023 <=s 3367617@32,
(-3367617)@32 <=s cg21123, cg21123 <=s 3367617@32,
(-3367617)@32 <=s cg21223, cg21223 <=s 3367617@32,
(-3367617)@32 <=s cg21024, cg21024 <=s 3367617@32,
(-3367617)@32 <=s cg21124, cg21124 <=s 3367617@32,
(-3367617)@32 <=s cg21224, cg21224 <=s 3367617@32,
(-3367617)@32 <=s cg21025, cg21025 <=s 3367617@32,
(-3367617)@32 <=s cg21125, cg21125 <=s 3367617@32,
(-3367617)@32 <=s cg21225, cg21225 <=s 3367617@32,
(-3367617)@32 <=s cg21026, cg21026 <=s 3367617@32,
(-3367617)@32 <=s cg21126, cg21126 <=s 3367617@32,
(-3367617)@32 <=s cg21226, cg21226 <=s 3367617@32,
(-3367617)@32 <=s cg21027, cg21027 <=s 3367617@32,
(-3367617)@32 <=s cg21127, cg21127 <=s 3367617@32,
(-3367617)@32 <=s cg21227, cg21227 <=s 3367617@32,
(-3367617)@32 <=s cg21028, cg21028 <=s 3367617@32,
(-3367617)@32 <=s cg21128, cg21128 <=s 3367617@32,
(-3367617)@32 <=s cg21228, cg21228 <=s 3367617@32,
(-3367617)@32 <=s cg21029, cg21029 <=s 3367617@32,
(-3367617)@32 <=s cg21129, cg21129 <=s 3367617@32,
(-3367617)@32 <=s cg21229, cg21229 <=s 3367617@32,
(-3367617)@32 <=s cg21030, cg21030 <=s 3367617@32,
(-3367617)@32 <=s cg21130, cg21130 <=s 3367617@32,
(-3367617)@32 <=s cg21230, cg21230 <=s 3367617@32,
(-3367617)@32 <=s cg21031, cg21031 <=s 3367617@32,
(-3367617)@32 <=s cg21131, cg21131 <=s 3367617@32,
(-3367617)@32 <=s cg21231, cg21231 <=s 3367617@32,
(-3367617)@32 <=s cg22000, cg22000 <=s 3367617@32,
(-3367617)@32 <=s cg22100, cg22100 <=s 3367617@32,
(-3367617)@32 <=s cg22200, cg22200 <=s 3367617@32,
(-3367617)@32 <=s cg22001, cg22001 <=s 3367617@32,
(-3367617)@32 <=s cg22101, cg22101 <=s 3367617@32,
(-3367617)@32 <=s cg22201, cg22201 <=s 3367617@32,
(-3367617)@32 <=s cg22002, cg22002 <=s 3367617@32,
(-3367617)@32 <=s cg22102, cg22102 <=s 3367617@32,
(-3367617)@32 <=s cg22202, cg22202 <=s 3367617@32,
(-3367617)@32 <=s cg22003, cg22003 <=s 3367617@32,
(-3367617)@32 <=s cg22103, cg22103 <=s 3367617@32,
(-3367617)@32 <=s cg22203, cg22203 <=s 3367617@32,
(-3367617)@32 <=s cg22004, cg22004 <=s 3367617@32,
(-3367617)@32 <=s cg22104, cg22104 <=s 3367617@32,
(-3367617)@32 <=s cg22204, cg22204 <=s 3367617@32,
(-3367617)@32 <=s cg22005, cg22005 <=s 3367617@32,
(-3367617)@32 <=s cg22105, cg22105 <=s 3367617@32,
(-3367617)@32 <=s cg22205, cg22205 <=s 3367617@32,
(-3367617)@32 <=s cg22006, cg22006 <=s 3367617@32,
(-3367617)@32 <=s cg22106, cg22106 <=s 3367617@32,
(-3367617)@32 <=s cg22206, cg22206 <=s 3367617@32,
(-3367617)@32 <=s cg22007, cg22007 <=s 3367617@32,
(-3367617)@32 <=s cg22107, cg22107 <=s 3367617@32,
(-3367617)@32 <=s cg22207, cg22207 <=s 3367617@32,
(-3367617)@32 <=s cg22008, cg22008 <=s 3367617@32,
(-3367617)@32 <=s cg22108, cg22108 <=s 3367617@32,
(-3367617)@32 <=s cg22208, cg22208 <=s 3367617@32,
(-3367617)@32 <=s cg22009, cg22009 <=s 3367617@32,
(-3367617)@32 <=s cg22109, cg22109 <=s 3367617@32,
(-3367617)@32 <=s cg22209, cg22209 <=s 3367617@32,
(-3367617)@32 <=s cg22010, cg22010 <=s 3367617@32,
(-3367617)@32 <=s cg22110, cg22110 <=s 3367617@32,
(-3367617)@32 <=s cg22210, cg22210 <=s 3367617@32,
(-3367617)@32 <=s cg22011, cg22011 <=s 3367617@32,
(-3367617)@32 <=s cg22111, cg22111 <=s 3367617@32,
(-3367617)@32 <=s cg22211, cg22211 <=s 3367617@32,
(-3367617)@32 <=s cg22012, cg22012 <=s 3367617@32,
(-3367617)@32 <=s cg22112, cg22112 <=s 3367617@32,
(-3367617)@32 <=s cg22212, cg22212 <=s 3367617@32,
(-3367617)@32 <=s cg22013, cg22013 <=s 3367617@32,
(-3367617)@32 <=s cg22113, cg22113 <=s 3367617@32,
(-3367617)@32 <=s cg22213, cg22213 <=s 3367617@32,
(-3367617)@32 <=s cg22014, cg22014 <=s 3367617@32,
(-3367617)@32 <=s cg22114, cg22114 <=s 3367617@32,
(-3367617)@32 <=s cg22214, cg22214 <=s 3367617@32,
(-3367617)@32 <=s cg22015, cg22015 <=s 3367617@32,
(-3367617)@32 <=s cg22115, cg22115 <=s 3367617@32,
(-3367617)@32 <=s cg22215, cg22215 <=s 3367617@32,
(-3367617)@32 <=s cg22016, cg22016 <=s 3367617@32,
(-3367617)@32 <=s cg22116, cg22116 <=s 3367617@32,
(-3367617)@32 <=s cg22216, cg22216 <=s 3367617@32,
(-3367617)@32 <=s cg22017, cg22017 <=s 3367617@32,
(-3367617)@32 <=s cg22117, cg22117 <=s 3367617@32,
(-3367617)@32 <=s cg22217, cg22217 <=s 3367617@32,
(-3367617)@32 <=s cg22018, cg22018 <=s 3367617@32,
(-3367617)@32 <=s cg22118, cg22118 <=s 3367617@32,
(-3367617)@32 <=s cg22218, cg22218 <=s 3367617@32,
(-3367617)@32 <=s cg22019, cg22019 <=s 3367617@32,
(-3367617)@32 <=s cg22119, cg22119 <=s 3367617@32,
(-3367617)@32 <=s cg22219, cg22219 <=s 3367617@32,
(-3367617)@32 <=s cg22020, cg22020 <=s 3367617@32,
(-3367617)@32 <=s cg22120, cg22120 <=s 3367617@32,
(-3367617)@32 <=s cg22220, cg22220 <=s 3367617@32,
(-3367617)@32 <=s cg22021, cg22021 <=s 3367617@32,
(-3367617)@32 <=s cg22121, cg22121 <=s 3367617@32,
(-3367617)@32 <=s cg22221, cg22221 <=s 3367617@32,
(-3367617)@32 <=s cg22022, cg22022 <=s 3367617@32,
(-3367617)@32 <=s cg22122, cg22122 <=s 3367617@32,
(-3367617)@32 <=s cg22222, cg22222 <=s 3367617@32,
(-3367617)@32 <=s cg22023, cg22023 <=s 3367617@32,
(-3367617)@32 <=s cg22123, cg22123 <=s 3367617@32,
(-3367617)@32 <=s cg22223, cg22223 <=s 3367617@32,
(-3367617)@32 <=s cg22024, cg22024 <=s 3367617@32,
(-3367617)@32 <=s cg22124, cg22124 <=s 3367617@32,
(-3367617)@32 <=s cg22224, cg22224 <=s 3367617@32,
(-3367617)@32 <=s cg22025, cg22025 <=s 3367617@32,
(-3367617)@32 <=s cg22125, cg22125 <=s 3367617@32,
(-3367617)@32 <=s cg22225, cg22225 <=s 3367617@32,
(-3367617)@32 <=s cg22026, cg22026 <=s 3367617@32,
(-3367617)@32 <=s cg22126, cg22126 <=s 3367617@32,
(-3367617)@32 <=s cg22226, cg22226 <=s 3367617@32,
(-3367617)@32 <=s cg22027, cg22027 <=s 3367617@32,
(-3367617)@32 <=s cg22127, cg22127 <=s 3367617@32,
(-3367617)@32 <=s cg22227, cg22227 <=s 3367617@32,
(-3367617)@32 <=s cg22028, cg22028 <=s 3367617@32,
(-3367617)@32 <=s cg22128, cg22128 <=s 3367617@32,
(-3367617)@32 <=s cg22228, cg22228 <=s 3367617@32,
(-3367617)@32 <=s cg22029, cg22029 <=s 3367617@32,
(-3367617)@32 <=s cg22129, cg22129 <=s 3367617@32,
(-3367617)@32 <=s cg22229, cg22229 <=s 3367617@32,
(-3367617)@32 <=s cg22030, cg22030 <=s 3367617@32,
(-3367617)@32 <=s cg22130, cg22130 <=s 3367617@32,
(-3367617)@32 <=s cg22230, cg22230 <=s 3367617@32,
(-3367617)@32 <=s cg22031, cg22031 <=s 3367617@32,
(-3367617)@32 <=s cg22131, cg22131 <=s 3367617@32,
(-3367617)@32 <=s cg22231, cg22231 <=s 3367617@32
,
(-3367617)@32 <=s cg30000, cg30000 <=s 3367617@32,
(-3367617)@32 <=s cg30100, cg30100 <=s 3367617@32,
(-3367617)@32 <=s cg30200, cg30200 <=s 3367617@32,
(-3367617)@32 <=s cg30001, cg30001 <=s 3367617@32,
(-3367617)@32 <=s cg30101, cg30101 <=s 3367617@32,
(-3367617)@32 <=s cg30201, cg30201 <=s 3367617@32,
(-3367617)@32 <=s cg30002, cg30002 <=s 3367617@32,
(-3367617)@32 <=s cg30102, cg30102 <=s 3367617@32,
(-3367617)@32 <=s cg30202, cg30202 <=s 3367617@32,
(-3367617)@32 <=s cg30003, cg30003 <=s 3367617@32,
(-3367617)@32 <=s cg30103, cg30103 <=s 3367617@32,
(-3367617)@32 <=s cg30203, cg30203 <=s 3367617@32,
(-3367617)@32 <=s cg30004, cg30004 <=s 3367617@32,
(-3367617)@32 <=s cg30104, cg30104 <=s 3367617@32,
(-3367617)@32 <=s cg30204, cg30204 <=s 3367617@32,
(-3367617)@32 <=s cg30005, cg30005 <=s 3367617@32,
(-3367617)@32 <=s cg30105, cg30105 <=s 3367617@32,
(-3367617)@32 <=s cg30205, cg30205 <=s 3367617@32,
(-3367617)@32 <=s cg30006, cg30006 <=s 3367617@32,
(-3367617)@32 <=s cg30106, cg30106 <=s 3367617@32,
(-3367617)@32 <=s cg30206, cg30206 <=s 3367617@32,
(-3367617)@32 <=s cg30007, cg30007 <=s 3367617@32,
(-3367617)@32 <=s cg30107, cg30107 <=s 3367617@32,
(-3367617)@32 <=s cg30207, cg30207 <=s 3367617@32,
(-3367617)@32 <=s cg30008, cg30008 <=s 3367617@32,
(-3367617)@32 <=s cg30108, cg30108 <=s 3367617@32,
(-3367617)@32 <=s cg30208, cg30208 <=s 3367617@32,
(-3367617)@32 <=s cg30009, cg30009 <=s 3367617@32,
(-3367617)@32 <=s cg30109, cg30109 <=s 3367617@32,
(-3367617)@32 <=s cg30209, cg30209 <=s 3367617@32,
(-3367617)@32 <=s cg30010, cg30010 <=s 3367617@32,
(-3367617)@32 <=s cg30110, cg30110 <=s 3367617@32,
(-3367617)@32 <=s cg30210, cg30210 <=s 3367617@32,
(-3367617)@32 <=s cg30011, cg30011 <=s 3367617@32,
(-3367617)@32 <=s cg30111, cg30111 <=s 3367617@32,
(-3367617)@32 <=s cg30211, cg30211 <=s 3367617@32,
(-3367617)@32 <=s cg30012, cg30012 <=s 3367617@32,
(-3367617)@32 <=s cg30112, cg30112 <=s 3367617@32,
(-3367617)@32 <=s cg30212, cg30212 <=s 3367617@32,
(-3367617)@32 <=s cg30013, cg30013 <=s 3367617@32,
(-3367617)@32 <=s cg30113, cg30113 <=s 3367617@32,
(-3367617)@32 <=s cg30213, cg30213 <=s 3367617@32,
(-3367617)@32 <=s cg30014, cg30014 <=s 3367617@32,
(-3367617)@32 <=s cg30114, cg30114 <=s 3367617@32,
(-3367617)@32 <=s cg30214, cg30214 <=s 3367617@32,
(-3367617)@32 <=s cg30015, cg30015 <=s 3367617@32,
(-3367617)@32 <=s cg30115, cg30115 <=s 3367617@32,
(-3367617)@32 <=s cg30215, cg30215 <=s 3367617@32,
(-3367617)@32 <=s cg30016, cg30016 <=s 3367617@32,
(-3367617)@32 <=s cg30116, cg30116 <=s 3367617@32,
(-3367617)@32 <=s cg30216, cg30216 <=s 3367617@32,
(-3367617)@32 <=s cg30017, cg30017 <=s 3367617@32,
(-3367617)@32 <=s cg30117, cg30117 <=s 3367617@32,
(-3367617)@32 <=s cg30217, cg30217 <=s 3367617@32,
(-3367617)@32 <=s cg30018, cg30018 <=s 3367617@32,
(-3367617)@32 <=s cg30118, cg30118 <=s 3367617@32,
(-3367617)@32 <=s cg30218, cg30218 <=s 3367617@32,
(-3367617)@32 <=s cg30019, cg30019 <=s 3367617@32,
(-3367617)@32 <=s cg30119, cg30119 <=s 3367617@32,
(-3367617)@32 <=s cg30219, cg30219 <=s 3367617@32,
(-3367617)@32 <=s cg30020, cg30020 <=s 3367617@32,
(-3367617)@32 <=s cg30120, cg30120 <=s 3367617@32,
(-3367617)@32 <=s cg30220, cg30220 <=s 3367617@32,
(-3367617)@32 <=s cg30021, cg30021 <=s 3367617@32,
(-3367617)@32 <=s cg30121, cg30121 <=s 3367617@32,
(-3367617)@32 <=s cg30221, cg30221 <=s 3367617@32,
(-3367617)@32 <=s cg30022, cg30022 <=s 3367617@32,
(-3367617)@32 <=s cg30122, cg30122 <=s 3367617@32,
(-3367617)@32 <=s cg30222, cg30222 <=s 3367617@32,
(-3367617)@32 <=s cg30023, cg30023 <=s 3367617@32,
(-3367617)@32 <=s cg30123, cg30123 <=s 3367617@32,
(-3367617)@32 <=s cg30223, cg30223 <=s 3367617@32,
(-3367617)@32 <=s cg30024, cg30024 <=s 3367617@32,
(-3367617)@32 <=s cg30124, cg30124 <=s 3367617@32,
(-3367617)@32 <=s cg30224, cg30224 <=s 3367617@32,
(-3367617)@32 <=s cg30025, cg30025 <=s 3367617@32,
(-3367617)@32 <=s cg30125, cg30125 <=s 3367617@32,
(-3367617)@32 <=s cg30225, cg30225 <=s 3367617@32,
(-3367617)@32 <=s cg30026, cg30026 <=s 3367617@32,
(-3367617)@32 <=s cg30126, cg30126 <=s 3367617@32,
(-3367617)@32 <=s cg30226, cg30226 <=s 3367617@32,
(-3367617)@32 <=s cg30027, cg30027 <=s 3367617@32,
(-3367617)@32 <=s cg30127, cg30127 <=s 3367617@32,
(-3367617)@32 <=s cg30227, cg30227 <=s 3367617@32,
(-3367617)@32 <=s cg30028, cg30028 <=s 3367617@32,
(-3367617)@32 <=s cg30128, cg30128 <=s 3367617@32,
(-3367617)@32 <=s cg30228, cg30228 <=s 3367617@32,
(-3367617)@32 <=s cg30029, cg30029 <=s 3367617@32,
(-3367617)@32 <=s cg30129, cg30129 <=s 3367617@32,
(-3367617)@32 <=s cg30229, cg30229 <=s 3367617@32,
(-3367617)@32 <=s cg30030, cg30030 <=s 3367617@32,
(-3367617)@32 <=s cg30130, cg30130 <=s 3367617@32,
(-3367617)@32 <=s cg30230, cg30230 <=s 3367617@32,
(-3367617)@32 <=s cg30031, cg30031 <=s 3367617@32,
(-3367617)@32 <=s cg30131, cg30131 <=s 3367617@32,
(-3367617)@32 <=s cg30231, cg30231 <=s 3367617@32,
(-3367617)@32 <=s cg31000, cg31000 <=s 3367617@32,
(-3367617)@32 <=s cg31100, cg31100 <=s 3367617@32,
(-3367617)@32 <=s cg31200, cg31200 <=s 3367617@32,
(-3367617)@32 <=s cg31001, cg31001 <=s 3367617@32,
(-3367617)@32 <=s cg31101, cg31101 <=s 3367617@32,
(-3367617)@32 <=s cg31201, cg31201 <=s 3367617@32,
(-3367617)@32 <=s cg31002, cg31002 <=s 3367617@32,
(-3367617)@32 <=s cg31102, cg31102 <=s 3367617@32,
(-3367617)@32 <=s cg31202, cg31202 <=s 3367617@32,
(-3367617)@32 <=s cg31003, cg31003 <=s 3367617@32,
(-3367617)@32 <=s cg31103, cg31103 <=s 3367617@32,
(-3367617)@32 <=s cg31203, cg31203 <=s 3367617@32,
(-3367617)@32 <=s cg31004, cg31004 <=s 3367617@32,
(-3367617)@32 <=s cg31104, cg31104 <=s 3367617@32,
(-3367617)@32 <=s cg31204, cg31204 <=s 3367617@32,
(-3367617)@32 <=s cg31005, cg31005 <=s 3367617@32,
(-3367617)@32 <=s cg31105, cg31105 <=s 3367617@32,
(-3367617)@32 <=s cg31205, cg31205 <=s 3367617@32,
(-3367617)@32 <=s cg31006, cg31006 <=s 3367617@32,
(-3367617)@32 <=s cg31106, cg31106 <=s 3367617@32,
(-3367617)@32 <=s cg31206, cg31206 <=s 3367617@32,
(-3367617)@32 <=s cg31007, cg31007 <=s 3367617@32,
(-3367617)@32 <=s cg31107, cg31107 <=s 3367617@32,
(-3367617)@32 <=s cg31207, cg31207 <=s 3367617@32,
(-3367617)@32 <=s cg31008, cg31008 <=s 3367617@32,
(-3367617)@32 <=s cg31108, cg31108 <=s 3367617@32,
(-3367617)@32 <=s cg31208, cg31208 <=s 3367617@32,
(-3367617)@32 <=s cg31009, cg31009 <=s 3367617@32,
(-3367617)@32 <=s cg31109, cg31109 <=s 3367617@32,
(-3367617)@32 <=s cg31209, cg31209 <=s 3367617@32,
(-3367617)@32 <=s cg31010, cg31010 <=s 3367617@32,
(-3367617)@32 <=s cg31110, cg31110 <=s 3367617@32,
(-3367617)@32 <=s cg31210, cg31210 <=s 3367617@32,
(-3367617)@32 <=s cg31011, cg31011 <=s 3367617@32,
(-3367617)@32 <=s cg31111, cg31111 <=s 3367617@32,
(-3367617)@32 <=s cg31211, cg31211 <=s 3367617@32,
(-3367617)@32 <=s cg31012, cg31012 <=s 3367617@32,
(-3367617)@32 <=s cg31112, cg31112 <=s 3367617@32,
(-3367617)@32 <=s cg31212, cg31212 <=s 3367617@32,
(-3367617)@32 <=s cg31013, cg31013 <=s 3367617@32,
(-3367617)@32 <=s cg31113, cg31113 <=s 3367617@32,
(-3367617)@32 <=s cg31213, cg31213 <=s 3367617@32,
(-3367617)@32 <=s cg31014, cg31014 <=s 3367617@32,
(-3367617)@32 <=s cg31114, cg31114 <=s 3367617@32,
(-3367617)@32 <=s cg31214, cg31214 <=s 3367617@32,
(-3367617)@32 <=s cg31015, cg31015 <=s 3367617@32,
(-3367617)@32 <=s cg31115, cg31115 <=s 3367617@32,
(-3367617)@32 <=s cg31215, cg31215 <=s 3367617@32,
(-3367617)@32 <=s cg31016, cg31016 <=s 3367617@32,
(-3367617)@32 <=s cg31116, cg31116 <=s 3367617@32,
(-3367617)@32 <=s cg31216, cg31216 <=s 3367617@32,
(-3367617)@32 <=s cg31017, cg31017 <=s 3367617@32,
(-3367617)@32 <=s cg31117, cg31117 <=s 3367617@32,
(-3367617)@32 <=s cg31217, cg31217 <=s 3367617@32,
(-3367617)@32 <=s cg31018, cg31018 <=s 3367617@32,
(-3367617)@32 <=s cg31118, cg31118 <=s 3367617@32,
(-3367617)@32 <=s cg31218, cg31218 <=s 3367617@32,
(-3367617)@32 <=s cg31019, cg31019 <=s 3367617@32,
(-3367617)@32 <=s cg31119, cg31119 <=s 3367617@32,
(-3367617)@32 <=s cg31219, cg31219 <=s 3367617@32,
(-3367617)@32 <=s cg31020, cg31020 <=s 3367617@32,
(-3367617)@32 <=s cg31120, cg31120 <=s 3367617@32,
(-3367617)@32 <=s cg31220, cg31220 <=s 3367617@32,
(-3367617)@32 <=s cg31021, cg31021 <=s 3367617@32,
(-3367617)@32 <=s cg31121, cg31121 <=s 3367617@32,
(-3367617)@32 <=s cg31221, cg31221 <=s 3367617@32,
(-3367617)@32 <=s cg31022, cg31022 <=s 3367617@32,
(-3367617)@32 <=s cg31122, cg31122 <=s 3367617@32,
(-3367617)@32 <=s cg31222, cg31222 <=s 3367617@32,
(-3367617)@32 <=s cg31023, cg31023 <=s 3367617@32,
(-3367617)@32 <=s cg31123, cg31123 <=s 3367617@32,
(-3367617)@32 <=s cg31223, cg31223 <=s 3367617@32,
(-3367617)@32 <=s cg31024, cg31024 <=s 3367617@32,
(-3367617)@32 <=s cg31124, cg31124 <=s 3367617@32,
(-3367617)@32 <=s cg31224, cg31224 <=s 3367617@32,
(-3367617)@32 <=s cg31025, cg31025 <=s 3367617@32,
(-3367617)@32 <=s cg31125, cg31125 <=s 3367617@32,
(-3367617)@32 <=s cg31225, cg31225 <=s 3367617@32,
(-3367617)@32 <=s cg31026, cg31026 <=s 3367617@32,
(-3367617)@32 <=s cg31126, cg31126 <=s 3367617@32,
(-3367617)@32 <=s cg31226, cg31226 <=s 3367617@32,
(-3367617)@32 <=s cg31027, cg31027 <=s 3367617@32,
(-3367617)@32 <=s cg31127, cg31127 <=s 3367617@32,
(-3367617)@32 <=s cg31227, cg31227 <=s 3367617@32,
(-3367617)@32 <=s cg31028, cg31028 <=s 3367617@32,
(-3367617)@32 <=s cg31128, cg31128 <=s 3367617@32,
(-3367617)@32 <=s cg31228, cg31228 <=s 3367617@32,
(-3367617)@32 <=s cg31029, cg31029 <=s 3367617@32,
(-3367617)@32 <=s cg31129, cg31129 <=s 3367617@32,
(-3367617)@32 <=s cg31229, cg31229 <=s 3367617@32,
(-3367617)@32 <=s cg31030, cg31030 <=s 3367617@32,
(-3367617)@32 <=s cg31130, cg31130 <=s 3367617@32,
(-3367617)@32 <=s cg31230, cg31230 <=s 3367617@32,
(-3367617)@32 <=s cg31031, cg31031 <=s 3367617@32,
(-3367617)@32 <=s cg31131, cg31131 <=s 3367617@32,
(-3367617)@32 <=s cg31231, cg31231 <=s 3367617@32,
(-3367617)@32 <=s cg32000, cg32000 <=s 3367617@32,
(-3367617)@32 <=s cg32100, cg32100 <=s 3367617@32,
(-3367617)@32 <=s cg32200, cg32200 <=s 3367617@32,
(-3367617)@32 <=s cg32001, cg32001 <=s 3367617@32,
(-3367617)@32 <=s cg32101, cg32101 <=s 3367617@32,
(-3367617)@32 <=s cg32201, cg32201 <=s 3367617@32,
(-3367617)@32 <=s cg32002, cg32002 <=s 3367617@32,
(-3367617)@32 <=s cg32102, cg32102 <=s 3367617@32,
(-3367617)@32 <=s cg32202, cg32202 <=s 3367617@32,
(-3367617)@32 <=s cg32003, cg32003 <=s 3367617@32,
(-3367617)@32 <=s cg32103, cg32103 <=s 3367617@32,
(-3367617)@32 <=s cg32203, cg32203 <=s 3367617@32,
(-3367617)@32 <=s cg32004, cg32004 <=s 3367617@32,
(-3367617)@32 <=s cg32104, cg32104 <=s 3367617@32,
(-3367617)@32 <=s cg32204, cg32204 <=s 3367617@32,
(-3367617)@32 <=s cg32005, cg32005 <=s 3367617@32,
(-3367617)@32 <=s cg32105, cg32105 <=s 3367617@32,
(-3367617)@32 <=s cg32205, cg32205 <=s 3367617@32,
(-3367617)@32 <=s cg32006, cg32006 <=s 3367617@32,
(-3367617)@32 <=s cg32106, cg32106 <=s 3367617@32,
(-3367617)@32 <=s cg32206, cg32206 <=s 3367617@32,
(-3367617)@32 <=s cg32007, cg32007 <=s 3367617@32,
(-3367617)@32 <=s cg32107, cg32107 <=s 3367617@32,
(-3367617)@32 <=s cg32207, cg32207 <=s 3367617@32,
(-3367617)@32 <=s cg32008, cg32008 <=s 3367617@32,
(-3367617)@32 <=s cg32108, cg32108 <=s 3367617@32,
(-3367617)@32 <=s cg32208, cg32208 <=s 3367617@32,
(-3367617)@32 <=s cg32009, cg32009 <=s 3367617@32,
(-3367617)@32 <=s cg32109, cg32109 <=s 3367617@32,
(-3367617)@32 <=s cg32209, cg32209 <=s 3367617@32,
(-3367617)@32 <=s cg32010, cg32010 <=s 3367617@32,
(-3367617)@32 <=s cg32110, cg32110 <=s 3367617@32,
(-3367617)@32 <=s cg32210, cg32210 <=s 3367617@32,
(-3367617)@32 <=s cg32011, cg32011 <=s 3367617@32,
(-3367617)@32 <=s cg32111, cg32111 <=s 3367617@32,
(-3367617)@32 <=s cg32211, cg32211 <=s 3367617@32,
(-3367617)@32 <=s cg32012, cg32012 <=s 3367617@32,
(-3367617)@32 <=s cg32112, cg32112 <=s 3367617@32,
(-3367617)@32 <=s cg32212, cg32212 <=s 3367617@32,
(-3367617)@32 <=s cg32013, cg32013 <=s 3367617@32,
(-3367617)@32 <=s cg32113, cg32113 <=s 3367617@32,
(-3367617)@32 <=s cg32213, cg32213 <=s 3367617@32,
(-3367617)@32 <=s cg32014, cg32014 <=s 3367617@32,
(-3367617)@32 <=s cg32114, cg32114 <=s 3367617@32,
(-3367617)@32 <=s cg32214, cg32214 <=s 3367617@32,
(-3367617)@32 <=s cg32015, cg32015 <=s 3367617@32,
(-3367617)@32 <=s cg32115, cg32115 <=s 3367617@32,
(-3367617)@32 <=s cg32215, cg32215 <=s 3367617@32,
(-3367617)@32 <=s cg32016, cg32016 <=s 3367617@32,
(-3367617)@32 <=s cg32116, cg32116 <=s 3367617@32,
(-3367617)@32 <=s cg32216, cg32216 <=s 3367617@32,
(-3367617)@32 <=s cg32017, cg32017 <=s 3367617@32,
(-3367617)@32 <=s cg32117, cg32117 <=s 3367617@32,
(-3367617)@32 <=s cg32217, cg32217 <=s 3367617@32,
(-3367617)@32 <=s cg32018, cg32018 <=s 3367617@32,
(-3367617)@32 <=s cg32118, cg32118 <=s 3367617@32,
(-3367617)@32 <=s cg32218, cg32218 <=s 3367617@32,
(-3367617)@32 <=s cg32019, cg32019 <=s 3367617@32,
(-3367617)@32 <=s cg32119, cg32119 <=s 3367617@32,
(-3367617)@32 <=s cg32219, cg32219 <=s 3367617@32,
(-3367617)@32 <=s cg32020, cg32020 <=s 3367617@32,
(-3367617)@32 <=s cg32120, cg32120 <=s 3367617@32,
(-3367617)@32 <=s cg32220, cg32220 <=s 3367617@32,
(-3367617)@32 <=s cg32021, cg32021 <=s 3367617@32,
(-3367617)@32 <=s cg32121, cg32121 <=s 3367617@32,
(-3367617)@32 <=s cg32221, cg32221 <=s 3367617@32,
(-3367617)@32 <=s cg32022, cg32022 <=s 3367617@32,
(-3367617)@32 <=s cg32122, cg32122 <=s 3367617@32,
(-3367617)@32 <=s cg32222, cg32222 <=s 3367617@32,
(-3367617)@32 <=s cg32023, cg32023 <=s 3367617@32,
(-3367617)@32 <=s cg32123, cg32123 <=s 3367617@32,
(-3367617)@32 <=s cg32223, cg32223 <=s 3367617@32,
(-3367617)@32 <=s cg32024, cg32024 <=s 3367617@32,
(-3367617)@32 <=s cg32124, cg32124 <=s 3367617@32,
(-3367617)@32 <=s cg32224, cg32224 <=s 3367617@32,
(-3367617)@32 <=s cg32025, cg32025 <=s 3367617@32,
(-3367617)@32 <=s cg32125, cg32125 <=s 3367617@32,
(-3367617)@32 <=s cg32225, cg32225 <=s 3367617@32,
(-3367617)@32 <=s cg32026, cg32026 <=s 3367617@32,
(-3367617)@32 <=s cg32126, cg32126 <=s 3367617@32,
(-3367617)@32 <=s cg32226, cg32226 <=s 3367617@32,
(-3367617)@32 <=s cg32027, cg32027 <=s 3367617@32,
(-3367617)@32 <=s cg32127, cg32127 <=s 3367617@32,
(-3367617)@32 <=s cg32227, cg32227 <=s 3367617@32,
(-3367617)@32 <=s cg32028, cg32028 <=s 3367617@32,
(-3367617)@32 <=s cg32128, cg32128 <=s 3367617@32,
(-3367617)@32 <=s cg32228, cg32228 <=s 3367617@32,
(-3367617)@32 <=s cg32029, cg32029 <=s 3367617@32,
(-3367617)@32 <=s cg32129, cg32129 <=s 3367617@32,
(-3367617)@32 <=s cg32229, cg32229 <=s 3367617@32,
(-3367617)@32 <=s cg32030, cg32030 <=s 3367617@32,
(-3367617)@32 <=s cg32130, cg32130 <=s 3367617@32,
(-3367617)@32 <=s cg32230, cg32230 <=s 3367617@32,
(-3367617)@32 <=s cg32031, cg32031 <=s 3367617@32,
(-3367617)@32 <=s cg32131, cg32131 <=s 3367617@32,
(-3367617)@32 <=s cg32231, cg32231 <=s 3367617@32
,
(-3367617)@32 <=s cg40000, cg40000 <=s 3367617@32,
(-3367617)@32 <=s cg40100, cg40100 <=s 3367617@32,
(-3367617)@32 <=s cg40200, cg40200 <=s 3367617@32,
(-3367617)@32 <=s cg40001, cg40001 <=s 3367617@32,
(-3367617)@32 <=s cg40101, cg40101 <=s 3367617@32,
(-3367617)@32 <=s cg40201, cg40201 <=s 3367617@32,
(-3367617)@32 <=s cg40002, cg40002 <=s 3367617@32,
(-3367617)@32 <=s cg40102, cg40102 <=s 3367617@32,
(-3367617)@32 <=s cg40202, cg40202 <=s 3367617@32,
(-3367617)@32 <=s cg40003, cg40003 <=s 3367617@32,
(-3367617)@32 <=s cg40103, cg40103 <=s 3367617@32,
(-3367617)@32 <=s cg40203, cg40203 <=s 3367617@32,
(-3367617)@32 <=s cg40004, cg40004 <=s 3367617@32,
(-3367617)@32 <=s cg40104, cg40104 <=s 3367617@32,
(-3367617)@32 <=s cg40204, cg40204 <=s 3367617@32,
(-3367617)@32 <=s cg40005, cg40005 <=s 3367617@32,
(-3367617)@32 <=s cg40105, cg40105 <=s 3367617@32,
(-3367617)@32 <=s cg40205, cg40205 <=s 3367617@32,
(-3367617)@32 <=s cg40006, cg40006 <=s 3367617@32,
(-3367617)@32 <=s cg40106, cg40106 <=s 3367617@32,
(-3367617)@32 <=s cg40206, cg40206 <=s 3367617@32,
(-3367617)@32 <=s cg40007, cg40007 <=s 3367617@32,
(-3367617)@32 <=s cg40107, cg40107 <=s 3367617@32,
(-3367617)@32 <=s cg40207, cg40207 <=s 3367617@32,
(-3367617)@32 <=s cg40008, cg40008 <=s 3367617@32,
(-3367617)@32 <=s cg40108, cg40108 <=s 3367617@32,
(-3367617)@32 <=s cg40208, cg40208 <=s 3367617@32,
(-3367617)@32 <=s cg40009, cg40009 <=s 3367617@32,
(-3367617)@32 <=s cg40109, cg40109 <=s 3367617@32,
(-3367617)@32 <=s cg40209, cg40209 <=s 3367617@32,
(-3367617)@32 <=s cg40010, cg40010 <=s 3367617@32,
(-3367617)@32 <=s cg40110, cg40110 <=s 3367617@32,
(-3367617)@32 <=s cg40210, cg40210 <=s 3367617@32,
(-3367617)@32 <=s cg40011, cg40011 <=s 3367617@32,
(-3367617)@32 <=s cg40111, cg40111 <=s 3367617@32,
(-3367617)@32 <=s cg40211, cg40211 <=s 3367617@32,
(-3367617)@32 <=s cg40012, cg40012 <=s 3367617@32,
(-3367617)@32 <=s cg40112, cg40112 <=s 3367617@32,
(-3367617)@32 <=s cg40212, cg40212 <=s 3367617@32,
(-3367617)@32 <=s cg40013, cg40013 <=s 3367617@32,
(-3367617)@32 <=s cg40113, cg40113 <=s 3367617@32,
(-3367617)@32 <=s cg40213, cg40213 <=s 3367617@32,
(-3367617)@32 <=s cg40014, cg40014 <=s 3367617@32,
(-3367617)@32 <=s cg40114, cg40114 <=s 3367617@32,
(-3367617)@32 <=s cg40214, cg40214 <=s 3367617@32,
(-3367617)@32 <=s cg40015, cg40015 <=s 3367617@32,
(-3367617)@32 <=s cg40115, cg40115 <=s 3367617@32,
(-3367617)@32 <=s cg40215, cg40215 <=s 3367617@32,
(-3367617)@32 <=s cg40016, cg40016 <=s 3367617@32,
(-3367617)@32 <=s cg40116, cg40116 <=s 3367617@32,
(-3367617)@32 <=s cg40216, cg40216 <=s 3367617@32,
(-3367617)@32 <=s cg40017, cg40017 <=s 3367617@32,
(-3367617)@32 <=s cg40117, cg40117 <=s 3367617@32,
(-3367617)@32 <=s cg40217, cg40217 <=s 3367617@32,
(-3367617)@32 <=s cg40018, cg40018 <=s 3367617@32,
(-3367617)@32 <=s cg40118, cg40118 <=s 3367617@32,
(-3367617)@32 <=s cg40218, cg40218 <=s 3367617@32,
(-3367617)@32 <=s cg40019, cg40019 <=s 3367617@32,
(-3367617)@32 <=s cg40119, cg40119 <=s 3367617@32,
(-3367617)@32 <=s cg40219, cg40219 <=s 3367617@32,
(-3367617)@32 <=s cg40020, cg40020 <=s 3367617@32,
(-3367617)@32 <=s cg40120, cg40120 <=s 3367617@32,
(-3367617)@32 <=s cg40220, cg40220 <=s 3367617@32,
(-3367617)@32 <=s cg40021, cg40021 <=s 3367617@32,
(-3367617)@32 <=s cg40121, cg40121 <=s 3367617@32,
(-3367617)@32 <=s cg40221, cg40221 <=s 3367617@32,
(-3367617)@32 <=s cg40022, cg40022 <=s 3367617@32,
(-3367617)@32 <=s cg40122, cg40122 <=s 3367617@32,
(-3367617)@32 <=s cg40222, cg40222 <=s 3367617@32,
(-3367617)@32 <=s cg40023, cg40023 <=s 3367617@32,
(-3367617)@32 <=s cg40123, cg40123 <=s 3367617@32,
(-3367617)@32 <=s cg40223, cg40223 <=s 3367617@32,
(-3367617)@32 <=s cg40024, cg40024 <=s 3367617@32,
(-3367617)@32 <=s cg40124, cg40124 <=s 3367617@32,
(-3367617)@32 <=s cg40224, cg40224 <=s 3367617@32,
(-3367617)@32 <=s cg40025, cg40025 <=s 3367617@32,
(-3367617)@32 <=s cg40125, cg40125 <=s 3367617@32,
(-3367617)@32 <=s cg40225, cg40225 <=s 3367617@32,
(-3367617)@32 <=s cg40026, cg40026 <=s 3367617@32,
(-3367617)@32 <=s cg40126, cg40126 <=s 3367617@32,
(-3367617)@32 <=s cg40226, cg40226 <=s 3367617@32,
(-3367617)@32 <=s cg40027, cg40027 <=s 3367617@32,
(-3367617)@32 <=s cg40127, cg40127 <=s 3367617@32,
(-3367617)@32 <=s cg40227, cg40227 <=s 3367617@32,
(-3367617)@32 <=s cg40028, cg40028 <=s 3367617@32,
(-3367617)@32 <=s cg40128, cg40128 <=s 3367617@32,
(-3367617)@32 <=s cg40228, cg40228 <=s 3367617@32,
(-3367617)@32 <=s cg40029, cg40029 <=s 3367617@32,
(-3367617)@32 <=s cg40129, cg40129 <=s 3367617@32,
(-3367617)@32 <=s cg40229, cg40229 <=s 3367617@32,
(-3367617)@32 <=s cg40030, cg40030 <=s 3367617@32,
(-3367617)@32 <=s cg40130, cg40130 <=s 3367617@32,
(-3367617)@32 <=s cg40230, cg40230 <=s 3367617@32,
(-3367617)@32 <=s cg40031, cg40031 <=s 3367617@32,
(-3367617)@32 <=s cg40131, cg40131 <=s 3367617@32,
(-3367617)@32 <=s cg40231, cg40231 <=s 3367617@32,
(-3367617)@32 <=s cg41000, cg41000 <=s 3367617@32,
(-3367617)@32 <=s cg41100, cg41100 <=s 3367617@32,
(-3367617)@32 <=s cg41200, cg41200 <=s 3367617@32,
(-3367617)@32 <=s cg41001, cg41001 <=s 3367617@32,
(-3367617)@32 <=s cg41101, cg41101 <=s 3367617@32,
(-3367617)@32 <=s cg41201, cg41201 <=s 3367617@32,
(-3367617)@32 <=s cg41002, cg41002 <=s 3367617@32,
(-3367617)@32 <=s cg41102, cg41102 <=s 3367617@32,
(-3367617)@32 <=s cg41202, cg41202 <=s 3367617@32,
(-3367617)@32 <=s cg41003, cg41003 <=s 3367617@32,
(-3367617)@32 <=s cg41103, cg41103 <=s 3367617@32,
(-3367617)@32 <=s cg41203, cg41203 <=s 3367617@32,
(-3367617)@32 <=s cg41004, cg41004 <=s 3367617@32,
(-3367617)@32 <=s cg41104, cg41104 <=s 3367617@32,
(-3367617)@32 <=s cg41204, cg41204 <=s 3367617@32,
(-3367617)@32 <=s cg41005, cg41005 <=s 3367617@32,
(-3367617)@32 <=s cg41105, cg41105 <=s 3367617@32,
(-3367617)@32 <=s cg41205, cg41205 <=s 3367617@32,
(-3367617)@32 <=s cg41006, cg41006 <=s 3367617@32,
(-3367617)@32 <=s cg41106, cg41106 <=s 3367617@32,
(-3367617)@32 <=s cg41206, cg41206 <=s 3367617@32,
(-3367617)@32 <=s cg41007, cg41007 <=s 3367617@32,
(-3367617)@32 <=s cg41107, cg41107 <=s 3367617@32,
(-3367617)@32 <=s cg41207, cg41207 <=s 3367617@32,
(-3367617)@32 <=s cg41008, cg41008 <=s 3367617@32,
(-3367617)@32 <=s cg41108, cg41108 <=s 3367617@32,
(-3367617)@32 <=s cg41208, cg41208 <=s 3367617@32,
(-3367617)@32 <=s cg41009, cg41009 <=s 3367617@32,
(-3367617)@32 <=s cg41109, cg41109 <=s 3367617@32,
(-3367617)@32 <=s cg41209, cg41209 <=s 3367617@32,
(-3367617)@32 <=s cg41010, cg41010 <=s 3367617@32,
(-3367617)@32 <=s cg41110, cg41110 <=s 3367617@32,
(-3367617)@32 <=s cg41210, cg41210 <=s 3367617@32,
(-3367617)@32 <=s cg41011, cg41011 <=s 3367617@32,
(-3367617)@32 <=s cg41111, cg41111 <=s 3367617@32,
(-3367617)@32 <=s cg41211, cg41211 <=s 3367617@32,
(-3367617)@32 <=s cg41012, cg41012 <=s 3367617@32,
(-3367617)@32 <=s cg41112, cg41112 <=s 3367617@32,
(-3367617)@32 <=s cg41212, cg41212 <=s 3367617@32,
(-3367617)@32 <=s cg41013, cg41013 <=s 3367617@32,
(-3367617)@32 <=s cg41113, cg41113 <=s 3367617@32,
(-3367617)@32 <=s cg41213, cg41213 <=s 3367617@32,
(-3367617)@32 <=s cg41014, cg41014 <=s 3367617@32,
(-3367617)@32 <=s cg41114, cg41114 <=s 3367617@32,
(-3367617)@32 <=s cg41214, cg41214 <=s 3367617@32,
(-3367617)@32 <=s cg41015, cg41015 <=s 3367617@32,
(-3367617)@32 <=s cg41115, cg41115 <=s 3367617@32,
(-3367617)@32 <=s cg41215, cg41215 <=s 3367617@32,
(-3367617)@32 <=s cg41016, cg41016 <=s 3367617@32,
(-3367617)@32 <=s cg41116, cg41116 <=s 3367617@32,
(-3367617)@32 <=s cg41216, cg41216 <=s 3367617@32,
(-3367617)@32 <=s cg41017, cg41017 <=s 3367617@32,
(-3367617)@32 <=s cg41117, cg41117 <=s 3367617@32,
(-3367617)@32 <=s cg41217, cg41217 <=s 3367617@32,
(-3367617)@32 <=s cg41018, cg41018 <=s 3367617@32,
(-3367617)@32 <=s cg41118, cg41118 <=s 3367617@32,
(-3367617)@32 <=s cg41218, cg41218 <=s 3367617@32,
(-3367617)@32 <=s cg41019, cg41019 <=s 3367617@32,
(-3367617)@32 <=s cg41119, cg41119 <=s 3367617@32,
(-3367617)@32 <=s cg41219, cg41219 <=s 3367617@32,
(-3367617)@32 <=s cg41020, cg41020 <=s 3367617@32,
(-3367617)@32 <=s cg41120, cg41120 <=s 3367617@32,
(-3367617)@32 <=s cg41220, cg41220 <=s 3367617@32,
(-3367617)@32 <=s cg41021, cg41021 <=s 3367617@32,
(-3367617)@32 <=s cg41121, cg41121 <=s 3367617@32,
(-3367617)@32 <=s cg41221, cg41221 <=s 3367617@32,
(-3367617)@32 <=s cg41022, cg41022 <=s 3367617@32,
(-3367617)@32 <=s cg41122, cg41122 <=s 3367617@32,
(-3367617)@32 <=s cg41222, cg41222 <=s 3367617@32,
(-3367617)@32 <=s cg41023, cg41023 <=s 3367617@32,
(-3367617)@32 <=s cg41123, cg41123 <=s 3367617@32,
(-3367617)@32 <=s cg41223, cg41223 <=s 3367617@32,
(-3367617)@32 <=s cg41024, cg41024 <=s 3367617@32,
(-3367617)@32 <=s cg41124, cg41124 <=s 3367617@32,
(-3367617)@32 <=s cg41224, cg41224 <=s 3367617@32,
(-3367617)@32 <=s cg41025, cg41025 <=s 3367617@32,
(-3367617)@32 <=s cg41125, cg41125 <=s 3367617@32,
(-3367617)@32 <=s cg41225, cg41225 <=s 3367617@32,
(-3367617)@32 <=s cg41026, cg41026 <=s 3367617@32,
(-3367617)@32 <=s cg41126, cg41126 <=s 3367617@32,
(-3367617)@32 <=s cg41226, cg41226 <=s 3367617@32,
(-3367617)@32 <=s cg41027, cg41027 <=s 3367617@32,
(-3367617)@32 <=s cg41127, cg41127 <=s 3367617@32,
(-3367617)@32 <=s cg41227, cg41227 <=s 3367617@32,
(-3367617)@32 <=s cg41028, cg41028 <=s 3367617@32,
(-3367617)@32 <=s cg41128, cg41128 <=s 3367617@32,
(-3367617)@32 <=s cg41228, cg41228 <=s 3367617@32,
(-3367617)@32 <=s cg41029, cg41029 <=s 3367617@32,
(-3367617)@32 <=s cg41129, cg41129 <=s 3367617@32,
(-3367617)@32 <=s cg41229, cg41229 <=s 3367617@32,
(-3367617)@32 <=s cg41030, cg41030 <=s 3367617@32,
(-3367617)@32 <=s cg41130, cg41130 <=s 3367617@32,
(-3367617)@32 <=s cg41230, cg41230 <=s 3367617@32,
(-3367617)@32 <=s cg41031, cg41031 <=s 3367617@32,
(-3367617)@32 <=s cg41131, cg41131 <=s 3367617@32,
(-3367617)@32 <=s cg41231, cg41231 <=s 3367617@32,
(-3367617)@32 <=s cg42000, cg42000 <=s 3367617@32,
(-3367617)@32 <=s cg42100, cg42100 <=s 3367617@32,
(-3367617)@32 <=s cg42200, cg42200 <=s 3367617@32,
(-3367617)@32 <=s cg42001, cg42001 <=s 3367617@32,
(-3367617)@32 <=s cg42101, cg42101 <=s 3367617@32,
(-3367617)@32 <=s cg42201, cg42201 <=s 3367617@32,
(-3367617)@32 <=s cg42002, cg42002 <=s 3367617@32,
(-3367617)@32 <=s cg42102, cg42102 <=s 3367617@32,
(-3367617)@32 <=s cg42202, cg42202 <=s 3367617@32,
(-3367617)@32 <=s cg42003, cg42003 <=s 3367617@32,
(-3367617)@32 <=s cg42103, cg42103 <=s 3367617@32,
(-3367617)@32 <=s cg42203, cg42203 <=s 3367617@32,
(-3367617)@32 <=s cg42004, cg42004 <=s 3367617@32,
(-3367617)@32 <=s cg42104, cg42104 <=s 3367617@32,
(-3367617)@32 <=s cg42204, cg42204 <=s 3367617@32,
(-3367617)@32 <=s cg42005, cg42005 <=s 3367617@32,
(-3367617)@32 <=s cg42105, cg42105 <=s 3367617@32,
(-3367617)@32 <=s cg42205, cg42205 <=s 3367617@32,
(-3367617)@32 <=s cg42006, cg42006 <=s 3367617@32,
(-3367617)@32 <=s cg42106, cg42106 <=s 3367617@32,
(-3367617)@32 <=s cg42206, cg42206 <=s 3367617@32,
(-3367617)@32 <=s cg42007, cg42007 <=s 3367617@32,
(-3367617)@32 <=s cg42107, cg42107 <=s 3367617@32,
(-3367617)@32 <=s cg42207, cg42207 <=s 3367617@32,
(-3367617)@32 <=s cg42008, cg42008 <=s 3367617@32,
(-3367617)@32 <=s cg42108, cg42108 <=s 3367617@32,
(-3367617)@32 <=s cg42208, cg42208 <=s 3367617@32,
(-3367617)@32 <=s cg42009, cg42009 <=s 3367617@32,
(-3367617)@32 <=s cg42109, cg42109 <=s 3367617@32,
(-3367617)@32 <=s cg42209, cg42209 <=s 3367617@32,
(-3367617)@32 <=s cg42010, cg42010 <=s 3367617@32,
(-3367617)@32 <=s cg42110, cg42110 <=s 3367617@32,
(-3367617)@32 <=s cg42210, cg42210 <=s 3367617@32,
(-3367617)@32 <=s cg42011, cg42011 <=s 3367617@32,
(-3367617)@32 <=s cg42111, cg42111 <=s 3367617@32,
(-3367617)@32 <=s cg42211, cg42211 <=s 3367617@32,
(-3367617)@32 <=s cg42012, cg42012 <=s 3367617@32,
(-3367617)@32 <=s cg42112, cg42112 <=s 3367617@32,
(-3367617)@32 <=s cg42212, cg42212 <=s 3367617@32,
(-3367617)@32 <=s cg42013, cg42013 <=s 3367617@32,
(-3367617)@32 <=s cg42113, cg42113 <=s 3367617@32,
(-3367617)@32 <=s cg42213, cg42213 <=s 3367617@32,
(-3367617)@32 <=s cg42014, cg42014 <=s 3367617@32,
(-3367617)@32 <=s cg42114, cg42114 <=s 3367617@32,
(-3367617)@32 <=s cg42214, cg42214 <=s 3367617@32,
(-3367617)@32 <=s cg42015, cg42015 <=s 3367617@32,
(-3367617)@32 <=s cg42115, cg42115 <=s 3367617@32,
(-3367617)@32 <=s cg42215, cg42215 <=s 3367617@32,
(-3367617)@32 <=s cg42016, cg42016 <=s 3367617@32,
(-3367617)@32 <=s cg42116, cg42116 <=s 3367617@32,
(-3367617)@32 <=s cg42216, cg42216 <=s 3367617@32,
(-3367617)@32 <=s cg42017, cg42017 <=s 3367617@32,
(-3367617)@32 <=s cg42117, cg42117 <=s 3367617@32,
(-3367617)@32 <=s cg42217, cg42217 <=s 3367617@32,
(-3367617)@32 <=s cg42018, cg42018 <=s 3367617@32,
(-3367617)@32 <=s cg42118, cg42118 <=s 3367617@32,
(-3367617)@32 <=s cg42218, cg42218 <=s 3367617@32,
(-3367617)@32 <=s cg42019, cg42019 <=s 3367617@32,
(-3367617)@32 <=s cg42119, cg42119 <=s 3367617@32,
(-3367617)@32 <=s cg42219, cg42219 <=s 3367617@32,
(-3367617)@32 <=s cg42020, cg42020 <=s 3367617@32,
(-3367617)@32 <=s cg42120, cg42120 <=s 3367617@32,
(-3367617)@32 <=s cg42220, cg42220 <=s 3367617@32,
(-3367617)@32 <=s cg42021, cg42021 <=s 3367617@32,
(-3367617)@32 <=s cg42121, cg42121 <=s 3367617@32,
(-3367617)@32 <=s cg42221, cg42221 <=s 3367617@32,
(-3367617)@32 <=s cg42022, cg42022 <=s 3367617@32,
(-3367617)@32 <=s cg42122, cg42122 <=s 3367617@32,
(-3367617)@32 <=s cg42222, cg42222 <=s 3367617@32,
(-3367617)@32 <=s cg42023, cg42023 <=s 3367617@32,
(-3367617)@32 <=s cg42123, cg42123 <=s 3367617@32,
(-3367617)@32 <=s cg42223, cg42223 <=s 3367617@32,
(-3367617)@32 <=s cg42024, cg42024 <=s 3367617@32,
(-3367617)@32 <=s cg42124, cg42124 <=s 3367617@32,
(-3367617)@32 <=s cg42224, cg42224 <=s 3367617@32,
(-3367617)@32 <=s cg42025, cg42025 <=s 3367617@32,
(-3367617)@32 <=s cg42125, cg42125 <=s 3367617@32,
(-3367617)@32 <=s cg42225, cg42225 <=s 3367617@32,
(-3367617)@32 <=s cg42026, cg42026 <=s 3367617@32,
(-3367617)@32 <=s cg42126, cg42126 <=s 3367617@32,
(-3367617)@32 <=s cg42226, cg42226 <=s 3367617@32,
(-3367617)@32 <=s cg42027, cg42027 <=s 3367617@32,
(-3367617)@32 <=s cg42127, cg42127 <=s 3367617@32,
(-3367617)@32 <=s cg42227, cg42227 <=s 3367617@32,
(-3367617)@32 <=s cg42028, cg42028 <=s 3367617@32,
(-3367617)@32 <=s cg42128, cg42128 <=s 3367617@32,
(-3367617)@32 <=s cg42228, cg42228 <=s 3367617@32,
(-3367617)@32 <=s cg42029, cg42029 <=s 3367617@32,
(-3367617)@32 <=s cg42129, cg42129 <=s 3367617@32,
(-3367617)@32 <=s cg42229, cg42229 <=s 3367617@32,
(-3367617)@32 <=s cg42030, cg42030 <=s 3367617@32,
(-3367617)@32 <=s cg42130, cg42130 <=s 3367617@32,
(-3367617)@32 <=s cg42230, cg42230 <=s 3367617@32,
(-3367617)@32 <=s cg42031, cg42031 <=s 3367617@32,
(-3367617)@32 <=s cg42131, cg42131 <=s 3367617@32,
(-3367617)@32 <=s cg42231, cg42231 <=s 3367617@32
,
(-3367617)@32 <=s cg50000, cg50000 <=s 3367617@32,
(-3367617)@32 <=s cg50100, cg50100 <=s 3367617@32,
(-3367617)@32 <=s cg50200, cg50200 <=s 3367617@32,
(-3367617)@32 <=s cg50001, cg50001 <=s 3367617@32,
(-3367617)@32 <=s cg50101, cg50101 <=s 3367617@32,
(-3367617)@32 <=s cg50201, cg50201 <=s 3367617@32,
(-3367617)@32 <=s cg50002, cg50002 <=s 3367617@32,
(-3367617)@32 <=s cg50102, cg50102 <=s 3367617@32,
(-3367617)@32 <=s cg50202, cg50202 <=s 3367617@32,
(-3367617)@32 <=s cg50003, cg50003 <=s 3367617@32,
(-3367617)@32 <=s cg50103, cg50103 <=s 3367617@32,
(-3367617)@32 <=s cg50203, cg50203 <=s 3367617@32,
(-3367617)@32 <=s cg50004, cg50004 <=s 3367617@32,
(-3367617)@32 <=s cg50104, cg50104 <=s 3367617@32,
(-3367617)@32 <=s cg50204, cg50204 <=s 3367617@32,
(-3367617)@32 <=s cg50005, cg50005 <=s 3367617@32,
(-3367617)@32 <=s cg50105, cg50105 <=s 3367617@32,
(-3367617)@32 <=s cg50205, cg50205 <=s 3367617@32,
(-3367617)@32 <=s cg50006, cg50006 <=s 3367617@32,
(-3367617)@32 <=s cg50106, cg50106 <=s 3367617@32,
(-3367617)@32 <=s cg50206, cg50206 <=s 3367617@32,
(-3367617)@32 <=s cg50007, cg50007 <=s 3367617@32,
(-3367617)@32 <=s cg50107, cg50107 <=s 3367617@32,
(-3367617)@32 <=s cg50207, cg50207 <=s 3367617@32,
(-3367617)@32 <=s cg50008, cg50008 <=s 3367617@32,
(-3367617)@32 <=s cg50108, cg50108 <=s 3367617@32,
(-3367617)@32 <=s cg50208, cg50208 <=s 3367617@32,
(-3367617)@32 <=s cg50009, cg50009 <=s 3367617@32,
(-3367617)@32 <=s cg50109, cg50109 <=s 3367617@32,
(-3367617)@32 <=s cg50209, cg50209 <=s 3367617@32,
(-3367617)@32 <=s cg50010, cg50010 <=s 3367617@32,
(-3367617)@32 <=s cg50110, cg50110 <=s 3367617@32,
(-3367617)@32 <=s cg50210, cg50210 <=s 3367617@32,
(-3367617)@32 <=s cg50011, cg50011 <=s 3367617@32,
(-3367617)@32 <=s cg50111, cg50111 <=s 3367617@32,
(-3367617)@32 <=s cg50211, cg50211 <=s 3367617@32,
(-3367617)@32 <=s cg50012, cg50012 <=s 3367617@32,
(-3367617)@32 <=s cg50112, cg50112 <=s 3367617@32,
(-3367617)@32 <=s cg50212, cg50212 <=s 3367617@32,
(-3367617)@32 <=s cg50013, cg50013 <=s 3367617@32,
(-3367617)@32 <=s cg50113, cg50113 <=s 3367617@32,
(-3367617)@32 <=s cg50213, cg50213 <=s 3367617@32,
(-3367617)@32 <=s cg50014, cg50014 <=s 3367617@32,
(-3367617)@32 <=s cg50114, cg50114 <=s 3367617@32,
(-3367617)@32 <=s cg50214, cg50214 <=s 3367617@32,
(-3367617)@32 <=s cg50015, cg50015 <=s 3367617@32,
(-3367617)@32 <=s cg50115, cg50115 <=s 3367617@32,
(-3367617)@32 <=s cg50215, cg50215 <=s 3367617@32,
(-3367617)@32 <=s cg50016, cg50016 <=s 3367617@32,
(-3367617)@32 <=s cg50116, cg50116 <=s 3367617@32,
(-3367617)@32 <=s cg50216, cg50216 <=s 3367617@32,
(-3367617)@32 <=s cg50017, cg50017 <=s 3367617@32,
(-3367617)@32 <=s cg50117, cg50117 <=s 3367617@32,
(-3367617)@32 <=s cg50217, cg50217 <=s 3367617@32,
(-3367617)@32 <=s cg50018, cg50018 <=s 3367617@32,
(-3367617)@32 <=s cg50118, cg50118 <=s 3367617@32,
(-3367617)@32 <=s cg50218, cg50218 <=s 3367617@32,
(-3367617)@32 <=s cg50019, cg50019 <=s 3367617@32,
(-3367617)@32 <=s cg50119, cg50119 <=s 3367617@32,
(-3367617)@32 <=s cg50219, cg50219 <=s 3367617@32,
(-3367617)@32 <=s cg50020, cg50020 <=s 3367617@32,
(-3367617)@32 <=s cg50120, cg50120 <=s 3367617@32,
(-3367617)@32 <=s cg50220, cg50220 <=s 3367617@32,
(-3367617)@32 <=s cg50021, cg50021 <=s 3367617@32,
(-3367617)@32 <=s cg50121, cg50121 <=s 3367617@32,
(-3367617)@32 <=s cg50221, cg50221 <=s 3367617@32,
(-3367617)@32 <=s cg50022, cg50022 <=s 3367617@32,
(-3367617)@32 <=s cg50122, cg50122 <=s 3367617@32,
(-3367617)@32 <=s cg50222, cg50222 <=s 3367617@32,
(-3367617)@32 <=s cg50023, cg50023 <=s 3367617@32,
(-3367617)@32 <=s cg50123, cg50123 <=s 3367617@32,
(-3367617)@32 <=s cg50223, cg50223 <=s 3367617@32,
(-3367617)@32 <=s cg50024, cg50024 <=s 3367617@32,
(-3367617)@32 <=s cg50124, cg50124 <=s 3367617@32,
(-3367617)@32 <=s cg50224, cg50224 <=s 3367617@32,
(-3367617)@32 <=s cg50025, cg50025 <=s 3367617@32,
(-3367617)@32 <=s cg50125, cg50125 <=s 3367617@32,
(-3367617)@32 <=s cg50225, cg50225 <=s 3367617@32,
(-3367617)@32 <=s cg50026, cg50026 <=s 3367617@32,
(-3367617)@32 <=s cg50126, cg50126 <=s 3367617@32,
(-3367617)@32 <=s cg50226, cg50226 <=s 3367617@32,
(-3367617)@32 <=s cg50027, cg50027 <=s 3367617@32,
(-3367617)@32 <=s cg50127, cg50127 <=s 3367617@32,
(-3367617)@32 <=s cg50227, cg50227 <=s 3367617@32,
(-3367617)@32 <=s cg50028, cg50028 <=s 3367617@32,
(-3367617)@32 <=s cg50128, cg50128 <=s 3367617@32,
(-3367617)@32 <=s cg50228, cg50228 <=s 3367617@32,
(-3367617)@32 <=s cg50029, cg50029 <=s 3367617@32,
(-3367617)@32 <=s cg50129, cg50129 <=s 3367617@32,
(-3367617)@32 <=s cg50229, cg50229 <=s 3367617@32,
(-3367617)@32 <=s cg50030, cg50030 <=s 3367617@32,
(-3367617)@32 <=s cg50130, cg50130 <=s 3367617@32,
(-3367617)@32 <=s cg50230, cg50230 <=s 3367617@32,
(-3367617)@32 <=s cg50031, cg50031 <=s 3367617@32,
(-3367617)@32 <=s cg50131, cg50131 <=s 3367617@32,
(-3367617)@32 <=s cg50231, cg50231 <=s 3367617@32,
(-3367617)@32 <=s cg51000, cg51000 <=s 3367617@32,
(-3367617)@32 <=s cg51100, cg51100 <=s 3367617@32,
(-3367617)@32 <=s cg51200, cg51200 <=s 3367617@32,
(-3367617)@32 <=s cg51001, cg51001 <=s 3367617@32,
(-3367617)@32 <=s cg51101, cg51101 <=s 3367617@32,
(-3367617)@32 <=s cg51201, cg51201 <=s 3367617@32,
(-3367617)@32 <=s cg51002, cg51002 <=s 3367617@32,
(-3367617)@32 <=s cg51102, cg51102 <=s 3367617@32,
(-3367617)@32 <=s cg51202, cg51202 <=s 3367617@32,
(-3367617)@32 <=s cg51003, cg51003 <=s 3367617@32,
(-3367617)@32 <=s cg51103, cg51103 <=s 3367617@32,
(-3367617)@32 <=s cg51203, cg51203 <=s 3367617@32,
(-3367617)@32 <=s cg51004, cg51004 <=s 3367617@32,
(-3367617)@32 <=s cg51104, cg51104 <=s 3367617@32,
(-3367617)@32 <=s cg51204, cg51204 <=s 3367617@32,
(-3367617)@32 <=s cg51005, cg51005 <=s 3367617@32,
(-3367617)@32 <=s cg51105, cg51105 <=s 3367617@32,
(-3367617)@32 <=s cg51205, cg51205 <=s 3367617@32,
(-3367617)@32 <=s cg51006, cg51006 <=s 3367617@32,
(-3367617)@32 <=s cg51106, cg51106 <=s 3367617@32,
(-3367617)@32 <=s cg51206, cg51206 <=s 3367617@32,
(-3367617)@32 <=s cg51007, cg51007 <=s 3367617@32,
(-3367617)@32 <=s cg51107, cg51107 <=s 3367617@32,
(-3367617)@32 <=s cg51207, cg51207 <=s 3367617@32,
(-3367617)@32 <=s cg51008, cg51008 <=s 3367617@32,
(-3367617)@32 <=s cg51108, cg51108 <=s 3367617@32,
(-3367617)@32 <=s cg51208, cg51208 <=s 3367617@32,
(-3367617)@32 <=s cg51009, cg51009 <=s 3367617@32,
(-3367617)@32 <=s cg51109, cg51109 <=s 3367617@32,
(-3367617)@32 <=s cg51209, cg51209 <=s 3367617@32,
(-3367617)@32 <=s cg51010, cg51010 <=s 3367617@32,
(-3367617)@32 <=s cg51110, cg51110 <=s 3367617@32,
(-3367617)@32 <=s cg51210, cg51210 <=s 3367617@32,
(-3367617)@32 <=s cg51011, cg51011 <=s 3367617@32,
(-3367617)@32 <=s cg51111, cg51111 <=s 3367617@32,
(-3367617)@32 <=s cg51211, cg51211 <=s 3367617@32,
(-3367617)@32 <=s cg51012, cg51012 <=s 3367617@32,
(-3367617)@32 <=s cg51112, cg51112 <=s 3367617@32,
(-3367617)@32 <=s cg51212, cg51212 <=s 3367617@32,
(-3367617)@32 <=s cg51013, cg51013 <=s 3367617@32,
(-3367617)@32 <=s cg51113, cg51113 <=s 3367617@32,
(-3367617)@32 <=s cg51213, cg51213 <=s 3367617@32,
(-3367617)@32 <=s cg51014, cg51014 <=s 3367617@32,
(-3367617)@32 <=s cg51114, cg51114 <=s 3367617@32,
(-3367617)@32 <=s cg51214, cg51214 <=s 3367617@32,
(-3367617)@32 <=s cg51015, cg51015 <=s 3367617@32,
(-3367617)@32 <=s cg51115, cg51115 <=s 3367617@32,
(-3367617)@32 <=s cg51215, cg51215 <=s 3367617@32,
(-3367617)@32 <=s cg51016, cg51016 <=s 3367617@32,
(-3367617)@32 <=s cg51116, cg51116 <=s 3367617@32,
(-3367617)@32 <=s cg51216, cg51216 <=s 3367617@32,
(-3367617)@32 <=s cg51017, cg51017 <=s 3367617@32,
(-3367617)@32 <=s cg51117, cg51117 <=s 3367617@32,
(-3367617)@32 <=s cg51217, cg51217 <=s 3367617@32,
(-3367617)@32 <=s cg51018, cg51018 <=s 3367617@32,
(-3367617)@32 <=s cg51118, cg51118 <=s 3367617@32,
(-3367617)@32 <=s cg51218, cg51218 <=s 3367617@32,
(-3367617)@32 <=s cg51019, cg51019 <=s 3367617@32,
(-3367617)@32 <=s cg51119, cg51119 <=s 3367617@32,
(-3367617)@32 <=s cg51219, cg51219 <=s 3367617@32,
(-3367617)@32 <=s cg51020, cg51020 <=s 3367617@32,
(-3367617)@32 <=s cg51120, cg51120 <=s 3367617@32,
(-3367617)@32 <=s cg51220, cg51220 <=s 3367617@32,
(-3367617)@32 <=s cg51021, cg51021 <=s 3367617@32,
(-3367617)@32 <=s cg51121, cg51121 <=s 3367617@32,
(-3367617)@32 <=s cg51221, cg51221 <=s 3367617@32,
(-3367617)@32 <=s cg51022, cg51022 <=s 3367617@32,
(-3367617)@32 <=s cg51122, cg51122 <=s 3367617@32,
(-3367617)@32 <=s cg51222, cg51222 <=s 3367617@32,
(-3367617)@32 <=s cg51023, cg51023 <=s 3367617@32,
(-3367617)@32 <=s cg51123, cg51123 <=s 3367617@32,
(-3367617)@32 <=s cg51223, cg51223 <=s 3367617@32,
(-3367617)@32 <=s cg51024, cg51024 <=s 3367617@32,
(-3367617)@32 <=s cg51124, cg51124 <=s 3367617@32,
(-3367617)@32 <=s cg51224, cg51224 <=s 3367617@32,
(-3367617)@32 <=s cg51025, cg51025 <=s 3367617@32,
(-3367617)@32 <=s cg51125, cg51125 <=s 3367617@32,
(-3367617)@32 <=s cg51225, cg51225 <=s 3367617@32,
(-3367617)@32 <=s cg51026, cg51026 <=s 3367617@32,
(-3367617)@32 <=s cg51126, cg51126 <=s 3367617@32,
(-3367617)@32 <=s cg51226, cg51226 <=s 3367617@32,
(-3367617)@32 <=s cg51027, cg51027 <=s 3367617@32,
(-3367617)@32 <=s cg51127, cg51127 <=s 3367617@32,
(-3367617)@32 <=s cg51227, cg51227 <=s 3367617@32,
(-3367617)@32 <=s cg51028, cg51028 <=s 3367617@32,
(-3367617)@32 <=s cg51128, cg51128 <=s 3367617@32,
(-3367617)@32 <=s cg51228, cg51228 <=s 3367617@32,
(-3367617)@32 <=s cg51029, cg51029 <=s 3367617@32,
(-3367617)@32 <=s cg51129, cg51129 <=s 3367617@32,
(-3367617)@32 <=s cg51229, cg51229 <=s 3367617@32,
(-3367617)@32 <=s cg51030, cg51030 <=s 3367617@32,
(-3367617)@32 <=s cg51130, cg51130 <=s 3367617@32,
(-3367617)@32 <=s cg51230, cg51230 <=s 3367617@32,
(-3367617)@32 <=s cg51031, cg51031 <=s 3367617@32,
(-3367617)@32 <=s cg51131, cg51131 <=s 3367617@32,
(-3367617)@32 <=s cg51231, cg51231 <=s 3367617@32,
(-3367617)@32 <=s cg52000, cg52000 <=s 3367617@32,
(-3367617)@32 <=s cg52100, cg52100 <=s 3367617@32,
(-3367617)@32 <=s cg52200, cg52200 <=s 3367617@32,
(-3367617)@32 <=s cg52001, cg52001 <=s 3367617@32,
(-3367617)@32 <=s cg52101, cg52101 <=s 3367617@32,
(-3367617)@32 <=s cg52201, cg52201 <=s 3367617@32,
(-3367617)@32 <=s cg52002, cg52002 <=s 3367617@32,
(-3367617)@32 <=s cg52102, cg52102 <=s 3367617@32,
(-3367617)@32 <=s cg52202, cg52202 <=s 3367617@32,
(-3367617)@32 <=s cg52003, cg52003 <=s 3367617@32,
(-3367617)@32 <=s cg52103, cg52103 <=s 3367617@32,
(-3367617)@32 <=s cg52203, cg52203 <=s 3367617@32,
(-3367617)@32 <=s cg52004, cg52004 <=s 3367617@32,
(-3367617)@32 <=s cg52104, cg52104 <=s 3367617@32,
(-3367617)@32 <=s cg52204, cg52204 <=s 3367617@32,
(-3367617)@32 <=s cg52005, cg52005 <=s 3367617@32,
(-3367617)@32 <=s cg52105, cg52105 <=s 3367617@32,
(-3367617)@32 <=s cg52205, cg52205 <=s 3367617@32,
(-3367617)@32 <=s cg52006, cg52006 <=s 3367617@32,
(-3367617)@32 <=s cg52106, cg52106 <=s 3367617@32,
(-3367617)@32 <=s cg52206, cg52206 <=s 3367617@32,
(-3367617)@32 <=s cg52007, cg52007 <=s 3367617@32,
(-3367617)@32 <=s cg52107, cg52107 <=s 3367617@32,
(-3367617)@32 <=s cg52207, cg52207 <=s 3367617@32,
(-3367617)@32 <=s cg52008, cg52008 <=s 3367617@32,
(-3367617)@32 <=s cg52108, cg52108 <=s 3367617@32,
(-3367617)@32 <=s cg52208, cg52208 <=s 3367617@32,
(-3367617)@32 <=s cg52009, cg52009 <=s 3367617@32,
(-3367617)@32 <=s cg52109, cg52109 <=s 3367617@32,
(-3367617)@32 <=s cg52209, cg52209 <=s 3367617@32,
(-3367617)@32 <=s cg52010, cg52010 <=s 3367617@32,
(-3367617)@32 <=s cg52110, cg52110 <=s 3367617@32,
(-3367617)@32 <=s cg52210, cg52210 <=s 3367617@32,
(-3367617)@32 <=s cg52011, cg52011 <=s 3367617@32,
(-3367617)@32 <=s cg52111, cg52111 <=s 3367617@32,
(-3367617)@32 <=s cg52211, cg52211 <=s 3367617@32,
(-3367617)@32 <=s cg52012, cg52012 <=s 3367617@32,
(-3367617)@32 <=s cg52112, cg52112 <=s 3367617@32,
(-3367617)@32 <=s cg52212, cg52212 <=s 3367617@32,
(-3367617)@32 <=s cg52013, cg52013 <=s 3367617@32,
(-3367617)@32 <=s cg52113, cg52113 <=s 3367617@32,
(-3367617)@32 <=s cg52213, cg52213 <=s 3367617@32,
(-3367617)@32 <=s cg52014, cg52014 <=s 3367617@32,
(-3367617)@32 <=s cg52114, cg52114 <=s 3367617@32,
(-3367617)@32 <=s cg52214, cg52214 <=s 3367617@32,
(-3367617)@32 <=s cg52015, cg52015 <=s 3367617@32,
(-3367617)@32 <=s cg52115, cg52115 <=s 3367617@32,
(-3367617)@32 <=s cg52215, cg52215 <=s 3367617@32,
(-3367617)@32 <=s cg52016, cg52016 <=s 3367617@32,
(-3367617)@32 <=s cg52116, cg52116 <=s 3367617@32,
(-3367617)@32 <=s cg52216, cg52216 <=s 3367617@32,
(-3367617)@32 <=s cg52017, cg52017 <=s 3367617@32,
(-3367617)@32 <=s cg52117, cg52117 <=s 3367617@32,
(-3367617)@32 <=s cg52217, cg52217 <=s 3367617@32,
(-3367617)@32 <=s cg52018, cg52018 <=s 3367617@32,
(-3367617)@32 <=s cg52118, cg52118 <=s 3367617@32,
(-3367617)@32 <=s cg52218, cg52218 <=s 3367617@32,
(-3367617)@32 <=s cg52019, cg52019 <=s 3367617@32,
(-3367617)@32 <=s cg52119, cg52119 <=s 3367617@32,
(-3367617)@32 <=s cg52219, cg52219 <=s 3367617@32,
(-3367617)@32 <=s cg52020, cg52020 <=s 3367617@32,
(-3367617)@32 <=s cg52120, cg52120 <=s 3367617@32,
(-3367617)@32 <=s cg52220, cg52220 <=s 3367617@32,
(-3367617)@32 <=s cg52021, cg52021 <=s 3367617@32,
(-3367617)@32 <=s cg52121, cg52121 <=s 3367617@32,
(-3367617)@32 <=s cg52221, cg52221 <=s 3367617@32,
(-3367617)@32 <=s cg52022, cg52022 <=s 3367617@32,
(-3367617)@32 <=s cg52122, cg52122 <=s 3367617@32,
(-3367617)@32 <=s cg52222, cg52222 <=s 3367617@32,
(-3367617)@32 <=s cg52023, cg52023 <=s 3367617@32,
(-3367617)@32 <=s cg52123, cg52123 <=s 3367617@32,
(-3367617)@32 <=s cg52223, cg52223 <=s 3367617@32,
(-3367617)@32 <=s cg52024, cg52024 <=s 3367617@32,
(-3367617)@32 <=s cg52124, cg52124 <=s 3367617@32,
(-3367617)@32 <=s cg52224, cg52224 <=s 3367617@32,
(-3367617)@32 <=s cg52025, cg52025 <=s 3367617@32,
(-3367617)@32 <=s cg52125, cg52125 <=s 3367617@32,
(-3367617)@32 <=s cg52225, cg52225 <=s 3367617@32,
(-3367617)@32 <=s cg52026, cg52026 <=s 3367617@32,
(-3367617)@32 <=s cg52126, cg52126 <=s 3367617@32,
(-3367617)@32 <=s cg52226, cg52226 <=s 3367617@32,
(-3367617)@32 <=s cg52027, cg52027 <=s 3367617@32,
(-3367617)@32 <=s cg52127, cg52127 <=s 3367617@32,
(-3367617)@32 <=s cg52227, cg52227 <=s 3367617@32,
(-3367617)@32 <=s cg52028, cg52028 <=s 3367617@32,
(-3367617)@32 <=s cg52128, cg52128 <=s 3367617@32,
(-3367617)@32 <=s cg52228, cg52228 <=s 3367617@32,
(-3367617)@32 <=s cg52029, cg52029 <=s 3367617@32,
(-3367617)@32 <=s cg52129, cg52129 <=s 3367617@32,
(-3367617)@32 <=s cg52229, cg52229 <=s 3367617@32,
(-3367617)@32 <=s cg52030, cg52030 <=s 3367617@32,
(-3367617)@32 <=s cg52130, cg52130 <=s 3367617@32,
(-3367617)@32 <=s cg52230, cg52230 <=s 3367617@32,
(-3367617)@32 <=s cg52031, cg52031 <=s 3367617@32,
(-3367617)@32 <=s cg52131, cg52131 <=s 3367617@32,
(-3367617)@32 <=s cg52231, cg52231 <=s 3367617@32
]
}

(******************** initialization ********************)

nondet r0@uint32; nondet lr@uint32;
mov L0x20014898 cg00000; mov L0x2001489c cg01000; mov L0x200148a0 cg02000;
mov L0x200148a4 cg00100; mov L0x200148a8 cg01100; mov L0x200148ac cg02100;
mov L0x200148b0 cg00200; mov L0x200148b4 cg01200; mov L0x200148b8 cg02200;
mov L0x20014904 cg00001; mov L0x20014908 cg01001; mov L0x2001490c cg02001;
mov L0x20014910 cg00101; mov L0x20014914 cg01101; mov L0x20014918 cg02101;
mov L0x2001491c cg00201; mov L0x20014920 cg01201; mov L0x20014924 cg02201;
mov L0x20014970 cg00002; mov L0x20014974 cg01002; mov L0x20014978 cg02002;
mov L0x2001497c cg00102; mov L0x20014980 cg01102; mov L0x20014984 cg02102;
mov L0x20014988 cg00202; mov L0x2001498c cg01202; mov L0x20014990 cg02202;
mov L0x200149dc cg00003; mov L0x200149e0 cg01003; mov L0x200149e4 cg02003;
mov L0x200149e8 cg00103; mov L0x200149ec cg01103; mov L0x200149f0 cg02103;
mov L0x200149f4 cg00203; mov L0x200149f8 cg01203; mov L0x200149fc cg02203;
mov L0x20014a48 cg00004; mov L0x20014a4c cg01004; mov L0x20014a50 cg02004;
mov L0x20014a54 cg00104; mov L0x20014a58 cg01104; mov L0x20014a5c cg02104;
mov L0x20014a60 cg00204; mov L0x20014a64 cg01204; mov L0x20014a68 cg02204;
mov L0x20014ab4 cg00005; mov L0x20014ab8 cg01005; mov L0x20014abc cg02005;
mov L0x20014ac0 cg00105; mov L0x20014ac4 cg01105; mov L0x20014ac8 cg02105;
mov L0x20014acc cg00205; mov L0x20014ad0 cg01205; mov L0x20014ad4 cg02205;
mov L0x20014b20 cg00006; mov L0x20014b24 cg01006; mov L0x20014b28 cg02006;
mov L0x20014b2c cg00106; mov L0x20014b30 cg01106; mov L0x20014b34 cg02106;
mov L0x20014b38 cg00206; mov L0x20014b3c cg01206; mov L0x20014b40 cg02206;
mov L0x20014b8c cg00007; mov L0x20014b90 cg01007; mov L0x20014b94 cg02007;
mov L0x20014b98 cg00107; mov L0x20014b9c cg01107; mov L0x20014ba0 cg02107;
mov L0x20014ba4 cg00207; mov L0x20014ba8 cg01207; mov L0x20014bac cg02207;
mov L0x20014bf8 cg00008; mov L0x20014bfc cg01008; mov L0x20014c00 cg02008;
mov L0x20014c04 cg00108; mov L0x20014c08 cg01108; mov L0x20014c0c cg02108;
mov L0x20014c10 cg00208; mov L0x20014c14 cg01208; mov L0x20014c18 cg02208;
mov L0x20014c64 cg00009; mov L0x20014c68 cg01009; mov L0x20014c6c cg02009;
mov L0x20014c70 cg00109; mov L0x20014c74 cg01109; mov L0x20014c78 cg02109;
mov L0x20014c7c cg00209; mov L0x20014c80 cg01209; mov L0x20014c84 cg02209;
mov L0x20014cd0 cg00010; mov L0x20014cd4 cg01010; mov L0x20014cd8 cg02010;
mov L0x20014cdc cg00110; mov L0x20014ce0 cg01110; mov L0x20014ce4 cg02110;
mov L0x20014ce8 cg00210; mov L0x20014cec cg01210; mov L0x20014cf0 cg02210;
mov L0x20014d3c cg00011; mov L0x20014d40 cg01011; mov L0x20014d44 cg02011;
mov L0x20014d48 cg00111; mov L0x20014d4c cg01111; mov L0x20014d50 cg02111;
mov L0x20014d54 cg00211; mov L0x20014d58 cg01211; mov L0x20014d5c cg02211;
mov L0x20014da8 cg00012; mov L0x20014dac cg01012; mov L0x20014db0 cg02012;
mov L0x20014db4 cg00112; mov L0x20014db8 cg01112; mov L0x20014dbc cg02112;
mov L0x20014dc0 cg00212; mov L0x20014dc4 cg01212; mov L0x20014dc8 cg02212;
mov L0x20014e14 cg00013; mov L0x20014e18 cg01013; mov L0x20014e1c cg02013;
mov L0x20014e20 cg00113; mov L0x20014e24 cg01113; mov L0x20014e28 cg02113;
mov L0x20014e2c cg00213; mov L0x20014e30 cg01213; mov L0x20014e34 cg02213;
mov L0x20014e80 cg00014; mov L0x20014e84 cg01014; mov L0x20014e88 cg02014;
mov L0x20014e8c cg00114; mov L0x20014e90 cg01114; mov L0x20014e94 cg02114;
mov L0x20014e98 cg00214; mov L0x20014e9c cg01214; mov L0x20014ea0 cg02214;
mov L0x20014eec cg00015; mov L0x20014ef0 cg01015; mov L0x20014ef4 cg02015;
mov L0x20014ef8 cg00115; mov L0x20014efc cg01115; mov L0x20014f00 cg02115;
mov L0x20014f04 cg00215; mov L0x20014f08 cg01215; mov L0x20014f0c cg02215;
mov L0x20014f58 cg00016; mov L0x20014f5c cg01016; mov L0x20014f60 cg02016;
mov L0x20014f64 cg00116; mov L0x20014f68 cg01116; mov L0x20014f6c cg02116;
mov L0x20014f70 cg00216; mov L0x20014f74 cg01216; mov L0x20014f78 cg02216;
mov L0x20014fc4 cg00017; mov L0x20014fc8 cg01017; mov L0x20014fcc cg02017;
mov L0x20014fd0 cg00117; mov L0x20014fd4 cg01117; mov L0x20014fd8 cg02117;
mov L0x20014fdc cg00217; mov L0x20014fe0 cg01217; mov L0x20014fe4 cg02217;
mov L0x20015030 cg00018; mov L0x20015034 cg01018; mov L0x20015038 cg02018;
mov L0x2001503c cg00118; mov L0x20015040 cg01118; mov L0x20015044 cg02118;
mov L0x20015048 cg00218; mov L0x2001504c cg01218; mov L0x20015050 cg02218;
mov L0x2001509c cg00019; mov L0x200150a0 cg01019; mov L0x200150a4 cg02019;
mov L0x200150a8 cg00119; mov L0x200150ac cg01119; mov L0x200150b0 cg02119;
mov L0x200150b4 cg00219; mov L0x200150b8 cg01219; mov L0x200150bc cg02219;
mov L0x20015108 cg00020; mov L0x2001510c cg01020; mov L0x20015110 cg02020;
mov L0x20015114 cg00120; mov L0x20015118 cg01120; mov L0x2001511c cg02120;
mov L0x20015120 cg00220; mov L0x20015124 cg01220; mov L0x20015128 cg02220;
mov L0x20015174 cg00021; mov L0x20015178 cg01021; mov L0x2001517c cg02021;
mov L0x20015180 cg00121; mov L0x20015184 cg01121; mov L0x20015188 cg02121;
mov L0x2001518c cg00221; mov L0x20015190 cg01221; mov L0x20015194 cg02221;
mov L0x200151e0 cg00022; mov L0x200151e4 cg01022; mov L0x200151e8 cg02022;
mov L0x200151ec cg00122; mov L0x200151f0 cg01122; mov L0x200151f4 cg02122;
mov L0x200151f8 cg00222; mov L0x200151fc cg01222; mov L0x20015200 cg02222;
mov L0x2001524c cg00023; mov L0x20015250 cg01023; mov L0x20015254 cg02023;
mov L0x20015258 cg00123; mov L0x2001525c cg01123; mov L0x20015260 cg02123;
mov L0x20015264 cg00223; mov L0x20015268 cg01223; mov L0x2001526c cg02223;
mov L0x200152b8 cg00024; mov L0x200152bc cg01024; mov L0x200152c0 cg02024;
mov L0x200152c4 cg00124; mov L0x200152c8 cg01124; mov L0x200152cc cg02124;
mov L0x200152d0 cg00224; mov L0x200152d4 cg01224; mov L0x200152d8 cg02224;
mov L0x20015324 cg00025; mov L0x20015328 cg01025; mov L0x2001532c cg02025;
mov L0x20015330 cg00125; mov L0x20015334 cg01125; mov L0x20015338 cg02125;
mov L0x2001533c cg00225; mov L0x20015340 cg01225; mov L0x20015344 cg02225;
mov L0x20015390 cg00026; mov L0x20015394 cg01026; mov L0x20015398 cg02026;
mov L0x2001539c cg00126; mov L0x200153a0 cg01126; mov L0x200153a4 cg02126;
mov L0x200153a8 cg00226; mov L0x200153ac cg01226; mov L0x200153b0 cg02226;
mov L0x200153fc cg00027; mov L0x20015400 cg01027; mov L0x20015404 cg02027;
mov L0x20015408 cg00127; mov L0x2001540c cg01127; mov L0x20015410 cg02127;
mov L0x20015414 cg00227; mov L0x20015418 cg01227; mov L0x2001541c cg02227;
mov L0x20015468 cg00028; mov L0x2001546c cg01028; mov L0x20015470 cg02028;
mov L0x20015474 cg00128; mov L0x20015478 cg01128; mov L0x2001547c cg02128;
mov L0x20015480 cg00228; mov L0x20015484 cg01228; mov L0x20015488 cg02228;
mov L0x200154d4 cg00029; mov L0x200154d8 cg01029; mov L0x200154dc cg02029;
mov L0x200154e0 cg00129; mov L0x200154e4 cg01129; mov L0x200154e8 cg02129;
mov L0x200154ec cg00229; mov L0x200154f0 cg01229; mov L0x200154f4 cg02229;
mov L0x20015540 cg00030; mov L0x20015544 cg01030; mov L0x20015548 cg02030;
mov L0x2001554c cg00130; mov L0x20015550 cg01130; mov L0x20015554 cg02130;
mov L0x20015558 cg00230; mov L0x2001555c cg01230; mov L0x20015560 cg02230;
mov L0x200155ac cg00031; mov L0x200155b0 cg01031; mov L0x200155b4 cg02031;
mov L0x200155b8 cg00131; mov L0x200155bc cg01131; mov L0x200155c0 cg02131;
mov L0x200155c4 cg00231; mov L0x200155c8 cg01231; mov L0x200155cc cg02231;
mov L0x200148bc cg10000; mov L0x200148c0 cg11000; mov L0x200148c4 cg12000;
mov L0x200148c8 cg10100; mov L0x200148cc cg11100; mov L0x200148d0 cg12100;
mov L0x200148d4 cg10200; mov L0x200148d8 cg11200; mov L0x200148dc cg12200;
mov L0x20014928 cg10001; mov L0x2001492c cg11001; mov L0x20014930 cg12001;
mov L0x20014934 cg10101; mov L0x20014938 cg11101; mov L0x2001493c cg12101;
mov L0x20014940 cg10201; mov L0x20014944 cg11201; mov L0x20014948 cg12201;
mov L0x20014994 cg10002; mov L0x20014998 cg11002; mov L0x2001499c cg12002;
mov L0x200149a0 cg10102; mov L0x200149a4 cg11102; mov L0x200149a8 cg12102;
mov L0x200149ac cg10202; mov L0x200149b0 cg11202; mov L0x200149b4 cg12202;
mov L0x20014a00 cg10003; mov L0x20014a04 cg11003; mov L0x20014a08 cg12003;
mov L0x20014a0c cg10103; mov L0x20014a10 cg11103; mov L0x20014a14 cg12103;
mov L0x20014a18 cg10203; mov L0x20014a1c cg11203; mov L0x20014a20 cg12203;
mov L0x20014a6c cg10004; mov L0x20014a70 cg11004; mov L0x20014a74 cg12004;
mov L0x20014a78 cg10104; mov L0x20014a7c cg11104; mov L0x20014a80 cg12104;
mov L0x20014a84 cg10204; mov L0x20014a88 cg11204; mov L0x20014a8c cg12204;
mov L0x20014ad8 cg10005; mov L0x20014adc cg11005; mov L0x20014ae0 cg12005;
mov L0x20014ae4 cg10105; mov L0x20014ae8 cg11105; mov L0x20014aec cg12105;
mov L0x20014af0 cg10205; mov L0x20014af4 cg11205; mov L0x20014af8 cg12205;
mov L0x20014b44 cg10006; mov L0x20014b48 cg11006; mov L0x20014b4c cg12006;
mov L0x20014b50 cg10106; mov L0x20014b54 cg11106; mov L0x20014b58 cg12106;
mov L0x20014b5c cg10206; mov L0x20014b60 cg11206; mov L0x20014b64 cg12206;
mov L0x20014bb0 cg10007; mov L0x20014bb4 cg11007; mov L0x20014bb8 cg12007;
mov L0x20014bbc cg10107; mov L0x20014bc0 cg11107; mov L0x20014bc4 cg12107;
mov L0x20014bc8 cg10207; mov L0x20014bcc cg11207; mov L0x20014bd0 cg12207;
mov L0x20014c1c cg10008; mov L0x20014c20 cg11008; mov L0x20014c24 cg12008;
mov L0x20014c28 cg10108; mov L0x20014c2c cg11108; mov L0x20014c30 cg12108;
mov L0x20014c34 cg10208; mov L0x20014c38 cg11208; mov L0x20014c3c cg12208;
mov L0x20014c88 cg10009; mov L0x20014c8c cg11009; mov L0x20014c90 cg12009;
mov L0x20014c94 cg10109; mov L0x20014c98 cg11109; mov L0x20014c9c cg12109;
mov L0x20014ca0 cg10209; mov L0x20014ca4 cg11209; mov L0x20014ca8 cg12209;
mov L0x20014cf4 cg10010; mov L0x20014cf8 cg11010; mov L0x20014cfc cg12010;
mov L0x20014d00 cg10110; mov L0x20014d04 cg11110; mov L0x20014d08 cg12110;
mov L0x20014d0c cg10210; mov L0x20014d10 cg11210; mov L0x20014d14 cg12210;
mov L0x20014d60 cg10011; mov L0x20014d64 cg11011; mov L0x20014d68 cg12011;
mov L0x20014d6c cg10111; mov L0x20014d70 cg11111; mov L0x20014d74 cg12111;
mov L0x20014d78 cg10211; mov L0x20014d7c cg11211; mov L0x20014d80 cg12211;
mov L0x20014dcc cg10012; mov L0x20014dd0 cg11012; mov L0x20014dd4 cg12012;
mov L0x20014dd8 cg10112; mov L0x20014ddc cg11112; mov L0x20014de0 cg12112;
mov L0x20014de4 cg10212; mov L0x20014de8 cg11212; mov L0x20014dec cg12212;
mov L0x20014e38 cg10013; mov L0x20014e3c cg11013; mov L0x20014e40 cg12013;
mov L0x20014e44 cg10113; mov L0x20014e48 cg11113; mov L0x20014e4c cg12113;
mov L0x20014e50 cg10213; mov L0x20014e54 cg11213; mov L0x20014e58 cg12213;
mov L0x20014ea4 cg10014; mov L0x20014ea8 cg11014; mov L0x20014eac cg12014;
mov L0x20014eb0 cg10114; mov L0x20014eb4 cg11114; mov L0x20014eb8 cg12114;
mov L0x20014ebc cg10214; mov L0x20014ec0 cg11214; mov L0x20014ec4 cg12214;
mov L0x20014f10 cg10015; mov L0x20014f14 cg11015; mov L0x20014f18 cg12015;
mov L0x20014f1c cg10115; mov L0x20014f20 cg11115; mov L0x20014f24 cg12115;
mov L0x20014f28 cg10215; mov L0x20014f2c cg11215; mov L0x20014f30 cg12215;
mov L0x20014f7c cg10016; mov L0x20014f80 cg11016; mov L0x20014f84 cg12016;
mov L0x20014f88 cg10116; mov L0x20014f8c cg11116; mov L0x20014f90 cg12116;
mov L0x20014f94 cg10216; mov L0x20014f98 cg11216; mov L0x20014f9c cg12216;
mov L0x20014fe8 cg10017; mov L0x20014fec cg11017; mov L0x20014ff0 cg12017;
mov L0x20014ff4 cg10117; mov L0x20014ff8 cg11117; mov L0x20014ffc cg12117;
mov L0x20015000 cg10217; mov L0x20015004 cg11217; mov L0x20015008 cg12217;
mov L0x20015054 cg10018; mov L0x20015058 cg11018; mov L0x2001505c cg12018;
mov L0x20015060 cg10118; mov L0x20015064 cg11118; mov L0x20015068 cg12118;
mov L0x2001506c cg10218; mov L0x20015070 cg11218; mov L0x20015074 cg12218;
mov L0x200150c0 cg10019; mov L0x200150c4 cg11019; mov L0x200150c8 cg12019;
mov L0x200150cc cg10119; mov L0x200150d0 cg11119; mov L0x200150d4 cg12119;
mov L0x200150d8 cg10219; mov L0x200150dc cg11219; mov L0x200150e0 cg12219;
mov L0x2001512c cg10020; mov L0x20015130 cg11020; mov L0x20015134 cg12020;
mov L0x20015138 cg10120; mov L0x2001513c cg11120; mov L0x20015140 cg12120;
mov L0x20015144 cg10220; mov L0x20015148 cg11220; mov L0x2001514c cg12220;
mov L0x20015198 cg10021; mov L0x2001519c cg11021; mov L0x200151a0 cg12021;
mov L0x200151a4 cg10121; mov L0x200151a8 cg11121; mov L0x200151ac cg12121;
mov L0x200151b0 cg10221; mov L0x200151b4 cg11221; mov L0x200151b8 cg12221;
mov L0x20015204 cg10022; mov L0x20015208 cg11022; mov L0x2001520c cg12022;
mov L0x20015210 cg10122; mov L0x20015214 cg11122; mov L0x20015218 cg12122;
mov L0x2001521c cg10222; mov L0x20015220 cg11222; mov L0x20015224 cg12222;
mov L0x20015270 cg10023; mov L0x20015274 cg11023; mov L0x20015278 cg12023;
mov L0x2001527c cg10123; mov L0x20015280 cg11123; mov L0x20015284 cg12123;
mov L0x20015288 cg10223; mov L0x2001528c cg11223; mov L0x20015290 cg12223;
mov L0x200152dc cg10024; mov L0x200152e0 cg11024; mov L0x200152e4 cg12024;
mov L0x200152e8 cg10124; mov L0x200152ec cg11124; mov L0x200152f0 cg12124;
mov L0x200152f4 cg10224; mov L0x200152f8 cg11224; mov L0x200152fc cg12224;
mov L0x20015348 cg10025; mov L0x2001534c cg11025; mov L0x20015350 cg12025;
mov L0x20015354 cg10125; mov L0x20015358 cg11125; mov L0x2001535c cg12125;
mov L0x20015360 cg10225; mov L0x20015364 cg11225; mov L0x20015368 cg12225;
mov L0x200153b4 cg10026; mov L0x200153b8 cg11026; mov L0x200153bc cg12026;
mov L0x200153c0 cg10126; mov L0x200153c4 cg11126; mov L0x200153c8 cg12126;
mov L0x200153cc cg10226; mov L0x200153d0 cg11226; mov L0x200153d4 cg12226;
mov L0x20015420 cg10027; mov L0x20015424 cg11027; mov L0x20015428 cg12027;
mov L0x2001542c cg10127; mov L0x20015430 cg11127; mov L0x20015434 cg12127;
mov L0x20015438 cg10227; mov L0x2001543c cg11227; mov L0x20015440 cg12227;
mov L0x2001548c cg10028; mov L0x20015490 cg11028; mov L0x20015494 cg12028;
mov L0x20015498 cg10128; mov L0x2001549c cg11128; mov L0x200154a0 cg12128;
mov L0x200154a4 cg10228; mov L0x200154a8 cg11228; mov L0x200154ac cg12228;
mov L0x200154f8 cg10029; mov L0x200154fc cg11029; mov L0x20015500 cg12029;
mov L0x20015504 cg10129; mov L0x20015508 cg11129; mov L0x2001550c cg12129;
mov L0x20015510 cg10229; mov L0x20015514 cg11229; mov L0x20015518 cg12229;
mov L0x20015564 cg10030; mov L0x20015568 cg11030; mov L0x2001556c cg12030;
mov L0x20015570 cg10130; mov L0x20015574 cg11130; mov L0x20015578 cg12130;
mov L0x2001557c cg10230; mov L0x20015580 cg11230; mov L0x20015584 cg12230;
mov L0x200155d0 cg10031; mov L0x200155d4 cg11031; mov L0x200155d8 cg12031;
mov L0x200155dc cg10131; mov L0x200155e0 cg11131; mov L0x200155e4 cg12131;
mov L0x200155e8 cg10231; mov L0x200155ec cg11231; mov L0x200155f0 cg12231;
mov L0x200148e0 cg20000; mov L0x200148e4 cg21000; mov L0x200148e8 cg22000;
mov L0x200148ec cg20100; mov L0x200148f0 cg21100; mov L0x200148f4 cg22100;
mov L0x200148f8 cg20200; mov L0x200148fc cg21200; mov L0x20014900 cg22200;
mov L0x2001494c cg20001; mov L0x20014950 cg21001; mov L0x20014954 cg22001;
mov L0x20014958 cg20101; mov L0x2001495c cg21101; mov L0x20014960 cg22101;
mov L0x20014964 cg20201; mov L0x20014968 cg21201; mov L0x2001496c cg22201;
mov L0x200149b8 cg20002; mov L0x200149bc cg21002; mov L0x200149c0 cg22002;
mov L0x200149c4 cg20102; mov L0x200149c8 cg21102; mov L0x200149cc cg22102;
mov L0x200149d0 cg20202; mov L0x200149d4 cg21202; mov L0x200149d8 cg22202;
mov L0x20014a24 cg20003; mov L0x20014a28 cg21003; mov L0x20014a2c cg22003;
mov L0x20014a30 cg20103; mov L0x20014a34 cg21103; mov L0x20014a38 cg22103;
mov L0x20014a3c cg20203; mov L0x20014a40 cg21203; mov L0x20014a44 cg22203;
mov L0x20014a90 cg20004; mov L0x20014a94 cg21004; mov L0x20014a98 cg22004;
mov L0x20014a9c cg20104; mov L0x20014aa0 cg21104; mov L0x20014aa4 cg22104;
mov L0x20014aa8 cg20204; mov L0x20014aac cg21204; mov L0x20014ab0 cg22204;
mov L0x20014afc cg20005; mov L0x20014b00 cg21005; mov L0x20014b04 cg22005;
mov L0x20014b08 cg20105; mov L0x20014b0c cg21105; mov L0x20014b10 cg22105;
mov L0x20014b14 cg20205; mov L0x20014b18 cg21205; mov L0x20014b1c cg22205;
mov L0x20014b68 cg20006; mov L0x20014b6c cg21006; mov L0x20014b70 cg22006;
mov L0x20014b74 cg20106; mov L0x20014b78 cg21106; mov L0x20014b7c cg22106;
mov L0x20014b80 cg20206; mov L0x20014b84 cg21206; mov L0x20014b88 cg22206;
mov L0x20014bd4 cg20007; mov L0x20014bd8 cg21007; mov L0x20014bdc cg22007;
mov L0x20014be0 cg20107; mov L0x20014be4 cg21107; mov L0x20014be8 cg22107;
mov L0x20014bec cg20207; mov L0x20014bf0 cg21207; mov L0x20014bf4 cg22207;
mov L0x20014c40 cg20008; mov L0x20014c44 cg21008; mov L0x20014c48 cg22008;
mov L0x20014c4c cg20108; mov L0x20014c50 cg21108; mov L0x20014c54 cg22108;
mov L0x20014c58 cg20208; mov L0x20014c5c cg21208; mov L0x20014c60 cg22208;
mov L0x20014cac cg20009; mov L0x20014cb0 cg21009; mov L0x20014cb4 cg22009;
mov L0x20014cb8 cg20109; mov L0x20014cbc cg21109; mov L0x20014cc0 cg22109;
mov L0x20014cc4 cg20209; mov L0x20014cc8 cg21209; mov L0x20014ccc cg22209;
mov L0x20014d18 cg20010; mov L0x20014d1c cg21010; mov L0x20014d20 cg22010;
mov L0x20014d24 cg20110; mov L0x20014d28 cg21110; mov L0x20014d2c cg22110;
mov L0x20014d30 cg20210; mov L0x20014d34 cg21210; mov L0x20014d38 cg22210;
mov L0x20014d84 cg20011; mov L0x20014d88 cg21011; mov L0x20014d8c cg22011;
mov L0x20014d90 cg20111; mov L0x20014d94 cg21111; mov L0x20014d98 cg22111;
mov L0x20014d9c cg20211; mov L0x20014da0 cg21211; mov L0x20014da4 cg22211;
mov L0x20014df0 cg20012; mov L0x20014df4 cg21012; mov L0x20014df8 cg22012;
mov L0x20014dfc cg20112; mov L0x20014e00 cg21112; mov L0x20014e04 cg22112;
mov L0x20014e08 cg20212; mov L0x20014e0c cg21212; mov L0x20014e10 cg22212;
mov L0x20014e5c cg20013; mov L0x20014e60 cg21013; mov L0x20014e64 cg22013;
mov L0x20014e68 cg20113; mov L0x20014e6c cg21113; mov L0x20014e70 cg22113;
mov L0x20014e74 cg20213; mov L0x20014e78 cg21213; mov L0x20014e7c cg22213;
mov L0x20014ec8 cg20014; mov L0x20014ecc cg21014; mov L0x20014ed0 cg22014;
mov L0x20014ed4 cg20114; mov L0x20014ed8 cg21114; mov L0x20014edc cg22114;
mov L0x20014ee0 cg20214; mov L0x20014ee4 cg21214; mov L0x20014ee8 cg22214;
mov L0x20014f34 cg20015; mov L0x20014f38 cg21015; mov L0x20014f3c cg22015;
mov L0x20014f40 cg20115; mov L0x20014f44 cg21115; mov L0x20014f48 cg22115;
mov L0x20014f4c cg20215; mov L0x20014f50 cg21215; mov L0x20014f54 cg22215;
mov L0x20014fa0 cg20016; mov L0x20014fa4 cg21016; mov L0x20014fa8 cg22016;
mov L0x20014fac cg20116; mov L0x20014fb0 cg21116; mov L0x20014fb4 cg22116;
mov L0x20014fb8 cg20216; mov L0x20014fbc cg21216; mov L0x20014fc0 cg22216;
mov L0x2001500c cg20017; mov L0x20015010 cg21017; mov L0x20015014 cg22017;
mov L0x20015018 cg20117; mov L0x2001501c cg21117; mov L0x20015020 cg22117;
mov L0x20015024 cg20217; mov L0x20015028 cg21217; mov L0x2001502c cg22217;
mov L0x20015078 cg20018; mov L0x2001507c cg21018; mov L0x20015080 cg22018;
mov L0x20015084 cg20118; mov L0x20015088 cg21118; mov L0x2001508c cg22118;
mov L0x20015090 cg20218; mov L0x20015094 cg21218; mov L0x20015098 cg22218;
mov L0x200150e4 cg20019; mov L0x200150e8 cg21019; mov L0x200150ec cg22019;
mov L0x200150f0 cg20119; mov L0x200150f4 cg21119; mov L0x200150f8 cg22119;
mov L0x200150fc cg20219; mov L0x20015100 cg21219; mov L0x20015104 cg22219;
mov L0x20015150 cg20020; mov L0x20015154 cg21020; mov L0x20015158 cg22020;
mov L0x2001515c cg20120; mov L0x20015160 cg21120; mov L0x20015164 cg22120;
mov L0x20015168 cg20220; mov L0x2001516c cg21220; mov L0x20015170 cg22220;
mov L0x200151bc cg20021; mov L0x200151c0 cg21021; mov L0x200151c4 cg22021;
mov L0x200151c8 cg20121; mov L0x200151cc cg21121; mov L0x200151d0 cg22121;
mov L0x200151d4 cg20221; mov L0x200151d8 cg21221; mov L0x200151dc cg22221;
mov L0x20015228 cg20022; mov L0x2001522c cg21022; mov L0x20015230 cg22022;
mov L0x20015234 cg20122; mov L0x20015238 cg21122; mov L0x2001523c cg22122;
mov L0x20015240 cg20222; mov L0x20015244 cg21222; mov L0x20015248 cg22222;
mov L0x20015294 cg20023; mov L0x20015298 cg21023; mov L0x2001529c cg22023;
mov L0x200152a0 cg20123; mov L0x200152a4 cg21123; mov L0x200152a8 cg22123;
mov L0x200152ac cg20223; mov L0x200152b0 cg21223; mov L0x200152b4 cg22223;
mov L0x20015300 cg20024; mov L0x20015304 cg21024; mov L0x20015308 cg22024;
mov L0x2001530c cg20124; mov L0x20015310 cg21124; mov L0x20015314 cg22124;
mov L0x20015318 cg20224; mov L0x2001531c cg21224; mov L0x20015320 cg22224;
mov L0x2001536c cg20025; mov L0x20015370 cg21025; mov L0x20015374 cg22025;
mov L0x20015378 cg20125; mov L0x2001537c cg21125; mov L0x20015380 cg22125;
mov L0x20015384 cg20225; mov L0x20015388 cg21225; mov L0x2001538c cg22225;
mov L0x200153d8 cg20026; mov L0x200153dc cg21026; mov L0x200153e0 cg22026;
mov L0x200153e4 cg20126; mov L0x200153e8 cg21126; mov L0x200153ec cg22126;
mov L0x200153f0 cg20226; mov L0x200153f4 cg21226; mov L0x200153f8 cg22226;
mov L0x20015444 cg20027; mov L0x20015448 cg21027; mov L0x2001544c cg22027;
mov L0x20015450 cg20127; mov L0x20015454 cg21127; mov L0x20015458 cg22127;
mov L0x2001545c cg20227; mov L0x20015460 cg21227; mov L0x20015464 cg22227;
mov L0x200154b0 cg20028; mov L0x200154b4 cg21028; mov L0x200154b8 cg22028;
mov L0x200154bc cg20128; mov L0x200154c0 cg21128; mov L0x200154c4 cg22128;
mov L0x200154c8 cg20228; mov L0x200154cc cg21228; mov L0x200154d0 cg22228;
mov L0x2001551c cg20029; mov L0x20015520 cg21029; mov L0x20015524 cg22029;
mov L0x20015528 cg20129; mov L0x2001552c cg21129; mov L0x20015530 cg22129;
mov L0x20015534 cg20229; mov L0x20015538 cg21229; mov L0x2001553c cg22229;
mov L0x20015588 cg20030; mov L0x2001558c cg21030; mov L0x20015590 cg22030;
mov L0x20015594 cg20130; mov L0x20015598 cg21130; mov L0x2001559c cg22130;
mov L0x200155a0 cg20230; mov L0x200155a4 cg21230; mov L0x200155a8 cg22230;
mov L0x200155f4 cg20031; mov L0x200155f8 cg21031; mov L0x200155fc cg22031;
mov L0x20015600 cg20131; mov L0x20015604 cg21131; mov L0x20015608 cg22131;
mov L0x2001560c cg20231; mov L0x20015610 cg21231; mov L0x20015614 cg22231;
mov L0x20015618 cg30000; mov L0x2001561c cg31000; mov L0x20015620 cg32000;
mov L0x20015624 cg30100; mov L0x20015628 cg31100; mov L0x2001562c cg32100;
mov L0x20015630 cg30200; mov L0x20015634 cg31200; mov L0x20015638 cg32200;
mov L0x20015684 cg30001; mov L0x20015688 cg31001; mov L0x2001568c cg32001;
mov L0x20015690 cg30101; mov L0x20015694 cg31101; mov L0x20015698 cg32101;
mov L0x2001569c cg30201; mov L0x200156a0 cg31201; mov L0x200156a4 cg32201;
mov L0x200156f0 cg30002; mov L0x200156f4 cg31002; mov L0x200156f8 cg32002;
mov L0x200156fc cg30102; mov L0x20015700 cg31102; mov L0x20015704 cg32102;
mov L0x20015708 cg30202; mov L0x2001570c cg31202; mov L0x20015710 cg32202;
mov L0x2001575c cg30003; mov L0x20015760 cg31003; mov L0x20015764 cg32003;
mov L0x20015768 cg30103; mov L0x2001576c cg31103; mov L0x20015770 cg32103;
mov L0x20015774 cg30203; mov L0x20015778 cg31203; mov L0x2001577c cg32203;
mov L0x200157c8 cg30004; mov L0x200157cc cg31004; mov L0x200157d0 cg32004;
mov L0x200157d4 cg30104; mov L0x200157d8 cg31104; mov L0x200157dc cg32104;
mov L0x200157e0 cg30204; mov L0x200157e4 cg31204; mov L0x200157e8 cg32204;
mov L0x20015834 cg30005; mov L0x20015838 cg31005; mov L0x2001583c cg32005;
mov L0x20015840 cg30105; mov L0x20015844 cg31105; mov L0x20015848 cg32105;
mov L0x2001584c cg30205; mov L0x20015850 cg31205; mov L0x20015854 cg32205;
mov L0x200158a0 cg30006; mov L0x200158a4 cg31006; mov L0x200158a8 cg32006;
mov L0x200158ac cg30106; mov L0x200158b0 cg31106; mov L0x200158b4 cg32106;
mov L0x200158b8 cg30206; mov L0x200158bc cg31206; mov L0x200158c0 cg32206;
mov L0x2001590c cg30007; mov L0x20015910 cg31007; mov L0x20015914 cg32007;
mov L0x20015918 cg30107; mov L0x2001591c cg31107; mov L0x20015920 cg32107;
mov L0x20015924 cg30207; mov L0x20015928 cg31207; mov L0x2001592c cg32207;
mov L0x20015978 cg30008; mov L0x2001597c cg31008; mov L0x20015980 cg32008;
mov L0x20015984 cg30108; mov L0x20015988 cg31108; mov L0x2001598c cg32108;
mov L0x20015990 cg30208; mov L0x20015994 cg31208; mov L0x20015998 cg32208;
mov L0x200159e4 cg30009; mov L0x200159e8 cg31009; mov L0x200159ec cg32009;
mov L0x200159f0 cg30109; mov L0x200159f4 cg31109; mov L0x200159f8 cg32109;
mov L0x200159fc cg30209; mov L0x20015a00 cg31209; mov L0x20015a04 cg32209;
mov L0x20015a50 cg30010; mov L0x20015a54 cg31010; mov L0x20015a58 cg32010;
mov L0x20015a5c cg30110; mov L0x20015a60 cg31110; mov L0x20015a64 cg32110;
mov L0x20015a68 cg30210; mov L0x20015a6c cg31210; mov L0x20015a70 cg32210;
mov L0x20015abc cg30011; mov L0x20015ac0 cg31011; mov L0x20015ac4 cg32011;
mov L0x20015ac8 cg30111; mov L0x20015acc cg31111; mov L0x20015ad0 cg32111;
mov L0x20015ad4 cg30211; mov L0x20015ad8 cg31211; mov L0x20015adc cg32211;
mov L0x20015b28 cg30012; mov L0x20015b2c cg31012; mov L0x20015b30 cg32012;
mov L0x20015b34 cg30112; mov L0x20015b38 cg31112; mov L0x20015b3c cg32112;
mov L0x20015b40 cg30212; mov L0x20015b44 cg31212; mov L0x20015b48 cg32212;
mov L0x20015b94 cg30013; mov L0x20015b98 cg31013; mov L0x20015b9c cg32013;
mov L0x20015ba0 cg30113; mov L0x20015ba4 cg31113; mov L0x20015ba8 cg32113;
mov L0x20015bac cg30213; mov L0x20015bb0 cg31213; mov L0x20015bb4 cg32213;
mov L0x20015c00 cg30014; mov L0x20015c04 cg31014; mov L0x20015c08 cg32014;
mov L0x20015c0c cg30114; mov L0x20015c10 cg31114; mov L0x20015c14 cg32114;
mov L0x20015c18 cg30214; mov L0x20015c1c cg31214; mov L0x20015c20 cg32214;
mov L0x20015c6c cg30015; mov L0x20015c70 cg31015; mov L0x20015c74 cg32015;
mov L0x20015c78 cg30115; mov L0x20015c7c cg31115; mov L0x20015c80 cg32115;
mov L0x20015c84 cg30215; mov L0x20015c88 cg31215; mov L0x20015c8c cg32215;
mov L0x20015cd8 cg30016; mov L0x20015cdc cg31016; mov L0x20015ce0 cg32016;
mov L0x20015ce4 cg30116; mov L0x20015ce8 cg31116; mov L0x20015cec cg32116;
mov L0x20015cf0 cg30216; mov L0x20015cf4 cg31216; mov L0x20015cf8 cg32216;
mov L0x20015d44 cg30017; mov L0x20015d48 cg31017; mov L0x20015d4c cg32017;
mov L0x20015d50 cg30117; mov L0x20015d54 cg31117; mov L0x20015d58 cg32117;
mov L0x20015d5c cg30217; mov L0x20015d60 cg31217; mov L0x20015d64 cg32217;
mov L0x20015db0 cg30018; mov L0x20015db4 cg31018; mov L0x20015db8 cg32018;
mov L0x20015dbc cg30118; mov L0x20015dc0 cg31118; mov L0x20015dc4 cg32118;
mov L0x20015dc8 cg30218; mov L0x20015dcc cg31218; mov L0x20015dd0 cg32218;
mov L0x20015e1c cg30019; mov L0x20015e20 cg31019; mov L0x20015e24 cg32019;
mov L0x20015e28 cg30119; mov L0x20015e2c cg31119; mov L0x20015e30 cg32119;
mov L0x20015e34 cg30219; mov L0x20015e38 cg31219; mov L0x20015e3c cg32219;
mov L0x20015e88 cg30020; mov L0x20015e8c cg31020; mov L0x20015e90 cg32020;
mov L0x20015e94 cg30120; mov L0x20015e98 cg31120; mov L0x20015e9c cg32120;
mov L0x20015ea0 cg30220; mov L0x20015ea4 cg31220; mov L0x20015ea8 cg32220;
mov L0x20015ef4 cg30021; mov L0x20015ef8 cg31021; mov L0x20015efc cg32021;
mov L0x20015f00 cg30121; mov L0x20015f04 cg31121; mov L0x20015f08 cg32121;
mov L0x20015f0c cg30221; mov L0x20015f10 cg31221; mov L0x20015f14 cg32221;
mov L0x20015f60 cg30022; mov L0x20015f64 cg31022; mov L0x20015f68 cg32022;
mov L0x20015f6c cg30122; mov L0x20015f70 cg31122; mov L0x20015f74 cg32122;
mov L0x20015f78 cg30222; mov L0x20015f7c cg31222; mov L0x20015f80 cg32222;
mov L0x20015fcc cg30023; mov L0x20015fd0 cg31023; mov L0x20015fd4 cg32023;
mov L0x20015fd8 cg30123; mov L0x20015fdc cg31123; mov L0x20015fe0 cg32123;
mov L0x20015fe4 cg30223; mov L0x20015fe8 cg31223; mov L0x20015fec cg32223;
mov L0x20016038 cg30024; mov L0x2001603c cg31024; mov L0x20016040 cg32024;
mov L0x20016044 cg30124; mov L0x20016048 cg31124; mov L0x2001604c cg32124;
mov L0x20016050 cg30224; mov L0x20016054 cg31224; mov L0x20016058 cg32224;
mov L0x200160a4 cg30025; mov L0x200160a8 cg31025; mov L0x200160ac cg32025;
mov L0x200160b0 cg30125; mov L0x200160b4 cg31125; mov L0x200160b8 cg32125;
mov L0x200160bc cg30225; mov L0x200160c0 cg31225; mov L0x200160c4 cg32225;
mov L0x20016110 cg30026; mov L0x20016114 cg31026; mov L0x20016118 cg32026;
mov L0x2001611c cg30126; mov L0x20016120 cg31126; mov L0x20016124 cg32126;
mov L0x20016128 cg30226; mov L0x2001612c cg31226; mov L0x20016130 cg32226;
mov L0x2001617c cg30027; mov L0x20016180 cg31027; mov L0x20016184 cg32027;
mov L0x20016188 cg30127; mov L0x2001618c cg31127; mov L0x20016190 cg32127;
mov L0x20016194 cg30227; mov L0x20016198 cg31227; mov L0x2001619c cg32227;
mov L0x200161e8 cg30028; mov L0x200161ec cg31028; mov L0x200161f0 cg32028;
mov L0x200161f4 cg30128; mov L0x200161f8 cg31128; mov L0x200161fc cg32128;
mov L0x20016200 cg30228; mov L0x20016204 cg31228; mov L0x20016208 cg32228;
mov L0x20016254 cg30029; mov L0x20016258 cg31029; mov L0x2001625c cg32029;
mov L0x20016260 cg30129; mov L0x20016264 cg31129; mov L0x20016268 cg32129;
mov L0x2001626c cg30229; mov L0x20016270 cg31229; mov L0x20016274 cg32229;
mov L0x200162c0 cg30030; mov L0x200162c4 cg31030; mov L0x200162c8 cg32030;
mov L0x200162cc cg30130; mov L0x200162d0 cg31130; mov L0x200162d4 cg32130;
mov L0x200162d8 cg30230; mov L0x200162dc cg31230; mov L0x200162e0 cg32230;
mov L0x2001632c cg30031; mov L0x20016330 cg31031; mov L0x20016334 cg32031;
mov L0x20016338 cg30131; mov L0x2001633c cg31131; mov L0x20016340 cg32131;
mov L0x20016344 cg30231; mov L0x20016348 cg31231; mov L0x2001634c cg32231;
mov L0x2001563c cg40000; mov L0x20015640 cg41000; mov L0x20015644 cg42000;
mov L0x20015648 cg40100; mov L0x2001564c cg41100; mov L0x20015650 cg42100;
mov L0x20015654 cg40200; mov L0x20015658 cg41200; mov L0x2001565c cg42200;
mov L0x200156a8 cg40001; mov L0x200156ac cg41001; mov L0x200156b0 cg42001;
mov L0x200156b4 cg40101; mov L0x200156b8 cg41101; mov L0x200156bc cg42101;
mov L0x200156c0 cg40201; mov L0x200156c4 cg41201; mov L0x200156c8 cg42201;
mov L0x20015714 cg40002; mov L0x20015718 cg41002; mov L0x2001571c cg42002;
mov L0x20015720 cg40102; mov L0x20015724 cg41102; mov L0x20015728 cg42102;
mov L0x2001572c cg40202; mov L0x20015730 cg41202; mov L0x20015734 cg42202;
mov L0x20015780 cg40003; mov L0x20015784 cg41003; mov L0x20015788 cg42003;
mov L0x2001578c cg40103; mov L0x20015790 cg41103; mov L0x20015794 cg42103;
mov L0x20015798 cg40203; mov L0x2001579c cg41203; mov L0x200157a0 cg42203;
mov L0x200157ec cg40004; mov L0x200157f0 cg41004; mov L0x200157f4 cg42004;
mov L0x200157f8 cg40104; mov L0x200157fc cg41104; mov L0x20015800 cg42104;
mov L0x20015804 cg40204; mov L0x20015808 cg41204; mov L0x2001580c cg42204;
mov L0x20015858 cg40005; mov L0x2001585c cg41005; mov L0x20015860 cg42005;
mov L0x20015864 cg40105; mov L0x20015868 cg41105; mov L0x2001586c cg42105;
mov L0x20015870 cg40205; mov L0x20015874 cg41205; mov L0x20015878 cg42205;
mov L0x200158c4 cg40006; mov L0x200158c8 cg41006; mov L0x200158cc cg42006;
mov L0x200158d0 cg40106; mov L0x200158d4 cg41106; mov L0x200158d8 cg42106;
mov L0x200158dc cg40206; mov L0x200158e0 cg41206; mov L0x200158e4 cg42206;
mov L0x20015930 cg40007; mov L0x20015934 cg41007; mov L0x20015938 cg42007;
mov L0x2001593c cg40107; mov L0x20015940 cg41107; mov L0x20015944 cg42107;
mov L0x20015948 cg40207; mov L0x2001594c cg41207; mov L0x20015950 cg42207;
mov L0x2001599c cg40008; mov L0x200159a0 cg41008; mov L0x200159a4 cg42008;
mov L0x200159a8 cg40108; mov L0x200159ac cg41108; mov L0x200159b0 cg42108;
mov L0x200159b4 cg40208; mov L0x200159b8 cg41208; mov L0x200159bc cg42208;
mov L0x20015a08 cg40009; mov L0x20015a0c cg41009; mov L0x20015a10 cg42009;
mov L0x20015a14 cg40109; mov L0x20015a18 cg41109; mov L0x20015a1c cg42109;
mov L0x20015a20 cg40209; mov L0x20015a24 cg41209; mov L0x20015a28 cg42209;
mov L0x20015a74 cg40010; mov L0x20015a78 cg41010; mov L0x20015a7c cg42010;
mov L0x20015a80 cg40110; mov L0x20015a84 cg41110; mov L0x20015a88 cg42110;
mov L0x20015a8c cg40210; mov L0x20015a90 cg41210; mov L0x20015a94 cg42210;
mov L0x20015ae0 cg40011; mov L0x20015ae4 cg41011; mov L0x20015ae8 cg42011;
mov L0x20015aec cg40111; mov L0x20015af0 cg41111; mov L0x20015af4 cg42111;
mov L0x20015af8 cg40211; mov L0x20015afc cg41211; mov L0x20015b00 cg42211;
mov L0x20015b4c cg40012; mov L0x20015b50 cg41012; mov L0x20015b54 cg42012;
mov L0x20015b58 cg40112; mov L0x20015b5c cg41112; mov L0x20015b60 cg42112;
mov L0x20015b64 cg40212; mov L0x20015b68 cg41212; mov L0x20015b6c cg42212;
mov L0x20015bb8 cg40013; mov L0x20015bbc cg41013; mov L0x20015bc0 cg42013;
mov L0x20015bc4 cg40113; mov L0x20015bc8 cg41113; mov L0x20015bcc cg42113;
mov L0x20015bd0 cg40213; mov L0x20015bd4 cg41213; mov L0x20015bd8 cg42213;
mov L0x20015c24 cg40014; mov L0x20015c28 cg41014; mov L0x20015c2c cg42014;
mov L0x20015c30 cg40114; mov L0x20015c34 cg41114; mov L0x20015c38 cg42114;
mov L0x20015c3c cg40214; mov L0x20015c40 cg41214; mov L0x20015c44 cg42214;
mov L0x20015c90 cg40015; mov L0x20015c94 cg41015; mov L0x20015c98 cg42015;
mov L0x20015c9c cg40115; mov L0x20015ca0 cg41115; mov L0x20015ca4 cg42115;
mov L0x20015ca8 cg40215; mov L0x20015cac cg41215; mov L0x20015cb0 cg42215;
mov L0x20015cfc cg40016; mov L0x20015d00 cg41016; mov L0x20015d04 cg42016;
mov L0x20015d08 cg40116; mov L0x20015d0c cg41116; mov L0x20015d10 cg42116;
mov L0x20015d14 cg40216; mov L0x20015d18 cg41216; mov L0x20015d1c cg42216;
mov L0x20015d68 cg40017; mov L0x20015d6c cg41017; mov L0x20015d70 cg42017;
mov L0x20015d74 cg40117; mov L0x20015d78 cg41117; mov L0x20015d7c cg42117;
mov L0x20015d80 cg40217; mov L0x20015d84 cg41217; mov L0x20015d88 cg42217;
mov L0x20015dd4 cg40018; mov L0x20015dd8 cg41018; mov L0x20015ddc cg42018;
mov L0x20015de0 cg40118; mov L0x20015de4 cg41118; mov L0x20015de8 cg42118;
mov L0x20015dec cg40218; mov L0x20015df0 cg41218; mov L0x20015df4 cg42218;
mov L0x20015e40 cg40019; mov L0x20015e44 cg41019; mov L0x20015e48 cg42019;
mov L0x20015e4c cg40119; mov L0x20015e50 cg41119; mov L0x20015e54 cg42119;
mov L0x20015e58 cg40219; mov L0x20015e5c cg41219; mov L0x20015e60 cg42219;
mov L0x20015eac cg40020; mov L0x20015eb0 cg41020; mov L0x20015eb4 cg42020;
mov L0x20015eb8 cg40120; mov L0x20015ebc cg41120; mov L0x20015ec0 cg42120;
mov L0x20015ec4 cg40220; mov L0x20015ec8 cg41220; mov L0x20015ecc cg42220;
mov L0x20015f18 cg40021; mov L0x20015f1c cg41021; mov L0x20015f20 cg42021;
mov L0x20015f24 cg40121; mov L0x20015f28 cg41121; mov L0x20015f2c cg42121;
mov L0x20015f30 cg40221; mov L0x20015f34 cg41221; mov L0x20015f38 cg42221;
mov L0x20015f84 cg40022; mov L0x20015f88 cg41022; mov L0x20015f8c cg42022;
mov L0x20015f90 cg40122; mov L0x20015f94 cg41122; mov L0x20015f98 cg42122;
mov L0x20015f9c cg40222; mov L0x20015fa0 cg41222; mov L0x20015fa4 cg42222;
mov L0x20015ff0 cg40023; mov L0x20015ff4 cg41023; mov L0x20015ff8 cg42023;
mov L0x20015ffc cg40123; mov L0x20016000 cg41123; mov L0x20016004 cg42123;
mov L0x20016008 cg40223; mov L0x2001600c cg41223; mov L0x20016010 cg42223;
mov L0x2001605c cg40024; mov L0x20016060 cg41024; mov L0x20016064 cg42024;
mov L0x20016068 cg40124; mov L0x2001606c cg41124; mov L0x20016070 cg42124;
mov L0x20016074 cg40224; mov L0x20016078 cg41224; mov L0x2001607c cg42224;
mov L0x200160c8 cg40025; mov L0x200160cc cg41025; mov L0x200160d0 cg42025;
mov L0x200160d4 cg40125; mov L0x200160d8 cg41125; mov L0x200160dc cg42125;
mov L0x200160e0 cg40225; mov L0x200160e4 cg41225; mov L0x200160e8 cg42225;
mov L0x20016134 cg40026; mov L0x20016138 cg41026; mov L0x2001613c cg42026;
mov L0x20016140 cg40126; mov L0x20016144 cg41126; mov L0x20016148 cg42126;
mov L0x2001614c cg40226; mov L0x20016150 cg41226; mov L0x20016154 cg42226;
mov L0x200161a0 cg40027; mov L0x200161a4 cg41027; mov L0x200161a8 cg42027;
mov L0x200161ac cg40127; mov L0x200161b0 cg41127; mov L0x200161b4 cg42127;
mov L0x200161b8 cg40227; mov L0x200161bc cg41227; mov L0x200161c0 cg42227;
mov L0x2001620c cg40028; mov L0x20016210 cg41028; mov L0x20016214 cg42028;
mov L0x20016218 cg40128; mov L0x2001621c cg41128; mov L0x20016220 cg42128;
mov L0x20016224 cg40228; mov L0x20016228 cg41228; mov L0x2001622c cg42228;
mov L0x20016278 cg40029; mov L0x2001627c cg41029; mov L0x20016280 cg42029;
mov L0x20016284 cg40129; mov L0x20016288 cg41129; mov L0x2001628c cg42129;
mov L0x20016290 cg40229; mov L0x20016294 cg41229; mov L0x20016298 cg42229;
mov L0x200162e4 cg40030; mov L0x200162e8 cg41030; mov L0x200162ec cg42030;
mov L0x200162f0 cg40130; mov L0x200162f4 cg41130; mov L0x200162f8 cg42130;
mov L0x200162fc cg40230; mov L0x20016300 cg41230; mov L0x20016304 cg42230;
mov L0x20016350 cg40031; mov L0x20016354 cg41031; mov L0x20016358 cg42031;
mov L0x2001635c cg40131; mov L0x20016360 cg41131; mov L0x20016364 cg42131;
mov L0x20016368 cg40231; mov L0x2001636c cg41231; mov L0x20016370 cg42231;
mov L0x20015660 cg50000; mov L0x20015664 cg51000; mov L0x20015668 cg52000;
mov L0x2001566c cg50100; mov L0x20015670 cg51100; mov L0x20015674 cg52100;
mov L0x20015678 cg50200; mov L0x2001567c cg51200; mov L0x20015680 cg52200;
mov L0x200156cc cg50001; mov L0x200156d0 cg51001; mov L0x200156d4 cg52001;
mov L0x200156d8 cg50101; mov L0x200156dc cg51101; mov L0x200156e0 cg52101;
mov L0x200156e4 cg50201; mov L0x200156e8 cg51201; mov L0x200156ec cg52201;
mov L0x20015738 cg50002; mov L0x2001573c cg51002; mov L0x20015740 cg52002;
mov L0x20015744 cg50102; mov L0x20015748 cg51102; mov L0x2001574c cg52102;
mov L0x20015750 cg50202; mov L0x20015754 cg51202; mov L0x20015758 cg52202;
mov L0x200157a4 cg50003; mov L0x200157a8 cg51003; mov L0x200157ac cg52003;
mov L0x200157b0 cg50103; mov L0x200157b4 cg51103; mov L0x200157b8 cg52103;
mov L0x200157bc cg50203; mov L0x200157c0 cg51203; mov L0x200157c4 cg52203;
mov L0x20015810 cg50004; mov L0x20015814 cg51004; mov L0x20015818 cg52004;
mov L0x2001581c cg50104; mov L0x20015820 cg51104; mov L0x20015824 cg52104;
mov L0x20015828 cg50204; mov L0x2001582c cg51204; mov L0x20015830 cg52204;
mov L0x2001587c cg50005; mov L0x20015880 cg51005; mov L0x20015884 cg52005;
mov L0x20015888 cg50105; mov L0x2001588c cg51105; mov L0x20015890 cg52105;
mov L0x20015894 cg50205; mov L0x20015898 cg51205; mov L0x2001589c cg52205;
mov L0x200158e8 cg50006; mov L0x200158ec cg51006; mov L0x200158f0 cg52006;
mov L0x200158f4 cg50106; mov L0x200158f8 cg51106; mov L0x200158fc cg52106;
mov L0x20015900 cg50206; mov L0x20015904 cg51206; mov L0x20015908 cg52206;
mov L0x20015954 cg50007; mov L0x20015958 cg51007; mov L0x2001595c cg52007;
mov L0x20015960 cg50107; mov L0x20015964 cg51107; mov L0x20015968 cg52107;
mov L0x2001596c cg50207; mov L0x20015970 cg51207; mov L0x20015974 cg52207;
mov L0x200159c0 cg50008; mov L0x200159c4 cg51008; mov L0x200159c8 cg52008;
mov L0x200159cc cg50108; mov L0x200159d0 cg51108; mov L0x200159d4 cg52108;
mov L0x200159d8 cg50208; mov L0x200159dc cg51208; mov L0x200159e0 cg52208;
mov L0x20015a2c cg50009; mov L0x20015a30 cg51009; mov L0x20015a34 cg52009;
mov L0x20015a38 cg50109; mov L0x20015a3c cg51109; mov L0x20015a40 cg52109;
mov L0x20015a44 cg50209; mov L0x20015a48 cg51209; mov L0x20015a4c cg52209;
mov L0x20015a98 cg50010; mov L0x20015a9c cg51010; mov L0x20015aa0 cg52010;
mov L0x20015aa4 cg50110; mov L0x20015aa8 cg51110; mov L0x20015aac cg52110;
mov L0x20015ab0 cg50210; mov L0x20015ab4 cg51210; mov L0x20015ab8 cg52210;
mov L0x20015b04 cg50011; mov L0x20015b08 cg51011; mov L0x20015b0c cg52011;
mov L0x20015b10 cg50111; mov L0x20015b14 cg51111; mov L0x20015b18 cg52111;
mov L0x20015b1c cg50211; mov L0x20015b20 cg51211; mov L0x20015b24 cg52211;
mov L0x20015b70 cg50012; mov L0x20015b74 cg51012; mov L0x20015b78 cg52012;
mov L0x20015b7c cg50112; mov L0x20015b80 cg51112; mov L0x20015b84 cg52112;
mov L0x20015b88 cg50212; mov L0x20015b8c cg51212; mov L0x20015b90 cg52212;
mov L0x20015bdc cg50013; mov L0x20015be0 cg51013; mov L0x20015be4 cg52013;
mov L0x20015be8 cg50113; mov L0x20015bec cg51113; mov L0x20015bf0 cg52113;
mov L0x20015bf4 cg50213; mov L0x20015bf8 cg51213; mov L0x20015bfc cg52213;
mov L0x20015c48 cg50014; mov L0x20015c4c cg51014; mov L0x20015c50 cg52014;
mov L0x20015c54 cg50114; mov L0x20015c58 cg51114; mov L0x20015c5c cg52114;
mov L0x20015c60 cg50214; mov L0x20015c64 cg51214; mov L0x20015c68 cg52214;
mov L0x20015cb4 cg50015; mov L0x20015cb8 cg51015; mov L0x20015cbc cg52015;
mov L0x20015cc0 cg50115; mov L0x20015cc4 cg51115; mov L0x20015cc8 cg52115;
mov L0x20015ccc cg50215; mov L0x20015cd0 cg51215; mov L0x20015cd4 cg52215;
mov L0x20015d20 cg50016; mov L0x20015d24 cg51016; mov L0x20015d28 cg52016;
mov L0x20015d2c cg50116; mov L0x20015d30 cg51116; mov L0x20015d34 cg52116;
mov L0x20015d38 cg50216; mov L0x20015d3c cg51216; mov L0x20015d40 cg52216;
mov L0x20015d8c cg50017; mov L0x20015d90 cg51017; mov L0x20015d94 cg52017;
mov L0x20015d98 cg50117; mov L0x20015d9c cg51117; mov L0x20015da0 cg52117;
mov L0x20015da4 cg50217; mov L0x20015da8 cg51217; mov L0x20015dac cg52217;
mov L0x20015df8 cg50018; mov L0x20015dfc cg51018; mov L0x20015e00 cg52018;
mov L0x20015e04 cg50118; mov L0x20015e08 cg51118; mov L0x20015e0c cg52118;
mov L0x20015e10 cg50218; mov L0x20015e14 cg51218; mov L0x20015e18 cg52218;
mov L0x20015e64 cg50019; mov L0x20015e68 cg51019; mov L0x20015e6c cg52019;
mov L0x20015e70 cg50119; mov L0x20015e74 cg51119; mov L0x20015e78 cg52119;
mov L0x20015e7c cg50219; mov L0x20015e80 cg51219; mov L0x20015e84 cg52219;
mov L0x20015ed0 cg50020; mov L0x20015ed4 cg51020; mov L0x20015ed8 cg52020;
mov L0x20015edc cg50120; mov L0x20015ee0 cg51120; mov L0x20015ee4 cg52120;
mov L0x20015ee8 cg50220; mov L0x20015eec cg51220; mov L0x20015ef0 cg52220;
mov L0x20015f3c cg50021; mov L0x20015f40 cg51021; mov L0x20015f44 cg52021;
mov L0x20015f48 cg50121; mov L0x20015f4c cg51121; mov L0x20015f50 cg52121;
mov L0x20015f54 cg50221; mov L0x20015f58 cg51221; mov L0x20015f5c cg52221;
mov L0x20015fa8 cg50022; mov L0x20015fac cg51022; mov L0x20015fb0 cg52022;
mov L0x20015fb4 cg50122; mov L0x20015fb8 cg51122; mov L0x20015fbc cg52122;
mov L0x20015fc0 cg50222; mov L0x20015fc4 cg51222; mov L0x20015fc8 cg52222;
mov L0x20016014 cg50023; mov L0x20016018 cg51023; mov L0x2001601c cg52023;
mov L0x20016020 cg50123; mov L0x20016024 cg51123; mov L0x20016028 cg52123;
mov L0x2001602c cg50223; mov L0x20016030 cg51223; mov L0x20016034 cg52223;
mov L0x20016080 cg50024; mov L0x20016084 cg51024; mov L0x20016088 cg52024;
mov L0x2001608c cg50124; mov L0x20016090 cg51124; mov L0x20016094 cg52124;
mov L0x20016098 cg50224; mov L0x2001609c cg51224; mov L0x200160a0 cg52224;
mov L0x200160ec cg50025; mov L0x200160f0 cg51025; mov L0x200160f4 cg52025;
mov L0x200160f8 cg50125; mov L0x200160fc cg51125; mov L0x20016100 cg52125;
mov L0x20016104 cg50225; mov L0x20016108 cg51225; mov L0x2001610c cg52225;
mov L0x20016158 cg50026; mov L0x2001615c cg51026; mov L0x20016160 cg52026;
mov L0x20016164 cg50126; mov L0x20016168 cg51126; mov L0x2001616c cg52126;
mov L0x20016170 cg50226; mov L0x20016174 cg51226; mov L0x20016178 cg52226;
mov L0x200161c4 cg50027; mov L0x200161c8 cg51027; mov L0x200161cc cg52027;
mov L0x200161d0 cg50127; mov L0x200161d4 cg51127; mov L0x200161d8 cg52127;
mov L0x200161dc cg50227; mov L0x200161e0 cg51227; mov L0x200161e4 cg52227;
mov L0x20016230 cg50028; mov L0x20016234 cg51028; mov L0x20016238 cg52028;
mov L0x2001623c cg50128; mov L0x20016240 cg51128; mov L0x20016244 cg52128;
mov L0x20016248 cg50228; mov L0x2001624c cg51228; mov L0x20016250 cg52228;
mov L0x2001629c cg50029; mov L0x200162a0 cg51029; mov L0x200162a4 cg52029;
mov L0x200162a8 cg50129; mov L0x200162ac cg51129; mov L0x200162b0 cg52129;
mov L0x200162b4 cg50229; mov L0x200162b8 cg51229; mov L0x200162bc cg52229;
mov L0x20016308 cg50030; mov L0x2001630c cg51030; mov L0x20016310 cg52030;
mov L0x20016314 cg50130; mov L0x20016318 cg51130; mov L0x2001631c cg52130;
mov L0x20016320 cg50230; mov L0x20016324 cg51230; mov L0x20016328 cg52230;
mov L0x20016374 cg50031; mov L0x20016378 cg51031; mov L0x2001637c cg52031;
mov L0x20016380 cg50131; mov L0x20016384 cg51131; mov L0x20016388 cg52131;
mov L0x2001638c cg50231; mov L0x20016390 cg51231; mov L0x20016394 cg52231;



(******************** constants ********************)

mov r2 1205062335@uint32; mov r3    3365569@sint32;
mov L0x8063178 (1883665)@sint32; mov L0x806317c ( 980652)@sint32;
mov L0x8063180 ( 501252)@sint32; mov L0x8063184 ( 501252)@sint32;
mov L0x8063188 ( 501252)@sint32; mov L0x806318c ( 501252)@sint32;
mov L0x8063190 (3346293)@sint32; mov L0x8063194 (2811707)@sint32;
mov L0x8063198 ( 501252)@sint32; mov L0x806319c (2811707)@sint32;
mov L0x80631a0 (1639017)@sint32; mov L0x80631a4 ( 813816)@sint32;
mov L0x80631a8 ( 813816)@sint32; mov L0x80631ac ( 813816)@sint32;
mov L0x80631b0 ( 813816)@sint32; mov L0x80631b4 (2997402)@sint32;
mov L0x80631b8 (1833606)@sint32; mov L0x80631bc ( 813816)@sint32;
mov L0x80631c0 (1833606)@sint32; mov L0x80631c4 (2087623)@sint32;

(**************** CUT   0, - *****************)

ecut true;

(* #! -> SP = 0x20014890 *)
#! 0x20014890 = 0x20014890;
(* #stmdb	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, lr}#! EA = L0x20014890; PC = 0x8037fe8 *)
#stmdb	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, %%lr}#! L0x20014890 = L0x20014890; 0x8037fe8 = 0x8037fe8;
(* #vpush	{s16-s22}                                 #! PC = 0x8037fec *)
#vpush	{%%s16-%%s22}                                 #! 0x8037fec = 0x8037fec;
(* ldmia.w	r1!, {r10, r11}                         #! EA = L0x8063178; Value = 0xffe96350; PC = 0x8037ff0 *)
mov r10 L0x8063178;
mov r11 L0x806317c;
(* vldmia	r1!, {s4-s21}                             #! EA = L0x8063180; PC = 0x8037ff4 *)
mov s4 L0x8063180;
mov s5 L0x8063184;
mov s6 L0x8063188;
mov s7 L0x806318c;
mov s8 L0x8063190;
mov s9 L0x8063194;
mov s10 L0x8063198;
mov s11 L0x806319c;
mov s12 L0x80631a0;
mov s13 L0x80631a4;
mov s14 L0x80631a8;
mov s15 L0x80631ac;
mov s16 L0x80631b0;
mov s17 L0x80631b4;
mov s18 L0x80631b8;
mov s19 L0x80631bc;
mov s20 L0x80631c0;
mov s21 L0x80631c4;
(* add.w	r12, r0, #12                              #! PC = 0x8037ff8 *)
adds dc r12 r0 12@uint32;
(* vmov	s2, r12                                    #! PC = 0x8037ffc *)
mov s2 r12;
(* add.w	lr, r0, #1728	; 0x6c0                     #! PC = 0x8038000 *)
adds dc lr r0 1728@uint32;
(* vmov	s3, lr                                     #! PC = 0x8038004 *)
mov s3 lr;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014898; Value = 0x00000000; PC = 0x8038008 *)
mov r4 L0x20014898;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200148a4; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x200148a4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200148b0; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x200148b0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f58; Value = 0xfffffffd; PC = 0x8038014 *)
mov r7 L0x20014f58;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f64; Value = 0x00000003; PC = 0x8038018 *)
mov r8 L0x20014f64;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f70; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x20014f70;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200148a4; PC = 0x8038074 *)
mov L0x200148a4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200148b0; PC = 0x8038078 *)
mov L0x200148b0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f58; PC = 0x803807c *)
mov L0x20014f58 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f64; PC = 0x8038080 *)
mov L0x20014f64 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f70; PC = 0x8038084 *)
mov L0x20014f70 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014898; PC = 0x8038088 *)
mov L0x20014898 r4;

(**************** CUT   1, - *****************)

ecut and [
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x20014898*x**0*z** 0 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x200148a4*x**0*z** 0 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x200148b0*x**0*z** 0 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x20014f58*x**0*z** 0 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x20014f64*x**0*z** 0 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x20014f70*x**0*z** 0 [ 3365569, y - 2912918, z**16 - 3365568 ]
];


(* ldr.w	r4, [r0]                                  #! EA = L0x200148bc; Value = 0xffeb4785; PC = 0x803808c *)
mov r4 L0x200148bc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200148c8; Value = 0x00000000; PC = 0x8038090 *)
mov r5 L0x200148c8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200148d4; Value = 0x00000003; PC = 0x8038094 *)
mov r6 L0x200148d4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f7c; Value = 0x0014b881; PC = 0x8038098 *)
mov r7 L0x20014f7c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f88; Value = 0x00000003; PC = 0x803809c *)
mov r8 L0x20014f88;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f94; Value = 0x002970ff; PC = 0x80380a0 *)
mov r9 L0x20014f94;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200148c8; PC = 0x8038130 *)
mov L0x200148c8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200148d4; PC = 0x8038134 *)
mov L0x200148d4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f7c; PC = 0x8038138 *)
mov L0x20014f7c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f88; PC = 0x803813c *)
mov L0x20014f88 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f94; PC = 0x8038140 *)
mov L0x20014f94 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200148bc; PC = 0x8038144 *)
mov L0x200148bc r4;

(**************** CUT   2, - *****************)

ecut and [
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x200148bc*x**0*z** 0 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x200148c8*x**0*z** 0 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x200148d4*x**0*z** 0 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x20014f7c*x**0*z** 0 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x20014f88*x**0*z** 0 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x20014f94*x**0*z** 0 [ 3365569, y - 1540404, z**16 - 3365568 ]
];

(* ldr.w	r4, [r0]                                  #! EA = L0x200148e0; Value = 0x0014b884; PC = 0x8038148 *)
mov r4 L0x200148e0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200148ec; Value = 0x00000000; PC = 0x803814c *)
mov r5 L0x200148ec;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200148f8; Value = 0x00000003; PC = 0x8038150 *)
mov r6 L0x200148f8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fa0; Value = 0xffeb4782; PC = 0x8038154 *)
mov r7 L0x20014fa0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fac; Value = 0x00000003; PC = 0x8038158 *)
mov r8 L0x20014fac;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fb8; Value = 0xffd68f01; PC = 0x803815c *)
mov r9 L0x20014fb8;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200148ec; PC = 0x80381ec *)
mov L0x200148ec r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200148f8; PC = 0x80381f0 *)
mov L0x200148f8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fa0; PC = 0x80381f4 *)
mov L0x20014fa0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fac; PC = 0x80381f8 *)
mov L0x20014fac r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fb8; PC = 0x80381fc *)
mov L0x20014fb8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200148e0; PC = 0x8038200 *)
mov L0x200148e0 r4;


(**************** CUT   3, - *****************)

ecut and [
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x200148e0*x**0*z** 0 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x200148ec*x**0*z** 0 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x200148f8*x**0*z** 0 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x20014fa0*x**0*z** 0 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x20014fac*x**0*z** 0 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x20014fb8*x**0*z** 0 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014904; Value = 0x00000000; PC = 0x8038008 *)
mov r4 L0x20014904;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014910; Value = 0xfffffffd; PC = 0x803800c *)
mov r5 L0x20014910;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001491c; Value = 0x00000000; PC = 0x8038010 *)
mov r6 L0x2001491c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fc4; Value = 0xfffffffd; PC = 0x8038014 *)
mov r7 L0x20014fc4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fd0; Value = 0x00000009; PC = 0x8038018 *)
mov r8 L0x20014fd0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fdc; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x20014fdc;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014910; PC = 0x8038074 *)
mov L0x20014910 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001491c; PC = 0x8038078 *)
mov L0x2001491c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fc4; PC = 0x803807c *)
mov L0x20014fc4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fd0; PC = 0x8038080 *)
mov L0x20014fd0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fdc; PC = 0x8038084 *)
mov L0x20014fdc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014904; PC = 0x8038088 *)
mov L0x20014904 r4;


(**************** CUT   4, - *****************)

ecut and [
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x20014904*x**0*z** 1 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x20014910*x**0*z** 1 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x2001491c*x**0*z** 1 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x20014fc4*x**0*z** 1 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x20014fd0*x**0*z** 1 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x20014fdc*x**0*z** 1 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014928; Value = 0x00000000; PC = 0x803808c *)
mov r4 L0x20014928;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014934; Value = 0xffd68efe; PC = 0x8038090 *)
mov r5 L0x20014934;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014940; Value = 0x00000000; PC = 0x8038094 *)
mov r6 L0x20014940;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fe8; Value = 0x0014b881; PC = 0x8038098 *)
mov r7 L0x20014fe8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014ff4; Value = 0x00000000; PC = 0x803809c *)
mov r8 L0x20014ff4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015000; Value = 0x0014b87b; PC = 0x80380a0 *)
mov r9 L0x20015000;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014934; PC = 0x8038130 *)
mov L0x20014934 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014940; PC = 0x8038134 *)
mov L0x20014940 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fe8; PC = 0x8038138 *)
mov L0x20014fe8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014ff4; PC = 0x803813c *)
mov L0x20014ff4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015000; PC = 0x8038140 *)
mov L0x20015000 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014928; PC = 0x8038144 *)
mov L0x20014928 r4;




(**************** CUT   5, - *****************)

ecut and [
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20014928*x**0*z** 1 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20014934*x**0*z** 1 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20014940*x**0*z** 1 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20014fe8*x**0*z** 1 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20014ff4*x**0*z** 1 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20015000*x**0*z** 1 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x2001494c; Value = 0x00000000; PC = 0x8038148 *)
mov r4 L0x2001494c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014958; Value = 0x002970fc; PC = 0x803814c *)
mov r5 L0x20014958;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014964; Value = 0x00000000; PC = 0x8038150 *)
mov r6 L0x20014964;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001500c; Value = 0xffeb4782; PC = 0x8038154 *)
mov r7 L0x2001500c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015018; Value = 0x00000000; PC = 0x8038158 *)
mov r8 L0x20015018;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015024; Value = 0xffeb477c; PC = 0x803815c *)
mov r9 L0x20015024;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014958; PC = 0x80381ec *)
mov L0x20014958 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014964; PC = 0x80381f0 *)
mov L0x20014964 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001500c; PC = 0x80381f4 *)
mov L0x2001500c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015018; PC = 0x80381f8 *)
mov L0x20015018 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015024; PC = 0x80381fc *)
mov L0x20015024 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001494c; PC = 0x8038200 *)
mov L0x2001494c r4;




(**************** CUT   6, - *****************)

ecut and [
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x2001494c*x**0*z** 1 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x20014958*x**0*z** 1 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x20014964*x**0*z** 1 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x2001500c*x**0*z** 1 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x20015018*x**0*z** 1 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x20015024*x**0*z** 1 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014970; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014970;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001497c; Value = 0xfffffff7; PC = 0x803800c *)
mov r5 L0x2001497c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014988; Value = 0x00000000; PC = 0x8038010 *)
mov r6 L0x20014988;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015030; Value = 0xfffffffd; PC = 0x8038014 *)
mov r7 L0x20015030;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001503c; Value = 0x00000003; PC = 0x8038018 *)
mov r8 L0x2001503c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015048; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x20015048;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001497c; PC = 0x8038074 *)
mov L0x2001497c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014988; PC = 0x8038078 *)
mov L0x20014988 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015030; PC = 0x803807c *)
mov L0x20015030 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001503c; PC = 0x8038080 *)
mov L0x2001503c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015048; PC = 0x8038084 *)
mov L0x20015048 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014970; PC = 0x8038088 *)
mov L0x20014970 r4;




(**************** CUT   7, - *****************)

ecut and [
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x20014970*x**0*z** 2 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x2001497c*x**0*z** 2 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x20014988*x**0*z** 2 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x20015030*x**0*z** 2 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x2001503c*x**0*z** 2 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x20015048*x**0*z** 2 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014994; Value = 0xfff6163b; PC = 0x803808c *)
mov r4 L0x20014994;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200149a0; Value = 0x00000000; PC = 0x8038090 *)
mov r5 L0x200149a0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200149ac; Value = 0x002970ff; PC = 0x8038094 *)
mov r6 L0x200149ac;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015054; Value = 0x0014b881; PC = 0x8038098 *)
mov r7 L0x20015054;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015060; Value = 0xffeb477f; PC = 0x803809c *)
mov r8 L0x20015060;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001506c; Value = 0x0014b881; PC = 0x80380a0 *)
mov r9 L0x2001506c;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200149a0; PC = 0x8038130 *)
mov L0x200149a0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200149ac; PC = 0x8038134 *)
mov L0x200149ac r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015054; PC = 0x8038138 *)
mov L0x20015054 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015060; PC = 0x803813c *)
mov L0x20015060 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001506c; PC = 0x8038140 *)
mov L0x2001506c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014994; PC = 0x8038144 *)
mov L0x20014994 r4;




(**************** CUT   8, - *****************)

ecut and [
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x20014994*x**0*z** 2 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x200149a0*x**0*z** 2 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x200149ac*x**0*z** 2 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x20015054*x**0*z** 2 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x20015060*x**0*z** 2 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x2001506c*x**0*z** 2 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200149b8; Value = 0x0009e9bf; PC = 0x8038148 *)
mov r4 L0x200149b8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200149c4; Value = 0x00000000; PC = 0x803814c *)
mov r5 L0x200149c4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200149d0; Value = 0xffd68f01; PC = 0x8038150 *)
mov r6 L0x200149d0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015078; Value = 0xffeb4782; PC = 0x8038154 *)
mov r7 L0x20015078;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015084; Value = 0x0014b87e; PC = 0x8038158 *)
mov r8 L0x20015084;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015090; Value = 0xffeb4782; PC = 0x803815c *)
mov r9 L0x20015090;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200149c4; PC = 0x80381ec *)
mov L0x200149c4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200149d0; PC = 0x80381f0 *)
mov L0x200149d0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015078; PC = 0x80381f4 *)
mov L0x20015078 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015084; PC = 0x80381f8 *)
mov L0x20015084 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015090; PC = 0x80381fc *)
mov L0x20015090 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200149b8; PC = 0x8038200 *)
mov L0x200149b8 r4;




(**************** CUT   9, - *****************)

ecut and [
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x200149b8*x**0*z** 2 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x200149c4*x**0*z** 2 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x200149d0*x**0*z** 2 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x20015078*x**0*z** 2 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x20015084*x**0*z** 2 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x20015090*x**0*z** 2 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x200149dc; Value = 0x00000000; PC = 0x8038008 *)
mov r4 L0x200149dc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200149e8; Value = 0x00000003; PC = 0x803800c *)
mov r5 L0x200149e8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200149f4; Value = 0x00000000; PC = 0x8038010 *)
mov r6 L0x200149f4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001509c; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x2001509c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150a8; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x200150a8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150b4; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x200150b4;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200149e8; PC = 0x8038074 *)
mov L0x200149e8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200149f4; PC = 0x8038078 *)
mov L0x200149f4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001509c; PC = 0x803807c *)
mov L0x2001509c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150a8; PC = 0x8038080 *)
mov L0x200150a8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150b4; PC = 0x8038084 *)
mov L0x200150b4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200149dc; PC = 0x8038088 *)
mov L0x200149dc r4;




(**************** CUT  10, - *****************)

ecut and [
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x200149dc*x**0*z** 3 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x200149e8*x**0*z** 3 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x200149f4*x**0*z** 3 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x2001509c*x**0*z** 3 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x200150a8*x**0*z** 3 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x200150b4*x**0*z** 3 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a00; Value = 0x0014b87b; PC = 0x803808c *)
mov r4 L0x20014a00;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a0c; Value = 0x00000003; PC = 0x8038090 *)
mov r5 L0x20014a0c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a18; Value = 0xffd68f01; PC = 0x8038094 *)
mov r6 L0x20014a18;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150c0; Value = 0x002970ff; PC = 0x8038098 *)
mov r7 L0x200150c0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150cc; Value = 0x00000000; PC = 0x803809c *)
mov r8 L0x200150cc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150d8; Value = 0xfffffffd; PC = 0x80380a0 *)
mov r9 L0x200150d8;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a0c; PC = 0x8038130 *)
mov L0x20014a0c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a18; PC = 0x8038134 *)
mov L0x20014a18 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150c0; PC = 0x8038138 *)
mov L0x200150c0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150cc; PC = 0x803813c *)
mov L0x200150cc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150d8; PC = 0x8038140 *)
mov L0x200150d8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a00; PC = 0x8038144 *)
mov L0x20014a00 r4;




(**************** CUT  11, - *****************)

ecut and [
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x20014a00*x**0*z** 3 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x20014a0c*x**0*z** 3 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x20014a18*x**0*z** 3 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x200150c0*x**0*z** 3 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x200150cc*x**0*z** 3 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x200150d8*x**0*z** 3 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a24; Value = 0xffeb477c; PC = 0x8038148 *)
mov r4 L0x20014a24;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a30; Value = 0x00000003; PC = 0x803814c *)
mov r5 L0x20014a30;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a3c; Value = 0x002970ff; PC = 0x8038150 *)
mov r6 L0x20014a3c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150e4; Value = 0xffd68f01; PC = 0x8038154 *)
mov r7 L0x200150e4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150f0; Value = 0x00000000; PC = 0x8038158 *)
mov r8 L0x200150f0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150fc; Value = 0xfffffffd; PC = 0x803815c *)
mov r9 L0x200150fc;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a30; PC = 0x80381ec *)
mov L0x20014a30 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a3c; PC = 0x80381f0 *)
mov L0x20014a3c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150e4; PC = 0x80381f4 *)
mov L0x200150e4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150f0; PC = 0x80381f8 *)
mov L0x200150f0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150fc; PC = 0x80381fc *)
mov L0x200150fc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a24; PC = 0x8038200 *)
mov L0x20014a24 r4;




(**************** CUT  12, - *****************)

ecut and [
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x20014a24*x**0*z** 3 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x20014a30*x**0*z** 3 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x20014a3c*x**0*z** 3 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x200150e4*x**0*z** 3 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x200150f0*x**0*z** 3 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x200150fc*x**0*z** 3 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014a48; Value = 0xfffffffa; PC = 0x8038008 *)
mov r4 L0x20014a48;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a54; Value = 0x00001000; PC = 0x803800c *)
mov r5 L0x20014a54;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a60; Value = 0xfffffffd; PC = 0x8038010 *)
mov r6 L0x20014a60;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015108; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x20015108;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015114; Value = 0xfffffffd; PC = 0x8038018 *)
mov r8 L0x20015114;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015120; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x20015120;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a54; PC = 0x8038074 *)
mov L0x20014a54 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a60; PC = 0x8038078 *)
mov L0x20014a60 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015108; PC = 0x803807c *)
mov L0x20015108 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015114; PC = 0x8038080 *)
mov L0x20015114 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015120; PC = 0x8038084 *)
mov L0x20015120 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a48; PC = 0x8038088 *)
mov L0x20014a48 r4;




(**************** CUT  13, - *****************)

ecut and [
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20014a48*x**0*z** 4 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20014a54*x**0*z** 4 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20014a60*x**0*z** 4 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20015108*x**0*z** 4 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20015114*x**0*z** 4 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20015120*x**0*z** 4 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a6c; Value = 0x00000003; PC = 0x803808c *)
mov r4 L0x20014a6c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a78; Value = 0xffe7032a; PC = 0x8038090 *)
mov r5 L0x20014a78;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a84; Value = 0x0009e9bf; PC = 0x8038094 *)
mov r6 L0x20014a84;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001512c; Value = 0x00000003; PC = 0x8038098 *)
mov r7 L0x2001512c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015138; Value = 0xffeb4782; PC = 0x803809c *)
mov r8 L0x20015138;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015144; Value = 0xffd68efe; PC = 0x80380a0 *)
mov r9 L0x20015144;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a78; PC = 0x8038130 *)
mov L0x20014a78 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a84; PC = 0x8038134 *)
mov L0x20014a84 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001512c; PC = 0x8038138 *)
mov L0x2001512c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015138; PC = 0x803813c *)
mov L0x20015138 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015144; PC = 0x8038140 *)
mov L0x20015144 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a6c; PC = 0x8038144 *)
mov L0x20014a6c r4;




(**************** CUT  14, - *****************)

ecut and [
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x20014a6c*x**0*z** 4 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x20014a78*x**0*z** 4 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x20014a84*x**0*z** 4 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x2001512c*x**0*z** 4 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x20015138*x**0*z** 4 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x20015144*x**0*z** 4 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a90; Value = 0x00000003; PC = 0x8038148 *)
mov r4 L0x20014a90;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a9c; Value = 0x0018ecdf; PC = 0x803814c *)
mov r5 L0x20014a9c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014aa8; Value = 0xfff6163b; PC = 0x8038150 *)
mov r6 L0x20014aa8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015150; Value = 0x00000003; PC = 0x8038154 *)
mov r7 L0x20015150;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001515c; Value = 0x0014b881; PC = 0x8038158 *)
mov r8 L0x2001515c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015168; Value = 0x002970fc; PC = 0x803815c *)
mov r9 L0x20015168;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a9c; PC = 0x80381ec *)
mov L0x20014a9c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014aa8; PC = 0x80381f0 *)
mov L0x20014aa8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015150; PC = 0x80381f4 *)
mov L0x20015150 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001515c; PC = 0x80381f8 *)
mov L0x2001515c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015168; PC = 0x80381fc *)
mov L0x20015168 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a90; PC = 0x8038200 *)
mov L0x20014a90 r4;




(**************** CUT  15, - *****************)

ecut and [
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x20014a90*x**0*z** 4 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x20014a9c*x**0*z** 4 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x20014aa8*x**0*z** 4 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x20015150*x**0*z** 4 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x2001515c*x**0*z** 4 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x20015168*x**0*z** 4 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014ab4; Value = 0xfffffffa; PC = 0x8038008 *)
mov r4 L0x20014ab4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ac0; Value = 0xfffffff7; PC = 0x803800c *)
mov r5 L0x20014ac0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014acc; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x20014acc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015174; Value = 0xfffffffa; PC = 0x8038014 *)
mov r7 L0x20015174;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015180; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x20015180;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001518c; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x2001518c;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ac0; PC = 0x8038074 *)
mov L0x20014ac0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014acc; PC = 0x8038078 *)
mov L0x20014acc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015174; PC = 0x803807c *)
mov L0x20015174 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015180; PC = 0x8038080 *)
mov L0x20015180 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001518c; PC = 0x8038084 *)
mov L0x2001518c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014ab4; PC = 0x8038088 *)
mov L0x20014ab4 r4;




(**************** CUT  16, - *****************)

ecut and [
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x20014ab4*x**0*z** 5 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x20014ac0*x**0*z** 5 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x20014acc*x**0*z** 5 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x20015174*x**0*z** 5 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x20015180*x**0*z** 5 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x2001518c*x**0*z** 5 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014ad8; Value = 0x00000003; PC = 0x803808c *)
mov r4 L0x20014ad8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ae4; Value = 0x00000000; PC = 0x8038090 *)
mov r5 L0x20014ae4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014af0; Value = 0xfff61641; PC = 0x8038094 *)
mov r6 L0x20014af0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015198; Value = 0xffeb477f; PC = 0x8038098 *)
mov r7 L0x20015198;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151a4; Value = 0xffeb4785; PC = 0x803809c *)
mov r8 L0x200151a4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151b0; Value = 0x002970ff; PC = 0x80380a0 *)
mov r9 L0x200151b0;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ae4; PC = 0x8038130 *)
mov L0x20014ae4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014af0; PC = 0x8038134 *)
mov L0x20014af0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015198; PC = 0x8038138 *)
mov L0x20015198 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151a4; PC = 0x803813c *)
mov L0x200151a4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151b0; PC = 0x8038140 *)
mov L0x200151b0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014ad8; PC = 0x8038144 *)
mov L0x20014ad8 r4;




(**************** CUT  17, - *****************)

ecut and [
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x20014ad8*x**0*z** 5 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x20014ae4*x**0*z** 5 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x20014af0*x**0*z** 5 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x20015198*x**0*z** 5 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x200151a4*x**0*z** 5 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x200151b0*x**0*z** 5 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014afc; Value = 0x00000003; PC = 0x8038148 *)
mov r4 L0x20014afc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b08; Value = 0x00000000; PC = 0x803814c *)
mov r5 L0x20014b08;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b14; Value = 0x0009e9c5; PC = 0x8038150 *)
mov r6 L0x20014b14;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151bc; Value = 0x0014b87e; PC = 0x8038154 *)
mov r7 L0x200151bc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151c8; Value = 0x0014b884; PC = 0x8038158 *)
mov r8 L0x200151c8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151d4; Value = 0xffd68f01; PC = 0x803815c *)
mov r9 L0x200151d4;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b08; PC = 0x80381ec *)
mov L0x20014b08 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b14; PC = 0x80381f0 *)
mov L0x20014b14 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151bc; PC = 0x80381f4 *)
mov L0x200151bc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151c8; PC = 0x80381f8 *)
mov L0x200151c8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151d4; PC = 0x80381fc *)
mov L0x200151d4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014afc; PC = 0x8038200 *)
mov L0x20014afc r4;




(**************** CUT  18, - *****************)

ecut and [
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x20014afc*x**0*z** 5 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x20014b08*x**0*z** 5 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x20014b14*x**0*z** 5 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x200151bc*x**0*z** 5 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x200151c8*x**0*z** 5 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x200151d4*x**0*z** 5 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014b20; Value = 0x00000006; PC = 0x8038008 *)
mov r4 L0x20014b20;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b2c; Value = 0x00000003; PC = 0x803800c *)
mov r5 L0x20014b2c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b38; Value = 0x00000006; PC = 0x8038010 *)
mov r6 L0x20014b38;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151e0; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x200151e0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151ec; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x200151ec;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151f8; Value = 0x00000006; PC = 0x803801c *)
mov r9 L0x200151f8;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b2c; PC = 0x8038074 *)
mov L0x20014b2c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b38; PC = 0x8038078 *)
mov L0x20014b38 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151e0; PC = 0x803807c *)
mov L0x200151e0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151ec; PC = 0x8038080 *)
mov L0x200151ec r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151f8; PC = 0x8038084 *)
mov L0x200151f8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b20; PC = 0x8038088 *)
mov L0x20014b20 r4;




(**************** CUT  19, - *****************)

ecut and [
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x20014b20*x**0*z** 6 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x20014b2c*x**0*z** 6 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x20014b38*x**0*z** 6 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x200151e0*x**0*z** 6 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x200151ec*x**0*z** 6 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x200151f8*x**0*z** 6 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014b44; Value = 0xffeb4782; PC = 0x803808c *)
mov r4 L0x20014b44;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b50; Value = 0x00000003; PC = 0x8038090 *)
mov r5 L0x20014b50;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b5c; Value = 0x0014b881; PC = 0x8038094 *)
mov r6 L0x20014b5c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015204; Value = 0x00000003; PC = 0x8038098 *)
mov r7 L0x20015204;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015210; Value = 0x00000000; PC = 0x803809c *)
mov r8 L0x20015210;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001521c; Value = 0x0014b881; PC = 0x80380a0 *)
mov r9 L0x2001521c;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b50; PC = 0x8038130 *)
mov L0x20014b50 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b5c; PC = 0x8038134 *)
mov L0x20014b5c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015204; PC = 0x8038138 *)
mov L0x20015204 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015210; PC = 0x803813c *)
mov L0x20015210 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001521c; PC = 0x8038140 *)
mov L0x2001521c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b44; PC = 0x8038144 *)
mov L0x20014b44 r4;




(**************** CUT  20, - *****************)

ecut and [
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x20014b44*x**0*z** 6 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x20014b50*x**0*z** 6 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x20014b5c*x**0*z** 6 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x20015204*x**0*z** 6 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x20015210*x**0*z** 6 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x2001521c*x**0*z** 6 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014b68; Value = 0x0014b881; PC = 0x8038148 *)
mov r4 L0x20014b68;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b74; Value = 0x00000003; PC = 0x803814c *)
mov r5 L0x20014b74;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b80; Value = 0xffeb4782; PC = 0x8038150 *)
mov r6 L0x20014b80;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015228; Value = 0x00000003; PC = 0x8038154 *)
mov r7 L0x20015228;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015234; Value = 0x00000000; PC = 0x8038158 *)
mov r8 L0x20015234;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015240; Value = 0xffeb4782; PC = 0x803815c *)
mov r9 L0x20015240;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b74; PC = 0x80381ec *)
mov L0x20014b74 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b80; PC = 0x80381f0 *)
mov L0x20014b80 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015228; PC = 0x80381f4 *)
mov L0x20015228 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015234; PC = 0x80381f8 *)
mov L0x20015234 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015240; PC = 0x80381fc *)
mov L0x20015240 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b68; PC = 0x8038200 *)
mov L0x20014b68 r4;




(**************** CUT  21, - *****************)

ecut and [
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20014b68*x**0*z** 6 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20014b74*x**0*z** 6 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20014b80*x**0*z** 6 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20015228*x**0*z** 6 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20015234*x**0*z** 6 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20015240*x**0*z** 6 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014b8c; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014b8c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b98; Value = 0xfffffffa; PC = 0x803800c *)
mov r5 L0x20014b98;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ba4; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x20014ba4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001524c; Value = 0xfffffffa; PC = 0x8038014 *)
mov r7 L0x2001524c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015258; Value = 0x00000009; PC = 0x8038018 *)
mov r8 L0x20015258;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015264; Value = 0x00000003; PC = 0x803801c *)
mov r9 L0x20015264;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b98; PC = 0x8038074 *)
mov L0x20014b98 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ba4; PC = 0x8038078 *)
mov L0x20014ba4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001524c; PC = 0x803807c *)
mov L0x2001524c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015258; PC = 0x8038080 *)
mov L0x20015258 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015264; PC = 0x8038084 *)
mov L0x20015264 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b8c; PC = 0x8038088 *)
mov L0x20014b8c r4;




(**************** CUT  22, - *****************)

ecut and [
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x20014b8c*x**0*z** 7 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x20014b98*x**0*z** 7 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x20014ba4*x**0*z** 7 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x2001524c*x**0*z** 7 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x20015258*x**0*z** 7 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x20015264*x**0*z** 7 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014bb0; Value = 0xfffffffd; PC = 0x803808c *)
mov r4 L0x20014bb0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014bbc; Value = 0x0014b87e; PC = 0x8038090 *)
mov r5 L0x20014bbc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014bc8; Value = 0x00000003; PC = 0x8038094 *)
mov r6 L0x20014bc8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015270; Value = 0x00000003; PC = 0x8038098 *)
mov r7 L0x20015270;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001527c; Value = 0x00000000; PC = 0x803809c *)
mov r8 L0x2001527c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015288; Value = 0x00000003; PC = 0x80380a0 *)
mov r9 L0x20015288;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014bbc; PC = 0x8038130 *)
mov L0x20014bbc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014bc8; PC = 0x8038134 *)
mov L0x20014bc8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015270; PC = 0x8038138 *)
mov L0x20015270 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001527c; PC = 0x803813c *)
mov L0x2001527c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015288; PC = 0x8038140 *)
mov L0x20015288 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014bb0; PC = 0x8038144 *)
mov L0x20014bb0 r4;




(**************** CUT  23, - *****************)

ecut and [
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x20014bb0*x**0*z** 7 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x20014bbc*x**0*z** 7 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x20014bc8*x**0*z** 7 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x20015270*x**0*z** 7 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x2001527c*x**0*z** 7 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x20015288*x**0*z** 7 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014bd4; Value = 0xfffffffd; PC = 0x8038148 *)
mov r4 L0x20014bd4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014be0; Value = 0xffeb477f; PC = 0x803814c *)
mov r5 L0x20014be0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014bec; Value = 0x00000003; PC = 0x8038150 *)
mov r6 L0x20014bec;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015294; Value = 0x00000003; PC = 0x8038154 *)
mov r7 L0x20015294;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152a0; Value = 0x00000000; PC = 0x8038158 *)
mov r8 L0x200152a0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152ac; Value = 0x00000003; PC = 0x803815c *)
mov r9 L0x200152ac;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014be0; PC = 0x80381ec *)
mov L0x20014be0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014bec; PC = 0x80381f0 *)
mov L0x20014bec r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015294; PC = 0x80381f4 *)
mov L0x20015294 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152a0; PC = 0x80381f8 *)
mov L0x200152a0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152ac; PC = 0x80381fc *)
mov L0x200152ac r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014bd4; PC = 0x8038200 *)
mov L0x20014bd4 r4;




(**************** CUT  24, - *****************)

ecut and [
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x20014bd4*x**0*z** 7 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x20014be0*x**0*z** 7 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x20014bec*x**0*z** 7 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x20015294*x**0*z** 7 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x200152a0*x**0*z** 7 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x200152ac*x**0*z** 7 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014bf8; Value = 0x00000009; PC = 0x8038008 *)
mov r4 L0x20014bf8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c04; Value = 0x00000003; PC = 0x803800c *)
mov r5 L0x20014c04;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c10; Value = 0x00000000; PC = 0x8038010 *)
mov r6 L0x20014c10;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152b8; Value = 0xfffffffa; PC = 0x8038014 *)
mov r7 L0x200152b8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152c4; Value = 0xfffffffd; PC = 0x8038018 *)
mov r8 L0x200152c4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152d0; Value = 0x00000003; PC = 0x803801c *)
mov r9 L0x200152d0;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c04; PC = 0x8038074 *)
mov L0x20014c04 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c10; PC = 0x8038078 *)
mov L0x20014c10 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152b8; PC = 0x803807c *)
mov L0x200152b8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152c4; PC = 0x8038080 *)
mov L0x200152c4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152d0; PC = 0x8038084 *)
mov L0x200152d0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014bf8; PC = 0x8038088 *)
mov L0x20014bf8 r4;




(**************** CUT  25, - *****************)

ecut and [
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x20014bf8*x**0*z** 8 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x20014c04*x**0*z** 8 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x20014c10*x**0*z** 8 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x200152b8*x**0*z** 8 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x200152c4*x**0*z** 8 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x200152d0*x**0*z** 8 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014c1c; Value = 0x00000000; PC = 0x803808c *)
mov r4 L0x20014c1c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c28; Value = 0xffeb477f; PC = 0x8038090 *)
mov r5 L0x20014c28;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c34; Value = 0x002970ff; PC = 0x8038094 *)
mov r6 L0x20014c34;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152dc; Value = 0x0014b87e; PC = 0x8038098 *)
mov r7 L0x200152dc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152e8; Value = 0x0014b881; PC = 0x803809c *)
mov r8 L0x200152e8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152f4; Value = 0x00000003; PC = 0x80380a0 *)
mov r9 L0x200152f4;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c28; PC = 0x8038130 *)
mov L0x20014c28 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c34; PC = 0x8038134 *)
mov L0x20014c34 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152dc; PC = 0x8038138 *)
mov L0x200152dc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152e8; PC = 0x803813c *)
mov L0x200152e8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152f4; PC = 0x8038140 *)
mov L0x200152f4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c1c; PC = 0x8038144 *)
mov L0x20014c1c r4;




(**************** CUT  26, - *****************)

ecut and [
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x20014c1c*x**0*z** 8 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x20014c28*x**0*z** 8 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x20014c34*x**0*z** 8 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x200152dc*x**0*z** 8 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x200152e8*x**0*z** 8 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x200152f4*x**0*z** 8 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014c40; Value = 0x00000000; PC = 0x8038148 *)
mov r4 L0x20014c40;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c4c; Value = 0x0014b87e; PC = 0x803814c *)
mov r5 L0x20014c4c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c58; Value = 0xffd68f01; PC = 0x8038150 *)
mov r6 L0x20014c58;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015300; Value = 0xffeb477f; PC = 0x8038154 *)
mov r7 L0x20015300;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001530c; Value = 0xffeb4782; PC = 0x8038158 *)
mov r8 L0x2001530c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015318; Value = 0x00000003; PC = 0x803815c *)
mov r9 L0x20015318;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c4c; PC = 0x80381ec *)
mov L0x20014c4c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c58; PC = 0x80381f0 *)
mov L0x20014c58 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015300; PC = 0x80381f4 *)
mov L0x20015300 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001530c; PC = 0x80381f8 *)
mov L0x2001530c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015318; PC = 0x80381fc *)
mov L0x20015318 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c40; PC = 0x8038200 *)
mov L0x20014c40 r4;




(**************** CUT  27, - *****************)

ecut and [
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x20014c40*x**0*z** 8 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x20014c4c*x**0*z** 8 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x20014c58*x**0*z** 8 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x20015300*x**0*z** 8 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x2001530c*x**0*z** 8 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x20015318*x**0*z** 8 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014c64; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014c64;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c70; Value = 0xfffffffd; PC = 0x803800c *)
mov r5 L0x20014c70;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c7c; Value = 0xfffffff7; PC = 0x8038010 *)
mov r6 L0x20014c7c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015324; Value = 0xfffffffa; PC = 0x8038014 *)
mov r7 L0x20015324;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015330; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x20015330;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001533c; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x2001533c;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c70; PC = 0x8038074 *)
mov L0x20014c70 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c7c; PC = 0x8038078 *)
mov L0x20014c7c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015324; PC = 0x803807c *)
mov L0x20015324 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015330; PC = 0x8038080 *)
mov L0x20015330 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001533c; PC = 0x8038084 *)
mov L0x2001533c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c64; PC = 0x8038088 *)
mov L0x20014c64 r4;




(**************** CUT  28, - *****************)

ecut and [
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x20014c64*x**0*z** 9 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x20014c70*x**0*z** 9 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x20014c7c*x**0*z** 9 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x20015324*x**0*z** 9 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x20015330*x**0*z** 9 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x2001533c*x**0*z** 9 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014c88; Value = 0xffd68efe; PC = 0x803808c *)
mov r4 L0x20014c88;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c94; Value = 0x0014b881; PC = 0x8038090 *)
mov r5 L0x20014c94;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ca0; Value = 0x00000000; PC = 0x8038094 *)
mov r6 L0x20014ca0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015348; Value = 0xffeb477f; PC = 0x8038098 *)
mov r7 L0x20015348;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015354; Value = 0xffeb4785; PC = 0x803809c *)
mov r8 L0x20015354;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015360; Value = 0x0014b881; PC = 0x80380a0 *)
mov r9 L0x20015360;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c94; PC = 0x8038130 *)
mov L0x20014c94 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ca0; PC = 0x8038134 *)
mov L0x20014ca0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015348; PC = 0x8038138 *)
mov L0x20015348 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015354; PC = 0x803813c *)
mov L0x20015354 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015360; PC = 0x8038140 *)
mov L0x20015360 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c88; PC = 0x8038144 *)
mov L0x20014c88 r4;




(**************** CUT  29, - *****************)

ecut and [
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20014c88*x**0*z** 9 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20014c94*x**0*z** 9 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20014ca0*x**0*z** 9 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20015348*x**0*z** 9 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20015354*x**0*z** 9 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20015360*x**0*z** 9 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014cac; Value = 0x002970fc; PC = 0x8038148 *)
mov r4 L0x20014cac;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014cb8; Value = 0xffeb4782; PC = 0x803814c *)
mov r5 L0x20014cb8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014cc4; Value = 0x00000000; PC = 0x8038150 *)
mov r6 L0x20014cc4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001536c; Value = 0x0014b87e; PC = 0x8038154 *)
mov r7 L0x2001536c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015378; Value = 0x0014b884; PC = 0x8038158 *)
mov r8 L0x20015378;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015384; Value = 0xffeb4782; PC = 0x803815c *)
mov r9 L0x20015384;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014cb8; PC = 0x80381ec *)
mov L0x20014cb8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014cc4; PC = 0x80381f0 *)
mov L0x20014cc4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001536c; PC = 0x80381f4 *)
mov L0x2001536c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015378; PC = 0x80381f8 *)
mov L0x20015378 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015384; PC = 0x80381fc *)
mov L0x20015384 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014cac; PC = 0x8038200 *)
mov L0x20014cac r4;




(**************** CUT  30, - *****************)

ecut and [
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x20014cac*x**0*z** 9 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x20014cb8*x**0*z** 9 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x20014cc4*x**0*z** 9 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x2001536c*x**0*z** 9 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x20015378*x**0*z** 9 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x20015384*x**0*z** 9 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014cd0; Value = 0x00000000; PC = 0x8038008 *)
mov r4 L0x20014cd0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014cdc; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x20014cdc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ce8; Value = 0xfffffffd; PC = 0x8038010 *)
mov r6 L0x20014ce8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015390; Value = 0xfffffffa; PC = 0x8038014 *)
mov r7 L0x20015390;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001539c; Value = 0xfffffffa; PC = 0x8038018 *)
mov r8 L0x2001539c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153a8; Value = 0xfffffffa; PC = 0x803801c *)
mov r9 L0x200153a8;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014cdc; PC = 0x8038074 *)
mov L0x20014cdc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ce8; PC = 0x8038078 *)
mov L0x20014ce8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015390; PC = 0x803807c *)
mov L0x20015390 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001539c; PC = 0x8038080 *)
mov L0x2001539c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153a8; PC = 0x8038084 *)
mov L0x200153a8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014cd0; PC = 0x8038088 *)
mov L0x20014cd0 r4;




(**************** CUT  31, - *****************)

ecut and [
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x20014cd0*x**0*z**10 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x20014cdc*x**0*z**10 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x20014ce8*x**0*z**10 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x20015390*x**0*z**10 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x2001539c*x**0*z**10 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x200153a8*x**0*z**10 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014cf4; Value = 0x00000000; PC = 0x803808c *)
mov r4 L0x20014cf4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d00; Value = 0xffd68f01; PC = 0x8038090 *)
mov r5 L0x20014d00;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d0c; Value = 0xffd68efe; PC = 0x8038094 *)
mov r6 L0x20014d0c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153b4; Value = 0x0014b87e; PC = 0x8038098 *)
mov r7 L0x200153b4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153c0; Value = 0xffeb477f; PC = 0x803809c *)
mov r8 L0x200153c0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153cc; Value = 0x0014b87e; PC = 0x80380a0 *)
mov r9 L0x200153cc;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d00; PC = 0x8038130 *)
mov L0x20014d00 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d0c; PC = 0x8038134 *)
mov L0x20014d0c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153b4; PC = 0x8038138 *)
mov L0x200153b4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153c0; PC = 0x803813c *)
mov L0x200153c0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153cc; PC = 0x8038140 *)
mov L0x200153cc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014cf4; PC = 0x8038144 *)
mov L0x20014cf4 r4;




(**************** CUT  32, - *****************)

ecut and [
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x20014cf4*x**0*z**10 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x20014d00*x**0*z**10 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x20014d0c*x**0*z**10 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x200153b4*x**0*z**10 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x200153c0*x**0*z**10 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x200153cc*x**0*z**10 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014d18; Value = 0x00000000; PC = 0x8038148 *)
mov r4 L0x20014d18;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d24; Value = 0x002970ff; PC = 0x803814c *)
mov r5 L0x20014d24;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d30; Value = 0x002970fc; PC = 0x8038150 *)
mov r6 L0x20014d30;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153d8; Value = 0xffeb477f; PC = 0x8038154 *)
mov r7 L0x200153d8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153e4; Value = 0x0014b87e; PC = 0x8038158 *)
mov r8 L0x200153e4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153f0; Value = 0xffeb477f; PC = 0x803815c *)
mov r9 L0x200153f0;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d24; PC = 0x80381ec *)
mov L0x20014d24 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d30; PC = 0x80381f0 *)
mov L0x20014d30 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153d8; PC = 0x80381f4 *)
mov L0x200153d8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153e4; PC = 0x80381f8 *)
mov L0x200153e4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153f0; PC = 0x80381fc *)
mov L0x200153f0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d18; PC = 0x8038200 *)
mov L0x20014d18 r4;




(**************** CUT  33, - *****************)

ecut and [
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x20014d18*x**0*z**10 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x20014d24*x**0*z**10 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x20014d30*x**0*z**10 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x200153d8*x**0*z**10 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x200153e4*x**0*z**10 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x200153f0*x**0*z**10 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014d3c; Value = 0x00000003; PC = 0x8038008 *)
mov r4 L0x20014d3c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d48; Value = 0x00000003; PC = 0x803800c *)
mov r5 L0x20014d48;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d54; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x20014d54;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153fc; Value = 0xfffffffd; PC = 0x8038014 *)
mov r7 L0x200153fc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015408; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x20015408;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015414; Value = 0xfffffffa; PC = 0x803801c *)
mov r9 L0x20015414;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d48; PC = 0x8038074 *)
mov L0x20014d48 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d54; PC = 0x8038078 *)
mov L0x20014d54 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153fc; PC = 0x803807c *)
mov L0x200153fc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015408; PC = 0x8038080 *)
mov L0x20015408 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015414; PC = 0x8038084 *)
mov L0x20015414 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d3c; PC = 0x8038088 *)
mov L0x20014d3c r4;




(**************** CUT  34, - *****************)

ecut and [
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x20014d3c*x**0*z**11 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x20014d48*x**0*z**11 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x20014d54*x**0*z**11 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x200153fc*x**0*z**11 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x20015408*x**0*z**11 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x20015414*x**0*z**11 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014d60; Value = 0xfff61641; PC = 0x803808c *)
mov r4 L0x20014d60;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d6c; Value = 0x00297102; PC = 0x8038090 *)
mov r5 L0x20014d6c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d78; Value = 0xffeb477f; PC = 0x8038094 *)
mov r6 L0x20014d78;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015420; Value = 0x00000006; PC = 0x8038098 *)
mov r7 L0x20015420;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001542c; Value = 0x0014b884; PC = 0x803809c *)
mov r8 L0x2001542c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015438; Value = 0x00000003; PC = 0x80380a0 *)
mov r9 L0x20015438;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d6c; PC = 0x8038130 *)
mov L0x20014d6c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d78; PC = 0x8038134 *)
mov L0x20014d78 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015420; PC = 0x8038138 *)
mov L0x20015420 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001542c; PC = 0x803813c *)
mov L0x2001542c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015438; PC = 0x8038140 *)
mov L0x20015438 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d60; PC = 0x8038144 *)
mov L0x20014d60 r4;




(**************** CUT  35, - *****************)

ecut and [
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x20014d60*x**0*z**11 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x20014d6c*x**0*z**11 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x20014d78*x**0*z**11 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x20015420*x**0*z**11 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x2001542c*x**0*z**11 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x20015438*x**0*z**11 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014d84; Value = 0x0009e9c5; PC = 0x8038148 *)
mov r4 L0x20014d84;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d90; Value = 0xffd68f04; PC = 0x803814c *)
mov r5 L0x20014d90;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d9c; Value = 0x0014b87e; PC = 0x8038150 *)
mov r6 L0x20014d9c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015444; Value = 0x00000006; PC = 0x8038154 *)
mov r7 L0x20015444;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015450; Value = 0xffeb4785; PC = 0x8038158 *)
mov r8 L0x20015450;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001545c; Value = 0x00000003; PC = 0x803815c *)
mov r9 L0x2001545c;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d90; PC = 0x80381ec *)
mov L0x20014d90 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d9c; PC = 0x80381f0 *)
mov L0x20014d9c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015444; PC = 0x80381f4 *)
mov L0x20015444 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015450; PC = 0x80381f8 *)
mov L0x20015450 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001545c; PC = 0x80381fc *)
mov L0x2001545c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d84; PC = 0x8038200 *)
mov L0x20014d84 r4;




(**************** CUT  36, - *****************)

ecut and [
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x20014d84*x**0*z**11 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x20014d90*x**0*z**11 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x20014d9c*x**0*z**11 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x20015444*x**0*z**11 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x20015450*x**0*z**11 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x2001545c*x**0*z**11 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014da8; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014da8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014db4; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x20014db4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014dc0; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x20014dc0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015468; Value = 0x00000009; PC = 0x8038014 *)
mov r7 L0x20015468;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015474; Value = 0xfffffffa; PC = 0x8038018 *)
mov r8 L0x20015474;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015480; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x20015480;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014db4; PC = 0x8038074 *)
mov L0x20014db4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014dc0; PC = 0x8038078 *)
mov L0x20014dc0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015468; PC = 0x803807c *)
mov L0x20015468 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015474; PC = 0x8038080 *)
mov L0x20015474 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015480; PC = 0x8038084 *)
mov L0x20015480 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014da8; PC = 0x8038088 *)
mov L0x20014da8 r4;




(**************** CUT  37, - *****************)

ecut and [
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20014da8*x**0*z**12 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20014db4*x**0*z**12 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20014dc0*x**0*z**12 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20015468*x**0*z**12 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20015474*x**0*z**12 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20015480*x**0*z**12 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014dcc; Value = 0xfffffffd; PC = 0x803808c *)
mov r4 L0x20014dcc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014dd8; Value = 0x00000000; PC = 0x8038090 *)
mov r5 L0x20014dd8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014de4; Value = 0xfffffffa; PC = 0x8038094 *)
mov r6 L0x20014de4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001548c; Value = 0x00000000; PC = 0x8038098 *)
mov r7 L0x2001548c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015498; Value = 0x0014b87e; PC = 0x803809c *)
mov r8 L0x20015498;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154a4; Value = 0x00000000; PC = 0x80380a0 *)
mov r9 L0x200154a4;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014dd8; PC = 0x8038130 *)
mov L0x20014dd8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014de4; PC = 0x8038134 *)
mov L0x20014de4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001548c; PC = 0x8038138 *)
mov L0x2001548c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015498; PC = 0x803813c *)
mov L0x20015498 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154a4; PC = 0x8038140 *)
mov L0x200154a4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014dcc; PC = 0x8038144 *)
mov L0x20014dcc r4;




(**************** CUT  38, - *****************)

ecut and [
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x20014dcc*x**0*z**12 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x20014dd8*x**0*z**12 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x20014de4*x**0*z**12 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x2001548c*x**0*z**12 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x20015498*x**0*z**12 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x200154a4*x**0*z**12 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014df0; Value = 0xfffffffd; PC = 0x8038148 *)
mov r4 L0x20014df0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014dfc; Value = 0x00000000; PC = 0x803814c *)
mov r5 L0x20014dfc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e08; Value = 0xfffffffa; PC = 0x8038150 *)
mov r6 L0x20014e08;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154b0; Value = 0x00000000; PC = 0x8038154 *)
mov r7 L0x200154b0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154bc; Value = 0xffeb477f; PC = 0x8038158 *)
mov r8 L0x200154bc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154c8; Value = 0x00000000; PC = 0x803815c *)
mov r9 L0x200154c8;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014dfc; PC = 0x80381ec *)
mov L0x20014dfc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e08; PC = 0x80381f0 *)
mov L0x20014e08 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154b0; PC = 0x80381f4 *)
mov L0x200154b0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154bc; PC = 0x80381f8 *)
mov L0x200154bc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154c8; PC = 0x80381fc *)
mov L0x200154c8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014df0; PC = 0x8038200 *)
mov L0x20014df0 r4;




(**************** CUT  39, - *****************)

ecut and [
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x20014df0*x**0*z**12 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x20014dfc*x**0*z**12 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x20014e08*x**0*z**12 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x200154b0*x**0*z**12 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x200154bc*x**0*z**12 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x200154c8*x**0*z**12 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014e14; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014e14;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e20; Value = 0x00000003; PC = 0x803800c *)
mov r5 L0x20014e20;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e2c; Value = 0xfffffffd; PC = 0x8038010 *)
mov r6 L0x20014e2c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154d4; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x200154d4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154e0; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x200154e0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154ec; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x200154ec;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e20; PC = 0x8038074 *)
mov L0x20014e20 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e2c; PC = 0x8038078 *)
mov L0x20014e2c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154d4; PC = 0x803807c *)
mov L0x200154d4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154e0; PC = 0x8038080 *)
mov L0x200154e0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154ec; PC = 0x8038084 *)
mov L0x200154ec r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e14; PC = 0x8038088 *)
mov L0x20014e14 r4;




(**************** CUT  40, - *****************)

ecut and [
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x20014e14*x**0*z**13 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x20014e20*x**0*z**13 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x20014e2c*x**0*z**13 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x200154d4*x**0*z**13 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x200154e0*x**0*z**13 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x200154ec*x**0*z**13 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014e38; Value = 0xfffffffd; PC = 0x803808c *)
mov r4 L0x20014e38;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e44; Value = 0xffeb477f; PC = 0x8038090 *)
mov r5 L0x20014e44;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e50; Value = 0xfff6163b; PC = 0x8038094 *)
mov r6 L0x20014e50;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154f8; Value = 0x00000000; PC = 0x8038098 *)
mov r7 L0x200154f8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015504; Value = 0x00000000; PC = 0x803809c *)
mov r8 L0x20015504;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015510; Value = 0xffeb4785; PC = 0x80380a0 *)
mov r9 L0x20015510;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e44; PC = 0x8038130 *)
mov L0x20014e44 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e50; PC = 0x8038134 *)
mov L0x20014e50 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154f8; PC = 0x8038138 *)
mov L0x200154f8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015504; PC = 0x803813c *)
mov L0x20015504 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015510; PC = 0x8038140 *)
mov L0x20015510 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e38; PC = 0x8038144 *)
mov L0x20014e38 r4;




(**************** CUT  41, - *****************)

ecut and [
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x20014e38*x**0*z**13 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x20014e44*x**0*z**13 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x20014e50*x**0*z**13 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x200154f8*x**0*z**13 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x20015504*x**0*z**13 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x20015510*x**0*z**13 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014e5c; Value = 0xfffffffd; PC = 0x8038148 *)
mov r4 L0x20014e5c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e68; Value = 0x0014b87e; PC = 0x803814c *)
mov r5 L0x20014e68;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e74; Value = 0x0009e9bf; PC = 0x8038150 *)
mov r6 L0x20014e74;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001551c; Value = 0x00000000; PC = 0x8038154 *)
mov r7 L0x2001551c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015528; Value = 0x00000000; PC = 0x8038158 *)
mov r8 L0x20015528;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015534; Value = 0x0014b884; PC = 0x803815c *)
mov r9 L0x20015534;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e68; PC = 0x80381ec *)
mov L0x20014e68 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e74; PC = 0x80381f0 *)
mov L0x20014e74 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001551c; PC = 0x80381f4 *)
mov L0x2001551c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015528; PC = 0x80381f8 *)
mov L0x20015528 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015534; PC = 0x80381fc *)
mov L0x20015534 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e5c; PC = 0x8038200 *)
mov L0x20014e5c r4;
(* vmov	lr, s3                                     #! PC = 0x8038204 *)




(**************** CUT  42, - *****************)

ecut and [
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x20014e5c*x**0*z**13 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x20014e68*x**0*z**13 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x20014e74*x**0*z**13 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x2001551c*x**0*z**13 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x20015528*x**0*z**13 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x20015534*x**0*z**13 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014e80; Value = 0x00000000; PC = 0x8038008 *)
mov r4 L0x20014e80;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e8c; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x20014e8c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e98; Value = 0x00000006; PC = 0x8038010 *)
mov r6 L0x20014e98;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015540; Value = 0xfffffffa; PC = 0x8038014 *)
mov r7 L0x20015540;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001554c; Value = 0x00000006; PC = 0x8038018 *)
mov r8 L0x2001554c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015558; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x20015558;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e8c; PC = 0x8038074 *)
mov L0x20014e8c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e98; PC = 0x8038078 *)
mov L0x20014e98 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015540; PC = 0x803807c *)
mov L0x20015540 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001554c; PC = 0x8038080 *)
mov L0x2001554c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015558; PC = 0x8038084 *)
mov L0x20015558 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e80; PC = 0x8038088 *)
mov L0x20014e80 r4;




(**************** CUT  43, - *****************)

ecut and [
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x20014e80*x**0*z**14 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x20014e8c*x**0*z**14 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x20014e98*x**0*z**14 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x20015540*x**0*z**14 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x2001554c*x**0*z**14 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x20015558*x**0*z**14 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014ea4; Value = 0xffeb4785; PC = 0x803808c *)
mov r4 L0x20014ea4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014eb0; Value = 0xffd68f01; PC = 0x8038090 *)
mov r5 L0x20014eb0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ebc; Value = 0xffeb4782; PC = 0x8038094 *)
mov r6 L0x20014ebc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015564; Value = 0x0014b87e; PC = 0x8038098 *)
mov r7 L0x20015564;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015570; Value = 0x0014b881; PC = 0x803809c *)
mov r8 L0x20015570;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001557c; Value = 0x0014b881; PC = 0x80380a0 *)
mov r9 L0x2001557c;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014eb0; PC = 0x8038130 *)
mov L0x20014eb0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ebc; PC = 0x8038134 *)
mov L0x20014ebc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015564; PC = 0x8038138 *)
mov L0x20015564 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015570; PC = 0x803813c *)
mov L0x20015570 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001557c; PC = 0x8038140 *)
mov L0x2001557c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014ea4; PC = 0x8038144 *)
mov L0x20014ea4 r4;




(**************** CUT  44, - *****************)

ecut and [
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x20014ea4*x**0*z**14 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x20014eb0*x**0*z**14 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x20014ebc*x**0*z**14 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x20015564*x**0*z**14 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x20015570*x**0*z**14 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x2001557c*x**0*z**14 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014ec8; Value = 0x0014b884; PC = 0x8038148 *)
mov r4 L0x20014ec8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ed4; Value = 0x002970ff; PC = 0x803814c *)
mov r5 L0x20014ed4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ee0; Value = 0x0014b881; PC = 0x8038150 *)
mov r6 L0x20014ee0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015588; Value = 0xffeb477f; PC = 0x8038154 *)
mov r7 L0x20015588;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015594; Value = 0xffeb4782; PC = 0x8038158 *)
mov r8 L0x20015594;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155a0; Value = 0xffeb4782; PC = 0x803815c *)
mov r9 L0x200155a0;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ed4; PC = 0x80381ec *)
mov L0x20014ed4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ee0; PC = 0x80381f0 *)
mov L0x20014ee0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015588; PC = 0x80381f4 *)
mov L0x20015588 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015594; PC = 0x80381f8 *)
mov L0x20015594 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155a0; PC = 0x80381fc *)
mov L0x200155a0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014ec8; PC = 0x8038200 *)
mov L0x20014ec8 r4;




(**************** CUT  45, - *****************)

ecut and [
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x20014ec8*x**0*z**14 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x20014ed4*x**0*z**14 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x20014ee0*x**0*z**14 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x20015588*x**0*z**14 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x20015594*x**0*z**14 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x200155a0*x**0*z**14 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014eec; Value = 0x00000006; PC = 0x8038008 *)
mov r4 L0x20014eec;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ef8; Value = 0x00000009; PC = 0x803800c *)
mov r5 L0x20014ef8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014f04; Value = 0x00000006; PC = 0x8038010 *)
mov r6 L0x20014f04;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155ac; Value = 0x00000006; PC = 0x8038014 *)
mov r7 L0x200155ac;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155b8; Value = 0xfffffffd; PC = 0x8038018 *)
mov r8 L0x200155b8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155c4; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x200155c4;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ef8; PC = 0x8038074 *)
mov L0x20014ef8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014f04; PC = 0x8038078 *)
mov L0x20014f04 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155ac; PC = 0x803807c *)
mov L0x200155ac r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155b8; PC = 0x8038080 *)
mov L0x200155b8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155c4; PC = 0x8038084 *)
mov L0x200155c4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014eec; PC = 0x8038088 *)
mov L0x20014eec r4;




(**************** CUT  46, - *****************)

ecut and [
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x20014eec*x**0*z**15 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x20014ef8*x**0*z**15 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x20014f04*x**0*z**15 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x200155ac*x**0*z**15 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x200155b8*x**0*z**15 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x200155c4*x**0*z**15 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014f10; Value = 0xffeb4782; PC = 0x803808c *)
mov r4 L0x20014f10;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014f1c; Value = 0x00000000; PC = 0x8038090 *)
mov r5 L0x20014f1c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014f28; Value = 0xffeb4782; PC = 0x8038094 *)
mov r6 L0x20014f28;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155d0; Value = 0xffeb4782; PC = 0x8038098 *)
mov r7 L0x200155d0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155dc; Value = 0xfffffffd; PC = 0x803809c *)
mov r8 L0x200155dc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155e8; Value = 0xffeb4782; PC = 0x80380a0 *)
mov r9 L0x200155e8;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014f1c; PC = 0x8038130 *)
mov L0x20014f1c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014f28; PC = 0x8038134 *)
mov L0x20014f28 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155d0; PC = 0x8038138 *)
mov L0x200155d0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155dc; PC = 0x803813c *)
mov L0x200155dc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155e8; PC = 0x8038140 *)
mov L0x200155e8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014f10; PC = 0x8038144 *)
mov L0x20014f10 r4;




(**************** CUT  47, - *****************)

ecut and [
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x20014f10*x**0*z**15 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x20014f1c*x**0*z**15 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x20014f28*x**0*z**15 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x200155d0*x**0*z**15 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x200155dc*x**0*z**15 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x200155e8*x**0*z**15 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014f34; Value = 0x0014b881; PC = 0x8038148 *)
mov r4 L0x20014f34;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014f40; Value = 0x00000000; PC = 0x803814c *)
mov r5 L0x20014f40;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014f4c; Value = 0x0014b881; PC = 0x8038150 *)
mov r6 L0x20014f4c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155f4; Value = 0x0014b881; PC = 0x8038154 *)
mov r7 L0x200155f4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015600; Value = 0xfffffffd; PC = 0x8038158 *)
mov r8 L0x20015600;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001560c; Value = 0x0014b881; PC = 0x803815c *)
mov r9 L0x2001560c;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014f40; PC = 0x80381ec *)
mov L0x20014f40 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014f4c; PC = 0x80381f0 *)
mov L0x20014f4c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155f4; PC = 0x80381f4 *)
mov L0x200155f4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015600; PC = 0x80381f8 *)
mov L0x20015600 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001560c; PC = 0x80381fc *)
mov L0x2001560c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014f34; PC = 0x8038200 *)
mov L0x20014f34 r4;




(**************** CUT  48, - *****************)

ecut and [
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x20014f34*x**0*z**15 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x20014f40*x**0*z**15 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x20014f4c*x**0*z**15 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x200155f4*x**0*z**15 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x20015600*x**0*z**15 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x2001560c*x**0*z**15 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* sub.w	r0, r0, #1728	; 0x6c0                     #! PC = 0x8038210 *)
subs dc r0 r0 1728@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8038214 *)
adds dc r0 r0 4@uint32;
(* vmov	r12, s2                                    #! PC = 0x8038218 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x803821c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8038000 <_3x2_light>                   #! PC = 0x8038220 *)
#bne.w	0x8038000 <_3x2_light>                   #! 0x8038220 = 0x8038220;
(* add.w	lr, r0, #1728	; 0x6c0                     #! PC = 0x8038000 *)
adds dc lr r0 1728@uint32;
(* vmov	s3, lr                                     #! PC = 0x8038004 *)
mov s3 lr;
(* ldr.w	r4, [r0]                                  #! EA = L0x2001489c; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x2001489c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200148a8; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x200148a8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200148b4; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x200148b4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f5c; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x20014f5c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f68; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x20014f68;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f74; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x20014f74;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200148a8; PC = 0x8038074 *)
mov L0x200148a8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200148b4; PC = 0x8038078 *)
mov L0x200148b4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f5c; PC = 0x803807c *)
mov L0x20014f5c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f68; PC = 0x8038080 *)
mov L0x20014f68 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f74; PC = 0x8038084 *)
mov L0x20014f74 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001489c; PC = 0x8038088 *)
mov L0x2001489c r4;




(**************** CUT  49, - *****************)

ecut and [
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x2001489c*x**1*z** 0 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x200148a8*x**1*z** 0 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x200148b4*x**1*z** 0 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x20014f5c*x**1*z** 0 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x20014f68*x**1*z** 0 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x20014f74*x**1*z** 0 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200148c0; Value = 0x00000006; PC = 0x803808c *)
mov r4 L0x200148c0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200148cc; Value = 0x00000000; PC = 0x8038090 *)
mov r5 L0x200148cc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200148d8; Value = 0x0014b87e; PC = 0x8038094 *)
mov r6 L0x200148d8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f80; Value = 0xfffffffa; PC = 0x8038098 *)
mov r7 L0x20014f80;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f8c; Value = 0xffeb477c; PC = 0x803809c *)
mov r8 L0x20014f8c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f98; Value = 0xffeb4785; PC = 0x80380a0 *)
mov r9 L0x20014f98;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200148cc; PC = 0x8038130 *)
mov L0x200148cc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200148d8; PC = 0x8038134 *)
mov L0x200148d8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f80; PC = 0x8038138 *)
mov L0x20014f80 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f8c; PC = 0x803813c *)
mov L0x20014f8c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f98; PC = 0x8038140 *)
mov L0x20014f98 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200148c0; PC = 0x8038144 *)
mov L0x200148c0 r4;




(**************** CUT  50, - *****************)

ecut and [
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x200148c0*x**1*z** 0 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x200148cc*x**1*z** 0 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x200148d8*x**1*z** 0 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x20014f80*x**1*z** 0 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x20014f8c*x**1*z** 0 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x20014f98*x**1*z** 0 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200148e4; Value = 0x00000006; PC = 0x8038148 *)
mov r4 L0x200148e4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200148f0; Value = 0x00000000; PC = 0x803814c *)
mov r5 L0x200148f0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200148fc; Value = 0xffeb477f; PC = 0x8038150 *)
mov r6 L0x200148fc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fa4; Value = 0xfffffffa; PC = 0x8038154 *)
mov r7 L0x20014fa4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fb0; Value = 0x0014b87b; PC = 0x8038158 *)
mov r8 L0x20014fb0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fbc; Value = 0x0014b884; PC = 0x803815c *)
mov r9 L0x20014fbc;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200148f0; PC = 0x80381ec *)
mov L0x200148f0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200148fc; PC = 0x80381f0 *)
mov L0x200148fc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fa4; PC = 0x80381f4 *)
mov L0x20014fa4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fb0; PC = 0x80381f8 *)
mov L0x20014fb0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fbc; PC = 0x80381fc *)
mov L0x20014fbc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200148e4; PC = 0x8038200 *)
mov L0x200148e4 r4;




(**************** CUT  51, - *****************)

ecut and [
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x200148e4*x**1*z** 0 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x200148f0*x**1*z** 0 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x200148fc*x**1*z** 0 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x20014fa4*x**1*z** 0 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x20014fb0*x**1*z** 0 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x20014fbc*x**1*z** 0 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014908; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014908;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014914; Value = 0xfffffff7; PC = 0x803800c *)
mov r5 L0x20014914;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014920; Value = 0xfffffffd; PC = 0x8038010 *)
mov r6 L0x20014920;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fc8; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x20014fc8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fd4; Value = 0x00000006; PC = 0x8038018 *)
mov r8 L0x20014fd4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fe0; Value = 0x00000003; PC = 0x803801c *)
mov r9 L0x20014fe0;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014914; PC = 0x8038074 *)
mov L0x20014914 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014920; PC = 0x8038078 *)
mov L0x20014920 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fc8; PC = 0x803807c *)
mov L0x20014fc8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fd4; PC = 0x8038080 *)
mov L0x20014fd4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fe0; PC = 0x8038084 *)
mov L0x20014fe0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014908; PC = 0x8038088 *)
mov L0x20014908 r4;




(**************** CUT  52, - *****************)

ecut and [
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014908*x**1*z** 1 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014914*x**1*z** 1 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014920*x**1*z** 1 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014fc8*x**1*z** 1 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014fd4*x**1*z** 1 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014fe0*x**1*z** 1 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x2001492c; Value = 0xffeb4782; PC = 0x803808c *)
mov r4 L0x2001492c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014938; Value = 0x00000000; PC = 0x8038090 *)
mov r5 L0x20014938;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014944; Value = 0xfffffffd; PC = 0x8038094 *)
mov r6 L0x20014944;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fec; Value = 0xffeb477c; PC = 0x8038098 *)
mov r7 L0x20014fec;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014ff8; Value = 0xffeb4782; PC = 0x803809c *)
mov r8 L0x20014ff8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015004; Value = 0xffd68f04; PC = 0x80380a0 *)
mov r9 L0x20015004;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014938; PC = 0x8038130 *)
mov L0x20014938 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014944; PC = 0x8038134 *)
mov L0x20014944 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fec; PC = 0x8038138 *)
mov L0x20014fec r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014ff8; PC = 0x803813c *)
mov L0x20014ff8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015004; PC = 0x8038140 *)
mov L0x20015004 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001492c; PC = 0x8038144 *)
mov L0x2001492c r4;




(**************** CUT  53, - *****************)

ecut and [
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x2001492c*x**1*z** 1 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x20014938*x**1*z** 1 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x20014944*x**1*z** 1 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x20014fec*x**1*z** 1 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x20014ff8*x**1*z** 1 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x20015004*x**1*z** 1 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014950; Value = 0x0014b881; PC = 0x8038148 *)
mov r4 L0x20014950;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001495c; Value = 0x00000000; PC = 0x803814c *)
mov r5 L0x2001495c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014968; Value = 0xfffffffd; PC = 0x8038150 *)
mov r6 L0x20014968;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015010; Value = 0x0014b87b; PC = 0x8038154 *)
mov r7 L0x20015010;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001501c; Value = 0x0014b881; PC = 0x8038158 *)
mov r8 L0x2001501c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015028; Value = 0x00297102; PC = 0x803815c *)
mov r9 L0x20015028;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001495c; PC = 0x80381ec *)
mov L0x2001495c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014968; PC = 0x80381f0 *)
mov L0x20014968 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015010; PC = 0x80381f4 *)
mov L0x20015010 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001501c; PC = 0x80381f8 *)
mov L0x2001501c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015028; PC = 0x80381fc *)
mov L0x20015028 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014950; PC = 0x8038200 *)
mov L0x20014950 r4;




(**************** CUT  54, - *****************)

ecut and [
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x20014950*x**1*z** 1 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x2001495c*x**1*z** 1 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x20014968*x**1*z** 1 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x20015010*x**1*z** 1 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x2001501c*x**1*z** 1 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x20015028*x**1*z** 1 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014974; Value = 0x00000000; PC = 0x8038008 *)
mov r4 L0x20014974;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014980; Value = 0xfffffffa; PC = 0x803800c *)
mov r5 L0x20014980;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001498c; Value = 0xfffffffd; PC = 0x8038010 *)
mov r6 L0x2001498c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015034; Value = 0x00000006; PC = 0x8038014 *)
mov r7 L0x20015034;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015040; Value = 0x00000003; PC = 0x8038018 *)
mov r8 L0x20015040;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001504c; Value = 0xfffffffa; PC = 0x803801c *)
mov r9 L0x2001504c;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014980; PC = 0x8038074 *)
mov L0x20014980 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001498c; PC = 0x8038078 *)
mov L0x2001498c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015034; PC = 0x803807c *)
mov L0x20015034 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015040; PC = 0x8038080 *)
mov L0x20015040 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001504c; PC = 0x8038084 *)
mov L0x2001504c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014974; PC = 0x8038088 *)
mov L0x20014974 r4;




(**************** CUT  55, - *****************)

ecut and [
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x20014974*x**1*z** 2 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x20014980*x**1*z** 2 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x2001498c*x**1*z** 2 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x20015034*x**1*z** 2 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x20015040*x**1*z** 2 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x2001504c*x**1*z** 2 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014998; Value = 0x00000000; PC = 0x803808c *)
mov r4 L0x20014998;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200149a4; Value = 0x0014b87e; PC = 0x8038090 *)
mov r5 L0x200149a4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200149b0; Value = 0xfffffffd; PC = 0x8038094 *)
mov r6 L0x200149b0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015058; Value = 0x0014b881; PC = 0x8038098 *)
mov r7 L0x20015058;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015064; Value = 0xffeb477f; PC = 0x803809c *)
mov r8 L0x20015064;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015070; Value = 0x0014b87e; PC = 0x80380a0 *)
mov r9 L0x20015070;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200149a4; PC = 0x8038130 *)
mov L0x200149a4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200149b0; PC = 0x8038134 *)
mov L0x200149b0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015058; PC = 0x8038138 *)
mov L0x20015058 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015064; PC = 0x803813c *)
mov L0x20015064 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015070; PC = 0x8038140 *)
mov L0x20015070 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014998; PC = 0x8038144 *)
mov L0x20014998 r4;




(**************** CUT  56, - *****************)

ecut and [
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x20014998*x**1*z** 2 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x200149a4*x**1*z** 2 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x200149b0*x**1*z** 2 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x20015058*x**1*z** 2 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x20015064*x**1*z** 2 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x20015070*x**1*z** 2 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200149bc; Value = 0x00000000; PC = 0x8038148 *)
mov r4 L0x200149bc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200149c8; Value = 0xffeb477f; PC = 0x803814c *)
mov r5 L0x200149c8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200149d4; Value = 0xfffffffd; PC = 0x8038150 *)
mov r6 L0x200149d4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001507c; Value = 0xffeb4782; PC = 0x8038154 *)
mov r7 L0x2001507c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015088; Value = 0x0014b87e; PC = 0x8038158 *)
mov r8 L0x20015088;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015094; Value = 0xffeb477f; PC = 0x803815c *)
mov r9 L0x20015094;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200149c8; PC = 0x80381ec *)
mov L0x200149c8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200149d4; PC = 0x80381f0 *)
mov L0x200149d4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001507c; PC = 0x80381f4 *)
mov L0x2001507c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015088; PC = 0x80381f8 *)
mov L0x20015088 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015094; PC = 0x80381fc *)
mov L0x20015094 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200149bc; PC = 0x8038200 *)
mov L0x200149bc r4;



(**************** CUT  57, - *****************)

ecut and [
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x200149bc*x**1*z** 2 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x200149c8*x**1*z** 2 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x200149d4*x**1*z** 2 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x2001507c*x**1*z** 2 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x20015088*x**1*z** 2 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x20015094*x**1*z** 2 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x200149e0; Value = 0x00000003; PC = 0x8038008 *)
mov r4 L0x200149e0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200149ec; Value = 0xfffffffd; PC = 0x803800c *)
mov r5 L0x200149ec;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200149f8; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x200149f8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150a0; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x200150a0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150ac; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x200150ac;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150b8; Value = 0x00000003; PC = 0x803801c *)
mov r9 L0x200150b8;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200149ec; PC = 0x8038074 *)
mov L0x200149ec r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200149f8; PC = 0x8038078 *)
mov L0x200149f8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150a0; PC = 0x803807c *)
mov L0x200150a0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150ac; PC = 0x8038080 *)
mov L0x200150ac r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150b8; PC = 0x8038084 *)
mov L0x200150b8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200149e0; PC = 0x8038088 *)
mov L0x200149e0 r4;




(**************** CUT  58, - *****************)

ecut and [
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200149e0*x**1*z** 3 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200149ec*x**1*z** 3 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200149f8*x**1*z** 3 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200150a0*x**1*z** 3 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200150ac*x**1*z** 3 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200150b8*x**1*z** 3 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a04; Value = 0x00000003; PC = 0x803808c *)
mov r4 L0x20014a04;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a10; Value = 0x0009e9bf; PC = 0x8038090 *)
mov r5 L0x20014a10;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a1c; Value = 0xffeb477f; PC = 0x8038094 *)
mov r6 L0x20014a1c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150c4; Value = 0x0014b87b; PC = 0x8038098 *)
mov r7 L0x200150c4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150d0; Value = 0x0014b884; PC = 0x803809c *)
mov r8 L0x200150d0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150dc; Value = 0xffeb477f; PC = 0x80380a0 *)
mov r9 L0x200150dc;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a10; PC = 0x8038130 *)
mov L0x20014a10 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a1c; PC = 0x8038134 *)
mov L0x20014a1c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150c4; PC = 0x8038138 *)
mov L0x200150c4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150d0; PC = 0x803813c *)
mov L0x200150d0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150dc; PC = 0x8038140 *)
mov L0x200150dc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a04; PC = 0x8038144 *)
mov L0x20014a04 r4;




(**************** CUT  59, - *****************)

ecut and [
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x20014a04*x**1*z** 3 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x20014a10*x**1*z** 3 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x20014a1c*x**1*z** 3 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x200150c4*x**1*z** 3 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x200150d0*x**1*z** 3 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x200150dc*x**1*z** 3 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a28; Value = 0x00000003; PC = 0x8038148 *)
mov r4 L0x20014a28;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a34; Value = 0xfff6163b; PC = 0x803814c *)
mov r5 L0x20014a34;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a40; Value = 0x0014b87e; PC = 0x8038150 *)
mov r6 L0x20014a40;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150e8; Value = 0xffeb477c; PC = 0x8038154 *)
mov r7 L0x200150e8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150f4; Value = 0xffeb4785; PC = 0x8038158 *)
mov r8 L0x200150f4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015100; Value = 0x0014b87e; PC = 0x803815c *)
mov r9 L0x20015100;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a34; PC = 0x80381ec *)
mov L0x20014a34 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a40; PC = 0x80381f0 *)
mov L0x20014a40 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150e8; PC = 0x80381f4 *)
mov L0x200150e8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150f4; PC = 0x80381f8 *)
mov L0x200150f4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015100; PC = 0x80381fc *)
mov L0x20015100 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a28; PC = 0x8038200 *)
mov L0x20014a28 r4;




(**************** CUT  60, - *****************)

ecut and [
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x20014a28*x**1*z** 3 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x20014a34*x**1*z** 3 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x20014a40*x**1*z** 3 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x200150e8*x**1*z** 3 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x200150f4*x**1*z** 3 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x20015100*x**1*z** 3 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014a4c; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014a4c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a58; Value = 0x00001000; PC = 0x803800c *)
mov r5 L0x20014a58;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a64; Value = 0xfffffffd; PC = 0x8038010 *)
mov r6 L0x20014a64;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001510c; Value = 0x00000006; PC = 0x8038014 *)
mov r7 L0x2001510c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015118; Value = 0xfffffffd; PC = 0x8038018 *)
mov r8 L0x20015118;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015124; Value = 0xfffffff7; PC = 0x803801c *)
mov r9 L0x20015124;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a58; PC = 0x8038074 *)
mov L0x20014a58 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a64; PC = 0x8038078 *)
mov L0x20014a64 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001510c; PC = 0x803807c *)
mov L0x2001510c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015118; PC = 0x8038080 *)
mov L0x20015118 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015124; PC = 0x8038084 *)
mov L0x20015124 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a4c; PC = 0x8038088 *)
mov L0x20014a4c r4;




(**************** CUT  61, - *****************)

ecut and [
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x20014a4c*x**1*z** 4 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x20014a58*x**1*z** 4 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x20014a64*x**1*z** 4 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x2001510c*x**1*z** 4 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x20015118*x**1*z** 4 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x20015124*x**1*z** 4 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a70; Value = 0xffeb4782; PC = 0x803808c *)
mov r4 L0x20014a70;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a7c; Value = 0xfffbbba5; PC = 0x8038090 *)
mov r5 L0x20014a7c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a88; Value = 0xffeb4782; PC = 0x8038094 *)
mov r6 L0x20014a88;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015130; Value = 0x0014b881; PC = 0x8038098 *)
mov r7 L0x20015130;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001513c; Value = 0x0014b881; PC = 0x803809c *)
mov r8 L0x2001513c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015148; Value = 0x00000000; PC = 0x80380a0 *)
mov r9 L0x20015148;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a7c; PC = 0x8038130 *)
mov L0x20014a7c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a88; PC = 0x8038134 *)
mov L0x20014a88 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015130; PC = 0x8038138 *)
mov L0x20015130 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001513c; PC = 0x803813c *)
mov L0x2001513c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015148; PC = 0x8038140 *)
mov L0x20015148 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a70; PC = 0x8038144 *)
mov L0x20014a70 r4;




(**************** CUT  62, - *****************)

ecut and [
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x20014a70*x**1*z** 4 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x20014a7c*x**1*z** 4 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x20014a88*x**1*z** 4 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x20015130*x**1*z** 4 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x2001513c*x**1*z** 4 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x20015148*x**1*z** 4 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a94; Value = 0x0014b881; PC = 0x8038148 *)
mov r4 L0x20014a94;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014aa0; Value = 0x0004345b; PC = 0x803814c *)
mov r5 L0x20014aa0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014aac; Value = 0x0014b881; PC = 0x8038150 *)
mov r6 L0x20014aac;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015154; Value = 0xffeb4782; PC = 0x8038154 *)
mov r7 L0x20015154;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015160; Value = 0xffeb4782; PC = 0x8038158 *)
mov r8 L0x20015160;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001516c; Value = 0x00000000; PC = 0x803815c *)
mov r9 L0x2001516c;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014aa0; PC = 0x80381ec *)
mov L0x20014aa0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014aac; PC = 0x80381f0 *)
mov L0x20014aac r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015154; PC = 0x80381f4 *)
mov L0x20015154 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015160; PC = 0x80381f8 *)
mov L0x20015160 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001516c; PC = 0x80381fc *)
mov L0x2001516c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a94; PC = 0x8038200 *)
mov L0x20014a94 r4;




(**************** CUT  63, - *****************)

ecut and [
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x20014a94*x**1*z** 4 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x20014aa0*x**1*z** 4 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x20014aac*x**1*z** 4 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x20015154*x**1*z** 4 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x20015160*x**1*z** 4 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x2001516c*x**1*z** 4 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014ab8; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014ab8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ac4; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x20014ac4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ad0; Value = 0x00000006; PC = 0x8038010 *)
mov r6 L0x20014ad0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015178; Value = 0xfffffffd; PC = 0x8038014 *)
mov r7 L0x20015178;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015184; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x20015184;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015190; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x20015190;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ac4; PC = 0x8038074 *)
mov L0x20014ac4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ad0; PC = 0x8038078 *)
mov L0x20014ad0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015178; PC = 0x803807c *)
mov L0x20015178 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015184; PC = 0x8038080 *)
mov L0x20015184 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015190; PC = 0x8038084 *)
mov L0x20015190 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014ab8; PC = 0x8038088 *)
mov L0x20014ab8 r4;




(**************** CUT  64, - *****************)

ecut and [
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20014ab8*x**1*z** 5 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20014ac4*x**1*z** 5 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20014ad0*x**1*z** 5 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20015178*x**1*z** 5 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20015184*x**1*z** 5 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20015190*x**1*z** 5 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014adc; Value = 0x002970fc; PC = 0x803808c *)
mov r4 L0x20014adc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ae8; Value = 0x0014b87b; PC = 0x8038090 *)
mov r5 L0x20014ae8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014af4; Value = 0xfffffffd; PC = 0x8038094 *)
mov r6 L0x20014af4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001519c; Value = 0x00000006; PC = 0x8038098 *)
mov r7 L0x2001519c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151a8; Value = 0x0014b87b; PC = 0x803809c *)
mov r8 L0x200151a8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151b4; Value = 0x002970ff; PC = 0x80380a0 *)
mov r9 L0x200151b4;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ae8; PC = 0x8038130 *)
mov L0x20014ae8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014af4; PC = 0x8038134 *)
mov L0x20014af4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001519c; PC = 0x8038138 *)
mov L0x2001519c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151a8; PC = 0x803813c *)
mov L0x200151a8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151b4; PC = 0x8038140 *)
mov L0x200151b4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014adc; PC = 0x8038144 *)
mov L0x20014adc r4;




(**************** CUT  65, - *****************)

ecut and [
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x20014adc*x**1*z** 5 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x20014ae8*x**1*z** 5 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x20014af4*x**1*z** 5 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x2001519c*x**1*z** 5 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x200151a8*x**1*z** 5 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x200151b4*x**1*z** 5 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014b00; Value = 0xffd68efe; PC = 0x8038148 *)
mov r4 L0x20014b00;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b0c; Value = 0xffeb477c; PC = 0x803814c *)
mov r5 L0x20014b0c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b18; Value = 0xfffffffd; PC = 0x8038150 *)
mov r6 L0x20014b18;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151c0; Value = 0x00000006; PC = 0x8038154 *)
mov r7 L0x200151c0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151cc; Value = 0xffeb477c; PC = 0x8038158 *)
mov r8 L0x200151cc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151d8; Value = 0xffd68f01; PC = 0x803815c *)
mov r9 L0x200151d8;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b0c; PC = 0x80381ec *)
mov L0x20014b0c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b18; PC = 0x80381f0 *)
mov L0x20014b18 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151c0; PC = 0x80381f4 *)
mov L0x200151c0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151cc; PC = 0x80381f8 *)
mov L0x200151cc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151d8; PC = 0x80381fc *)
mov L0x200151d8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b00; PC = 0x8038200 *)
mov L0x20014b00 r4;




(**************** CUT  66, - *****************)

ecut and [
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x20014b00*x**1*z** 5 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x20014b0c*x**1*z** 5 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x20014b18*x**1*z** 5 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x200151c0*x**1*z** 5 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x200151cc*x**1*z** 5 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x200151d8*x**1*z** 5 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014b24; Value = 0x00000006; PC = 0x8038008 *)
mov r4 L0x20014b24;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b30; Value = 0xfffffffa; PC = 0x803800c *)
mov r5 L0x20014b30;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b3c; Value = 0xfffffffa; PC = 0x8038010 *)
mov r6 L0x20014b3c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151e4; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x200151e4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151f0; Value = 0x00000ffa; PC = 0x8038018 *)
mov r8 L0x200151f0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151fc; Value = 0x00000003; PC = 0x803801c *)
mov r9 L0x200151fc;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b30; PC = 0x8038074 *)
mov L0x20014b30 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b3c; PC = 0x8038078 *)
mov L0x20014b3c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151e4; PC = 0x803807c *)
mov L0x200151e4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151f0; PC = 0x8038080 *)
mov L0x200151f0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151fc; PC = 0x8038084 *)
mov L0x200151fc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b24; PC = 0x8038088 *)
mov L0x20014b24 r4;




(**************** CUT  67, - *****************)

ecut and [
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x20014b24*x**1*z** 6 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x20014b30*x**1*z** 6 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x20014b3c*x**1*z** 6 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x200151e4*x**1*z** 6 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x200151f0*x**1*z** 6 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x200151fc*x**1*z** 6 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014b48; Value = 0x0014b881; PC = 0x803808c *)
mov r4 L0x20014b48;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b54; Value = 0x00000003; PC = 0x8038090 *)
mov r5 L0x20014b54;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b60; Value = 0xffeb477f; PC = 0x8038094 *)
mov r6 L0x20014b60;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015208; Value = 0xffeb477c; PC = 0x8038098 *)
mov r7 L0x20015208;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015214; Value = 0x0005a56a; PC = 0x803809c *)
mov r8 L0x20015214;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015220; Value = 0x0014b87e; PC = 0x80380a0 *)
mov r9 L0x20015220;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b54; PC = 0x8038130 *)
mov L0x20014b54 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b60; PC = 0x8038134 *)
mov L0x20014b60 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015208; PC = 0x8038138 *)
mov L0x20015208 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015214; PC = 0x803813c *)
mov L0x20015214 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015220; PC = 0x8038140 *)
mov L0x20015220 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b48; PC = 0x8038144 *)
mov L0x20014b48 r4;




(**************** CUT  68, - *****************)

ecut and [
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20014b48*x**1*z** 6 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20014b54*x**1*z** 6 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20014b60*x**1*z** 6 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20015208*x**1*z** 6 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20015214*x**1*z** 6 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20015220*x**1*z** 6 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014b6c; Value = 0xffeb4782; PC = 0x8038148 *)
mov r4 L0x20014b6c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b78; Value = 0x00000003; PC = 0x803814c *)
mov r5 L0x20014b78;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b84; Value = 0x0014b87e; PC = 0x8038150 *)
mov r6 L0x20014b84;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001522c; Value = 0x0014b87b; PC = 0x8038154 *)
mov r7 L0x2001522c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015238; Value = 0xfffa4a9c; PC = 0x8038158 *)
mov r8 L0x20015238;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015244; Value = 0xffeb477f; PC = 0x803815c *)
mov r9 L0x20015244;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b78; PC = 0x80381ec *)
mov L0x20014b78 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b84; PC = 0x80381f0 *)
mov L0x20014b84 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001522c; PC = 0x80381f4 *)
mov L0x2001522c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015238; PC = 0x80381f8 *)
mov L0x20015238 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015244; PC = 0x80381fc *)
mov L0x20015244 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b6c; PC = 0x8038200 *)
mov L0x20014b6c r4;




(**************** CUT  69, - *****************)

ecut and [
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x20014b6c*x**1*z** 6 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x20014b78*x**1*z** 6 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x20014b84*x**1*z** 6 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x2001522c*x**1*z** 6 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x20015238*x**1*z** 6 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x20015244*x**1*z** 6 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014b90; Value = 0x00000003; PC = 0x8038008 *)
mov r4 L0x20014b90;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b9c; Value = 0x00000006; PC = 0x803800c *)
mov r5 L0x20014b9c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ba8; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x20014ba8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015250; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x20015250;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001525c; Value = 0xfffffffd; PC = 0x8038018 *)
mov r8 L0x2001525c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015268; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x20015268;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b9c; PC = 0x8038074 *)
mov L0x20014b9c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ba8; PC = 0x8038078 *)
mov L0x20014ba8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015250; PC = 0x803807c *)
mov L0x20015250 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001525c; PC = 0x8038080 *)
mov L0x2001525c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015268; PC = 0x8038084 *)
mov L0x20015268 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b90; PC = 0x8038088 *)
mov L0x20014b90 r4;




(**************** CUT  70, - *****************)

ecut and [
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x20014b90*x**1*z** 7 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x20014b9c*x**1*z** 7 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x20014ba8*x**1*z** 7 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x20015250*x**1*z** 7 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x2001525c*x**1*z** 7 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x20015268*x**1*z** 7 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014bb4; Value = 0xfffffffa; PC = 0x803808c *)
mov r4 L0x20014bb4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014bc0; Value = 0xfffffffd; PC = 0x8038090 *)
mov r5 L0x20014bc0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014bcc; Value = 0x0014b87e; PC = 0x8038094 *)
mov r6 L0x20014bcc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015274; Value = 0x0014b87e; PC = 0x8038098 *)
mov r7 L0x20015274;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015280; Value = 0xffeb4782; PC = 0x803809c *)
mov r8 L0x20015280;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001528c; Value = 0xffeb4782; PC = 0x80380a0 *)
mov r9 L0x2001528c;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014bc0; PC = 0x8038130 *)
mov L0x20014bc0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014bcc; PC = 0x8038134 *)
mov L0x20014bcc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015274; PC = 0x8038138 *)
mov L0x20015274 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015280; PC = 0x803813c *)
mov L0x20015280 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001528c; PC = 0x8038140 *)
mov L0x2001528c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014bb4; PC = 0x8038144 *)
mov L0x20014bb4 r4;




(**************** CUT  71, - *****************)

ecut and [
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x20014bb4*x**1*z** 7 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x20014bc0*x**1*z** 7 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x20014bcc*x**1*z** 7 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x20015274*x**1*z** 7 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x20015280*x**1*z** 7 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x2001528c*x**1*z** 7 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014bd8; Value = 0xfffffffa; PC = 0x8038148 *)
mov r4 L0x20014bd8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014be4; Value = 0xfffffffd; PC = 0x803814c *)
mov r5 L0x20014be4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014bf0; Value = 0xffeb477f; PC = 0x8038150 *)
mov r6 L0x20014bf0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015298; Value = 0xffeb477f; PC = 0x8038154 *)
mov r7 L0x20015298;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152a4; Value = 0x0014b881; PC = 0x8038158 *)
mov r8 L0x200152a4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152b0; Value = 0x0014b881; PC = 0x803815c *)
mov r9 L0x200152b0;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014be4; PC = 0x80381ec *)
mov L0x20014be4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014bf0; PC = 0x80381f0 *)
mov L0x20014bf0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015298; PC = 0x80381f4 *)
mov L0x20015298 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152a4; PC = 0x80381f8 *)
mov L0x200152a4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152b0; PC = 0x80381fc *)
mov L0x200152b0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014bd8; PC = 0x8038200 *)
mov L0x20014bd8 r4;




(**************** CUT  72, - *****************)

ecut and [
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x20014bd8*x**1*z** 7 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x20014be4*x**1*z** 7 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x20014bf0*x**1*z** 7 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x20015298*x**1*z** 7 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x200152a4*x**1*z** 7 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x200152b0*x**1*z** 7 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014bfc; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014bfc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c08; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x20014c08;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c14; Value = 0x00000000; PC = 0x8038010 *)
mov r6 L0x20014c14;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152bc; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x200152bc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152c8; Value = 0x00000003; PC = 0x8038018 *)
mov r8 L0x200152c8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152d4; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x200152d4;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c08; PC = 0x8038074 *)
mov L0x20014c08 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c14; PC = 0x8038078 *)
mov L0x20014c14 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152bc; PC = 0x803807c *)
mov L0x200152bc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152c8; PC = 0x8038080 *)
mov L0x200152c8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152d4; PC = 0x8038084 *)
mov L0x200152d4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014bfc; PC = 0x8038088 *)
mov L0x20014bfc r4;




(**************** CUT  73, - *****************)

ecut and [
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x20014bfc*x**1*z** 8 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x20014c08*x**1*z** 8 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x20014c14*x**1*z** 8 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x200152bc*x**1*z** 8 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x200152c8*x**1*z** 8 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x200152d4*x**1*z** 8 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014c20; Value = 0x002970fc; PC = 0x803808c *)
mov r4 L0x20014c20;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c2c; Value = 0x00000000; PC = 0x8038090 *)
mov r5 L0x20014c2c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c38; Value = 0x00000000; PC = 0x8038094 *)
mov r6 L0x20014c38;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152e0; Value = 0xffeb477f; PC = 0x8038098 *)
mov r7 L0x200152e0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152ec; Value = 0xffeb477f; PC = 0x803809c *)
mov r8 L0x200152ec;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152f8; Value = 0x00000006; PC = 0x80380a0 *)
mov r9 L0x200152f8;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c2c; PC = 0x8038130 *)
mov L0x20014c2c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c38; PC = 0x8038134 *)
mov L0x20014c38 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152e0; PC = 0x8038138 *)
mov L0x200152e0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152ec; PC = 0x803813c *)
mov L0x200152ec r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152f8; PC = 0x8038140 *)
mov L0x200152f8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c20; PC = 0x8038144 *)
mov L0x20014c20 r4;




(**************** CUT  74, - *****************)

ecut and [
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x20014c20*x**1*z** 8 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x20014c2c*x**1*z** 8 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x20014c38*x**1*z** 8 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x200152e0*x**1*z** 8 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x200152ec*x**1*z** 8 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x200152f8*x**1*z** 8 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014c44; Value = 0xffd68efe; PC = 0x8038148 *)
mov r4 L0x20014c44;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c50; Value = 0x00000000; PC = 0x803814c *)
mov r5 L0x20014c50;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c5c; Value = 0x00000000; PC = 0x8038150 *)
mov r6 L0x20014c5c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015304; Value = 0x0014b87e; PC = 0x8038154 *)
mov r7 L0x20015304;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015310; Value = 0x0014b87e; PC = 0x8038158 *)
mov r8 L0x20015310;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001531c; Value = 0x00000006; PC = 0x803815c *)
mov r9 L0x2001531c;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c50; PC = 0x80381ec *)
mov L0x20014c50 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c5c; PC = 0x80381f0 *)
mov L0x20014c5c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015304; PC = 0x80381f4 *)
mov L0x20015304 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015310; PC = 0x80381f8 *)
mov L0x20015310 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001531c; PC = 0x80381fc *)
mov L0x2001531c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c44; PC = 0x8038200 *)
mov L0x20014c44 r4;




(**************** CUT  75, - *****************)

ecut and [
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x20014c44*x**1*z** 8 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x20014c50*x**1*z** 8 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x20014c5c*x**1*z** 8 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x20015304*x**1*z** 8 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x20015310*x**1*z** 8 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x2001531c*x**1*z** 8 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014c68; Value = 0x00000003; PC = 0x8038008 *)
mov r4 L0x20014c68;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c74; Value = 0xfffffffa; PC = 0x803800c *)
mov r5 L0x20014c74;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c80; Value = 0x00000000; PC = 0x8038010 *)
mov r6 L0x20014c80;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015328; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x20015328;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015334; Value = 0xfffffffd; PC = 0x8038018 *)
mov r8 L0x20015334;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015340; Value = 0x00000006; PC = 0x803801c *)
mov r9 L0x20015340;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c74; PC = 0x8038074 *)
mov L0x20014c74 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c80; PC = 0x8038078 *)
mov L0x20014c80 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015328; PC = 0x803807c *)
mov L0x20015328 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015334; PC = 0x8038080 *)
mov L0x20015334 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015340; PC = 0x8038084 *)
mov L0x20015340 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c68; PC = 0x8038088 *)
mov L0x20014c68 r4;




(**************** CUT  76, - *****************)

ecut and [
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20014c68*x**1*z** 9 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20014c74*x**1*z** 9 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20014c80*x**1*z** 9 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20015328*x**1*z** 9 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20015334*x**1*z** 9 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20015340*x**1*z** 9 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014c8c; Value = 0xffd68f04; PC = 0x803808c *)
mov r4 L0x20014c8c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c98; Value = 0xffeb477f; PC = 0x8038090 *)
mov r5 L0x20014c98;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ca4; Value = 0x0014b884; PC = 0x8038094 *)
mov r6 L0x20014ca4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001534c; Value = 0x0014b884; PC = 0x8038098 *)
mov r7 L0x2001534c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015358; Value = 0x00000006; PC = 0x803809c *)
mov r8 L0x20015358;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015364; Value = 0xfffffffd; PC = 0x80380a0 *)
mov r9 L0x20015364;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c98; PC = 0x8038130 *)
mov L0x20014c98 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ca4; PC = 0x8038134 *)
mov L0x20014ca4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001534c; PC = 0x8038138 *)
mov L0x2001534c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015358; PC = 0x803813c *)
mov L0x20015358 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015364; PC = 0x8038140 *)
mov L0x20015364 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c8c; PC = 0x8038144 *)
mov L0x20014c8c r4;




(**************** CUT  77, - *****************)

ecut and [
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x20014c8c*x**1*z** 9 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x20014c98*x**1*z** 9 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x20014ca4*x**1*z** 9 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x2001534c*x**1*z** 9 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x20015358*x**1*z** 9 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x20015364*x**1*z** 9 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014cb0; Value = 0x00297102; PC = 0x8038148 *)
mov r4 L0x20014cb0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014cbc; Value = 0x0014b87e; PC = 0x803814c *)
mov r5 L0x20014cbc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014cc8; Value = 0xffeb4785; PC = 0x8038150 *)
mov r6 L0x20014cc8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015370; Value = 0xffeb4785; PC = 0x8038154 *)
mov r7 L0x20015370;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001537c; Value = 0x00000006; PC = 0x8038158 *)
mov r8 L0x2001537c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015388; Value = 0xfffffffd; PC = 0x803815c *)
mov r9 L0x20015388;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014cbc; PC = 0x80381ec *)
mov L0x20014cbc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014cc8; PC = 0x80381f0 *)
mov L0x20014cc8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015370; PC = 0x80381f4 *)
mov L0x20015370 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001537c; PC = 0x80381f8 *)
mov L0x2001537c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015388; PC = 0x80381fc *)
mov L0x20015388 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014cb0; PC = 0x8038200 *)
mov L0x20014cb0 r4;




(**************** CUT  78, - *****************)

ecut and [
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x20014cb0*x**1*z** 9 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x20014cbc*x**1*z** 9 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x20014cc8*x**1*z** 9 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x20015370*x**1*z** 9 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x2001537c*x**1*z** 9 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x20015388*x**1*z** 9 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014cd4; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014cd4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ce0; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x20014ce0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014cec; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x20014cec;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015394; Value = 0xfffffffd; PC = 0x8038014 *)
mov r7 L0x20015394;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153a0; Value = 0x00000003; PC = 0x8038018 *)
mov r8 L0x200153a0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153ac; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x200153ac;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ce0; PC = 0x8038074 *)
mov L0x20014ce0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014cec; PC = 0x8038078 *)
mov L0x20014cec r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015394; PC = 0x803807c *)
mov L0x20015394 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153a0; PC = 0x8038080 *)
mov L0x200153a0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153ac; PC = 0x8038084 *)
mov L0x200153ac r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014cd4; PC = 0x8038088 *)
mov L0x20014cd4 r4;




(**************** CUT  79, - *****************)

ecut and [
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x20014cd4*x**1*z**10 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x20014ce0*x**1*z**10 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x20014cec*x**1*z**10 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x20015394*x**1*z**10 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x200153a0*x**1*z**10 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x200153ac*x**1*z**10 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014cf8; Value = 0x0014b881; PC = 0x803808c *)
mov r4 L0x20014cf8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d04; Value = 0x00000000; PC = 0x8038090 *)
mov r5 L0x20014d04;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d10; Value = 0xfffffffa; PC = 0x8038094 *)
mov r6 L0x20014d10;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153b8; Value = 0x0014b881; PC = 0x8038098 *)
mov r7 L0x200153b8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153c4; Value = 0x0009e9c5; PC = 0x803809c *)
mov r8 L0x200153c4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153d0; Value = 0x0014b884; PC = 0x80380a0 *)
mov r9 L0x200153d0;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d04; PC = 0x8038130 *)
mov L0x20014d04 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d10; PC = 0x8038134 *)
mov L0x20014d10 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153b8; PC = 0x8038138 *)
mov L0x200153b8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153c4; PC = 0x803813c *)
mov L0x200153c4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153d0; PC = 0x8038140 *)
mov L0x200153d0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014cf8; PC = 0x8038144 *)
mov L0x20014cf8 r4;




(**************** CUT  80, - *****************)

ecut and [
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x20014cf8*x**1*z**10 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x20014d04*x**1*z**10 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x20014d10*x**1*z**10 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x200153b8*x**1*z**10 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x200153c4*x**1*z**10 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x200153d0*x**1*z**10 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014d1c; Value = 0xffeb4782; PC = 0x8038148 *)
mov r4 L0x20014d1c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d28; Value = 0x00000000; PC = 0x803814c *)
mov r5 L0x20014d28;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d34; Value = 0xfffffffa; PC = 0x8038150 *)
mov r6 L0x20014d34;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153dc; Value = 0xffeb4782; PC = 0x8038154 *)
mov r7 L0x200153dc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153e8; Value = 0xfff61641; PC = 0x8038158 *)
mov r8 L0x200153e8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153f4; Value = 0xffeb4785; PC = 0x803815c *)
mov r9 L0x200153f4;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d28; PC = 0x80381ec *)
mov L0x20014d28 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d34; PC = 0x80381f0 *)
mov L0x20014d34 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153dc; PC = 0x80381f4 *)
mov L0x200153dc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153e8; PC = 0x80381f8 *)
mov L0x200153e8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153f4; PC = 0x80381fc *)
mov L0x200153f4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d1c; PC = 0x8038200 *)
mov L0x20014d1c r4;




(**************** CUT  81, - *****************)

ecut and [
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x20014d1c*x**1*z**10 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x20014d28*x**1*z**10 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x20014d34*x**1*z**10 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x200153dc*x**1*z**10 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x200153e8*x**1*z**10 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x200153f4*x**1*z**10 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014d40; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014d40;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d4c; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x20014d4c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d58; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x20014d58;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015400; Value = 0x00000009; PC = 0x8038014 *)
mov r7 L0x20015400;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001540c; Value = 0x00000003; PC = 0x8038018 *)
mov r8 L0x2001540c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015418; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x20015418;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d4c; PC = 0x8038074 *)
mov L0x20014d4c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d58; PC = 0x8038078 *)
mov L0x20014d58 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015400; PC = 0x803807c *)
mov L0x20015400 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001540c; PC = 0x8038080 *)
mov L0x2001540c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015418; PC = 0x8038084 *)
mov L0x20015418 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d40; PC = 0x8038088 *)
mov L0x20014d40 r4;




(**************** CUT  82, - *****************)

ecut and [
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x20014d40*x**1*z**11 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x20014d4c*x**1*z**11 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x20014d58*x**1*z**11 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x20015400*x**1*z**11 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x2001540c*x**1*z**11 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x20015418*x**1*z**11 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014d64; Value = 0x0014b881; PC = 0x803808c *)
mov r4 L0x20014d64;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d70; Value = 0x0014b87b; PC = 0x8038090 *)
mov r5 L0x20014d70;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d7c; Value = 0xfffffffa; PC = 0x8038094 *)
mov r6 L0x20014d7c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015424; Value = 0x00000000; PC = 0x8038098 *)
mov r7 L0x20015424;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015430; Value = 0xffeb477f; PC = 0x803809c *)
mov r8 L0x20015430;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001543c; Value = 0x00000006; PC = 0x80380a0 *)
mov r9 L0x2001543c;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d70; PC = 0x8038130 *)
mov L0x20014d70 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d7c; PC = 0x8038134 *)
mov L0x20014d7c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015424; PC = 0x8038138 *)
mov L0x20015424 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015430; PC = 0x803813c *)
mov L0x20015430 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001543c; PC = 0x8038140 *)
mov L0x2001543c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d64; PC = 0x8038144 *)
mov L0x20014d64 r4;




(**************** CUT  83, - *****************)

ecut and [
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x20014d64*x**1*z**11 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x20014d70*x**1*z**11 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x20014d7c*x**1*z**11 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x20015424*x**1*z**11 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x20015430*x**1*z**11 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x2001543c*x**1*z**11 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014d88; Value = 0xffeb4782; PC = 0x8038148 *)
mov r4 L0x20014d88;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d94; Value = 0xffeb477c; PC = 0x803814c *)
mov r5 L0x20014d94;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014da0; Value = 0xfffffffa; PC = 0x8038150 *)
mov r6 L0x20014da0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015448; Value = 0x00000000; PC = 0x8038154 *)
mov r7 L0x20015448;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015454; Value = 0x0014b87e; PC = 0x8038158 *)
mov r8 L0x20015454;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015460; Value = 0x00000006; PC = 0x803815c *)
mov r9 L0x20015460;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d94; PC = 0x80381ec *)
mov L0x20014d94 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014da0; PC = 0x80381f0 *)
mov L0x20014da0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015448; PC = 0x80381f4 *)
mov L0x20015448 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015454; PC = 0x80381f8 *)
mov L0x20015454 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015460; PC = 0x80381fc *)
mov L0x20015460 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d88; PC = 0x8038200 *)
mov L0x20014d88 r4;



(**************** CUT  84, - *****************)

ecut and [
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20014d88*x**1*z**11 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20014d94*x**1*z**11 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20014da0*x**1*z**11 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20015448*x**1*z**11 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20015454*x**1*z**11 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20015460*x**1*z**11 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014dac; Value = 0x00000000; PC = 0x8038008 *)
mov r4 L0x20014dac;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014db8; Value = 0x00000009; PC = 0x803800c *)
mov r5 L0x20014db8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014dc4; Value = 0xfffffffd; PC = 0x8038010 *)
mov r6 L0x20014dc4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001546c; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x2001546c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015478; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x20015478;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015484; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x20015484;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014db8; PC = 0x8038074 *)
mov L0x20014db8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014dc4; PC = 0x8038078 *)
mov L0x20014dc4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001546c; PC = 0x803807c *)
mov L0x2001546c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015478; PC = 0x8038080 *)
mov L0x20015478 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015484; PC = 0x8038084 *)
mov L0x20015484 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014dac; PC = 0x8038088 *)
mov L0x20014dac r4;




(**************** CUT  85, - *****************)

ecut and [
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x20014dac*x**1*z**12 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x20014db8*x**1*z**12 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x20014dc4*x**1*z**12 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x2001546c*x**1*z**12 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x20015478*x**1*z**12 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x20015484*x**1*z**12 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014dd0; Value = 0xffeb4785; PC = 0x803808c *)
mov r4 L0x20014dd0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ddc; Value = 0x00000000; PC = 0x8038090 *)
mov r5 L0x20014ddc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014de8; Value = 0xffd68efe; PC = 0x8038094 *)
mov r6 L0x20014de8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015490; Value = 0xffeb477c; PC = 0x8038098 *)
mov r7 L0x20015490;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001549c; Value = 0x00000000; PC = 0x803809c *)
mov r8 L0x2001549c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154a8; Value = 0xffeb477c; PC = 0x80380a0 *)
mov r9 L0x200154a8;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ddc; PC = 0x8038130 *)
mov L0x20014ddc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014de8; PC = 0x8038134 *)
mov L0x20014de8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015490; PC = 0x8038138 *)
mov L0x20015490 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001549c; PC = 0x803813c *)
mov L0x2001549c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154a8; PC = 0x8038140 *)
mov L0x200154a8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014dd0; PC = 0x8038144 *)
mov L0x20014dd0 r4;


(**************** CUT  86, - *****************)

ecut and [
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x20014dd0*x**1*z**12 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x20014ddc*x**1*z**12 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x20014de8*x**1*z**12 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x20015490*x**1*z**12 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x2001549c*x**1*z**12 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x200154a8*x**1*z**12 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014df4; Value = 0x0014b884; PC = 0x8038148 *)
mov r4 L0x20014df4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e00; Value = 0x00000000; PC = 0x803814c *)
mov r5 L0x20014e00;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e0c; Value = 0x002970fc; PC = 0x8038150 *)
mov r6 L0x20014e0c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154b4; Value = 0x0014b87b; PC = 0x8038154 *)
mov r7 L0x200154b4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154c0; Value = 0x00000000; PC = 0x8038158 *)
mov r8 L0x200154c0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154cc; Value = 0x0014b87b; PC = 0x803815c *)
mov r9 L0x200154cc;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e00; PC = 0x80381ec *)
mov L0x20014e00 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e0c; PC = 0x80381f0 *)
mov L0x20014e0c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154b4; PC = 0x80381f4 *)
mov L0x200154b4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154c0; PC = 0x80381f8 *)
mov L0x200154c0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154cc; PC = 0x80381fc *)
mov L0x200154cc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014df4; PC = 0x8038200 *)
mov L0x20014df4 r4;




(**************** CUT  87, - *****************)

ecut and [
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x20014df4*x**1*z**12 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x20014e00*x**1*z**12 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x20014e0c*x**1*z**12 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x200154b4*x**1*z**12 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x200154c0*x**1*z**12 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x200154cc*x**1*z**12 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014e18; Value = 0x00000006; PC = 0x8038008 *)
mov r4 L0x20014e18;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e24; Value = 0x00000ff7; PC = 0x803800c *)
mov r5 L0x20014e24;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e30; Value = 0xfffffffd; PC = 0x8038010 *)
mov r6 L0x20014e30;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154d8; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x200154d8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154e4; Value = 0x00000006; PC = 0x8038018 *)
mov r8 L0x200154e4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154f0; Value = 0xfffffffa; PC = 0x803801c *)
mov r9 L0x200154f0;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e24; PC = 0x8038074 *)
mov L0x20014e24 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e30; PC = 0x8038078 *)
mov L0x20014e30 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154d8; PC = 0x803807c *)
mov L0x200154d8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154e4; PC = 0x8038080 *)
mov L0x200154e4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154f0; PC = 0x8038084 *)
mov L0x200154f0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e18; PC = 0x8038088 *)
mov L0x20014e18 r4;




(**************** CUT  88, - *****************)

ecut and [
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x20014e18*x**1*z**13 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x20014e24*x**1*z**13 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x20014e30*x**1*z**13 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x200154d8*x**1*z**13 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x200154e4*x**1*z**13 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x200154f0*x**1*z**13 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014e3c; Value = 0xfffffffd; PC = 0x803808c *)
mov r4 L0x20014e3c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e48; Value = 0x0005a567; PC = 0x8038090 *)
mov r5 L0x20014e48;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e54; Value = 0x00000006; PC = 0x8038094 *)
mov r6 L0x20014e54;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154fc; Value = 0xffeb4785; PC = 0x8038098 *)
mov r7 L0x200154fc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015508; Value = 0xffeb4782; PC = 0x803809c *)
mov r8 L0x20015508;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015514; Value = 0x00000003; PC = 0x80380a0 *)
mov r9 L0x20015514;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e48; PC = 0x8038130 *)
mov L0x20014e48 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e54; PC = 0x8038134 *)
mov L0x20014e54 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154fc; PC = 0x8038138 *)
mov L0x200154fc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015508; PC = 0x803813c *)
mov L0x20015508 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015514; PC = 0x8038140 *)
mov L0x20015514 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e3c; PC = 0x8038144 *)
mov L0x20014e3c r4;




(**************** CUT  89, - *****************)

ecut and [
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x20014e3c*x**1*z**13 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x20014e48*x**1*z**13 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x20014e54*x**1*z**13 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x200154fc*x**1*z**13 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x20015508*x**1*z**13 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x20015514*x**1*z**13 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014e60; Value = 0xfffffffd; PC = 0x8038148 *)
mov r4 L0x20014e60;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e6c; Value = 0xfffa4a99; PC = 0x803814c *)
mov r5 L0x20014e6c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e78; Value = 0x00000006; PC = 0x8038150 *)
mov r6 L0x20014e78;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015520; Value = 0x0014b884; PC = 0x8038154 *)
mov r7 L0x20015520;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001552c; Value = 0x0014b881; PC = 0x8038158 *)
mov r8 L0x2001552c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015538; Value = 0x00000003; PC = 0x803815c *)
mov r9 L0x20015538;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e6c; PC = 0x80381ec *)
mov L0x20014e6c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e78; PC = 0x80381f0 *)
mov L0x20014e78 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015520; PC = 0x80381f4 *)
mov L0x20015520 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001552c; PC = 0x80381f8 *)
mov L0x2001552c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015538; PC = 0x80381fc *)
mov L0x20015538 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e60; PC = 0x8038200 *)
mov L0x20014e60 r4;




(**************** CUT  90, - *****************)

ecut and [
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x20014e60*x**1*z**13 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x20014e6c*x**1*z**13 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x20014e78*x**1*z**13 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x20015520*x**1*z**13 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x2001552c*x**1*z**13 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x20015538*x**1*z**13 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014e84; Value = 0x00000006; PC = 0x8038008 *)
mov r4 L0x20014e84;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e90; Value = 0x00000003; PC = 0x803800c *)
mov r5 L0x20014e90;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e9c; Value = 0x00000009; PC = 0x8038010 *)
mov r6 L0x20014e9c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015544; Value = 0xfffffffd; PC = 0x8038014 *)
mov r7 L0x20015544;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015550; Value = 0x00000003; PC = 0x8038018 *)
mov r8 L0x20015550;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001555c; Value = 0x00000006; PC = 0x803801c *)
mov r9 L0x2001555c;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e90; PC = 0x8038074 *)
mov L0x20014e90 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e9c; PC = 0x8038078 *)
mov L0x20014e9c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015544; PC = 0x803807c *)
mov L0x20015544 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015550; PC = 0x8038080 *)
mov L0x20015550 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001555c; PC = 0x8038084 *)
mov L0x2001555c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e84; PC = 0x8038088 *)
mov L0x20014e84 r4;




(**************** CUT  91, - *****************)

ecut and [
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x20014e84*x**1*z**14 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x20014e90*x**1*z**14 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x20014e9c*x**1*z**14 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x20015544*x**1*z**14 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x20015550*x**1*z**14 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x2001555c*x**1*z**14 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014ea8; Value = 0xfffffffd; PC = 0x803808c *)
mov r4 L0x20014ea8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014eb4; Value = 0xffeb477f; PC = 0x8038090 *)
mov r5 L0x20014eb4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ec0; Value = 0x00000000; PC = 0x8038094 *)
mov r6 L0x20014ec0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015568; Value = 0xfff6163b; PC = 0x8038098 *)
mov r7 L0x20015568;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015574; Value = 0x0014b87e; PC = 0x803809c *)
mov r8 L0x20015574;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015580; Value = 0x0014b881; PC = 0x80380a0 *)
mov r9 L0x20015580;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014eb4; PC = 0x8038130 *)
mov L0x20014eb4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ec0; PC = 0x8038134 *)
mov L0x20014ec0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015568; PC = 0x8038138 *)
mov L0x20015568 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015574; PC = 0x803813c *)
mov L0x20015574 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015580; PC = 0x8038140 *)
mov L0x20015580 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014ea8; PC = 0x8038144 *)
mov L0x20014ea8 r4;




(**************** CUT  92, - *****************)

ecut and [
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20014ea8*x**1*z**14 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20014eb4*x**1*z**14 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20014ec0*x**1*z**14 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20015568*x**1*z**14 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20015574*x**1*z**14 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20015580*x**1*z**14 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014ecc; Value = 0xfffffffd; PC = 0x8038148 *)
mov r4 L0x20014ecc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ed8; Value = 0x0014b87e; PC = 0x803814c *)
mov r5 L0x20014ed8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ee4; Value = 0x00000000; PC = 0x8038150 *)
mov r6 L0x20014ee4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001558c; Value = 0x0009e9bf; PC = 0x8038154 *)
mov r7 L0x2001558c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015598; Value = 0xffeb477f; PC = 0x8038158 *)
mov r8 L0x20015598;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155a4; Value = 0xffeb4782; PC = 0x803815c *)
mov r9 L0x200155a4;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ed8; PC = 0x80381ec *)
mov L0x20014ed8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ee4; PC = 0x80381f0 *)
mov L0x20014ee4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001558c; PC = 0x80381f4 *)
mov L0x2001558c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015598; PC = 0x80381f8 *)
mov L0x20015598 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155a4; PC = 0x80381fc *)
mov L0x200155a4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014ecc; PC = 0x8038200 *)
mov L0x20014ecc r4;




(**************** CUT  93, - *****************)

ecut and [
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x20014ecc*x**1*z**14 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x20014ed8*x**1*z**14 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x20014ee4*x**1*z**14 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x2001558c*x**1*z**14 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x20015598*x**1*z**14 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x200155a4*x**1*z**14 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014ef0; Value = 0x00000003; PC = 0x8038008 *)
mov r4 L0x20014ef0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014efc; Value = 0x00000006; PC = 0x803800c *)
mov r5 L0x20014efc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014f08; Value = 0x00000000; PC = 0x8038010 *)
mov r6 L0x20014f08;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155b0; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x200155b0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155bc; Value = 0x00000003; PC = 0x8038018 *)
mov r8 L0x200155bc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155c8; Value = 0x00000006; PC = 0x803801c *)
mov r9 L0x200155c8;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014efc; PC = 0x8038074 *)
mov L0x20014efc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014f08; PC = 0x8038078 *)
mov L0x20014f08 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155b0; PC = 0x803807c *)
mov L0x200155b0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155bc; PC = 0x8038080 *)
mov L0x200155bc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155c8; PC = 0x8038084 *)
mov L0x200155c8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014ef0; PC = 0x8038088 *)
mov L0x20014ef0 r4;




(**************** CUT  94, - *****************)

ecut and [
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x20014ef0*x**1*z**15 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x20014efc*x**1*z**15 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x20014f08*x**1*z**15 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x200155b0*x**1*z**15 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x200155bc*x**1*z**15 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x200155c8*x**1*z**15 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014f14; Value = 0x00000003; PC = 0x803808c *)
mov r4 L0x20014f14;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014f20; Value = 0xffeb4782; PC = 0x8038090 *)
mov r5 L0x20014f20;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014f2c; Value = 0xffeb4785; PC = 0x8038094 *)
mov r6 L0x20014f2c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155d4; Value = 0xffeb477f; PC = 0x8038098 *)
mov r7 L0x200155d4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155e0; Value = 0xffeb477f; PC = 0x803809c *)
mov r8 L0x200155e0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155ec; Value = 0x0014b881; PC = 0x80380a0 *)
mov r9 L0x200155ec;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014f20; PC = 0x8038130 *)
mov L0x20014f20 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014f2c; PC = 0x8038134 *)
mov L0x20014f2c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155d4; PC = 0x8038138 *)
mov L0x200155d4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155e0; PC = 0x803813c *)
mov L0x200155e0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155ec; PC = 0x8038140 *)
mov L0x200155ec r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014f14; PC = 0x8038144 *)
mov L0x20014f14 r4;




(**************** CUT  95, - *****************)

ecut and [
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x20014f14*x**1*z**15 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x20014f20*x**1*z**15 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x20014f2c*x**1*z**15 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x200155d4*x**1*z**15 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x200155e0*x**1*z**15 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x200155ec*x**1*z**15 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014f38; Value = 0x00000003; PC = 0x8038148 *)
mov r4 L0x20014f38;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014f44; Value = 0x0014b881; PC = 0x803814c *)
mov r5 L0x20014f44;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014f50; Value = 0x0014b884; PC = 0x8038150 *)
mov r6 L0x20014f50;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155f8; Value = 0x0014b87e; PC = 0x8038154 *)
mov r7 L0x200155f8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015604; Value = 0x0014b87e; PC = 0x8038158 *)
mov r8 L0x20015604;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015610; Value = 0xffeb4782; PC = 0x803815c *)
mov r9 L0x20015610;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014f44; PC = 0x80381ec *)
mov L0x20014f44 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014f50; PC = 0x80381f0 *)
mov L0x20014f50 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155f8; PC = 0x80381f4 *)
mov L0x200155f8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015604; PC = 0x80381f8 *)
mov L0x20015604 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015610; PC = 0x80381fc *)
mov L0x20015610 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014f38; PC = 0x8038200 *)
mov L0x20014f38 r4;




(**************** CUT  96, - *****************)

ecut and [
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x20014f38*x**1*z**15 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x20014f44*x**1*z**15 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x20014f50*x**1*z**15 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x200155f8*x**1*z**15 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x20015604*x**1*z**15 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x20015610*x**1*z**15 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* sub.w	r0, r0, #1728	; 0x6c0                     #! PC = 0x8038210 *)
subs dc r0 r0 1728@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8038214 *)
adds dc r0 r0 4@uint32;
(* vmov	r12, s2                                    #! PC = 0x8038218 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x803821c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8038000 <_3x2_light>                   #! PC = 0x8038220 *)
#bne.w	0x8038000 <_3x2_light>                   #! 0x8038220 = 0x8038220;
(* add.w	lr, r0, #1728	; 0x6c0                     #! PC = 0x8038000 *)
adds dc lr r0 1728@uint32;
(* vmov	s3, lr                                     #! PC = 0x8038004 *)
mov s3 lr;
(* ldr.w	r4, [r0]                                  #! EA = L0x200148a0; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x200148a0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200148ac; Value = 0xfffffffd; PC = 0x803800c *)
mov r5 L0x200148ac;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200148b8; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x200148b8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f60; Value = 0xfffffff7; PC = 0x8038014 *)
mov r7 L0x20014f60;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f6c; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x20014f6c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f78; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x20014f78;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200148ac; PC = 0x8038074 *)
mov L0x200148ac r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200148b8; PC = 0x8038078 *)
mov L0x200148b8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f60; PC = 0x803807c *)
mov L0x20014f60 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f6c; PC = 0x8038080 *)
mov L0x20014f6c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f78; PC = 0x8038084 *)
mov L0x20014f78 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200148a0; PC = 0x8038088 *)
mov L0x200148a0 r4;




(**************** CUT  97, - *****************)

ecut and [
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x200148a0*x**2*z** 0 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x200148ac*x**2*z** 0 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x200148b8*x**2*z** 0 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x20014f60*x**2*z** 0 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x20014f6c*x**2*z** 0 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x20014f78*x**2*z** 0 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200148c4; Value = 0xfffffffd; PC = 0x803808c *)
mov r4 L0x200148c4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200148d0; Value = 0x002970fc; PC = 0x8038090 *)
mov r5 L0x200148d0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200148dc; Value = 0x0009e9c5; PC = 0x8038094 *)
mov r6 L0x200148dc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f84; Value = 0x00000000; PC = 0x8038098 *)
mov r7 L0x20014f84;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f90; Value = 0xffeb477c; PC = 0x803809c *)
mov r8 L0x20014f90;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f9c; Value = 0x002970ff; PC = 0x80380a0 *)
mov r9 L0x20014f9c;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200148d0; PC = 0x8038130 *)
mov L0x200148d0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200148dc; PC = 0x8038134 *)
mov L0x200148dc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014f84; PC = 0x8038138 *)
mov L0x20014f84 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014f90; PC = 0x803813c *)
mov L0x20014f90 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014f9c; PC = 0x8038140 *)
mov L0x20014f9c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200148c4; PC = 0x8038144 *)
mov L0x200148c4 r4;




(**************** CUT  98, - *****************)

ecut and [
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x200148c4*x**2*z** 0 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x200148d0*x**2*z** 0 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x200148dc*x**2*z** 0 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x20014f84*x**2*z** 0 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x20014f90*x**2*z** 0 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x20014f9c*x**2*z** 0 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200148e8; Value = 0xfffffffd; PC = 0x8038148 *)
mov r4 L0x200148e8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200148f4; Value = 0xffd68efe; PC = 0x803814c *)
mov r5 L0x200148f4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014900; Value = 0xfff61641; PC = 0x8038150 *)
mov r6 L0x20014900;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fa8; Value = 0x00000000; PC = 0x8038154 *)
mov r7 L0x20014fa8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fb4; Value = 0x0014b87b; PC = 0x8038158 *)
mov r8 L0x20014fb4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fc0; Value = 0xffd68f01; PC = 0x803815c *)
mov r9 L0x20014fc0;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200148f4; PC = 0x80381ec *)
mov L0x200148f4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014900; PC = 0x80381f0 *)
mov L0x20014900 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fa8; PC = 0x80381f4 *)
mov L0x20014fa8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fb4; PC = 0x80381f8 *)
mov L0x20014fb4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fc0; PC = 0x80381fc *)
mov L0x20014fc0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200148e8; PC = 0x8038200 *)
mov L0x200148e8 r4;




(**************** CUT  99, - *****************)

ecut and [
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x200148e8*x**2*z** 0 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x200148f4*x**2*z** 0 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x20014900*x**2*z** 0 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x20014fa8*x**2*z** 0 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x20014fb4*x**2*z** 0 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x20014fc0*x**2*z** 0 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x2001490c; Value = 0x00000003; PC = 0x8038008 *)
mov r4 L0x2001490c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014918; Value = 0x00000006; PC = 0x803800c *)
mov r5 L0x20014918;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014924; Value = 0x00000000; PC = 0x8038010 *)
mov r6 L0x20014924;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fcc; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x20014fcc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fd8; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x20014fd8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fe4; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x20014fe4;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014918; PC = 0x8038074 *)
mov L0x20014918 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014924; PC = 0x8038078 *)
mov L0x20014924 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014fcc; PC = 0x803807c *)
mov L0x20014fcc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014fd8; PC = 0x8038080 *)
mov L0x20014fd8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20014fe4; PC = 0x8038084 *)
mov L0x20014fe4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001490c; PC = 0x8038088 *)
mov L0x2001490c r4;




(**************** CUT 100, - *****************)

ecut and [
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x2001490c*x**2*z** 1 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x20014918*x**2*z** 1 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x20014924*x**2*z** 1 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x20014fcc*x**2*z** 1 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x20014fd8*x**2*z** 1 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x20014fe4*x**2*z** 1 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014930; Value = 0x00000003; PC = 0x803808c *)
mov r4 L0x20014930;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001493c; Value = 0xfffffffd; PC = 0x8038090 *)
mov r5 L0x2001493c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014948; Value = 0x00000000; PC = 0x8038094 *)
mov r6 L0x20014948;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014ff0; Value = 0x0014b884; PC = 0x8038098 *)
mov r7 L0x20014ff0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014ffc; Value = 0x0009e9c2; PC = 0x803809c *)
mov r8 L0x20014ffc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015008; Value = 0x0014b881; PC = 0x80380a0 *)
mov r9 L0x20015008;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001493c; PC = 0x8038130 *)
mov L0x2001493c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014948; PC = 0x8038134 *)
mov L0x20014948 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20014ff0; PC = 0x8038138 *)
mov L0x20014ff0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20014ffc; PC = 0x803813c *)
mov L0x20014ffc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015008; PC = 0x8038140 *)
mov L0x20015008 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014930; PC = 0x8038144 *)
mov L0x20014930 r4;




(**************** CUT 101, - *****************)

ecut and [
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x20014930*x**2*z** 1 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x2001493c*x**2*z** 1 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x20014948*x**2*z** 1 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x20014ff0*x**2*z** 1 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x20014ffc*x**2*z** 1 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x20015008*x**2*z** 1 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014954; Value = 0x00000003; PC = 0x8038148 *)
mov r4 L0x20014954;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014960; Value = 0xfffffffd; PC = 0x803814c *)
mov r5 L0x20014960;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001496c; Value = 0x00000000; PC = 0x8038150 *)
mov r6 L0x2001496c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015014; Value = 0xffeb4785; PC = 0x8038154 *)
mov r7 L0x20015014;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015020; Value = 0xfff6163e; PC = 0x8038158 *)
mov r8 L0x20015020;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001502c; Value = 0xffeb4782; PC = 0x803815c *)
mov r9 L0x2001502c;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014960; PC = 0x80381ec *)
mov L0x20014960 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001496c; PC = 0x80381f0 *)
mov L0x2001496c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015014; PC = 0x80381f4 *)
mov L0x20015014 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015020; PC = 0x80381f8 *)
mov L0x20015020 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001502c; PC = 0x80381fc *)
mov L0x2001502c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014954; PC = 0x8038200 *)
mov L0x20014954 r4;




(**************** CUT 102, - *****************)

ecut and [
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x20014954*x**2*z** 1 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x20014960*x**2*z** 1 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x2001496c*x**2*z** 1 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x20015014*x**2*z** 1 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x20015020*x**2*z** 1 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x2001502c*x**2*z** 1 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014978; Value = 0x00000009; PC = 0x8038008 *)
mov r4 L0x20014978;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014984; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x20014984;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014990; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x20014990;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015038; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x20015038;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015044; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x20015044;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015050; Value = 0x00000003; PC = 0x803801c *)
mov r9 L0x20015050;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014984; PC = 0x8038074 *)
mov L0x20014984 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014990; PC = 0x8038078 *)
mov L0x20014990 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015038; PC = 0x803807c *)
mov L0x20015038 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015044; PC = 0x8038080 *)
mov L0x20015044 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015050; PC = 0x8038084 *)
mov L0x20015050 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014978; PC = 0x8038088 *)
mov L0x20014978 r4;




(**************** CUT 103, - *****************)

ecut and [
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20014978*x**2*z** 2 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20014984*x**2*z** 2 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20014990*x**2*z** 2 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20015038*x**2*z** 2 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20015044*x**2*z** 2 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20015050*x**2*z** 2 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x2001499c; Value = 0x00000000; PC = 0x803808c *)
mov r4 L0x2001499c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200149a8; Value = 0x0014b884; PC = 0x8038090 *)
mov r5 L0x200149a8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200149b4; Value = 0x00297102; PC = 0x8038094 *)
mov r6 L0x200149b4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001505c; Value = 0x0014b884; PC = 0x8038098 *)
mov r7 L0x2001505c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015068; Value = 0xffeb477c; PC = 0x803809c *)
mov r8 L0x20015068;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015074; Value = 0x0014b87e; PC = 0x80380a0 *)
mov r9 L0x20015074;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200149a8; PC = 0x8038130 *)
mov L0x200149a8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200149b4; PC = 0x8038134 *)
mov L0x200149b4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001505c; PC = 0x8038138 *)
mov L0x2001505c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015068; PC = 0x803813c *)
mov L0x20015068 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015074; PC = 0x8038140 *)
mov L0x20015074 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001499c; PC = 0x8038144 *)
mov L0x2001499c r4;


(**************** CUT 104, - *****************)

ecut and [
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x2001499c*x**2*z** 2 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x200149a8*x**2*z** 2 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x200149b4*x**2*z** 2 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x2001505c*x**2*z** 2 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x20015068*x**2*z** 2 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x20015074*x**2*z** 2 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200149c0; Value = 0x00000000; PC = 0x8038148 *)
mov r4 L0x200149c0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200149cc; Value = 0xffeb4785; PC = 0x803814c *)
mov r5 L0x200149cc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200149d8; Value = 0xffd68f04; PC = 0x8038150 *)
mov r6 L0x200149d8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015080; Value = 0xffeb4785; PC = 0x8038154 *)
mov r7 L0x20015080;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001508c; Value = 0x0014b87b; PC = 0x8038158 *)
mov r8 L0x2001508c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015098; Value = 0xffeb477f; PC = 0x803815c *)
mov r9 L0x20015098;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200149cc; PC = 0x80381ec *)
mov L0x200149cc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200149d8; PC = 0x80381f0 *)
mov L0x200149d8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015080; PC = 0x80381f4 *)
mov L0x20015080 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001508c; PC = 0x80381f8 *)
mov L0x2001508c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015098; PC = 0x80381fc *)
mov L0x20015098 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200149c0; PC = 0x8038200 *)
mov L0x200149c0 r4;




(**************** CUT 105, - *****************)

ecut and [
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x200149c0*x**2*z** 2 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x200149cc*x**2*z** 2 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x200149d8*x**2*z** 2 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x20015080*x**2*z** 2 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x2001508c*x**2*z** 2 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x20015098*x**2*z** 2 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x200149e4; Value = 0xfffffff7; PC = 0x8038008 *)
mov r4 L0x200149e4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200149f0; Value = 0x00000003; PC = 0x803800c *)
mov r5 L0x200149f0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200149fc; Value = 0x00000009; PC = 0x8038010 *)
mov r6 L0x200149fc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150a4; Value = 0xfffffff7; PC = 0x8038014 *)
mov r7 L0x200150a4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150b0; Value = 0x00000006; PC = 0x8038018 *)
mov r8 L0x200150b0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150bc; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x200150bc;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200149f0; PC = 0x8038074 *)
mov L0x200149f0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200149fc; PC = 0x8038078 *)
mov L0x200149fc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150a4; PC = 0x803807c *)
mov L0x200150a4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150b0; PC = 0x8038080 *)
mov L0x200150b0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150bc; PC = 0x8038084 *)
mov L0x200150bc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200149e4; PC = 0x8038088 *)
mov L0x200149e4 r4;




(**************** CUT 106, - *****************)

ecut and [
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200149e4*x**2*z** 3 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200149f0*x**2*z** 3 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200149fc*x**2*z** 3 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200150a4*x**2*z** 3 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200150b0*x**2*z** 3 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200150bc*x**2*z** 3 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a08; Value = 0x00000000; PC = 0x803808c *)
mov r4 L0x20014a08;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a14; Value = 0x0009e9c5; PC = 0x8038090 *)
mov r5 L0x20014a14;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a20; Value = 0x00000000; PC = 0x8038094 *)
mov r6 L0x20014a20;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150c8; Value = 0x00000000; PC = 0x8038098 *)
mov r7 L0x200150c8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150d4; Value = 0xffeb4782; PC = 0x803809c *)
mov r8 L0x200150d4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150e0; Value = 0x0014b884; PC = 0x80380a0 *)
mov r9 L0x200150e0;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a14; PC = 0x8038130 *)
mov L0x20014a14 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a20; PC = 0x8038134 *)
mov L0x20014a20 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150c8; PC = 0x8038138 *)
mov L0x200150c8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150d4; PC = 0x803813c *)
mov L0x200150d4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200150e0; PC = 0x8038140 *)
mov L0x200150e0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a08; PC = 0x8038144 *)
mov L0x20014a08 r4;




(**************** CUT 107, - *****************)

ecut and [
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x20014a08*x**2*z** 3 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x20014a14*x**2*z** 3 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x20014a20*x**2*z** 3 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x200150c8*x**2*z** 3 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x200150d4*x**2*z** 3 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x200150e0*x**2*z** 3 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a2c; Value = 0x00000000; PC = 0x8038148 *)
mov r4 L0x20014a2c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a38; Value = 0xfff61641; PC = 0x803814c *)
mov r5 L0x20014a38;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a44; Value = 0x00000000; PC = 0x8038150 *)
mov r6 L0x20014a44;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150ec; Value = 0x00000000; PC = 0x8038154 *)
mov r7 L0x200150ec;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150f8; Value = 0x0014b881; PC = 0x8038158 *)
mov r8 L0x200150f8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015104; Value = 0xffeb4785; PC = 0x803815c *)
mov r9 L0x20015104;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a38; PC = 0x80381ec *)
mov L0x20014a38 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a44; PC = 0x80381f0 *)
mov L0x20014a44 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200150ec; PC = 0x80381f4 *)
mov L0x200150ec r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200150f8; PC = 0x80381f8 *)
mov L0x200150f8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015104; PC = 0x80381fc *)
mov L0x20015104 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a2c; PC = 0x8038200 *)
mov L0x20014a2c r4;




(**************** CUT 108, - *****************)

ecut and [
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x20014a2c*x**2*z** 3 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x20014a38*x**2*z** 3 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x20014a44*x**2*z** 3 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x200150ec*x**2*z** 3 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x200150f8*x**2*z** 3 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x20015104*x**2*z** 3 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014a50; Value = 0x00000000; PC = 0x8038008 *)
mov r4 L0x20014a50;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a5c; Value = 0xfffffffd; PC = 0x803800c *)
mov r5 L0x20014a5c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a68; Value = 0x00000006; PC = 0x8038010 *)
mov r6 L0x20014a68;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015110; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x20015110;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001511c; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x2001511c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015128; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x20015128;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a5c; PC = 0x8038074 *)
mov L0x20014a5c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a68; PC = 0x8038078 *)
mov L0x20014a68 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015110; PC = 0x803807c *)
mov L0x20015110 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001511c; PC = 0x8038080 *)
mov L0x2001511c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015128; PC = 0x8038084 *)
mov L0x20015128 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a50; PC = 0x8038088 *)
mov L0x20014a50 r4;




(**************** CUT 109, - *****************)

ecut and [
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x20014a50*x**2*z** 4 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x20014a5c*x**2*z** 4 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x20014a68*x**2*z** 4 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x20015110*x**2*z** 4 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x2001511c*x**2*z** 4 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x20015128*x**2*z** 4 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a74; Value = 0xffeb4785; PC = 0x803808c *)
mov r4 L0x20014a74;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014a80; Value = 0xffeb4782; PC = 0x8038090 *)
mov r5 L0x20014a80;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014a8c; Value = 0xfffffffd; PC = 0x8038094 *)
mov r6 L0x20014a8c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015134; Value = 0x0014b87e; PC = 0x8038098 *)
mov r7 L0x20015134;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015140; Value = 0x00000000; PC = 0x803809c *)
mov r8 L0x20015140;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001514c; Value = 0x00000000; PC = 0x80380a0 *)
mov r9 L0x2001514c;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014a80; PC = 0x8038130 *)
mov L0x20014a80 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014a8c; PC = 0x8038134 *)
mov L0x20014a8c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015134; PC = 0x8038138 *)
mov L0x20015134 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015140; PC = 0x803813c *)
mov L0x20015140 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001514c; PC = 0x8038140 *)
mov L0x2001514c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a74; PC = 0x8038144 *)
mov L0x20014a74 r4;




(**************** CUT 110, - *****************)

ecut and [
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x20014a74*x**2*z** 4 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x20014a80*x**2*z** 4 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x20014a8c*x**2*z** 4 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x20015134*x**2*z** 4 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x20015140*x**2*z** 4 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x2001514c*x**2*z** 4 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014a98; Value = 0x0014b884; PC = 0x8038148 *)
mov r4 L0x20014a98;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014aa4; Value = 0x0014b881; PC = 0x803814c *)
mov r5 L0x20014aa4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ab0; Value = 0xfffffffd; PC = 0x8038150 *)
mov r6 L0x20014ab0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015158; Value = 0xffeb477f; PC = 0x8038154 *)
mov r7 L0x20015158;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015164; Value = 0x00000000; PC = 0x8038158 *)
mov r8 L0x20015164;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015170; Value = 0x00000000; PC = 0x803815c *)
mov r9 L0x20015170;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014aa4; PC = 0x80381ec *)
mov L0x20014aa4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ab0; PC = 0x80381f0 *)
mov L0x20014ab0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015158; PC = 0x80381f4 *)
mov L0x20015158 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015164; PC = 0x80381f8 *)
mov L0x20015164 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015170; PC = 0x80381fc *)
mov L0x20015170 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014a98; PC = 0x8038200 *)
mov L0x20014a98 r4;




(**************** CUT 111, - *****************)

ecut and [
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20014a98*x**2*z** 4 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20014aa4*x**2*z** 4 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20014ab0*x**2*z** 4 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20015158*x**2*z** 4 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20015164*x**2*z** 4 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20015170*x**2*z** 4 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014abc; Value = 0xfffffffa; PC = 0x8038008 *)
mov r4 L0x20014abc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ac8; Value = 0x00000003; PC = 0x803800c *)
mov r5 L0x20014ac8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ad4; Value = 0xfffffff7; PC = 0x8038010 *)
mov r6 L0x20014ad4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001517c; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x2001517c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015188; Value = 0xfffffffd; PC = 0x8038018 *)
mov r8 L0x20015188;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015194; Value = 0x00000003; PC = 0x803801c *)
mov r9 L0x20015194;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ac8; PC = 0x8038074 *)
mov L0x20014ac8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ad4; PC = 0x8038078 *)
mov L0x20014ad4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001517c; PC = 0x803807c *)
mov L0x2001517c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015188; PC = 0x8038080 *)
mov L0x20015188 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015194; PC = 0x8038084 *)
mov L0x20015194 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014abc; PC = 0x8038088 *)
mov L0x20014abc r4;




(**************** CUT 112, - *****************)

ecut and [
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x20014abc*x**2*z** 5 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x20014ac8*x**2*z** 5 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x20014ad4*x**2*z** 5 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x2001517c*x**2*z** 5 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x20015188*x**2*z** 5 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x20015194*x**2*z** 5 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014ae0; Value = 0x00000003; PC = 0x803808c *)
mov r4 L0x20014ae0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014aec; Value = 0x0014b87e; PC = 0x8038090 *)
mov r5 L0x20014aec;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014af8; Value = 0x00000000; PC = 0x8038094 *)
mov r6 L0x20014af8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151a0; Value = 0xffeb477f; PC = 0x8038098 *)
mov r7 L0x200151a0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151ac; Value = 0xffeb4782; PC = 0x803809c *)
mov r8 L0x200151ac;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151b8; Value = 0x0014b87e; PC = 0x80380a0 *)
mov r9 L0x200151b8;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014aec; PC = 0x8038130 *)
mov L0x20014aec r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014af8; PC = 0x8038134 *)
mov L0x20014af8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151a0; PC = 0x8038138 *)
mov L0x200151a0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151ac; PC = 0x803813c *)
mov L0x200151ac r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151b8; PC = 0x8038140 *)
mov L0x200151b8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014ae0; PC = 0x8038144 *)
mov L0x20014ae0 r4;




(**************** CUT 113, - *****************)

ecut and [
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x20014ae0*x**2*z** 5 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x20014aec*x**2*z** 5 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x20014af8*x**2*z** 5 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x200151a0*x**2*z** 5 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x200151ac*x**2*z** 5 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x200151b8*x**2*z** 5 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014b04; Value = 0x00000003; PC = 0x8038148 *)
mov r4 L0x20014b04;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b10; Value = 0xffeb477f; PC = 0x803814c *)
mov r5 L0x20014b10;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b1c; Value = 0x00000000; PC = 0x8038150 *)
mov r6 L0x20014b1c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151c4; Value = 0x0014b87e; PC = 0x8038154 *)
mov r7 L0x200151c4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151d0; Value = 0x0014b881; PC = 0x8038158 *)
mov r8 L0x200151d0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151dc; Value = 0xffeb477f; PC = 0x803815c *)
mov r9 L0x200151dc;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b10; PC = 0x80381ec *)
mov L0x20014b10 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b1c; PC = 0x80381f0 *)
mov L0x20014b1c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151c4; PC = 0x80381f4 *)
mov L0x200151c4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151d0; PC = 0x80381f8 *)
mov L0x200151d0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200151dc; PC = 0x80381fc *)
mov L0x200151dc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b04; PC = 0x8038200 *)
mov L0x20014b04 r4;




(**************** CUT 114, - *****************)

ecut and [
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x20014b04*x**2*z** 5 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x20014b10*x**2*z** 5 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x20014b1c*x**2*z** 5 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x200151c4*x**2*z** 5 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x200151d0*x**2*z** 5 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x200151dc*x**2*z** 5 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014b28; Value = 0x00000003; PC = 0x8038008 *)
mov r4 L0x20014b28;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b34; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x20014b34;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b40; Value = 0xfffffffa; PC = 0x8038010 *)
mov r6 L0x20014b40;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151e8; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x200151e8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151f4; Value = 0xfffffffd; PC = 0x8038018 *)
mov r8 L0x200151f4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015200; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x20015200;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b34; PC = 0x8038074 *)
mov L0x20014b34 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b40; PC = 0x8038078 *)
mov L0x20014b40 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200151e8; PC = 0x803807c *)
mov L0x200151e8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200151f4; PC = 0x8038080 *)
mov L0x200151f4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015200; PC = 0x8038084 *)
mov L0x20015200 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b28; PC = 0x8038088 *)
mov L0x20014b28 r4;




(**************** CUT 115, - *****************)

ecut and [
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x20014b28*x**2*z** 6 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x20014b34*x**2*z** 6 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x20014b40*x**2*z** 6 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x200151e8*x**2*z** 6 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x200151f4*x**2*z** 6 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x20015200*x**2*z** 6 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014b4c; Value = 0x0014b87e; PC = 0x803808c *)
mov r4 L0x20014b4c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b58; Value = 0xffeb477c; PC = 0x8038090 *)
mov r5 L0x20014b58;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b64; Value = 0x0014b87e; PC = 0x8038094 *)
mov r6 L0x20014b64;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001520c; Value = 0xffd68f01; PC = 0x8038098 *)
mov r7 L0x2001520c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015218; Value = 0xfffffffd; PC = 0x803809c *)
mov r8 L0x20015218;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015224; Value = 0xfffffffd; PC = 0x80380a0 *)
mov r9 L0x20015224;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b58; PC = 0x8038130 *)
mov L0x20014b58 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b64; PC = 0x8038134 *)
mov L0x20014b64 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001520c; PC = 0x8038138 *)
mov L0x2001520c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015218; PC = 0x803813c *)
mov L0x20015218 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015224; PC = 0x8038140 *)
mov L0x20015224 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b4c; PC = 0x8038144 *)
mov L0x20014b4c r4;




(**************** CUT 116, - *****************)

ecut and [
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x20014b4c*x**2*z** 6 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x20014b58*x**2*z** 6 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x20014b64*x**2*z** 6 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x2001520c*x**2*z** 6 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x20015218*x**2*z** 6 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x20015224*x**2*z** 6 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014b70; Value = 0xffeb477f; PC = 0x8038148 *)
mov r4 L0x20014b70;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014b7c; Value = 0x0014b87b; PC = 0x803814c *)
mov r5 L0x20014b7c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014b88; Value = 0xffeb477f; PC = 0x8038150 *)
mov r6 L0x20014b88;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015230; Value = 0x002970ff; PC = 0x8038154 *)
mov r7 L0x20015230;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001523c; Value = 0xfffffffd; PC = 0x8038158 *)
mov r8 L0x2001523c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015248; Value = 0xfffffffd; PC = 0x803815c *)
mov r9 L0x20015248;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014b7c; PC = 0x80381ec *)
mov L0x20014b7c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014b88; PC = 0x80381f0 *)
mov L0x20014b88 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015230; PC = 0x80381f4 *)
mov L0x20015230 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001523c; PC = 0x80381f8 *)
mov L0x2001523c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015248; PC = 0x80381fc *)
mov L0x20015248 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b70; PC = 0x8038200 *)
mov L0x20014b70 r4;




(**************** CUT 117, - *****************)

ecut and [
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x20014b70*x**2*z** 6 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x20014b7c*x**2*z** 6 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x20014b88*x**2*z** 6 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x20015230*x**2*z** 6 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x2001523c*x**2*z** 6 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x20015248*x**2*z** 6 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014b94; Value = 0x00000006; PC = 0x8038008 *)
mov r4 L0x20014b94;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ba0; Value = 0x00000000; PC = 0x803800c *)
mov r5 L0x20014ba0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014bac; Value = 0x00000009; PC = 0x8038010 *)
mov r6 L0x20014bac;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015254; Value = 0xfffffffd; PC = 0x8038014 *)
mov r7 L0x20015254;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015260; Value = 0xfffffffa; PC = 0x8038018 *)
mov r8 L0x20015260;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001526c; Value = 0x00000006; PC = 0x803801c *)
mov r9 L0x2001526c;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ba0; PC = 0x8038074 *)
mov L0x20014ba0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014bac; PC = 0x8038078 *)
mov L0x20014bac r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015254; PC = 0x803807c *)
mov L0x20015254 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015260; PC = 0x8038080 *)
mov L0x20015260 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001526c; PC = 0x8038084 *)
mov L0x2001526c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014b94; PC = 0x8038088 *)
mov L0x20014b94 r4;




(**************** CUT 118, - *****************)

ecut and [
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x20014b94*x**2*z** 7 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x20014ba0*x**2*z** 7 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x20014bac*x**2*z** 7 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x20015254*x**2*z** 7 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x20015260*x**2*z** 7 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x2001526c*x**2*z** 7 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014bb8; Value = 0xffeb4782; PC = 0x803808c *)
mov r4 L0x20014bb8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014bc4; Value = 0xffeb4785; PC = 0x8038090 *)
mov r5 L0x20014bc4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014bd0; Value = 0x00000000; PC = 0x8038094 *)
mov r6 L0x20014bd0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015278; Value = 0xffeb4782; PC = 0x8038098 *)
mov r7 L0x20015278;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015284; Value = 0x00000003; PC = 0x803809c *)
mov r8 L0x20015284;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015290; Value = 0x0014b881; PC = 0x80380a0 *)
mov r9 L0x20015290;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014bc4; PC = 0x8038130 *)
mov L0x20014bc4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014bd0; PC = 0x8038134 *)
mov L0x20014bd0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015278; PC = 0x8038138 *)
mov L0x20015278 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015284; PC = 0x803813c *)
mov L0x20015284 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015290; PC = 0x8038140 *)
mov L0x20015290 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014bb8; PC = 0x8038144 *)
mov L0x20014bb8 r4;




(**************** CUT 119, - *****************)

ecut and [
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20014bb8*x**2*z** 7 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20014bc4*x**2*z** 7 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20014bd0*x**2*z** 7 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20015278*x**2*z** 7 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20015284*x**2*z** 7 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20015290*x**2*z** 7 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014bdc; Value = 0x0014b881; PC = 0x8038148 *)
mov r4 L0x20014bdc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014be8; Value = 0x0014b884; PC = 0x803814c *)
mov r5 L0x20014be8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014bf4; Value = 0x00000000; PC = 0x8038150 *)
mov r6 L0x20014bf4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001529c; Value = 0x0014b881; PC = 0x8038154 *)
mov r7 L0x2001529c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152a8; Value = 0x00000003; PC = 0x8038158 *)
mov r8 L0x200152a8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152b4; Value = 0xffeb4782; PC = 0x803815c *)
mov r9 L0x200152b4;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014be8; PC = 0x80381ec *)
mov L0x20014be8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014bf4; PC = 0x80381f0 *)
mov L0x20014bf4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001529c; PC = 0x80381f4 *)
mov L0x2001529c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152a8; PC = 0x80381f8 *)
mov L0x200152a8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152b4; PC = 0x80381fc *)
mov L0x200152b4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014bdc; PC = 0x8038200 *)
mov L0x20014bdc r4;




(**************** CUT 120, - *****************)

ecut and [
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x20014bdc*x**2*z** 7 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x20014be8*x**2*z** 7 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x20014bf4*x**2*z** 7 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x2001529c*x**2*z** 7 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x200152a8*x**2*z** 7 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x200152b4*x**2*z** 7 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014c00; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014c00;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c0c; Value = 0x00000003; PC = 0x803800c *)
mov r5 L0x20014c0c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c18; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x20014c18;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152c0; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x200152c0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152cc; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x200152cc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152d8; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x200152d8;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c0c; PC = 0x8038074 *)
mov L0x20014c0c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c18; PC = 0x8038078 *)
mov L0x20014c18 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152c0; PC = 0x803807c *)
mov L0x200152c0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152cc; PC = 0x8038080 *)
mov L0x200152cc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152d8; PC = 0x8038084 *)
mov L0x200152d8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c00; PC = 0x8038088 *)
mov L0x20014c00 r4;




(**************** CUT 121, - *****************)

ecut and [
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x20014c00*x**2*z** 8 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x20014c0c*x**2*z** 8 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x20014c18*x**2*z** 8 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x200152c0*x**2*z** 8 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x200152cc*x**2*z** 8 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x200152d8*x**2*z** 8 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014c24; Value = 0xffd68efe; PC = 0x803808c *)
mov r4 L0x20014c24;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c30; Value = 0xffeb477f; PC = 0x8038090 *)
mov r5 L0x20014c30;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c3c; Value = 0x0014b87e; PC = 0x8038094 *)
mov r6 L0x20014c3c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152e4; Value = 0x00000000; PC = 0x8038098 *)
mov r7 L0x200152e4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152f0; Value = 0x002970ff; PC = 0x803809c *)
mov r8 L0x200152f0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152fc; Value = 0xffd68f01; PC = 0x80380a0 *)
mov r9 L0x200152fc;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c30; PC = 0x8038130 *)
mov L0x20014c30 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c3c; PC = 0x8038134 *)
mov L0x20014c3c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200152e4; PC = 0x8038138 *)
mov L0x200152e4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200152f0; PC = 0x803813c *)
mov L0x200152f0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200152fc; PC = 0x8038140 *)
mov L0x200152fc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c24; PC = 0x8038144 *)
mov L0x20014c24 r4;




(**************** CUT 122, - *****************)

ecut and [
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x20014c24*x**2*z** 8 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x20014c30*x**2*z** 8 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x20014c3c*x**2*z** 8 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x200152e4*x**2*z** 8 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x200152f0*x**2*z** 8 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x200152fc*x**2*z** 8 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014c48; Value = 0x002970fc; PC = 0x8038148 *)
mov r4 L0x20014c48;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c54; Value = 0x0014b87e; PC = 0x803814c *)
mov r5 L0x20014c54;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c60; Value = 0xffeb477f; PC = 0x8038150 *)
mov r6 L0x20014c60;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015308; Value = 0x00000000; PC = 0x8038154 *)
mov r7 L0x20015308;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015314; Value = 0xffd68f01; PC = 0x8038158 *)
mov r8 L0x20015314;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015320; Value = 0x002970ff; PC = 0x803815c *)
mov r9 L0x20015320;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c54; PC = 0x80381ec *)
mov L0x20014c54 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c60; PC = 0x80381f0 *)
mov L0x20014c60 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015308; PC = 0x80381f4 *)
mov L0x20015308 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015314; PC = 0x80381f8 *)
mov L0x20015314 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015320; PC = 0x80381fc *)
mov L0x20015320 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c48; PC = 0x8038200 *)
mov L0x20014c48 r4;




(**************** CUT 123, - *****************)

ecut and [
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20014c48*x**2*z** 8 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20014c54*x**2*z** 8 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20014c60*x**2*z** 8 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20015308*x**2*z** 8 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20015314*x**2*z** 8 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20015320*x**2*z** 8 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014c6c; Value = 0x00000000; PC = 0x8038008 *)
mov r4 L0x20014c6c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c78; Value = 0xfffffffd; PC = 0x803800c *)
mov r5 L0x20014c78;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014c84; Value = 0xfffffffa; PC = 0x8038010 *)
mov r6 L0x20014c84;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001532c; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x2001532c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015338; Value = 0x00000003; PC = 0x8038018 *)
mov r8 L0x20015338;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015344; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x20015344;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c78; PC = 0x8038074 *)
mov L0x20014c78 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014c84; PC = 0x8038078 *)
mov L0x20014c84 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001532c; PC = 0x803807c *)
mov L0x2001532c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015338; PC = 0x8038080 *)
mov L0x20015338 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015344; PC = 0x8038084 *)
mov L0x20015344 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c6c; PC = 0x8038088 *)
mov L0x20014c6c r4;




(**************** CUT 124, - *****************)

ecut and [
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x20014c6c*x**2*z** 9 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x20014c78*x**2*z** 9 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x20014c84*x**2*z** 9 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x2001532c*x**2*z** 9 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x20015338*x**2*z** 9 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x20015344*x**2*z** 9 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014c90; Value = 0x0014b87b; PC = 0x803808c *)
mov r4 L0x20014c90;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014c9c; Value = 0xfffffffd; PC = 0x8038090 *)
mov r5 L0x20014c9c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ca8; Value = 0xffeb477f; PC = 0x8038094 *)
mov r6 L0x20014ca8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015350; Value = 0x0014b884; PC = 0x8038098 *)
mov r7 L0x20015350;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001535c; Value = 0x00000003; PC = 0x803809c *)
mov r8 L0x2001535c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015368; Value = 0xffeb4782; PC = 0x80380a0 *)
mov r9 L0x20015368;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014c9c; PC = 0x8038130 *)
mov L0x20014c9c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ca8; PC = 0x8038134 *)
mov L0x20014ca8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015350; PC = 0x8038138 *)
mov L0x20015350 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001535c; PC = 0x803813c *)
mov L0x2001535c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015368; PC = 0x8038140 *)
mov L0x20015368 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014c90; PC = 0x8038144 *)
mov L0x20014c90 r4;




(**************** CUT 125, - *****************)

ecut and [
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x20014c90*x**2*z** 9 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x20014c9c*x**2*z** 9 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x20014ca8*x**2*z** 9 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x20015350*x**2*z** 9 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x2001535c*x**2*z** 9 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x20015368*x**2*z** 9 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014cb4; Value = 0xffeb477c; PC = 0x8038148 *)
mov r4 L0x20014cb4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014cc0; Value = 0xfffffffd; PC = 0x803814c *)
mov r5 L0x20014cc0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ccc; Value = 0x0014b87e; PC = 0x8038150 *)
mov r6 L0x20014ccc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015374; Value = 0xffeb4785; PC = 0x8038154 *)
mov r7 L0x20015374;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015380; Value = 0x00000003; PC = 0x8038158 *)
mov r8 L0x20015380;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001538c; Value = 0x0014b881; PC = 0x803815c *)
mov r9 L0x2001538c;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014cc0; PC = 0x80381ec *)
mov L0x20014cc0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ccc; PC = 0x80381f0 *)
mov L0x20014ccc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015374; PC = 0x80381f4 *)
mov L0x20015374 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015380; PC = 0x80381f8 *)
mov L0x20015380 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001538c; PC = 0x80381fc *)
mov L0x2001538c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014cb4; PC = 0x8038200 *)
mov L0x20014cb4 r4;




(**************** CUT 126, - *****************)

ecut and [
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x20014cb4*x**2*z** 9 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x20014cc0*x**2*z** 9 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x20014ccc*x**2*z** 9 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x20015374*x**2*z** 9 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x20015380*x**2*z** 9 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x2001538c*x**2*z** 9 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014cd8; Value = 0xfffffffa; PC = 0x8038008 *)
mov r4 L0x20014cd8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014ce4; Value = 0x00000003; PC = 0x803800c *)
mov r5 L0x20014ce4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014cf0; Value = 0x00000000; PC = 0x8038010 *)
mov r6 L0x20014cf0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015398; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x20015398;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153a4; Value = 0x00000003; PC = 0x8038018 *)
mov r8 L0x200153a4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153b0; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x200153b0;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014ce4; PC = 0x8038074 *)
mov L0x20014ce4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014cf0; PC = 0x8038078 *)
mov L0x20014cf0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015398; PC = 0x803807c *)
mov L0x20015398 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153a4; PC = 0x8038080 *)
mov L0x200153a4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153b0; PC = 0x8038084 *)
mov L0x200153b0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014cd8; PC = 0x8038088 *)
mov L0x20014cd8 r4;




(**************** CUT 127, - *****************)

ecut and [
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x20014cd8*x**2*z**10 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x20014ce4*x**2*z**10 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x20014cf0*x**2*z**10 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x20015398*x**2*z**10 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x200153a4*x**2*z**10 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x200153b0*x**2*z**10 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014cfc; Value = 0xffeb477f; PC = 0x803808c *)
mov r4 L0x20014cfc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d08; Value = 0xffeb477f; PC = 0x8038090 *)
mov r5 L0x20014d08;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d14; Value = 0xffd68f01; PC = 0x8038094 *)
mov r6 L0x20014d14;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153bc; Value = 0xffeb477f; PC = 0x8038098 *)
mov r7 L0x200153bc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153c8; Value = 0xffeb477f; PC = 0x803809c *)
mov r8 L0x200153c8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153d4; Value = 0xffd68efe; PC = 0x80380a0 *)
mov r9 L0x200153d4;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d08; PC = 0x8038130 *)
mov L0x20014d08 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d14; PC = 0x8038134 *)
mov L0x20014d14 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153bc; PC = 0x8038138 *)
mov L0x200153bc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153c8; PC = 0x803813c *)
mov L0x200153c8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153d4; PC = 0x8038140 *)
mov L0x200153d4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014cfc; PC = 0x8038144 *)
mov L0x20014cfc r4;




(**************** CUT 128, - *****************)

ecut and [
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x20014cfc*x**2*z**10 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x20014d08*x**2*z**10 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x20014d14*x**2*z**10 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x200153bc*x**2*z**10 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x200153c8*x**2*z**10 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x200153d4*x**2*z**10 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014d20; Value = 0x0014b87e; PC = 0x8038148 *)
mov r4 L0x20014d20;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d2c; Value = 0x0014b87e; PC = 0x803814c *)
mov r5 L0x20014d2c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d38; Value = 0x002970ff; PC = 0x8038150 *)
mov r6 L0x20014d38;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153e0; Value = 0x0014b87e; PC = 0x8038154 *)
mov r7 L0x200153e0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153ec; Value = 0x0014b87e; PC = 0x8038158 *)
mov r8 L0x200153ec;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153f8; Value = 0x002970fc; PC = 0x803815c *)
mov r9 L0x200153f8;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d2c; PC = 0x80381ec *)
mov L0x20014d2c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d38; PC = 0x80381f0 *)
mov L0x20014d38 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200153e0; PC = 0x80381f4 *)
mov L0x200153e0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200153ec; PC = 0x80381f8 *)
mov L0x200153ec r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200153f8; PC = 0x80381fc *)
mov L0x200153f8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d20; PC = 0x8038200 *)
mov L0x20014d20 r4;




(**************** CUT 129, - *****************)

ecut and [
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x20014d20*x**2*z**10 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x20014d2c*x**2*z**10 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x20014d38*x**2*z**10 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x200153e0*x**2*z**10 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x200153ec*x**2*z**10 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x200153f8*x**2*z**10 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014d44; Value = 0xfffffff7; PC = 0x8038008 *)
mov r4 L0x20014d44;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d50; Value = 0xfffffffa; PC = 0x803800c *)
mov r5 L0x20014d50;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d5c; Value = 0x00000006; PC = 0x8038010 *)
mov r6 L0x20014d5c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015404; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x20015404;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015410; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x20015410;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001541c; Value = 0x00000003; PC = 0x803801c *)
mov r9 L0x2001541c;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d50; PC = 0x8038074 *)
mov L0x20014d50 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d5c; PC = 0x8038078 *)
mov L0x20014d5c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015404; PC = 0x803807c *)
mov L0x20015404 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015410; PC = 0x8038080 *)
mov L0x20015410 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001541c; PC = 0x8038084 *)
mov L0x2001541c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d44; PC = 0x8038088 *)
mov L0x20014d44 r4;




(**************** CUT 130, - *****************)

ecut and [
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x20014d44*x**2*z**11 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x20014d50*x**2*z**11 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x20014d5c*x**2*z**11 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x20015404*x**2*z**11 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x20015410*x**2*z**11 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x2001541c*x**2*z**11 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014d68; Value = 0x00000000; PC = 0x803808c *)
mov r4 L0x20014d68;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d74; Value = 0x00000003; PC = 0x8038090 *)
mov r5 L0x20014d74;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014d80; Value = 0x0014b881; PC = 0x8038094 *)
mov r6 L0x20014d80;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015428; Value = 0x00297102; PC = 0x8038098 *)
mov r7 L0x20015428;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015434; Value = 0xffeb477c; PC = 0x803809c *)
mov r8 L0x20015434;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015440; Value = 0x0014b87e; PC = 0x80380a0 *)
mov r9 L0x20015440;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d74; PC = 0x8038130 *)
mov L0x20014d74 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014d80; PC = 0x8038134 *)
mov L0x20014d80 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015428; PC = 0x8038138 *)
mov L0x20015428 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015434; PC = 0x803813c *)
mov L0x20015434 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015440; PC = 0x8038140 *)
mov L0x20015440 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d68; PC = 0x8038144 *)
mov L0x20014d68 r4;




(**************** CUT 131, - *****************)

ecut and [
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20014d68*x**2*z**11 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20014d74*x**2*z**11 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20014d80*x**2*z**11 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20015428*x**2*z**11 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20015434*x**2*z**11 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20015440*x**2*z**11 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014d8c; Value = 0x00000000; PC = 0x8038148 *)
mov r4 L0x20014d8c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014d98; Value = 0x00000003; PC = 0x803814c *)
mov r5 L0x20014d98;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014da4; Value = 0xffeb4782; PC = 0x8038150 *)
mov r6 L0x20014da4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001544c; Value = 0xffd68f04; PC = 0x8038154 *)
mov r7 L0x2001544c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015458; Value = 0x0014b87b; PC = 0x8038158 *)
mov r8 L0x20015458;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015464; Value = 0xffeb477f; PC = 0x803815c *)
mov r9 L0x20015464;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014d98; PC = 0x80381ec *)
mov L0x20014d98 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014da4; PC = 0x80381f0 *)
mov L0x20014da4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001544c; PC = 0x80381f4 *)
mov L0x2001544c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015458; PC = 0x80381f8 *)
mov L0x20015458 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015464; PC = 0x80381fc *)
mov L0x20015464 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014d8c; PC = 0x8038200 *)
mov L0x20014d8c r4;




(**************** CUT 132, - *****************)

ecut and [
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x20014d8c*x**2*z**11 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x20014d98*x**2*z**11 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x20014da4*x**2*z**11 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x2001544c*x**2*z**11 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x20015458*x**2*z**11 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x20015464*x**2*z**11 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014db0; Value = 0x00000003; PC = 0x8038008 *)
mov r4 L0x20014db0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014dbc; Value = 0xfffffff7; PC = 0x803800c *)
mov r5 L0x20014dbc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014dc8; Value = 0xfffffff7; PC = 0x8038010 *)
mov r6 L0x20014dc8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015470; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x20015470;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001547c; Value = 0x00000006; PC = 0x8038018 *)
mov r8 L0x2001547c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015488; Value = 0xfffffffd; PC = 0x803801c *)
mov r9 L0x20015488;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014dbc; PC = 0x8038074 *)
mov L0x20014dbc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014dc8; PC = 0x8038078 *)
mov L0x20014dc8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015470; PC = 0x803807c *)
mov L0x20015470 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001547c; PC = 0x8038080 *)
mov L0x2001547c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015488; PC = 0x8038084 *)
mov L0x20015488 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014db0; PC = 0x8038088 *)
mov L0x20014db0 r4;




(**************** CUT 133, - *****************)

ecut and [
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x20014db0*x**2*z**12 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x20014dbc*x**2*z**12 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x20014dc8*x**2*z**12 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x20015470*x**2*z**12 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x2001547c*x**2*z**12 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x20015488*x**2*z**12 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014dd4; Value = 0x00000003; PC = 0x803808c *)
mov r4 L0x20014dd4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014de0; Value = 0x00000000; PC = 0x8038090 *)
mov r5 L0x20014de0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014dec; Value = 0x00000000; PC = 0x8038094 *)
mov r6 L0x20014dec;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015494; Value = 0xfffffffa; PC = 0x8038098 *)
mov r7 L0x20015494;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154a0; Value = 0xfffffffd; PC = 0x803809c *)
mov r8 L0x200154a0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154ac; Value = 0x0014b881; PC = 0x80380a0 *)
mov r9 L0x200154ac;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014de0; PC = 0x8038130 *)
mov L0x20014de0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014dec; PC = 0x8038134 *)
mov L0x20014dec r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015494; PC = 0x8038138 *)
mov L0x20015494 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154a0; PC = 0x803813c *)
mov L0x200154a0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154ac; PC = 0x8038140 *)
mov L0x200154ac r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014dd4; PC = 0x8038144 *)
mov L0x20014dd4 r4;




(**************** CUT 134, - *****************)

ecut and [
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x20014dd4*x**2*z**12 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x20014de0*x**2*z**12 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x20014dec*x**2*z**12 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x20015494*x**2*z**12 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x200154a0*x**2*z**12 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x200154ac*x**2*z**12 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014df8; Value = 0x00000003; PC = 0x8038148 *)
mov r4 L0x20014df8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e04; Value = 0x00000000; PC = 0x803814c *)
mov r5 L0x20014e04;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e10; Value = 0x00000000; PC = 0x8038150 *)
mov r6 L0x20014e10;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154b8; Value = 0xfffffffa; PC = 0x8038154 *)
mov r7 L0x200154b8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154c4; Value = 0xfffffffd; PC = 0x8038158 *)
mov r8 L0x200154c4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154d0; Value = 0xffeb4782; PC = 0x803815c *)
mov r9 L0x200154d0;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e04; PC = 0x80381ec *)
mov L0x20014e04 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e10; PC = 0x80381f0 *)
mov L0x20014e10 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154b8; PC = 0x80381f4 *)
mov L0x200154b8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154c4; PC = 0x80381f8 *)
mov L0x200154c4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154d0; PC = 0x80381fc *)
mov L0x200154d0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014df8; PC = 0x8038200 *)
mov L0x20014df8 r4;




(**************** CUT 135, - *****************)

ecut and [
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x20014df8*x**2*z**12 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x20014e04*x**2*z**12 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x20014e10*x**2*z**12 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x200154b8*x**2*z**12 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x200154c4*x**2*z**12 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x200154d0*x**2*z**12 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014e1c; Value = 0xfffffff7; PC = 0x8038008 *)
mov r4 L0x20014e1c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e28; Value = 0x00000006; PC = 0x803800c *)
mov r5 L0x20014e28;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e34; Value = 0x00000000; PC = 0x8038010 *)
mov r6 L0x20014e34;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154dc; Value = 0x00000003; PC = 0x8038014 *)
mov r7 L0x200154dc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154e8; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x200154e8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154f4; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x200154f4;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e28; PC = 0x8038074 *)
mov L0x20014e28 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e34; PC = 0x8038078 *)
mov L0x20014e34 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200154dc; PC = 0x803807c *)
mov L0x200154dc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200154e8; PC = 0x8038080 *)
mov L0x200154e8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200154f4; PC = 0x8038084 *)
mov L0x200154f4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e1c; PC = 0x8038088 *)
mov L0x20014e1c r4;




(**************** CUT 136, - *****************)

ecut and [
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x20014e1c*x**2*z**13 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x20014e28*x**2*z**13 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x20014e34*x**2*z**13 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x200154dc*x**2*z**13 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x200154e8*x**2*z**13 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x200154f4*x**2*z**13 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014e40; Value = 0x00000000; PC = 0x803808c *)
mov r4 L0x20014e40;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e4c; Value = 0xfffffffd; PC = 0x8038090 *)
mov r5 L0x20014e4c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e58; Value = 0xfff6163e; PC = 0x8038094 *)
mov r6 L0x20014e58;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015500; Value = 0x00000003; PC = 0x8038098 *)
mov r7 L0x20015500;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001550c; Value = 0x00000000; PC = 0x803809c *)
mov r8 L0x2001550c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015518; Value = 0xffeb4785; PC = 0x80380a0 *)
mov r9 L0x20015518;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e4c; PC = 0x8038130 *)
mov L0x20014e4c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e58; PC = 0x8038134 *)
mov L0x20014e58 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015500; PC = 0x8038138 *)
mov L0x20015500 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001550c; PC = 0x803813c *)
mov L0x2001550c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015518; PC = 0x8038140 *)
mov L0x20015518 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e40; PC = 0x8038144 *)
mov L0x20014e40 r4;




(**************** CUT 137, - *****************)

ecut and [
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x20014e40*x**2*z**13 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x20014e4c*x**2*z**13 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x20014e58*x**2*z**13 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x20015500*x**2*z**13 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x2001550c*x**2*z**13 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x20015518*x**2*z**13 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014e64; Value = 0x00000000; PC = 0x8038148 *)
mov r4 L0x20014e64;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e70; Value = 0xfffffffd; PC = 0x803814c *)
mov r5 L0x20014e70;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014e7c; Value = 0x0009e9c2; PC = 0x8038150 *)
mov r6 L0x20014e7c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015524; Value = 0x00000003; PC = 0x8038154 *)
mov r7 L0x20015524;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015530; Value = 0x00000000; PC = 0x8038158 *)
mov r8 L0x20015530;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001553c; Value = 0x0014b884; PC = 0x803815c *)
mov r9 L0x2001553c;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e70; PC = 0x80381ec *)
mov L0x20014e70 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014e7c; PC = 0x80381f0 *)
mov L0x20014e7c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015524; PC = 0x80381f4 *)
mov L0x20015524 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015530; PC = 0x80381f8 *)
mov L0x20015530 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001553c; PC = 0x80381fc *)
mov L0x2001553c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e64; PC = 0x8038200 *)
mov L0x20014e64 r4;




(**************** CUT 138, - *****************)

ecut and [
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x20014e64*x**2*z**13 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x20014e70*x**2*z**13 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x20014e7c*x**2*z**13 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x20015524*x**2*z**13 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x20015530*x**2*z**13 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x2001553c*x**2*z**13 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014e88; Value = 0xfffffffd; PC = 0x8038008 *)
mov r4 L0x20014e88;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014e94; Value = 0xfffffffd; PC = 0x803800c *)
mov r5 L0x20014e94;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ea0; Value = 0x00000006; PC = 0x8038010 *)
mov r6 L0x20014ea0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015548; Value = 0xfffffffd; PC = 0x8038014 *)
mov r7 L0x20015548;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015554; Value = 0x00000000; PC = 0x8038018 *)
mov r8 L0x20015554;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015560; Value = 0x00000003; PC = 0x803801c *)
mov r9 L0x20015560;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014e94; PC = 0x8038074 *)
mov L0x20014e94 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ea0; PC = 0x8038078 *)
mov L0x20014ea0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015548; PC = 0x803807c *)
mov L0x20015548 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015554; PC = 0x8038080 *)
mov L0x20015554 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015560; PC = 0x8038084 *)
mov L0x20015560 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014e88; PC = 0x8038088 *)
mov L0x20014e88 r4;




(**************** CUT 139, - *****************)

ecut and [
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20014e88*x**2*z**14 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20014e94*x**2*z**14 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20014ea0*x**2*z**14 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20015548*x**2*z**14 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20015554*x**2*z**14 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20015560*x**2*z**14 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014eac; Value = 0x0014b881; PC = 0x803808c *)
mov r4 L0x20014eac;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014eb8; Value = 0xffeb4782; PC = 0x8038090 *)
mov r5 L0x20014eb8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ec4; Value = 0xffeb4782; PC = 0x8038094 *)
mov r6 L0x20014ec4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001556c; Value = 0xffeb4782; PC = 0x8038098 *)
mov r7 L0x2001556c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015578; Value = 0xffeb4785; PC = 0x803809c *)
mov r8 L0x20015578;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015584; Value = 0x0014b87e; PC = 0x80380a0 *)
mov r9 L0x20015584;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014eb8; PC = 0x8038130 *)
mov L0x20014eb8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ec4; PC = 0x8038134 *)
mov L0x20014ec4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001556c; PC = 0x8038138 *)
mov L0x2001556c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015578; PC = 0x803813c *)
mov L0x20015578 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015584; PC = 0x8038140 *)
mov L0x20015584 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014eac; PC = 0x8038144 *)
mov L0x20014eac r4;




(**************** CUT 140, - *****************)

ecut and [
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x20014eac*x**2*z**14 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x20014eb8*x**2*z**14 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x20014ec4*x**2*z**14 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x2001556c*x**2*z**14 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x20015578*x**2*z**14 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x20015584*x**2*z**14 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014ed0; Value = 0xffeb4782; PC = 0x8038148 *)
mov r4 L0x20014ed0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014edc; Value = 0x0014b881; PC = 0x803814c *)
mov r5 L0x20014edc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014ee8; Value = 0x0014b881; PC = 0x8038150 *)
mov r6 L0x20014ee8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015590; Value = 0x0014b881; PC = 0x8038154 *)
mov r7 L0x20015590;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001559c; Value = 0x0014b884; PC = 0x8038158 *)
mov r8 L0x2001559c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155a8; Value = 0xffeb477f; PC = 0x803815c *)
mov r9 L0x200155a8;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014edc; PC = 0x80381ec *)
mov L0x20014edc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014ee8; PC = 0x80381f0 *)
mov L0x20014ee8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015590; PC = 0x80381f4 *)
mov L0x20015590 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001559c; PC = 0x80381f8 *)
mov L0x2001559c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155a8; PC = 0x80381fc *)
mov L0x200155a8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014ed0; PC = 0x8038200 *)
mov L0x20014ed0 r4;




(**************** CUT 141, - *****************)

ecut and [
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x20014ed0*x**2*z**14 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x20014edc*x**2*z**14 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x20014ee8*x**2*z**14 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x20015590*x**2*z**14 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x2001559c*x**2*z**14 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x200155a8*x**2*z**14 [ 3365569, y - 3070820, z**16 - 3365568 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* ldr.w	r4, [r0]                                  #! EA = L0x20014ef4; Value = 0x00000006; PC = 0x8038008 *)
mov r4 L0x20014ef4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014f00; Value = 0xfffffffd; PC = 0x803800c *)
mov r5 L0x20014f00;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014f0c; Value = 0x00000003; PC = 0x8038010 *)
mov r6 L0x20014f0c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155b4; Value = 0x00000000; PC = 0x8038014 *)
mov r7 L0x200155b4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155c0; Value = 0xfffffffa; PC = 0x8038018 *)
mov r8 L0x200155c0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155cc; Value = 0x00000000; PC = 0x803801c *)
mov r9 L0x200155cc;
(* add	r4, r7                                      #! PC = 0x8038020 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038022 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038024 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038028 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x803802c *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038030 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038034 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038038 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x803803c *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038040 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038044 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038048 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x803804c *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038050 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x8038052 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038054 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038058 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x803805c *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038060 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038064 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038068 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x803806c *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x8038070 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x8038072 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014f00; PC = 0x8038074 *)
mov L0x20014f00 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014f0c; PC = 0x8038078 *)
mov L0x20014f0c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155b4; PC = 0x803807c *)
mov L0x200155b4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155c0; PC = 0x8038080 *)
mov L0x200155c0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155cc; PC = 0x8038084 *)
mov L0x200155cc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014ef4; PC = 0x8038088 *)
mov L0x20014ef4 r4;




(**************** CUT 142, - *****************)

ecut and [
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x20014ef4*x**2*z**15 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x20014f00*x**2*z**15 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x20014f0c*x**2*z**15 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x200155b4*x**2*z**15 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x200155c0*x**2*z**15 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x200155cc*x**2*z**15 [ 3365569, y - 2912918, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014f18; Value = 0xffeb4782; PC = 0x803808c *)
mov r4 L0x20014f18;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014f24; Value = 0xffeb4782; PC = 0x8038090 *)
mov r5 L0x20014f24;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014f30; Value = 0xffeb477f; PC = 0x8038094 *)
mov r6 L0x20014f30;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155d8; Value = 0xffd68f01; PC = 0x8038098 *)
mov r7 L0x200155d8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155e4; Value = 0xffeb477f; PC = 0x803809c *)
mov r8 L0x200155e4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155f0; Value = 0x0014b87b; PC = 0x80380a0 *)
mov r9 L0x200155f0;
(* vmov	r1, s8                                     #! PC = 0x80380a4 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x80380a8 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380ac *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80380b0 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x80380b4 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380b8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x80380bc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x80380c0 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x80380c4 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380c8 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80380cc *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x80380d0 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80380d4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x80380d8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x80380dc *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x80380de *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x80380e0 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80380e4 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80380e8 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80380ec *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80380f0 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80380f4 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80380f8 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80380fc *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038100 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038104 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038108 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803810c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803810e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038110 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038114 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038118 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803811c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038120 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038124 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038128 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803812c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803812e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014f24; PC = 0x8038130 *)
mov L0x20014f24 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014f30; PC = 0x8038134 *)
mov L0x20014f30 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155d8; PC = 0x8038138 *)
mov L0x200155d8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200155e4; PC = 0x803813c *)
mov L0x200155e4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200155f0; PC = 0x8038140 *)
mov L0x200155f0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014f18; PC = 0x8038144 *)
mov L0x20014f18 r4;




(**************** CUT 143, - *****************)

ecut and [
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x20014f18*x**2*z**15 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x20014f24*x**2*z**15 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x20014f30*x**2*z**15 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x200155d8*x**2*z**15 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x200155e4*x**2*z**15 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x200155f0*x**2*z**15 [ 3365569, y - 1540404, z**16 - 3365568 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20014f3c; Value = 0x0014b881; PC = 0x8038148 *)
mov r4 L0x20014f3c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20014f48; Value = 0x0014b881; PC = 0x803814c *)
mov r5 L0x20014f48;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20014f54; Value = 0x0014b87e; PC = 0x8038150 *)
mov r6 L0x20014f54;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155fc; Value = 0x002970ff; PC = 0x8038154 *)
mov r7 L0x200155fc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015608; Value = 0x0014b87e; PC = 0x8038158 *)
mov r8 L0x20015608;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015614; Value = 0xffeb477c; PC = 0x803815c *)
mov r9 L0x20015614;
(* vmov	r1, s11                                    #! PC = 0x8038160 *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x8038164 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038168 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x803816c *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038170 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038174 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038178 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x803817c *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x8038180 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038184 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038188 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* smull	r12, r9, r9, r1                           #! PC = 0x803818c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038190 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038194 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038198 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803819a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803819c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x80381a0 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x80381a4 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x80381a8 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x80381ac *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x80381b0 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80381b4 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80381b8 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80381bc *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80381c0 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80381c4 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80381c8 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80381ca *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80381cc *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80381d0 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80381d4 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80381d8 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80381dc *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80381e0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80381e4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80381e8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80381ea *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20014f48; PC = 0x80381ec *)
mov L0x20014f48 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20014f54; PC = 0x80381f0 *)
mov L0x20014f54 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200155fc; PC = 0x80381f4 *)
mov L0x200155fc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015608; PC = 0x80381f8 *)
mov L0x20015608 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015614; PC = 0x80381fc *)
mov L0x20015614 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20014f3c; PC = 0x8038200 *)
mov L0x20014f3c r4;




(**************** CUT 144, - *****************)

ecut and [
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x20014f3c*x**2*z**15 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x20014f48*x**2*z**15 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x20014f54*x**2*z**15 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x200155fc*x**2*z**15 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x20015608*x**2*z**15 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x20015614*x**2*z**15 [ 3365569, y - 3070820, z**16 - 3365568 ]
];


(**************** CUT -,   0 *****************)

rcut and [
(-20205702)@32 <=s L0x20014898, L0x20014898 <=s 20205702@32,
(-16836037)@32 <=s L0x200148a4, L0x200148a4 <=s 16836037@32,
(-23571271)@32 <=s L0x200148b0, L0x200148b0 <=s 23571271@32,
(-20205702)@32 <=s L0x20014f58, L0x20014f58 <=s 20205702@32,
(-16836037)@32 <=s L0x20014f64, L0x20014f64 <=s 16836037@32,
(-23571271)@32 <=s L0x20014f70, L0x20014f70 <=s 23571271@32
,
(-20207750)@32 <=s L0x200148bc, L0x200148bc <=s 20207750@32,
(-16831941)@32 <=s L0x200148c8, L0x200148c8 <=s 16831941@32,
(-23565127)@32 <=s L0x200148d4, L0x200148d4 <=s 23565127@32,
(-20207750)@32 <=s L0x20014f7c, L0x20014f7c <=s 20207750@32,
(-16831941)@32 <=s L0x20014f88, L0x20014f88 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f94, L0x20014f94 <=s 23565127@32
,
(-20207750)@32 <=s L0x200148e0, L0x200148e0 <=s 20207750@32,
(-16831941)@32 <=s L0x200148ec, L0x200148ec <=s 16831941@32,
(-23565127)@32 <=s L0x200148f8, L0x200148f8 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fa0, L0x20014fa0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014fac, L0x20014fac <=s 16831941@32,
(-23565127)@32 <=s L0x20014fb8, L0x20014fb8 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014904, L0x20014904 <=s 20205702@32,
(-16836037)@32 <=s L0x20014910, L0x20014910 <=s 16836037@32,
(-23571271)@32 <=s L0x2001491c, L0x2001491c <=s 23571271@32,
(-20205702)@32 <=s L0x20014fc4, L0x20014fc4 <=s 20205702@32,
(-16836037)@32 <=s L0x20014fd0, L0x20014fd0 <=s 16836037@32,
(-23571271)@32 <=s L0x20014fdc, L0x20014fdc <=s 23571271@32
,
(-20207750)@32 <=s L0x20014928, L0x20014928 <=s 20207750@32,
(-16831941)@32 <=s L0x20014934, L0x20014934 <=s 16831941@32,
(-23565127)@32 <=s L0x20014940, L0x20014940 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fe8, L0x20014fe8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ff4, L0x20014ff4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015000, L0x20015000 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001494c, L0x2001494c <=s 20207750@32,
(-16831941)@32 <=s L0x20014958, L0x20014958 <=s 16831941@32,
(-23565127)@32 <=s L0x20014964, L0x20014964 <=s 23565127@32,
(-20207750)@32 <=s L0x2001500c, L0x2001500c <=s 20207750@32,
(-16831941)@32 <=s L0x20015018, L0x20015018 <=s 16831941@32,
(-23565127)@32 <=s L0x20015024, L0x20015024 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014970, L0x20014970 <=s 20205702@32,
(-16836037)@32 <=s L0x2001497c, L0x2001497c <=s 16836037@32,
(-23571271)@32 <=s L0x20014988, L0x20014988 <=s 23571271@32,
(-20205702)@32 <=s L0x20015030, L0x20015030 <=s 20205702@32,
(-16836037)@32 <=s L0x2001503c, L0x2001503c <=s 16836037@32,
(-23571271)@32 <=s L0x20015048, L0x20015048 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014994, L0x20014994 <=s 20207750@32,
(-16831941)@32 <=s L0x200149a0, L0x200149a0 <=s 16831941@32,
(-23565127)@32 <=s L0x200149ac, L0x200149ac <=s 23565127@32,
(-20207750)@32 <=s L0x20015054, L0x20015054 <=s 20207750@32,
(-16831941)@32 <=s L0x20015060, L0x20015060 <=s 16831941@32,
(-23565127)@32 <=s L0x2001506c, L0x2001506c <=s 23565127@32
,
(-20207750)@32 <=s L0x200149b8, L0x200149b8 <=s 20207750@32,
(-16831941)@32 <=s L0x200149c4, L0x200149c4 <=s 16831941@32,
(-23565127)@32 <=s L0x200149d0, L0x200149d0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015078, L0x20015078 <=s 20207750@32,
(-16831941)@32 <=s L0x20015084, L0x20015084 <=s 16831941@32,
(-23565127)@32 <=s L0x20015090, L0x20015090 <=s 23565127@32
,
(-20205702)@32 <=s L0x200149dc, L0x200149dc <=s 20205702@32,
(-16836037)@32 <=s L0x200149e8, L0x200149e8 <=s 16836037@32,
(-23571271)@32 <=s L0x200149f4, L0x200149f4 <=s 23571271@32,
(-20205702)@32 <=s L0x2001509c, L0x2001509c <=s 20205702@32,
(-16836037)@32 <=s L0x200150a8, L0x200150a8 <=s 16836037@32,
(-23571271)@32 <=s L0x200150b4, L0x200150b4 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014a00, L0x20014a00 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a0c, L0x20014a0c <=s 16831941@32,
(-23565127)@32 <=s L0x20014a18, L0x20014a18 <=s 23565127@32,
(-20207750)@32 <=s L0x200150c0, L0x200150c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200150cc, L0x200150cc <=s 16831941@32,
(-23565127)@32 <=s L0x200150d8, L0x200150d8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a24, L0x20014a24 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a30, L0x20014a30 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a3c, L0x20014a3c <=s 23565127@32,
(-20207750)@32 <=s L0x200150e4, L0x200150e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200150f0, L0x200150f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200150fc, L0x200150fc <=s 23565127@32
,
(-20205702)@32 <=s L0x20014a48, L0x20014a48 <=s 20205702@32,
(-16836037)@32 <=s L0x20014a54, L0x20014a54 <=s 16836037@32,
(-23571271)@32 <=s L0x20014a60, L0x20014a60 <=s 23571271@32,
(-20205702)@32 <=s L0x20015108, L0x20015108 <=s 20205702@32,
(-16836037)@32 <=s L0x20015114, L0x20015114 <=s 16836037@32,
(-23571271)@32 <=s L0x20015120, L0x20015120 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014a6c, L0x20014a6c <=s 20207750@32,
(-16831941)@32 <=s L0x20014a78, L0x20014a78 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a84, L0x20014a84 <=s 23565127@32,
(-20207750)@32 <=s L0x2001512c, L0x2001512c <=s 20207750@32,
(-16831941)@32 <=s L0x20015138, L0x20015138 <=s 16831941@32,
(-23565127)@32 <=s L0x20015144, L0x20015144 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a90, L0x20014a90 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a9c, L0x20014a9c <=s 16831941@32,
(-23565127)@32 <=s L0x20014aa8, L0x20014aa8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015150, L0x20015150 <=s 20207750@32,
(-16831941)@32 <=s L0x2001515c, L0x2001515c <=s 16831941@32,
(-23565127)@32 <=s L0x20015168, L0x20015168 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014ab4, L0x20014ab4 <=s 20205702@32,
(-16836037)@32 <=s L0x20014ac0, L0x20014ac0 <=s 16836037@32,
(-23571271)@32 <=s L0x20014acc, L0x20014acc <=s 23571271@32,
(-20205702)@32 <=s L0x20015174, L0x20015174 <=s 20205702@32,
(-16836037)@32 <=s L0x20015180, L0x20015180 <=s 16836037@32,
(-23571271)@32 <=s L0x2001518c, L0x2001518c <=s 23571271@32
,
(-20207750)@32 <=s L0x20014ad8, L0x20014ad8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ae4, L0x20014ae4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014af0, L0x20014af0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015198, L0x20015198 <=s 20207750@32,
(-16831941)@32 <=s L0x200151a4, L0x200151a4 <=s 16831941@32,
(-23565127)@32 <=s L0x200151b0, L0x200151b0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014afc, L0x20014afc <=s 20207750@32,
(-16831941)@32 <=s L0x20014b08, L0x20014b08 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b14, L0x20014b14 <=s 23565127@32,
(-20207750)@32 <=s L0x200151bc, L0x200151bc <=s 20207750@32,
(-16831941)@32 <=s L0x200151c8, L0x200151c8 <=s 16831941@32,
(-23565127)@32 <=s L0x200151d4, L0x200151d4 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014b20, L0x20014b20 <=s 20205702@32,
(-16836037)@32 <=s L0x20014b2c, L0x20014b2c <=s 16836037@32,
(-23571271)@32 <=s L0x20014b38, L0x20014b38 <=s 23571271@32,
(-20205702)@32 <=s L0x200151e0, L0x200151e0 <=s 20205702@32,
(-16836037)@32 <=s L0x200151ec, L0x200151ec <=s 16836037@32,
(-23571271)@32 <=s L0x200151f8, L0x200151f8 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014b44, L0x20014b44 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b50, L0x20014b50 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b5c, L0x20014b5c <=s 23565127@32,
(-20207750)@32 <=s L0x20015204, L0x20015204 <=s 20207750@32,
(-16831941)@32 <=s L0x20015210, L0x20015210 <=s 16831941@32,
(-23565127)@32 <=s L0x2001521c, L0x2001521c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b68, L0x20014b68 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b74, L0x20014b74 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b80, L0x20014b80 <=s 23565127@32,
(-20207750)@32 <=s L0x20015228, L0x20015228 <=s 20207750@32,
(-16831941)@32 <=s L0x20015234, L0x20015234 <=s 16831941@32,
(-23565127)@32 <=s L0x20015240, L0x20015240 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014b8c, L0x20014b8c <=s 20205702@32,
(-16836037)@32 <=s L0x20014b98, L0x20014b98 <=s 16836037@32,
(-23571271)@32 <=s L0x20014ba4, L0x20014ba4 <=s 23571271@32,
(-20205702)@32 <=s L0x2001524c, L0x2001524c <=s 20205702@32,
(-16836037)@32 <=s L0x20015258, L0x20015258 <=s 16836037@32,
(-23571271)@32 <=s L0x20015264, L0x20015264 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014bb0, L0x20014bb0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014bbc, L0x20014bbc <=s 16831941@32,
(-23565127)@32 <=s L0x20014bc8, L0x20014bc8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015270, L0x20015270 <=s 20207750@32,
(-16831941)@32 <=s L0x2001527c, L0x2001527c <=s 16831941@32,
(-23565127)@32 <=s L0x20015288, L0x20015288 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bd4, L0x20014bd4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014be0, L0x20014be0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bec, L0x20014bec <=s 23565127@32,
(-20207750)@32 <=s L0x20015294, L0x20015294 <=s 20207750@32,
(-16831941)@32 <=s L0x200152a0, L0x200152a0 <=s 16831941@32,
(-23565127)@32 <=s L0x200152ac, L0x200152ac <=s 23565127@32
,
(-20205702)@32 <=s L0x20014bf8, L0x20014bf8 <=s 20205702@32,
(-16836037)@32 <=s L0x20014c04, L0x20014c04 <=s 16836037@32,
(-23571271)@32 <=s L0x20014c10, L0x20014c10 <=s 23571271@32,
(-20205702)@32 <=s L0x200152b8, L0x200152b8 <=s 20205702@32,
(-16836037)@32 <=s L0x200152c4, L0x200152c4 <=s 16836037@32,
(-23571271)@32 <=s L0x200152d0, L0x200152d0 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014c1c, L0x20014c1c <=s 20207750@32,
(-16831941)@32 <=s L0x20014c28, L0x20014c28 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c34, L0x20014c34 <=s 23565127@32,
(-20207750)@32 <=s L0x200152dc, L0x200152dc <=s 20207750@32,
(-16831941)@32 <=s L0x200152e8, L0x200152e8 <=s 16831941@32,
(-23565127)@32 <=s L0x200152f4, L0x200152f4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c40, L0x20014c40 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c4c, L0x20014c4c <=s 16831941@32,
(-23565127)@32 <=s L0x20014c58, L0x20014c58 <=s 23565127@32,
(-20207750)@32 <=s L0x20015300, L0x20015300 <=s 20207750@32,
(-16831941)@32 <=s L0x2001530c, L0x2001530c <=s 16831941@32,
(-23565127)@32 <=s L0x20015318, L0x20015318 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014c64, L0x20014c64 <=s 20205702@32,
(-16836037)@32 <=s L0x20014c70, L0x20014c70 <=s 16836037@32,
(-23571271)@32 <=s L0x20014c7c, L0x20014c7c <=s 23571271@32,
(-20205702)@32 <=s L0x20015324, L0x20015324 <=s 20205702@32,
(-16836037)@32 <=s L0x20015330, L0x20015330 <=s 16836037@32,
(-23571271)@32 <=s L0x2001533c, L0x2001533c <=s 23571271@32
,
(-20207750)@32 <=s L0x20014c88, L0x20014c88 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c94, L0x20014c94 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ca0, L0x20014ca0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015348, L0x20015348 <=s 20207750@32,
(-16831941)@32 <=s L0x20015354, L0x20015354 <=s 16831941@32,
(-23565127)@32 <=s L0x20015360, L0x20015360 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cac, L0x20014cac <=s 20207750@32,
(-16831941)@32 <=s L0x20014cb8, L0x20014cb8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014cc4, L0x20014cc4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001536c, L0x2001536c <=s 20207750@32,
(-16831941)@32 <=s L0x20015378, L0x20015378 <=s 16831941@32,
(-23565127)@32 <=s L0x20015384, L0x20015384 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014cd0, L0x20014cd0 <=s 20205702@32,
(-16836037)@32 <=s L0x20014cdc, L0x20014cdc <=s 16836037@32,
(-23571271)@32 <=s L0x20014ce8, L0x20014ce8 <=s 23571271@32,
(-20205702)@32 <=s L0x20015390, L0x20015390 <=s 20205702@32,
(-16836037)@32 <=s L0x2001539c, L0x2001539c <=s 16836037@32,
(-23571271)@32 <=s L0x200153a8, L0x200153a8 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014cf4, L0x20014cf4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d00, L0x20014d00 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d0c, L0x20014d0c <=s 23565127@32,
(-20207750)@32 <=s L0x200153b4, L0x200153b4 <=s 20207750@32,
(-16831941)@32 <=s L0x200153c0, L0x200153c0 <=s 16831941@32,
(-23565127)@32 <=s L0x200153cc, L0x200153cc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d18, L0x20014d18 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d24, L0x20014d24 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d30, L0x20014d30 <=s 23565127@32,
(-20207750)@32 <=s L0x200153d8, L0x200153d8 <=s 20207750@32,
(-16831941)@32 <=s L0x200153e4, L0x200153e4 <=s 16831941@32,
(-23565127)@32 <=s L0x200153f0, L0x200153f0 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014d3c, L0x20014d3c <=s 20205702@32,
(-16836037)@32 <=s L0x20014d48, L0x20014d48 <=s 16836037@32,
(-23571271)@32 <=s L0x20014d54, L0x20014d54 <=s 23571271@32,
(-20205702)@32 <=s L0x200153fc, L0x200153fc <=s 20205702@32,
(-16836037)@32 <=s L0x20015408, L0x20015408 <=s 16836037@32,
(-23571271)@32 <=s L0x20015414, L0x20015414 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014d60, L0x20014d60 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d6c, L0x20014d6c <=s 16831941@32,
(-23565127)@32 <=s L0x20014d78, L0x20014d78 <=s 23565127@32,
(-20207750)@32 <=s L0x20015420, L0x20015420 <=s 20207750@32,
(-16831941)@32 <=s L0x2001542c, L0x2001542c <=s 16831941@32,
(-23565127)@32 <=s L0x20015438, L0x20015438 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d84, L0x20014d84 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d90, L0x20014d90 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d9c, L0x20014d9c <=s 23565127@32,
(-20207750)@32 <=s L0x20015444, L0x20015444 <=s 20207750@32,
(-16831941)@32 <=s L0x20015450, L0x20015450 <=s 16831941@32,
(-23565127)@32 <=s L0x2001545c, L0x2001545c <=s 23565127@32
,
(-20205702)@32 <=s L0x20014da8, L0x20014da8 <=s 20205702@32,
(-16836037)@32 <=s L0x20014db4, L0x20014db4 <=s 16836037@32,
(-23571271)@32 <=s L0x20014dc0, L0x20014dc0 <=s 23571271@32,
(-20205702)@32 <=s L0x20015468, L0x20015468 <=s 20205702@32,
(-16836037)@32 <=s L0x20015474, L0x20015474 <=s 16836037@32,
(-23571271)@32 <=s L0x20015480, L0x20015480 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014dcc, L0x20014dcc <=s 20207750@32,
(-16831941)@32 <=s L0x20014dd8, L0x20014dd8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014de4, L0x20014de4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001548c, L0x2001548c <=s 20207750@32,
(-16831941)@32 <=s L0x20015498, L0x20015498 <=s 16831941@32,
(-23565127)@32 <=s L0x200154a4, L0x200154a4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014df0, L0x20014df0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014dfc, L0x20014dfc <=s 16831941@32,
(-23565127)@32 <=s L0x20014e08, L0x20014e08 <=s 23565127@32,
(-20207750)@32 <=s L0x200154b0, L0x200154b0 <=s 20207750@32,
(-16831941)@32 <=s L0x200154bc, L0x200154bc <=s 16831941@32,
(-23565127)@32 <=s L0x200154c8, L0x200154c8 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014e14, L0x20014e14 <=s 20205702@32,
(-16836037)@32 <=s L0x20014e20, L0x20014e20 <=s 16836037@32,
(-23571271)@32 <=s L0x20014e2c, L0x20014e2c <=s 23571271@32,
(-20205702)@32 <=s L0x200154d4, L0x200154d4 <=s 20205702@32,
(-16836037)@32 <=s L0x200154e0, L0x200154e0 <=s 16836037@32,
(-23571271)@32 <=s L0x200154ec, L0x200154ec <=s 23571271@32
,
(-20207750)@32 <=s L0x20014e38, L0x20014e38 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e44, L0x20014e44 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e50, L0x20014e50 <=s 23565127@32,
(-20207750)@32 <=s L0x200154f8, L0x200154f8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015504, L0x20015504 <=s 16831941@32,
(-23565127)@32 <=s L0x20015510, L0x20015510 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e5c, L0x20014e5c <=s 20207750@32,
(-16831941)@32 <=s L0x20014e68, L0x20014e68 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e74, L0x20014e74 <=s 23565127@32,
(-20207750)@32 <=s L0x2001551c, L0x2001551c <=s 20207750@32,
(-16831941)@32 <=s L0x20015528, L0x20015528 <=s 16831941@32,
(-23565127)@32 <=s L0x20015534, L0x20015534 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014e80, L0x20014e80 <=s 20205702@32,
(-16836037)@32 <=s L0x20014e8c, L0x20014e8c <=s 16836037@32,
(-23571271)@32 <=s L0x20014e98, L0x20014e98 <=s 23571271@32,
(-20205702)@32 <=s L0x20015540, L0x20015540 <=s 20205702@32,
(-16836037)@32 <=s L0x2001554c, L0x2001554c <=s 16836037@32,
(-23571271)@32 <=s L0x20015558, L0x20015558 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014ea4, L0x20014ea4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014eb0, L0x20014eb0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ebc, L0x20014ebc <=s 23565127@32,
(-20207750)@32 <=s L0x20015564, L0x20015564 <=s 20207750@32,
(-16831941)@32 <=s L0x20015570, L0x20015570 <=s 16831941@32,
(-23565127)@32 <=s L0x2001557c, L0x2001557c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ec8, L0x20014ec8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ed4, L0x20014ed4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ee0, L0x20014ee0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015588, L0x20015588 <=s 20207750@32,
(-16831941)@32 <=s L0x20015594, L0x20015594 <=s 16831941@32,
(-23565127)@32 <=s L0x200155a0, L0x200155a0 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014eec, L0x20014eec <=s 20205702@32,
(-16836037)@32 <=s L0x20014ef8, L0x20014ef8 <=s 16836037@32,
(-23571271)@32 <=s L0x20014f04, L0x20014f04 <=s 23571271@32,
(-20205702)@32 <=s L0x200155ac, L0x200155ac <=s 20205702@32,
(-16836037)@32 <=s L0x200155b8, L0x200155b8 <=s 16836037@32,
(-23571271)@32 <=s L0x200155c4, L0x200155c4 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014f10, L0x20014f10 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f1c, L0x20014f1c <=s 16831941@32,
(-23565127)@32 <=s L0x20014f28, L0x20014f28 <=s 23565127@32,
(-20207750)@32 <=s L0x200155d0, L0x200155d0 <=s 20207750@32,
(-16831941)@32 <=s L0x200155dc, L0x200155dc <=s 16831941@32,
(-23565127)@32 <=s L0x200155e8, L0x200155e8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014f34, L0x20014f34 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f40, L0x20014f40 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f4c, L0x20014f4c <=s 23565127@32,
(-20207750)@32 <=s L0x200155f4, L0x200155f4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015600, L0x20015600 <=s 16831941@32,
(-23565127)@32 <=s L0x2001560c, L0x2001560c <=s 23565127@32
,
(-20207750)@32 <=s L0x2001489c, L0x2001489c <=s 20207750@32,
(-16831941)@32 <=s L0x200148a8, L0x200148a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200148b4, L0x200148b4 <=s 23565127@32,
(-20207750)@32 <=s L0x20014f5c, L0x20014f5c <=s 20207750@32,
(-16831941)@32 <=s L0x20014f68, L0x20014f68 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f74, L0x20014f74 <=s 23565127@32
,
(-20207750)@32 <=s L0x200148c0, L0x200148c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200148cc, L0x200148cc <=s 16831941@32,
(-23565127)@32 <=s L0x200148d8, L0x200148d8 <=s 23565127@32,
(-20207750)@32 <=s L0x20014f80, L0x20014f80 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f8c, L0x20014f8c <=s 16831941@32,
(-23565127)@32 <=s L0x20014f98, L0x20014f98 <=s 23565127@32
,
(-20207750)@32 <=s L0x200148e4, L0x200148e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200148f0, L0x200148f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200148fc, L0x200148fc <=s 23565127@32,
(-20207750)@32 <=s L0x20014fa4, L0x20014fa4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014fb0, L0x20014fb0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014fbc, L0x20014fbc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014908, L0x20014908 <=s 20207750@32,
(-16831941)@32 <=s L0x20014914, L0x20014914 <=s 16831941@32,
(-23565127)@32 <=s L0x20014920, L0x20014920 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fc8, L0x20014fc8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014fd4, L0x20014fd4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014fe0, L0x20014fe0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001492c, L0x2001492c <=s 20207750@32,
(-16831941)@32 <=s L0x20014938, L0x20014938 <=s 16831941@32,
(-23565127)@32 <=s L0x20014944, L0x20014944 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fec, L0x20014fec <=s 20207750@32,
(-16831941)@32 <=s L0x20014ff8, L0x20014ff8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015004, L0x20015004 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014950, L0x20014950 <=s 20207750@32,
(-16831941)@32 <=s L0x2001495c, L0x2001495c <=s 16831941@32,
(-23565127)@32 <=s L0x20014968, L0x20014968 <=s 23565127@32,
(-20207750)@32 <=s L0x20015010, L0x20015010 <=s 20207750@32,
(-16831941)@32 <=s L0x2001501c, L0x2001501c <=s 16831941@32,
(-23565127)@32 <=s L0x20015028, L0x20015028 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014974, L0x20014974 <=s 20207750@32,
(-16831941)@32 <=s L0x20014980, L0x20014980 <=s 16831941@32,
(-23565127)@32 <=s L0x2001498c, L0x2001498c <=s 23565127@32,
(-20207750)@32 <=s L0x20015034, L0x20015034 <=s 20207750@32,
(-16831941)@32 <=s L0x20015040, L0x20015040 <=s 16831941@32,
(-23565127)@32 <=s L0x2001504c, L0x2001504c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014998, L0x20014998 <=s 20207750@32,
(-16831941)@32 <=s L0x200149a4, L0x200149a4 <=s 16831941@32,
(-23565127)@32 <=s L0x200149b0, L0x200149b0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015058, L0x20015058 <=s 20207750@32,
(-16831941)@32 <=s L0x20015064, L0x20015064 <=s 16831941@32,
(-23565127)@32 <=s L0x20015070, L0x20015070 <=s 23565127@32
,
(-20207750)@32 <=s L0x200149bc, L0x200149bc <=s 20207750@32,
(-16831941)@32 <=s L0x200149c8, L0x200149c8 <=s 16831941@32,
(-23565127)@32 <=s L0x200149d4, L0x200149d4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001507c, L0x2001507c <=s 20207750@32,
(-16831941)@32 <=s L0x20015088, L0x20015088 <=s 16831941@32,
(-23565127)@32 <=s L0x20015094, L0x20015094 <=s 23565127@32
,
(-20207750)@32 <=s L0x200149e0, L0x200149e0 <=s 20207750@32,
(-16831941)@32 <=s L0x200149ec, L0x200149ec <=s 16831941@32,
(-23565127)@32 <=s L0x200149f8, L0x200149f8 <=s 23565127@32,
(-20207750)@32 <=s L0x200150a0, L0x200150a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200150ac, L0x200150ac <=s 16831941@32,
(-23565127)@32 <=s L0x200150b8, L0x200150b8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a04, L0x20014a04 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a10, L0x20014a10 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a1c, L0x20014a1c <=s 23565127@32,
(-20207750)@32 <=s L0x200150c4, L0x200150c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200150d0, L0x200150d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200150dc, L0x200150dc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a28, L0x20014a28 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a34, L0x20014a34 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a40, L0x20014a40 <=s 23565127@32,
(-20207750)@32 <=s L0x200150e8, L0x200150e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200150f4, L0x200150f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015100, L0x20015100 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a4c, L0x20014a4c <=s 20207750@32,
(-16831941)@32 <=s L0x20014a58, L0x20014a58 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a64, L0x20014a64 <=s 23565127@32,
(-20207750)@32 <=s L0x2001510c, L0x2001510c <=s 20207750@32,
(-16831941)@32 <=s L0x20015118, L0x20015118 <=s 16831941@32,
(-23565127)@32 <=s L0x20015124, L0x20015124 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a70, L0x20014a70 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a7c, L0x20014a7c <=s 16831941@32,
(-23565127)@32 <=s L0x20014a88, L0x20014a88 <=s 23565127@32,
(-20207750)@32 <=s L0x20015130, L0x20015130 <=s 20207750@32,
(-16831941)@32 <=s L0x2001513c, L0x2001513c <=s 16831941@32,
(-23565127)@32 <=s L0x20015148, L0x20015148 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a94, L0x20014a94 <=s 20207750@32,
(-16831941)@32 <=s L0x20014aa0, L0x20014aa0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014aac, L0x20014aac <=s 23565127@32,
(-20207750)@32 <=s L0x20015154, L0x20015154 <=s 20207750@32,
(-16831941)@32 <=s L0x20015160, L0x20015160 <=s 16831941@32,
(-23565127)@32 <=s L0x2001516c, L0x2001516c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ab8, L0x20014ab8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ac4, L0x20014ac4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ad0, L0x20014ad0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015178, L0x20015178 <=s 20207750@32,
(-16831941)@32 <=s L0x20015184, L0x20015184 <=s 16831941@32,
(-23565127)@32 <=s L0x20015190, L0x20015190 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014adc, L0x20014adc <=s 20207750@32,
(-16831941)@32 <=s L0x20014ae8, L0x20014ae8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014af4, L0x20014af4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001519c, L0x2001519c <=s 20207750@32,
(-16831941)@32 <=s L0x200151a8, L0x200151a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200151b4, L0x200151b4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b00, L0x20014b00 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b0c, L0x20014b0c <=s 16831941@32,
(-23565127)@32 <=s L0x20014b18, L0x20014b18 <=s 23565127@32,
(-20207750)@32 <=s L0x200151c0, L0x200151c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200151cc, L0x200151cc <=s 16831941@32,
(-23565127)@32 <=s L0x200151d8, L0x200151d8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b24, L0x20014b24 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b30, L0x20014b30 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b3c, L0x20014b3c <=s 23565127@32,
(-20207750)@32 <=s L0x200151e4, L0x200151e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200151f0, L0x200151f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200151fc, L0x200151fc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b48, L0x20014b48 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b54, L0x20014b54 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b60, L0x20014b60 <=s 23565127@32,
(-20207750)@32 <=s L0x20015208, L0x20015208 <=s 20207750@32,
(-16831941)@32 <=s L0x20015214, L0x20015214 <=s 16831941@32,
(-23565127)@32 <=s L0x20015220, L0x20015220 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b6c, L0x20014b6c <=s 20207750@32,
(-16831941)@32 <=s L0x20014b78, L0x20014b78 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b84, L0x20014b84 <=s 23565127@32,
(-20207750)@32 <=s L0x2001522c, L0x2001522c <=s 20207750@32,
(-16831941)@32 <=s L0x20015238, L0x20015238 <=s 16831941@32,
(-23565127)@32 <=s L0x20015244, L0x20015244 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b90, L0x20014b90 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b9c, L0x20014b9c <=s 16831941@32,
(-23565127)@32 <=s L0x20014ba8, L0x20014ba8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015250, L0x20015250 <=s 20207750@32,
(-16831941)@32 <=s L0x2001525c, L0x2001525c <=s 16831941@32,
(-23565127)@32 <=s L0x20015268, L0x20015268 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bb4, L0x20014bb4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014bc0, L0x20014bc0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bcc, L0x20014bcc <=s 23565127@32,
(-20207750)@32 <=s L0x20015274, L0x20015274 <=s 20207750@32,
(-16831941)@32 <=s L0x20015280, L0x20015280 <=s 16831941@32,
(-23565127)@32 <=s L0x2001528c, L0x2001528c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bd8, L0x20014bd8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014be4, L0x20014be4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bf0, L0x20014bf0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015298, L0x20015298 <=s 20207750@32,
(-16831941)@32 <=s L0x200152a4, L0x200152a4 <=s 16831941@32,
(-23565127)@32 <=s L0x200152b0, L0x200152b0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bfc, L0x20014bfc <=s 20207750@32,
(-16831941)@32 <=s L0x20014c08, L0x20014c08 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c14, L0x20014c14 <=s 23565127@32,
(-20207750)@32 <=s L0x200152bc, L0x200152bc <=s 20207750@32,
(-16831941)@32 <=s L0x200152c8, L0x200152c8 <=s 16831941@32,
(-23565127)@32 <=s L0x200152d4, L0x200152d4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c20, L0x20014c20 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c2c, L0x20014c2c <=s 16831941@32,
(-23565127)@32 <=s L0x20014c38, L0x20014c38 <=s 23565127@32,
(-20207750)@32 <=s L0x200152e0, L0x200152e0 <=s 20207750@32,
(-16831941)@32 <=s L0x200152ec, L0x200152ec <=s 16831941@32,
(-23565127)@32 <=s L0x200152f8, L0x200152f8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c44, L0x20014c44 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c50, L0x20014c50 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c5c, L0x20014c5c <=s 23565127@32,
(-20207750)@32 <=s L0x20015304, L0x20015304 <=s 20207750@32,
(-16831941)@32 <=s L0x20015310, L0x20015310 <=s 16831941@32,
(-23565127)@32 <=s L0x2001531c, L0x2001531c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c68, L0x20014c68 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c74, L0x20014c74 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c80, L0x20014c80 <=s 23565127@32,
(-20207750)@32 <=s L0x20015328, L0x20015328 <=s 20207750@32,
(-16831941)@32 <=s L0x20015334, L0x20015334 <=s 16831941@32,
(-23565127)@32 <=s L0x20015340, L0x20015340 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c8c, L0x20014c8c <=s 20207750@32,
(-16831941)@32 <=s L0x20014c98, L0x20014c98 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ca4, L0x20014ca4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001534c, L0x2001534c <=s 20207750@32,
(-16831941)@32 <=s L0x20015358, L0x20015358 <=s 16831941@32,
(-23565127)@32 <=s L0x20015364, L0x20015364 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cb0, L0x20014cb0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014cbc, L0x20014cbc <=s 16831941@32,
(-23565127)@32 <=s L0x20014cc8, L0x20014cc8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015370, L0x20015370 <=s 20207750@32,
(-16831941)@32 <=s L0x2001537c, L0x2001537c <=s 16831941@32,
(-23565127)@32 <=s L0x20015388, L0x20015388 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cd4, L0x20014cd4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ce0, L0x20014ce0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014cec, L0x20014cec <=s 23565127@32,
(-20207750)@32 <=s L0x20015394, L0x20015394 <=s 20207750@32,
(-16831941)@32 <=s L0x200153a0, L0x200153a0 <=s 16831941@32,
(-23565127)@32 <=s L0x200153ac, L0x200153ac <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cf8, L0x20014cf8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d04, L0x20014d04 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d10, L0x20014d10 <=s 23565127@32,
(-20207750)@32 <=s L0x200153b8, L0x200153b8 <=s 20207750@32,
(-16831941)@32 <=s L0x200153c4, L0x200153c4 <=s 16831941@32,
(-23565127)@32 <=s L0x200153d0, L0x200153d0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d1c, L0x20014d1c <=s 20207750@32,
(-16831941)@32 <=s L0x20014d28, L0x20014d28 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d34, L0x20014d34 <=s 23565127@32,
(-20207750)@32 <=s L0x200153dc, L0x200153dc <=s 20207750@32,
(-16831941)@32 <=s L0x200153e8, L0x200153e8 <=s 16831941@32,
(-23565127)@32 <=s L0x200153f4, L0x200153f4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d40, L0x20014d40 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d4c, L0x20014d4c <=s 16831941@32,
(-23565127)@32 <=s L0x20014d58, L0x20014d58 <=s 23565127@32,
(-20207750)@32 <=s L0x20015400, L0x20015400 <=s 20207750@32,
(-16831941)@32 <=s L0x2001540c, L0x2001540c <=s 16831941@32,
(-23565127)@32 <=s L0x20015418, L0x20015418 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d64, L0x20014d64 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d70, L0x20014d70 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d7c, L0x20014d7c <=s 23565127@32,
(-20207750)@32 <=s L0x20015424, L0x20015424 <=s 20207750@32,
(-16831941)@32 <=s L0x20015430, L0x20015430 <=s 16831941@32,
(-23565127)@32 <=s L0x2001543c, L0x2001543c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d88, L0x20014d88 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d94, L0x20014d94 <=s 16831941@32,
(-23565127)@32 <=s L0x20014da0, L0x20014da0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015448, L0x20015448 <=s 20207750@32,
(-16831941)@32 <=s L0x20015454, L0x20015454 <=s 16831941@32,
(-23565127)@32 <=s L0x20015460, L0x20015460 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014dac, L0x20014dac <=s 20207750@32,
(-16831941)@32 <=s L0x20014db8, L0x20014db8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014dc4, L0x20014dc4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001546c, L0x2001546c <=s 20207750@32,
(-16831941)@32 <=s L0x20015478, L0x20015478 <=s 16831941@32,
(-23565127)@32 <=s L0x20015484, L0x20015484 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014dd0, L0x20014dd0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ddc, L0x20014ddc <=s 16831941@32,
(-23565127)@32 <=s L0x20014de8, L0x20014de8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015490, L0x20015490 <=s 20207750@32,
(-16831941)@32 <=s L0x2001549c, L0x2001549c <=s 16831941@32,
(-23565127)@32 <=s L0x200154a8, L0x200154a8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014df4, L0x20014df4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e00, L0x20014e00 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e0c, L0x20014e0c <=s 23565127@32,
(-20207750)@32 <=s L0x200154b4, L0x200154b4 <=s 20207750@32,
(-16831941)@32 <=s L0x200154c0, L0x200154c0 <=s 16831941@32,
(-23565127)@32 <=s L0x200154cc, L0x200154cc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e18, L0x20014e18 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e24, L0x20014e24 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e30, L0x20014e30 <=s 23565127@32,
(-20207750)@32 <=s L0x200154d8, L0x200154d8 <=s 20207750@32,
(-16831941)@32 <=s L0x200154e4, L0x200154e4 <=s 16831941@32,
(-23565127)@32 <=s L0x200154f0, L0x200154f0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e3c, L0x20014e3c <=s 20207750@32,
(-16831941)@32 <=s L0x20014e48, L0x20014e48 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e54, L0x20014e54 <=s 23565127@32,
(-20207750)@32 <=s L0x200154fc, L0x200154fc <=s 20207750@32,
(-16831941)@32 <=s L0x20015508, L0x20015508 <=s 16831941@32,
(-23565127)@32 <=s L0x20015514, L0x20015514 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e60, L0x20014e60 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e6c, L0x20014e6c <=s 16831941@32,
(-23565127)@32 <=s L0x20014e78, L0x20014e78 <=s 23565127@32,
(-20207750)@32 <=s L0x20015520, L0x20015520 <=s 20207750@32,
(-16831941)@32 <=s L0x2001552c, L0x2001552c <=s 16831941@32,
(-23565127)@32 <=s L0x20015538, L0x20015538 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e84, L0x20014e84 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e90, L0x20014e90 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e9c, L0x20014e9c <=s 23565127@32,
(-20207750)@32 <=s L0x20015544, L0x20015544 <=s 20207750@32,
(-16831941)@32 <=s L0x20015550, L0x20015550 <=s 16831941@32,
(-23565127)@32 <=s L0x2001555c, L0x2001555c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ea8, L0x20014ea8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014eb4, L0x20014eb4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ec0, L0x20014ec0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015568, L0x20015568 <=s 20207750@32,
(-16831941)@32 <=s L0x20015574, L0x20015574 <=s 16831941@32,
(-23565127)@32 <=s L0x20015580, L0x20015580 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ecc, L0x20014ecc <=s 20207750@32,
(-16831941)@32 <=s L0x20014ed8, L0x20014ed8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ee4, L0x20014ee4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001558c, L0x2001558c <=s 20207750@32,
(-16831941)@32 <=s L0x20015598, L0x20015598 <=s 16831941@32,
(-23565127)@32 <=s L0x200155a4, L0x200155a4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ef0, L0x20014ef0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014efc, L0x20014efc <=s 16831941@32,
(-23565127)@32 <=s L0x20014f08, L0x20014f08 <=s 23565127@32,
(-20207750)@32 <=s L0x200155b0, L0x200155b0 <=s 20207750@32,
(-16831941)@32 <=s L0x200155bc, L0x200155bc <=s 16831941@32,
(-23565127)@32 <=s L0x200155c8, L0x200155c8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014f14, L0x20014f14 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f20, L0x20014f20 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f2c, L0x20014f2c <=s 23565127@32,
(-20207750)@32 <=s L0x200155d4, L0x200155d4 <=s 20207750@32,
(-16831941)@32 <=s L0x200155e0, L0x200155e0 <=s 16831941@32,
(-23565127)@32 <=s L0x200155ec, L0x200155ec <=s 23565127@32
,
(-20207750)@32 <=s L0x20014f38, L0x20014f38 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f44, L0x20014f44 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f50, L0x20014f50 <=s 23565127@32,
(-20207750)@32 <=s L0x200155f8, L0x200155f8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015604, L0x20015604 <=s 16831941@32,
(-23565127)@32 <=s L0x20015610, L0x20015610 <=s 23565127@32
,
(-20207750)@32 <=s L0x200148a0, L0x200148a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200148ac, L0x200148ac <=s 16831941@32,
(-23565127)@32 <=s L0x200148b8, L0x200148b8 <=s 23565127@32,
(-20207750)@32 <=s L0x20014f60, L0x20014f60 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f6c, L0x20014f6c <=s 16831941@32,
(-23565127)@32 <=s L0x20014f78, L0x20014f78 <=s 23565127@32
,
(-20207750)@32 <=s L0x200148c4, L0x200148c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200148d0, L0x200148d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200148dc, L0x200148dc <=s 23565127@32,
(-20207750)@32 <=s L0x20014f84, L0x20014f84 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f90, L0x20014f90 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f9c, L0x20014f9c <=s 23565127@32
,
(-20207750)@32 <=s L0x200148e8, L0x200148e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200148f4, L0x200148f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014900, L0x20014900 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fa8, L0x20014fa8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014fb4, L0x20014fb4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014fc0, L0x20014fc0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001490c, L0x2001490c <=s 20207750@32,
(-16831941)@32 <=s L0x20014918, L0x20014918 <=s 16831941@32,
(-23565127)@32 <=s L0x20014924, L0x20014924 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fcc, L0x20014fcc <=s 20207750@32,
(-16831941)@32 <=s L0x20014fd8, L0x20014fd8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014fe4, L0x20014fe4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014930, L0x20014930 <=s 20207750@32,
(-16831941)@32 <=s L0x2001493c, L0x2001493c <=s 16831941@32,
(-23565127)@32 <=s L0x20014948, L0x20014948 <=s 23565127@32,
(-20207750)@32 <=s L0x20014ff0, L0x20014ff0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ffc, L0x20014ffc <=s 16831941@32,
(-23565127)@32 <=s L0x20015008, L0x20015008 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014954, L0x20014954 <=s 20207750@32,
(-16831941)@32 <=s L0x20014960, L0x20014960 <=s 16831941@32,
(-23565127)@32 <=s L0x2001496c, L0x2001496c <=s 23565127@32,
(-20207750)@32 <=s L0x20015014, L0x20015014 <=s 20207750@32,
(-16831941)@32 <=s L0x20015020, L0x20015020 <=s 16831941@32,
(-23565127)@32 <=s L0x2001502c, L0x2001502c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014978, L0x20014978 <=s 20207750@32,
(-16831941)@32 <=s L0x20014984, L0x20014984 <=s 16831941@32,
(-23565127)@32 <=s L0x20014990, L0x20014990 <=s 23565127@32,
(-20207750)@32 <=s L0x20015038, L0x20015038 <=s 20207750@32,
(-16831941)@32 <=s L0x20015044, L0x20015044 <=s 16831941@32,
(-23565127)@32 <=s L0x20015050, L0x20015050 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001499c, L0x2001499c <=s 20207750@32,
(-16831941)@32 <=s L0x200149a8, L0x200149a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200149b4, L0x200149b4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001505c, L0x2001505c <=s 20207750@32,
(-16831941)@32 <=s L0x20015068, L0x20015068 <=s 16831941@32,
(-23565127)@32 <=s L0x20015074, L0x20015074 <=s 23565127@32
,
(-20207750)@32 <=s L0x200149c0, L0x200149c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200149cc, L0x200149cc <=s 16831941@32,
(-23565127)@32 <=s L0x200149d8, L0x200149d8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015080, L0x20015080 <=s 20207750@32,
(-16831941)@32 <=s L0x2001508c, L0x2001508c <=s 16831941@32,
(-23565127)@32 <=s L0x20015098, L0x20015098 <=s 23565127@32
,
(-20207750)@32 <=s L0x200149e4, L0x200149e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200149f0, L0x200149f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200149fc, L0x200149fc <=s 23565127@32,
(-20207750)@32 <=s L0x200150a4, L0x200150a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200150b0, L0x200150b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200150bc, L0x200150bc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a08, L0x20014a08 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a14, L0x20014a14 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a20, L0x20014a20 <=s 23565127@32,
(-20207750)@32 <=s L0x200150c8, L0x200150c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200150d4, L0x200150d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200150e0, L0x200150e0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a2c, L0x20014a2c <=s 20207750@32,
(-16831941)@32 <=s L0x20014a38, L0x20014a38 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a44, L0x20014a44 <=s 23565127@32,
(-20207750)@32 <=s L0x200150ec, L0x200150ec <=s 20207750@32,
(-16831941)@32 <=s L0x200150f8, L0x200150f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015104, L0x20015104 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a50, L0x20014a50 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a5c, L0x20014a5c <=s 16831941@32,
(-23565127)@32 <=s L0x20014a68, L0x20014a68 <=s 23565127@32,
(-20207750)@32 <=s L0x20015110, L0x20015110 <=s 20207750@32,
(-16831941)@32 <=s L0x2001511c, L0x2001511c <=s 16831941@32,
(-23565127)@32 <=s L0x20015128, L0x20015128 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a74, L0x20014a74 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a80, L0x20014a80 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a8c, L0x20014a8c <=s 23565127@32,
(-20207750)@32 <=s L0x20015134, L0x20015134 <=s 20207750@32,
(-16831941)@32 <=s L0x20015140, L0x20015140 <=s 16831941@32,
(-23565127)@32 <=s L0x2001514c, L0x2001514c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a98, L0x20014a98 <=s 20207750@32,
(-16831941)@32 <=s L0x20014aa4, L0x20014aa4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ab0, L0x20014ab0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015158, L0x20015158 <=s 20207750@32,
(-16831941)@32 <=s L0x20015164, L0x20015164 <=s 16831941@32,
(-23565127)@32 <=s L0x20015170, L0x20015170 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014abc, L0x20014abc <=s 20207750@32,
(-16831941)@32 <=s L0x20014ac8, L0x20014ac8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ad4, L0x20014ad4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001517c, L0x2001517c <=s 20207750@32,
(-16831941)@32 <=s L0x20015188, L0x20015188 <=s 16831941@32,
(-23565127)@32 <=s L0x20015194, L0x20015194 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ae0, L0x20014ae0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014aec, L0x20014aec <=s 16831941@32,
(-23565127)@32 <=s L0x20014af8, L0x20014af8 <=s 23565127@32,
(-20207750)@32 <=s L0x200151a0, L0x200151a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200151ac, L0x200151ac <=s 16831941@32,
(-23565127)@32 <=s L0x200151b8, L0x200151b8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b04, L0x20014b04 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b10, L0x20014b10 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b1c, L0x20014b1c <=s 23565127@32,
(-20207750)@32 <=s L0x200151c4, L0x200151c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200151d0, L0x200151d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200151dc, L0x200151dc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b28, L0x20014b28 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b34, L0x20014b34 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b40, L0x20014b40 <=s 23565127@32,
(-20207750)@32 <=s L0x200151e8, L0x200151e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200151f4, L0x200151f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015200, L0x20015200 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b4c, L0x20014b4c <=s 20207750@32,
(-16831941)@32 <=s L0x20014b58, L0x20014b58 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b64, L0x20014b64 <=s 23565127@32,
(-20207750)@32 <=s L0x2001520c, L0x2001520c <=s 20207750@32,
(-16831941)@32 <=s L0x20015218, L0x20015218 <=s 16831941@32,
(-23565127)@32 <=s L0x20015224, L0x20015224 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b70, L0x20014b70 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b7c, L0x20014b7c <=s 16831941@32,
(-23565127)@32 <=s L0x20014b88, L0x20014b88 <=s 23565127@32,
(-20207750)@32 <=s L0x20015230, L0x20015230 <=s 20207750@32,
(-16831941)@32 <=s L0x2001523c, L0x2001523c <=s 16831941@32,
(-23565127)@32 <=s L0x20015248, L0x20015248 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b94, L0x20014b94 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ba0, L0x20014ba0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bac, L0x20014bac <=s 23565127@32,
(-20207750)@32 <=s L0x20015254, L0x20015254 <=s 20207750@32,
(-16831941)@32 <=s L0x20015260, L0x20015260 <=s 16831941@32,
(-23565127)@32 <=s L0x2001526c, L0x2001526c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bb8, L0x20014bb8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014bc4, L0x20014bc4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bd0, L0x20014bd0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015278, L0x20015278 <=s 20207750@32,
(-16831941)@32 <=s L0x20015284, L0x20015284 <=s 16831941@32,
(-23565127)@32 <=s L0x20015290, L0x20015290 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bdc, L0x20014bdc <=s 20207750@32,
(-16831941)@32 <=s L0x20014be8, L0x20014be8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bf4, L0x20014bf4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001529c, L0x2001529c <=s 20207750@32,
(-16831941)@32 <=s L0x200152a8, L0x200152a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200152b4, L0x200152b4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c00, L0x20014c00 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c0c, L0x20014c0c <=s 16831941@32,
(-23565127)@32 <=s L0x20014c18, L0x20014c18 <=s 23565127@32,
(-20207750)@32 <=s L0x200152c0, L0x200152c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200152cc, L0x200152cc <=s 16831941@32,
(-23565127)@32 <=s L0x200152d8, L0x200152d8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c24, L0x20014c24 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c30, L0x20014c30 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c3c, L0x20014c3c <=s 23565127@32,
(-20207750)@32 <=s L0x200152e4, L0x200152e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200152f0, L0x200152f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200152fc, L0x200152fc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c48, L0x20014c48 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c54, L0x20014c54 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c60, L0x20014c60 <=s 23565127@32,
(-20207750)@32 <=s L0x20015308, L0x20015308 <=s 20207750@32,
(-16831941)@32 <=s L0x20015314, L0x20015314 <=s 16831941@32,
(-23565127)@32 <=s L0x20015320, L0x20015320 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c6c, L0x20014c6c <=s 20207750@32,
(-16831941)@32 <=s L0x20014c78, L0x20014c78 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c84, L0x20014c84 <=s 23565127@32,
(-20207750)@32 <=s L0x2001532c, L0x2001532c <=s 20207750@32,
(-16831941)@32 <=s L0x20015338, L0x20015338 <=s 16831941@32,
(-23565127)@32 <=s L0x20015344, L0x20015344 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c90, L0x20014c90 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c9c, L0x20014c9c <=s 16831941@32,
(-23565127)@32 <=s L0x20014ca8, L0x20014ca8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015350, L0x20015350 <=s 20207750@32,
(-16831941)@32 <=s L0x2001535c, L0x2001535c <=s 16831941@32,
(-23565127)@32 <=s L0x20015368, L0x20015368 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cb4, L0x20014cb4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014cc0, L0x20014cc0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ccc, L0x20014ccc <=s 23565127@32,
(-20207750)@32 <=s L0x20015374, L0x20015374 <=s 20207750@32,
(-16831941)@32 <=s L0x20015380, L0x20015380 <=s 16831941@32,
(-23565127)@32 <=s L0x2001538c, L0x2001538c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cd8, L0x20014cd8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ce4, L0x20014ce4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014cf0, L0x20014cf0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015398, L0x20015398 <=s 20207750@32,
(-16831941)@32 <=s L0x200153a4, L0x200153a4 <=s 16831941@32,
(-23565127)@32 <=s L0x200153b0, L0x200153b0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cfc, L0x20014cfc <=s 20207750@32,
(-16831941)@32 <=s L0x20014d08, L0x20014d08 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d14, L0x20014d14 <=s 23565127@32,
(-20207750)@32 <=s L0x200153bc, L0x200153bc <=s 20207750@32,
(-16831941)@32 <=s L0x200153c8, L0x200153c8 <=s 16831941@32,
(-23565127)@32 <=s L0x200153d4, L0x200153d4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d20, L0x20014d20 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d2c, L0x20014d2c <=s 16831941@32,
(-23565127)@32 <=s L0x20014d38, L0x20014d38 <=s 23565127@32,
(-20207750)@32 <=s L0x200153e0, L0x200153e0 <=s 20207750@32,
(-16831941)@32 <=s L0x200153ec, L0x200153ec <=s 16831941@32,
(-23565127)@32 <=s L0x200153f8, L0x200153f8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d44, L0x20014d44 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d50, L0x20014d50 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d5c, L0x20014d5c <=s 23565127@32,
(-20207750)@32 <=s L0x20015404, L0x20015404 <=s 20207750@32,
(-16831941)@32 <=s L0x20015410, L0x20015410 <=s 16831941@32,
(-23565127)@32 <=s L0x2001541c, L0x2001541c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d68, L0x20014d68 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d74, L0x20014d74 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d80, L0x20014d80 <=s 23565127@32,
(-20207750)@32 <=s L0x20015428, L0x20015428 <=s 20207750@32,
(-16831941)@32 <=s L0x20015434, L0x20015434 <=s 16831941@32,
(-23565127)@32 <=s L0x20015440, L0x20015440 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d8c, L0x20014d8c <=s 20207750@32,
(-16831941)@32 <=s L0x20014d98, L0x20014d98 <=s 16831941@32,
(-23565127)@32 <=s L0x20014da4, L0x20014da4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001544c, L0x2001544c <=s 20207750@32,
(-16831941)@32 <=s L0x20015458, L0x20015458 <=s 16831941@32,
(-23565127)@32 <=s L0x20015464, L0x20015464 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014db0, L0x20014db0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014dbc, L0x20014dbc <=s 16831941@32,
(-23565127)@32 <=s L0x20014dc8, L0x20014dc8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015470, L0x20015470 <=s 20207750@32,
(-16831941)@32 <=s L0x2001547c, L0x2001547c <=s 16831941@32,
(-23565127)@32 <=s L0x20015488, L0x20015488 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014dd4, L0x20014dd4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014de0, L0x20014de0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014dec, L0x20014dec <=s 23565127@32,
(-20207750)@32 <=s L0x20015494, L0x20015494 <=s 20207750@32,
(-16831941)@32 <=s L0x200154a0, L0x200154a0 <=s 16831941@32,
(-23565127)@32 <=s L0x200154ac, L0x200154ac <=s 23565127@32
,
(-20207750)@32 <=s L0x20014df8, L0x20014df8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e04, L0x20014e04 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e10, L0x20014e10 <=s 23565127@32,
(-20207750)@32 <=s L0x200154b8, L0x200154b8 <=s 20207750@32,
(-16831941)@32 <=s L0x200154c4, L0x200154c4 <=s 16831941@32,
(-23565127)@32 <=s L0x200154d0, L0x200154d0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e1c, L0x20014e1c <=s 20207750@32,
(-16831941)@32 <=s L0x20014e28, L0x20014e28 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e34, L0x20014e34 <=s 23565127@32,
(-20207750)@32 <=s L0x200154dc, L0x200154dc <=s 20207750@32,
(-16831941)@32 <=s L0x200154e8, L0x200154e8 <=s 16831941@32,
(-23565127)@32 <=s L0x200154f4, L0x200154f4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e40, L0x20014e40 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e4c, L0x20014e4c <=s 16831941@32,
(-23565127)@32 <=s L0x20014e58, L0x20014e58 <=s 23565127@32,
(-20207750)@32 <=s L0x20015500, L0x20015500 <=s 20207750@32,
(-16831941)@32 <=s L0x2001550c, L0x2001550c <=s 16831941@32,
(-23565127)@32 <=s L0x20015518, L0x20015518 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e64, L0x20014e64 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e70, L0x20014e70 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e7c, L0x20014e7c <=s 23565127@32,
(-20207750)@32 <=s L0x20015524, L0x20015524 <=s 20207750@32,
(-16831941)@32 <=s L0x20015530, L0x20015530 <=s 16831941@32,
(-23565127)@32 <=s L0x2001553c, L0x2001553c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e88, L0x20014e88 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e94, L0x20014e94 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ea0, L0x20014ea0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015548, L0x20015548 <=s 20207750@32,
(-16831941)@32 <=s L0x20015554, L0x20015554 <=s 16831941@32,
(-23565127)@32 <=s L0x20015560, L0x20015560 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014eac, L0x20014eac <=s 20207750@32,
(-16831941)@32 <=s L0x20014eb8, L0x20014eb8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ec4, L0x20014ec4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001556c, L0x2001556c <=s 20207750@32,
(-16831941)@32 <=s L0x20015578, L0x20015578 <=s 16831941@32,
(-23565127)@32 <=s L0x20015584, L0x20015584 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ed0, L0x20014ed0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014edc, L0x20014edc <=s 16831941@32,
(-23565127)@32 <=s L0x20014ee8, L0x20014ee8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015590, L0x20015590 <=s 20207750@32,
(-16831941)@32 <=s L0x2001559c, L0x2001559c <=s 16831941@32,
(-23565127)@32 <=s L0x200155a8, L0x200155a8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ef4, L0x20014ef4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f00, L0x20014f00 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f0c, L0x20014f0c <=s 23565127@32,
(-20207750)@32 <=s L0x200155b4, L0x200155b4 <=s 20207750@32,
(-16831941)@32 <=s L0x200155c0, L0x200155c0 <=s 16831941@32,
(-23565127)@32 <=s L0x200155cc, L0x200155cc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014f18, L0x20014f18 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f24, L0x20014f24 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f30, L0x20014f30 <=s 23565127@32,
(-20207750)@32 <=s L0x200155d8, L0x200155d8 <=s 20207750@32,
(-16831941)@32 <=s L0x200155e4, L0x200155e4 <=s 16831941@32,
(-23565127)@32 <=s L0x200155f0, L0x200155f0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014f3c, L0x20014f3c <=s 20207750@32,
(-16831941)@32 <=s L0x20014f48, L0x20014f48 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f54, L0x20014f54 <=s 23565127@32,
(-20207750)@32 <=s L0x200155fc, L0x200155fc <=s 20207750@32,
(-16831941)@32 <=s L0x20015608, L0x20015608 <=s 16831941@32,
(-23565127)@32 <=s L0x20015614, L0x20015614 <=s 23565127@32
] prove with [ precondition ];



(* vmov	lr, s3                                     #! PC = 0x8038204 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x8038208 *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x8038008 <_3x2_light_inner>             #! PC = 0x803820c *)
#bne.w	0x8038008 <_3x2_light_inner>             #! 0x803820c = 0x803820c;
(* sub.w	r0, r0, #1728	; 0x6c0                     #! PC = 0x8038210 *)
subs dc r0 r0 1728@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x8038214 *)
adds dc r0 r0 4@uint32;
(* vmov	r12, s2                                    #! PC = 0x8038218 *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x803821c *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8038000 <_3x2_light>                   #! PC = 0x8038220 *)
#bne.w	0x8038000 <_3x2_light>                   #! 0x8038220 = 0x8038220;
(* sub.w	r0, r0, #12                               #! PC = 0x8038224 *)
subs dc r0 r0 12@uint32;
(* add.w	r0, r0, #3456	; 0xd80                     #! PC = 0x8038228 *)
adds dc r0 r0 3456@uint32;
(* add.w	r12, r0, #12                              #! PC = 0x803822c *)
adds dc r12 r0 12@uint32;
(* vmov	s2, r12                                    #! PC = 0x8038230 *)
mov s2 r12;
(* add.w	lr, r0, #1728	; 0x6c0                     #! PC = 0x8038234 *)
adds dc lr r0 1728@uint32;
(* vmov	s3, lr                                     #! PC = 0x8038238 *)
mov s3 lr;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015618; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x20015618;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015624; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x20015624;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015630; Value = 0x00000003; PC = 0x8038244 *)
mov r6 L0x20015630;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015cd8; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x20015cd8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ce4; Value = 0x00000003; PC = 0x803824c *)
mov r8 L0x20015ce4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015cf0; Value = 0xfffffffa; PC = 0x8038250 *)
mov r9 L0x20015cf0;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015624; PC = 0x80382d8 *)
mov L0x20015624 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015630; PC = 0x80382dc *)
mov L0x20015630 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015cd8; PC = 0x80382e0 *)
mov L0x20015cd8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ce4; PC = 0x80382e4 *)
mov L0x20015ce4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015cf0; PC = 0x80382e8 *)
mov L0x20015cf0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015618; PC = 0x80382ec *)
mov L0x20015618 r4;




(**************** CUT 145, - *****************)

ecut and [
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015618*x**0*z** 0 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015624*x**0*z** 0 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015630*x**0*z** 0 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015cd8*x**0*z** 0 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015ce4*x**0*z** 0 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015cf0*x**0*z** 0 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x2001563c; Value = 0xffeb4785; PC = 0x80382f0 *)
mov r4 L0x2001563c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015648; Value = 0x00000000; PC = 0x80382f4 *)
mov r5 L0x20015648;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015654; Value = 0x00000003; PC = 0x80382f8 *)
mov r6 L0x20015654;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015cfc; Value = 0xffeb477f; PC = 0x80382fc *)
mov r7 L0x20015cfc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d08; Value = 0x00000003; PC = 0x8038300 *)
mov r8 L0x20015d08;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d14; Value = 0x00000003; PC = 0x8038304 *)
mov r9 L0x20015d14;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015648; PC = 0x80383ac *)
mov L0x20015648 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015654; PC = 0x80383b0 *)
mov L0x20015654 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015cfc; PC = 0x80383b4 *)
mov L0x20015cfc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d08; PC = 0x80383b8 *)
mov L0x20015d08 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d14; PC = 0x80383bc *)
mov L0x20015d14 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001563c; PC = 0x80383c0 *)
mov L0x2001563c r4;



(**************** CUT 146, - *****************)

ecut and [
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x2001563c*x**0*z** 0 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x20015648*x**0*z** 0 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x20015654*x**0*z** 0 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x20015cfc*x**0*z** 0 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x20015d08*x**0*z** 0 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x20015d14*x**0*z** 0 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015660; Value = 0x0014b884; PC = 0x80383c4 *)
mov r4 L0x20015660;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001566c; Value = 0x00000000; PC = 0x80383c8 *)
mov r5 L0x2001566c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015678; Value = 0x00000003; PC = 0x80383cc *)
mov r6 L0x20015678;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d20; Value = 0x0014b87e; PC = 0x80383d0 *)
mov r7 L0x20015d20;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d2c; Value = 0x00000003; PC = 0x80383d4 *)
mov r8 L0x20015d2c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d38; Value = 0x00000003; PC = 0x80383d8 *)
mov r9 L0x20015d38;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001566c; PC = 0x8038480 *)
mov L0x2001566c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015678; PC = 0x8038484 *)
mov L0x20015678 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d20; PC = 0x8038488 *)
mov L0x20015d20 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d2c; PC = 0x803848c *)
mov L0x20015d2c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d38; PC = 0x8038490 *)
mov L0x20015d38 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015660; PC = 0x8038494 *)
mov L0x20015660 r4;




(**************** CUT 147, - *****************)

ecut and [
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x20015660*x**0*z** 0 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x2001566c*x**0*z** 0 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x20015678*x**0*z** 0 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x20015d20*x**0*z** 0 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x20015d2c*x**0*z** 0 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x20015d38*x**0*z** 0 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015684; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x20015684;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015690; Value = 0xfffffff7; PC = 0x8038240 *)
mov r5 L0x20015690;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001569c; Value = 0x00000000; PC = 0x8038244 *)
mov r6 L0x2001569c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d44; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x20015d44;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d50; Value = 0xfffffffd; PC = 0x803824c *)
mov r8 L0x20015d50;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d5c; Value = 0xfffffffa; PC = 0x8038250 *)
mov r9 L0x20015d5c;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015690; PC = 0x80382d8 *)
mov L0x20015690 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001569c; PC = 0x80382dc *)
mov L0x2001569c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d44; PC = 0x80382e0 *)
mov L0x20015d44 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d50; PC = 0x80382e4 *)
mov L0x20015d50 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d5c; PC = 0x80382e8 *)
mov L0x20015d5c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015684; PC = 0x80382ec *)
mov L0x20015684 r4;




(**************** CUT 148, - *****************)

ecut and [
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x20015684*x**0*z** 1 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x20015690*x**0*z** 1 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x2001569c*x**0*z** 1 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x20015d44*x**0*z** 1 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x20015d50*x**0*z** 1 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x20015d5c*x**0*z** 1 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200156a8; Value = 0x00000000; PC = 0x80382f0 *)
mov r4 L0x200156a8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200156b4; Value = 0x00000000; PC = 0x80382f4 *)
mov r5 L0x200156b4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200156c0; Value = 0x00000000; PC = 0x80382f8 *)
mov r6 L0x200156c0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d68; Value = 0xffeb477f; PC = 0x80382fc *)
mov r7 L0x20015d68;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d74; Value = 0x00000006; PC = 0x8038300 *)
mov r8 L0x20015d74;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d80; Value = 0xffeb477f; PC = 0x8038304 *)
mov r9 L0x20015d80;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200156b4; PC = 0x80383ac *)
mov L0x200156b4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200156c0; PC = 0x80383b0 *)
mov L0x200156c0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d68; PC = 0x80383b4 *)
mov L0x20015d68 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d74; PC = 0x80383b8 *)
mov L0x20015d74 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d80; PC = 0x80383bc *)
mov L0x20015d80 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200156a8; PC = 0x80383c0 *)
mov L0x200156a8 r4;




(**************** CUT 149, - *****************)

ecut and [
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x200156a8*x**0*z** 1 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x200156b4*x**0*z** 1 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x200156c0*x**0*z** 1 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x20015d68*x**0*z** 1 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x20015d74*x**0*z** 1 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x20015d80*x**0*z** 1 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200156cc; Value = 0x00000000; PC = 0x80383c4 *)
mov r4 L0x200156cc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200156d8; Value = 0x00000000; PC = 0x80383c8 *)
mov r5 L0x200156d8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200156e4; Value = 0x00000000; PC = 0x80383cc *)
mov r6 L0x200156e4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d8c; Value = 0x0014b87e; PC = 0x80383d0 *)
mov r7 L0x20015d8c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d98; Value = 0x00000006; PC = 0x80383d4 *)
mov r8 L0x20015d98;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015da4; Value = 0x0014b87e; PC = 0x80383d8 *)
mov r9 L0x20015da4;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200156d8; PC = 0x8038480 *)
mov L0x200156d8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200156e4; PC = 0x8038484 *)
mov L0x200156e4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d8c; PC = 0x8038488 *)
mov L0x20015d8c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d98; PC = 0x803848c *)
mov L0x20015d98 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015da4; PC = 0x8038490 *)
mov L0x20015da4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200156cc; PC = 0x8038494 *)
mov L0x200156cc r4;




(**************** CUT 150, - *****************)

ecut and [
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x200156cc*x**0*z** 1 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x200156d8*x**0*z** 1 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x200156e4*x**0*z** 1 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x20015d8c*x**0*z** 1 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x20015d98*x**0*z** 1 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x20015da4*x**0*z** 1 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200156f0; Value = 0x00000003; PC = 0x803823c *)
mov r4 L0x200156f0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200156fc; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x200156fc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015708; Value = 0x00000006; PC = 0x8038244 *)
mov r6 L0x20015708;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015db0; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x20015db0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015dbc; Value = 0xfffffffd; PC = 0x803824c *)
mov r8 L0x20015dbc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015dc8; Value = 0x00000003; PC = 0x8038250 *)
mov r9 L0x20015dc8;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200156fc; PC = 0x80382d8 *)
mov L0x200156fc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015708; PC = 0x80382dc *)
mov L0x20015708 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015db0; PC = 0x80382e0 *)
mov L0x20015db0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015dbc; PC = 0x80382e4 *)
mov L0x20015dbc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015dc8; PC = 0x80382e8 *)
mov L0x20015dc8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200156f0; PC = 0x80382ec *)
mov L0x200156f0 r4;




(**************** CUT 151, - *****************)

ecut and [
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x200156f0*x**0*z** 2 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x200156fc*x**0*z** 2 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x20015708*x**0*z** 2 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x20015db0*x**0*z** 2 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x20015dbc*x**0*z** 2 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x20015dc8*x**0*z** 2 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015714; Value = 0xfff61641; PC = 0x80382f0 *)
mov r4 L0x20015714;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015720; Value = 0xffd68f04; PC = 0x80382f4 *)
mov r5 L0x20015720;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001572c; Value = 0xfffffffd; PC = 0x80382f8 *)
mov r6 L0x2001572c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015dd4; Value = 0xffeb477f; PC = 0x80382fc *)
mov r7 L0x20015dd4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015de0; Value = 0x0014b881; PC = 0x8038300 *)
mov r8 L0x20015de0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015dec; Value = 0xffeb477f; PC = 0x8038304 *)
mov r9 L0x20015dec;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015720; PC = 0x80383ac *)
mov L0x20015720 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001572c; PC = 0x80383b0 *)
mov L0x2001572c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015dd4; PC = 0x80383b4 *)
mov L0x20015dd4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015de0; PC = 0x80383b8 *)
mov L0x20015de0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015dec; PC = 0x80383bc *)
mov L0x20015dec r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015714; PC = 0x80383c0 *)
mov L0x20015714 r4;




(**************** CUT 152, - *****************)

ecut and [
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x20015714*x**0*z** 2 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x20015720*x**0*z** 2 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x2001572c*x**0*z** 2 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x20015dd4*x**0*z** 2 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x20015de0*x**0*z** 2 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x20015dec*x**0*z** 2 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015738; Value = 0x0009e9c5; PC = 0x80383c4 *)
mov r4 L0x20015738;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015744; Value = 0x00297102; PC = 0x80383c8 *)
mov r5 L0x20015744;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015750; Value = 0xfffffffd; PC = 0x80383cc *)
mov r6 L0x20015750;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015df8; Value = 0x0014b87e; PC = 0x80383d0 *)
mov r7 L0x20015df8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e04; Value = 0xffeb4782; PC = 0x80383d4 *)
mov r8 L0x20015e04;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e10; Value = 0x0014b87e; PC = 0x80383d8 *)
mov r9 L0x20015e10;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015744; PC = 0x8038480 *)
mov L0x20015744 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015750; PC = 0x8038484 *)
mov L0x20015750 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015df8; PC = 0x8038488 *)
mov L0x20015df8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e04; PC = 0x803848c *)
mov L0x20015e04 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e10; PC = 0x8038490 *)
mov L0x20015e10 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015738; PC = 0x8038494 *)
mov L0x20015738 r4;




(**************** CUT 153, - *****************)

ecut and [
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015738*x**0*z** 2 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015744*x**0*z** 2 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015750*x**0*z** 2 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015df8*x**0*z** 2 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015e04*x**0*z** 2 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015e10*x**0*z** 2 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x2001575c; Value = 0x00000006; PC = 0x803823c *)
mov r4 L0x2001575c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015768; Value = 0xfffffffd; PC = 0x8038240 *)
mov r5 L0x20015768;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015774; Value = 0xfffffffa; PC = 0x8038244 *)
mov r6 L0x20015774;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e1c; Value = 0x00000006; PC = 0x8038248 *)
mov r7 L0x20015e1c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e28; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20015e28;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e34; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x20015e34;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015768; PC = 0x80382d8 *)
mov L0x20015768 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015774; PC = 0x80382dc *)
mov L0x20015774 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e1c; PC = 0x80382e0 *)
mov L0x20015e1c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e28; PC = 0x80382e4 *)
mov L0x20015e28 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e34; PC = 0x80382e8 *)
mov L0x20015e34 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001575c; PC = 0x80382ec *)
mov L0x2001575c r4;




(**************** CUT 154, - *****************)

ecut and [
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x2001575c*x**0*z** 3 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x20015768*x**0*z** 3 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x20015774*x**0*z** 3 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x20015e1c*x**0*z** 3 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x20015e28*x**0*z** 3 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x20015e34*x**0*z** 3 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015780; Value = 0x0014b881; PC = 0x80382f0 *)
mov r4 L0x20015780;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001578c; Value = 0xfffffffd; PC = 0x80382f4 *)
mov r5 L0x2001578c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015798; Value = 0x00000003; PC = 0x80382f8 *)
mov r6 L0x20015798;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e40; Value = 0xfffffffd; PC = 0x80382fc *)
mov r7 L0x20015e40;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e4c; Value = 0x00000000; PC = 0x8038300 *)
mov r8 L0x20015e4c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e58; Value = 0xfffffffd; PC = 0x8038304 *)
mov r9 L0x20015e58;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001578c; PC = 0x80383ac *)
mov L0x2001578c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015798; PC = 0x80383b0 *)
mov L0x20015798 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e40; PC = 0x80383b4 *)
mov L0x20015e40 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e4c; PC = 0x80383b8 *)
mov L0x20015e4c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e58; PC = 0x80383bc *)
mov L0x20015e58 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015780; PC = 0x80383c0 *)
mov L0x20015780 r4;




(**************** CUT 155, - *****************)

ecut and [
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x20015780*x**0*z** 3 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x2001578c*x**0*z** 3 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x20015798*x**0*z** 3 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x20015e40*x**0*z** 3 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x20015e4c*x**0*z** 3 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x20015e58*x**0*z** 3 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200157a4; Value = 0xffeb4782; PC = 0x80383c4 *)
mov r4 L0x200157a4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200157b0; Value = 0xfffffffd; PC = 0x80383c8 *)
mov r5 L0x200157b0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200157bc; Value = 0x00000003; PC = 0x80383cc *)
mov r6 L0x200157bc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e64; Value = 0xfffffffd; PC = 0x80383d0 *)
mov r7 L0x20015e64;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e70; Value = 0x00000000; PC = 0x80383d4 *)
mov r8 L0x20015e70;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e7c; Value = 0xfffffffd; PC = 0x80383d8 *)
mov r9 L0x20015e7c;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200157b0; PC = 0x8038480 *)
mov L0x200157b0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200157bc; PC = 0x8038484 *)
mov L0x200157bc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e64; PC = 0x8038488 *)
mov L0x20015e64 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e70; PC = 0x803848c *)
mov L0x20015e70 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e7c; PC = 0x8038490 *)
mov L0x20015e7c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200157a4; PC = 0x8038494 *)
mov L0x200157a4 r4;




(**************** CUT 156, - *****************)

ecut and [
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x200157a4*x**0*z** 3 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x200157b0*x**0*z** 3 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x200157bc*x**0*z** 3 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x20015e64*x**0*z** 3 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x20015e70*x**0*z** 3 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x20015e7c*x**0*z** 3 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200157c8; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x200157c8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200157d4; Value = 0x00000ffa; PC = 0x8038240 *)
mov r5 L0x200157d4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200157e0; Value = 0x00000003; PC = 0x8038244 *)
mov r6 L0x200157e0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e88; Value = 0xfffffffd; PC = 0x8038248 *)
mov r7 L0x20015e88;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e94; Value = 0xfffffffd; PC = 0x803824c *)
mov r8 L0x20015e94;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ea0; Value = 0xfffffff7; PC = 0x8038250 *)
mov r9 L0x20015ea0;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200157d4; PC = 0x80382d8 *)
mov L0x200157d4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200157e0; PC = 0x80382dc *)
mov L0x200157e0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e88; PC = 0x80382e0 *)
mov L0x20015e88 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e94; PC = 0x80382e4 *)
mov L0x20015e94 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ea0; PC = 0x80382e8 *)
mov L0x20015ea0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200157c8; PC = 0x80382ec *)
mov L0x200157c8 r4;




(**************** CUT 157, - *****************)

ecut and [
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x200157c8*x**0*z** 4 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x200157d4*x**0*z** 4 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x200157e0*x**0*z** 4 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x20015e88*x**0*z** 4 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x20015e94*x**0*z** 4 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x20015ea0*x**0*z** 4 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200157ec; Value = 0x002970ff; PC = 0x80382f0 *)
mov r4 L0x200157ec;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200157f8; Value = 0xffe70324; PC = 0x80382f4 *)
mov r5 L0x200157f8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015804; Value = 0x0009e9c5; PC = 0x80382f8 *)
mov r6 L0x20015804;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015eac; Value = 0xfffffffd; PC = 0x80382fc *)
mov r7 L0x20015eac;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015eb8; Value = 0xffeb4782; PC = 0x8038300 *)
mov r8 L0x20015eb8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ec4; Value = 0x00000000; PC = 0x8038304 *)
mov r9 L0x20015ec4;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200157f8; PC = 0x80383ac *)
mov L0x200157f8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015804; PC = 0x80383b0 *)
mov L0x20015804 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015eac; PC = 0x80383b4 *)
mov L0x20015eac r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015eb8; PC = 0x80383b8 *)
mov L0x20015eb8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ec4; PC = 0x80383bc *)
mov L0x20015ec4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200157ec; PC = 0x80383c0 *)
mov L0x200157ec r4;




(**************** CUT 158, - *****************)

ecut and [
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x200157ec*x**0*z** 4 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x200157f8*x**0*z** 4 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x20015804*x**0*z** 4 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x20015eac*x**0*z** 4 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x20015eb8*x**0*z** 4 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x20015ec4*x**0*z** 4 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015810; Value = 0xffd68f01; PC = 0x80383c4 *)
mov r4 L0x20015810;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001581c; Value = 0x0018ecd9; PC = 0x80383c8 *)
mov r5 L0x2001581c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015828; Value = 0xfff61641; PC = 0x80383cc *)
mov r6 L0x20015828;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ed0; Value = 0xfffffffd; PC = 0x80383d0 *)
mov r7 L0x20015ed0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015edc; Value = 0x0014b881; PC = 0x80383d4 *)
mov r8 L0x20015edc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ee8; Value = 0x00000000; PC = 0x80383d8 *)
mov r9 L0x20015ee8;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001581c; PC = 0x8038480 *)
mov L0x2001581c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015828; PC = 0x8038484 *)
mov L0x20015828 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ed0; PC = 0x8038488 *)
mov L0x20015ed0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015edc; PC = 0x803848c *)
mov L0x20015edc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ee8; PC = 0x8038490 *)
mov L0x20015ee8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015810; PC = 0x8038494 *)
mov L0x20015810 r4;




(**************** CUT 159, - *****************)

ecut and [
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x20015810*x**0*z** 4 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x2001581c*x**0*z** 4 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x20015828*x**0*z** 4 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x20015ed0*x**0*z** 4 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x20015edc*x**0*z** 4 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x20015ee8*x**0*z** 4 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015834; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x20015834;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015840; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x20015840;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001584c; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x2001584c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ef4; Value = 0x00000000; PC = 0x8038248 *)
mov r7 L0x20015ef4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f00; Value = 0xfffffffa; PC = 0x803824c *)
mov r8 L0x20015f00;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f0c; Value = 0x00000006; PC = 0x8038250 *)
mov r9 L0x20015f0c;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015840; PC = 0x80382d8 *)
mov L0x20015840 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001584c; PC = 0x80382dc *)
mov L0x2001584c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ef4; PC = 0x80382e0 *)
mov L0x20015ef4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f00; PC = 0x80382e4 *)
mov L0x20015f00 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f0c; PC = 0x80382e8 *)
mov L0x20015f0c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015834; PC = 0x80382ec *)
mov L0x20015834 r4;




(**************** CUT 160, - *****************)

ecut and [
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x20015834*x**0*z** 5 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x20015840*x**0*z** 5 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x2001584c*x**0*z** 5 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x20015ef4*x**0*z** 5 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x20015f00*x**0*z** 5 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x20015f0c*x**0*z** 5 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015858; Value = 0x002970ff; PC = 0x80382f0 *)
mov r4 L0x20015858;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015864; Value = 0x00297102; PC = 0x80382f4 *)
mov r5 L0x20015864;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015870; Value = 0xfff6163b; PC = 0x80382f8 *)
mov r6 L0x20015870;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f18; Value = 0xffeb4785; PC = 0x80382fc *)
mov r7 L0x20015f18;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f24; Value = 0xffeb477f; PC = 0x8038300 *)
mov r8 L0x20015f24;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f30; Value = 0xfffffffd; PC = 0x8038304 *)
mov r9 L0x20015f30;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015864; PC = 0x80383ac *)
mov L0x20015864 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015870; PC = 0x80383b0 *)
mov L0x20015870 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f18; PC = 0x80383b4 *)
mov L0x20015f18 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f24; PC = 0x80383b8 *)
mov L0x20015f24 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f30; PC = 0x80383bc *)
mov L0x20015f30 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015858; PC = 0x80383c0 *)
mov L0x20015858 r4;




(**************** CUT 161, - *****************)

ecut and [
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015858*x**0*z** 5 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015864*x**0*z** 5 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015870*x**0*z** 5 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015f18*x**0*z** 5 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015f24*x**0*z** 5 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015f30*x**0*z** 5 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x2001587c; Value = 0xffd68f01; PC = 0x80383c4 *)
mov r4 L0x2001587c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015888; Value = 0xffd68f04; PC = 0x80383c8 *)
mov r5 L0x20015888;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015894; Value = 0x0009e9bf; PC = 0x80383cc *)
mov r6 L0x20015894;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f3c; Value = 0x0014b884; PC = 0x80383d0 *)
mov r7 L0x20015f3c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f48; Value = 0x0014b87e; PC = 0x80383d4 *)
mov r8 L0x20015f48;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f54; Value = 0xfffffffd; PC = 0x80383d8 *)
mov r9 L0x20015f54;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015888; PC = 0x8038480 *)
mov L0x20015888 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015894; PC = 0x8038484 *)
mov L0x20015894 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f3c; PC = 0x8038488 *)
mov L0x20015f3c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f48; PC = 0x803848c *)
mov L0x20015f48 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f54; PC = 0x8038490 *)
mov L0x20015f54 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001587c; PC = 0x8038494 *)
mov L0x2001587c r4;




(**************** CUT 162, - *****************)

ecut and [
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x2001587c*x**0*z** 5 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x20015888*x**0*z** 5 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x20015894*x**0*z** 5 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x20015f3c*x**0*z** 5 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x20015f48*x**0*z** 5 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x20015f54*x**0*z** 5 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200158a0; Value = 0x00000006; PC = 0x803823c *)
mov r4 L0x200158a0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200158ac; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x200158ac;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200158b8; Value = 0xfffffffa; PC = 0x8038244 *)
mov r6 L0x200158b8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f60; Value = 0xfffffffd; PC = 0x8038248 *)
mov r7 L0x20015f60;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f6c; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20015f6c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f78; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x20015f78;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200158ac; PC = 0x80382d8 *)
mov L0x200158ac r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200158b8; PC = 0x80382dc *)
mov L0x200158b8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f60; PC = 0x80382e0 *)
mov L0x20015f60 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f6c; PC = 0x80382e4 *)
mov L0x20015f6c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f78; PC = 0x80382e8 *)
mov L0x20015f78 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200158a0; PC = 0x80382ec *)
mov L0x200158a0 r4;




(**************** CUT 163, - *****************)

ecut and [
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x200158a0*x**0*z** 6 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x200158ac*x**0*z** 6 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x200158b8*x**0*z** 6 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x20015f60*x**0*z** 6 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x20015f6c*x**0*z** 6 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x20015f78*x**0*z** 6 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200158c4; Value = 0xffeb4782; PC = 0x80382f0 *)
mov r4 L0x200158c4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200158d0; Value = 0x00000003; PC = 0x80382f4 *)
mov r5 L0x200158d0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200158dc; Value = 0xffeb477f; PC = 0x80382f8 *)
mov r6 L0x200158dc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f84; Value = 0xfffffffd; PC = 0x80382fc *)
mov r7 L0x20015f84;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f90; Value = 0x00000000; PC = 0x8038300 *)
mov r8 L0x20015f90;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f9c; Value = 0x0014b87b; PC = 0x8038304 *)
mov r9 L0x20015f9c;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200158d0; PC = 0x80383ac *)
mov L0x200158d0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200158dc; PC = 0x80383b0 *)
mov L0x200158dc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f84; PC = 0x80383b4 *)
mov L0x20015f84 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f90; PC = 0x80383b8 *)
mov L0x20015f90 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f9c; PC = 0x80383bc *)
mov L0x20015f9c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200158c4; PC = 0x80383c0 *)
mov L0x200158c4 r4;




(**************** CUT 164, - *****************)

ecut and [
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x200158c4*x**0*z** 6 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x200158d0*x**0*z** 6 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x200158dc*x**0*z** 6 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x20015f84*x**0*z** 6 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x20015f90*x**0*z** 6 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x20015f9c*x**0*z** 6 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200158e8; Value = 0x0014b881; PC = 0x80383c4 *)
mov r4 L0x200158e8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200158f4; Value = 0x00000003; PC = 0x80383c8 *)
mov r5 L0x200158f4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015900; Value = 0x0014b87e; PC = 0x80383cc *)
mov r6 L0x20015900;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fa8; Value = 0xfffffffd; PC = 0x80383d0 *)
mov r7 L0x20015fa8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fb4; Value = 0x00000000; PC = 0x80383d4 *)
mov r8 L0x20015fb4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fc0; Value = 0xffeb477c; PC = 0x80383d8 *)
mov r9 L0x20015fc0;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200158f4; PC = 0x8038480 *)
mov L0x200158f4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015900; PC = 0x8038484 *)
mov L0x20015900 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fa8; PC = 0x8038488 *)
mov L0x20015fa8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fb4; PC = 0x803848c *)
mov L0x20015fb4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fc0; PC = 0x8038490 *)
mov L0x20015fc0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200158e8; PC = 0x8038494 *)
mov L0x200158e8 r4;




(**************** CUT 165, - *****************)

ecut and [
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x200158e8*x**0*z** 6 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x200158f4*x**0*z** 6 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x20015900*x**0*z** 6 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x20015fa8*x**0*z** 6 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x20015fb4*x**0*z** 6 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x20015fc0*x**0*z** 6 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x2001590c; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x2001590c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015918; Value = 0xfffffffa; PC = 0x8038240 *)
mov r5 L0x20015918;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015924; Value = 0x00000003; PC = 0x8038244 *)
mov r6 L0x20015924;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fcc; Value = 0x00000000; PC = 0x8038248 *)
mov r7 L0x20015fcc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fd8; Value = 0xfffffffd; PC = 0x803824c *)
mov r8 L0x20015fd8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fe4; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x20015fe4;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015918; PC = 0x80382d8 *)
mov L0x20015918 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015924; PC = 0x80382dc *)
mov L0x20015924 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fcc; PC = 0x80382e0 *)
mov L0x20015fcc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fd8; PC = 0x80382e4 *)
mov L0x20015fd8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fe4; PC = 0x80382e8 *)
mov L0x20015fe4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001590c; PC = 0x80382ec *)
mov L0x2001590c r4;



(**************** CUT 166, - *****************)

ecut and [
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x2001590c*x**0*z** 7 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x20015918*x**0*z** 7 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x20015924*x**0*z** 7 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x20015fcc*x**0*z** 7 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x20015fd8*x**0*z** 7 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x20015fe4*x**0*z** 7 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015930; Value = 0xfffffffd; PC = 0x80382f0 *)
mov r4 L0x20015930;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001593c; Value = 0x0014b87e; PC = 0x80382f4 *)
mov r5 L0x2001593c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015948; Value = 0x00000003; PC = 0x80382f8 *)
mov r6 L0x20015948;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ff0; Value = 0x002970ff; PC = 0x80382fc *)
mov r7 L0x20015ff0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ffc; Value = 0xffd68efe; PC = 0x8038300 *)
mov r8 L0x20015ffc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016008; Value = 0xfffffffd; PC = 0x8038304 *)
mov r9 L0x20016008;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001593c; PC = 0x80383ac *)
mov L0x2001593c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015948; PC = 0x80383b0 *)
mov L0x20015948 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ff0; PC = 0x80383b4 *)
mov L0x20015ff0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ffc; PC = 0x80383b8 *)
mov L0x20015ffc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016008; PC = 0x80383bc *)
mov L0x20016008 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015930; PC = 0x80383c0 *)
mov L0x20015930 r4;




(**************** CUT 167, - *****************)

ecut and [
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x20015930*x**0*z** 7 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x2001593c*x**0*z** 7 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x20015948*x**0*z** 7 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x20015ff0*x**0*z** 7 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x20015ffc*x**0*z** 7 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x20016008*x**0*z** 7 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015954; Value = 0xfffffffd; PC = 0x80383c4 *)
mov r4 L0x20015954;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015960; Value = 0xffeb477f; PC = 0x80383c8 *)
mov r5 L0x20015960;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001596c; Value = 0x00000003; PC = 0x80383cc *)
mov r6 L0x2001596c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016014; Value = 0xffd68f01; PC = 0x80383d0 *)
mov r7 L0x20016014;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016020; Value = 0x002970fc; PC = 0x80383d4 *)
mov r8 L0x20016020;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001602c; Value = 0xfffffffd; PC = 0x80383d8 *)
mov r9 L0x2001602c;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015960; PC = 0x8038480 *)
mov L0x20015960 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001596c; PC = 0x8038484 *)
mov L0x2001596c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016014; PC = 0x8038488 *)
mov L0x20016014 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016020; PC = 0x803848c *)
mov L0x20016020 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001602c; PC = 0x8038490 *)
mov L0x2001602c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015954; PC = 0x8038494 *)
mov L0x20015954 r4;




(**************** CUT 168, - *****************)

ecut and [
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x20015954*x**0*z** 7 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x20015960*x**0*z** 7 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x2001596c*x**0*z** 7 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x20016014*x**0*z** 7 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x20016020*x**0*z** 7 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x2001602c*x**0*z** 7 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015978; Value = 0x00000003; PC = 0x803823c *)
mov r4 L0x20015978;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015984; Value = 0xfffffffd; PC = 0x8038240 *)
mov r5 L0x20015984;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015990; Value = 0xfffffffa; PC = 0x8038244 *)
mov r6 L0x20015990;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016038; Value = 0xfffffffa; PC = 0x8038248 *)
mov r7 L0x20016038;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016044; Value = 0xfffffffd; PC = 0x803824c *)
mov r8 L0x20016044;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016050; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x20016050;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015984; PC = 0x80382d8 *)
mov L0x20015984 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015990; PC = 0x80382dc *)
mov L0x20015990 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016038; PC = 0x80382e0 *)
mov L0x20016038 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016044; PC = 0x80382e4 *)
mov L0x20016044 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016050; PC = 0x80382e8 *)
mov L0x20016050 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015978; PC = 0x80382ec *)
mov L0x20015978 r4;




(**************** CUT 169, - *****************)

ecut and [
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20015978*x**0*z** 8 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20015984*x**0*z** 8 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20015990*x**0*z** 8 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20016038*x**0*z** 8 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20016044*x**0*z** 8 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20016050*x**0*z** 8 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x2001599c; Value = 0x00297102; PC = 0x80382f0 *)
mov r4 L0x2001599c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200159a8; Value = 0x0014b881; PC = 0x80382f4 *)
mov r5 L0x200159a8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200159b4; Value = 0x00000003; PC = 0x80382f8 *)
mov r6 L0x200159b4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001605c; Value = 0x0014b87e; PC = 0x80382fc *)
mov r7 L0x2001605c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016068; Value = 0x0014b881; PC = 0x8038300 *)
mov r8 L0x20016068;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016074; Value = 0xfffffffd; PC = 0x8038304 *)
mov r9 L0x20016074;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200159a8; PC = 0x80383ac *)
mov L0x200159a8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200159b4; PC = 0x80383b0 *)
mov L0x200159b4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001605c; PC = 0x80383b4 *)
mov L0x2001605c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016068; PC = 0x80383b8 *)
mov L0x20016068 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016074; PC = 0x80383bc *)
mov L0x20016074 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001599c; PC = 0x80383c0 *)
mov L0x2001599c r4;




(**************** CUT 170, - *****************)

ecut and [
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x2001599c*x**0*z** 8 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x200159a8*x**0*z** 8 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x200159b4*x**0*z** 8 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x2001605c*x**0*z** 8 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x20016068*x**0*z** 8 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x20016074*x**0*z** 8 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200159c0; Value = 0xffd68f04; PC = 0x80383c4 *)
mov r4 L0x200159c0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200159cc; Value = 0xffeb4782; PC = 0x80383c8 *)
mov r5 L0x200159cc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200159d8; Value = 0x00000003; PC = 0x80383cc *)
mov r6 L0x200159d8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016080; Value = 0xffeb477f; PC = 0x80383d0 *)
mov r7 L0x20016080;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001608c; Value = 0xffeb4782; PC = 0x80383d4 *)
mov r8 L0x2001608c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016098; Value = 0xfffffffd; PC = 0x80383d8 *)
mov r9 L0x20016098;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200159cc; PC = 0x8038480 *)
mov L0x200159cc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200159d8; PC = 0x8038484 *)
mov L0x200159d8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016080; PC = 0x8038488 *)
mov L0x20016080 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001608c; PC = 0x803848c *)
mov L0x2001608c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016098; PC = 0x8038490 *)
mov L0x20016098 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200159c0; PC = 0x8038494 *)
mov L0x200159c0 r4;




(**************** CUT 171, - *****************)

ecut and [
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x200159c0*x**0*z** 8 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x200159cc*x**0*z** 8 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x200159d8*x**0*z** 8 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x20016080*x**0*z** 8 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x2001608c*x**0*z** 8 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x20016098*x**0*z** 8 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200159e4; Value = 0xfffffff7; PC = 0x803823c *)
mov r4 L0x200159e4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200159f0; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x200159f0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200159fc; Value = 0x00000003; PC = 0x8038244 *)
mov r6 L0x200159fc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160a4; Value = 0x00000000; PC = 0x8038248 *)
mov r7 L0x200160a4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160b0; Value = 0x00000006; PC = 0x803824c *)
mov r8 L0x200160b0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160bc; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x200160bc;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200159f0; PC = 0x80382d8 *)
mov L0x200159f0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200159fc; PC = 0x80382dc *)
mov L0x200159fc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160a4; PC = 0x80382e0 *)
mov L0x200160a4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160b0; PC = 0x80382e4 *)
mov L0x200160b0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160bc; PC = 0x80382e8 *)
mov L0x200160bc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200159e4; PC = 0x80382ec *)
mov L0x200159e4 r4;




(**************** CUT 172, - *****************)

ecut and [
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200159e4*x**0*z** 9 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200159f0*x**0*z** 9 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200159fc*x**0*z** 9 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200160a4*x**0*z** 9 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200160b0*x**0*z** 9 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200160bc*x**0*z** 9 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015a08; Value = 0x00000000; PC = 0x80382f0 *)
mov r4 L0x20015a08;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a14; Value = 0xffeb477f; PC = 0x80382f4 *)
mov r5 L0x20015a14;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a20; Value = 0xfffffffa; PC = 0x80382f8 *)
mov r6 L0x20015a20;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160c8; Value = 0x0014b87b; PC = 0x80382fc *)
mov r7 L0x200160c8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160d4; Value = 0x0014b881; PC = 0x8038300 *)
mov r8 L0x200160d4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160e0; Value = 0x0014b881; PC = 0x8038304 *)
mov r9 L0x200160e0;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a14; PC = 0x80383ac *)
mov L0x20015a14 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a20; PC = 0x80383b0 *)
mov L0x20015a20 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160c8; PC = 0x80383b4 *)
mov L0x200160c8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160d4; PC = 0x80383b8 *)
mov L0x200160d4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160e0; PC = 0x80383bc *)
mov L0x200160e0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a08; PC = 0x80383c0 *)
mov L0x20015a08 r4;




(**************** CUT 173, - *****************)

ecut and [
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x20015a08*x**0*z** 9 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x20015a14*x**0*z** 9 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x20015a20*x**0*z** 9 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x200160c8*x**0*z** 9 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x200160d4*x**0*z** 9 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x200160e0*x**0*z** 9 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015a2c; Value = 0x00000000; PC = 0x80383c4 *)
mov r4 L0x20015a2c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a38; Value = 0x0014b87e; PC = 0x80383c8 *)
mov r5 L0x20015a38;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a44; Value = 0xfffffffa; PC = 0x80383cc *)
mov r6 L0x20015a44;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160ec; Value = 0xffeb477c; PC = 0x80383d0 *)
mov r7 L0x200160ec;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160f8; Value = 0xffeb4782; PC = 0x80383d4 *)
mov r8 L0x200160f8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016104; Value = 0xffeb4782; PC = 0x80383d8 *)
mov r9 L0x20016104;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a38; PC = 0x8038480 *)
mov L0x20015a38 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a44; PC = 0x8038484 *)
mov L0x20015a44 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160ec; PC = 0x8038488 *)
mov L0x200160ec r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160f8; PC = 0x803848c *)
mov L0x200160f8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016104; PC = 0x8038490 *)
mov L0x20016104 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a2c; PC = 0x8038494 *)
mov L0x20015a2c r4;




(**************** CUT 174, - *****************)

ecut and [
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x20015a2c*x**0*z** 9 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x20015a38*x**0*z** 9 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x20015a44*x**0*z** 9 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x200160ec*x**0*z** 9 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x200160f8*x**0*z** 9 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x20016104*x**0*z** 9 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015a50; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x20015a50;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a5c; Value = 0xfffffffa; PC = 0x8038240 *)
mov r5 L0x20015a5c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a68; Value = 0xfffffff7; PC = 0x8038244 *)
mov r6 L0x20015a68;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016110; Value = 0x00000000; PC = 0x8038248 *)
mov r7 L0x20016110;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001611c; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x2001611c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016128; Value = 0xfffffffa; PC = 0x8038250 *)
mov r9 L0x20016128;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a5c; PC = 0x80382d8 *)
mov L0x20015a5c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a68; PC = 0x80382dc *)
mov L0x20015a68 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016110; PC = 0x80382e0 *)
mov L0x20016110 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001611c; PC = 0x80382e4 *)
mov L0x2001611c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016128; PC = 0x80382e8 *)
mov L0x20016128 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a50; PC = 0x80382ec *)
mov L0x20015a50 r4;




(**************** CUT 175, - *****************)

ecut and [
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x20015a50*x**0*z**10 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x20015a5c*x**0*z**10 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x20015a68*x**0*z**10 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x20016110*x**0*z**10 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x2001611c*x**0*z**10 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x20016128*x**0*z**10 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015a74; Value = 0x00000000; PC = 0x80382f0 *)
mov r4 L0x20015a74;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a80; Value = 0x00000003; PC = 0x80382f4 *)
mov r5 L0x20015a80;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a8c; Value = 0x00000000; PC = 0x80382f8 *)
mov r6 L0x20015a8c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016134; Value = 0xffeb477c; PC = 0x80382fc *)
mov r7 L0x20016134;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016140; Value = 0x0014b87b; PC = 0x8038300 *)
mov r8 L0x20016140;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001614c; Value = 0x0014b87e; PC = 0x8038304 *)
mov r9 L0x2001614c;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a80; PC = 0x80383ac *)
mov L0x20015a80 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a8c; PC = 0x80383b0 *)
mov L0x20015a8c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016134; PC = 0x80383b4 *)
mov L0x20016134 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016140; PC = 0x80383b8 *)
mov L0x20016140 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001614c; PC = 0x80383bc *)
mov L0x2001614c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a74; PC = 0x80383c0 *)
mov L0x20015a74 r4;




(**************** CUT 176, - *****************)

ecut and [
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x20015a74*x**0*z**10 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x20015a80*x**0*z**10 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x20015a8c*x**0*z**10 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x20016134*x**0*z**10 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x20016140*x**0*z**10 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x2001614c*x**0*z**10 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015a98; Value = 0x00000000; PC = 0x80383c4 *)
mov r4 L0x20015a98;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015aa4; Value = 0x00000003; PC = 0x80383c8 *)
mov r5 L0x20015aa4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015ab0; Value = 0x00000000; PC = 0x80383cc *)
mov r6 L0x20015ab0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016158; Value = 0x0014b87b; PC = 0x80383d0 *)
mov r7 L0x20016158;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016164; Value = 0xffeb477c; PC = 0x80383d4 *)
mov r8 L0x20016164;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016170; Value = 0xffeb477f; PC = 0x80383d8 *)
mov r9 L0x20016170;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015aa4; PC = 0x8038480 *)
mov L0x20015aa4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015ab0; PC = 0x8038484 *)
mov L0x20015ab0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016158; PC = 0x8038488 *)
mov L0x20016158 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016164; PC = 0x803848c *)
mov L0x20016164 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016170; PC = 0x8038490 *)
mov L0x20016170 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a98; PC = 0x8038494 *)
mov L0x20015a98 r4;




(**************** CUT 177, - *****************)

ecut and [
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20015a98*x**0*z**10 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20015aa4*x**0*z**10 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20015ab0*x**0*z**10 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20016158*x**0*z**10 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20016164*x**0*z**10 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20016170*x**0*z**10 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015abc; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x20015abc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015ac8; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x20015ac8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015ad4; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x20015ad4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001617c; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x2001617c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016188; Value = 0x00000006; PC = 0x803824c *)
mov r8 L0x20016188;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016194; Value = 0x00000006; PC = 0x8038250 *)
mov r9 L0x20016194;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015ac8; PC = 0x80382d8 *)
mov L0x20015ac8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015ad4; PC = 0x80382dc *)
mov L0x20015ad4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001617c; PC = 0x80382e0 *)
mov L0x2001617c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016188; PC = 0x80382e4 *)
mov L0x20016188 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016194; PC = 0x80382e8 *)
mov L0x20016194 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015abc; PC = 0x80382ec *)
mov L0x20015abc r4;




(**************** CUT 178, - *****************)

ecut and [
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x20015abc*x**0*z**11 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x20015ac8*x**0*z**11 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x20015ad4*x**0*z**11 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x2001617c*x**0*z**11 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x20016188*x**0*z**11 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x20016194*x**0*z**11 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015ae0; Value = 0xfff6163b; PC = 0x80382f0 *)
mov r4 L0x20015ae0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015aec; Value = 0xfffffffa; PC = 0x80382f4 *)
mov r5 L0x20015aec;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015af8; Value = 0x0014b881; PC = 0x80382f8 *)
mov r6 L0x20015af8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161a0; Value = 0xffd68f04; PC = 0x80382fc *)
mov r7 L0x200161a0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161ac; Value = 0xffeb4782; PC = 0x8038300 *)
mov r8 L0x200161ac;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161b8; Value = 0xfffffffd; PC = 0x8038304 *)
mov r9 L0x200161b8;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015aec; PC = 0x80383ac *)
mov L0x20015aec r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015af8; PC = 0x80383b0 *)
mov L0x20015af8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161a0; PC = 0x80383b4 *)
mov L0x200161a0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161ac; PC = 0x80383b8 *)
mov L0x200161ac r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161b8; PC = 0x80383bc *)
mov L0x200161b8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015ae0; PC = 0x80383c0 *)
mov L0x20015ae0 r4;




(**************** CUT 179, - *****************)

ecut and [
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x20015ae0*x**0*z**11 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x20015aec*x**0*z**11 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x20015af8*x**0*z**11 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x200161a0*x**0*z**11 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x200161ac*x**0*z**11 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x200161b8*x**0*z**11 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015b04; Value = 0x0009e9bf; PC = 0x80383c4 *)
mov r4 L0x20015b04;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b10; Value = 0xfffffffa; PC = 0x80383c8 *)
mov r5 L0x20015b10;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b1c; Value = 0xffeb4782; PC = 0x80383cc *)
mov r6 L0x20015b1c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161c4; Value = 0x00297102; PC = 0x80383d0 *)
mov r7 L0x200161c4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161d0; Value = 0x0014b881; PC = 0x80383d4 *)
mov r8 L0x200161d0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161dc; Value = 0xfffffffd; PC = 0x80383d8 *)
mov r9 L0x200161dc;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b10; PC = 0x8038480 *)
mov L0x20015b10 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b1c; PC = 0x8038484 *)
mov L0x20015b1c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161c4; PC = 0x8038488 *)
mov L0x200161c4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161d0; PC = 0x803848c *)
mov L0x200161d0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161dc; PC = 0x8038490 *)
mov L0x200161dc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b04; PC = 0x8038494 *)
mov L0x20015b04 r4;




(**************** CUT 180, - *****************)

ecut and [
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x20015b04*x**0*z**11 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x20015b10*x**0*z**11 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x20015b1c*x**0*z**11 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x200161c4*x**0*z**11 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x200161d0*x**0*z**11 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x200161dc*x**0*z**11 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015b28; Value = 0x00000003; PC = 0x803823c *)
mov r4 L0x20015b28;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b34; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x20015b34;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b40; Value = 0x00000003; PC = 0x8038244 *)
mov r6 L0x20015b40;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161e8; Value = 0xfffffffd; PC = 0x8038248 *)
mov r7 L0x200161e8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161f4; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x200161f4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016200; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x20016200;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b34; PC = 0x80382d8 *)
mov L0x20015b34 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b40; PC = 0x80382dc *)
mov L0x20015b40 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161e8; PC = 0x80382e0 *)
mov L0x200161e8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161f4; PC = 0x80382e4 *)
mov L0x200161f4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016200; PC = 0x80382e8 *)
mov L0x20016200 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b28; PC = 0x80382ec *)
mov L0x20015b28 r4;




(**************** CUT 181, - *****************)

ecut and [
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x20015b28*x**0*z**12 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x20015b34*x**0*z**12 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x20015b40*x**0*z**12 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x200161e8*x**0*z**12 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x200161f4*x**0*z**12 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x20016200*x**0*z**12 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015b4c; Value = 0x00000003; PC = 0x80382f0 *)
mov r4 L0x20015b4c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b58; Value = 0x00000000; PC = 0x80382f4 *)
mov r5 L0x20015b58;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b64; Value = 0x00297102; PC = 0x80382f8 *)
mov r6 L0x20015b64;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001620c; Value = 0x002970fc; PC = 0x80382fc *)
mov r7 L0x2001620c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016218; Value = 0xffeb477c; PC = 0x8038300 *)
mov r8 L0x20016218;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016224; Value = 0x00000000; PC = 0x8038304 *)
mov r9 L0x20016224;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b58; PC = 0x80383ac *)
mov L0x20015b58 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b64; PC = 0x80383b0 *)
mov L0x20015b64 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001620c; PC = 0x80383b4 *)
mov L0x2001620c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016218; PC = 0x80383b8 *)
mov L0x20016218 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016224; PC = 0x80383bc *)
mov L0x20016224 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b4c; PC = 0x80383c0 *)
mov L0x20015b4c r4;




(**************** CUT 182, - *****************)

ecut and [
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x20015b4c*x**0*z**12 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x20015b58*x**0*z**12 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x20015b64*x**0*z**12 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x2001620c*x**0*z**12 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x20016218*x**0*z**12 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x20016224*x**0*z**12 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015b70; Value = 0x00000003; PC = 0x80383c4 *)
mov r4 L0x20015b70;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b7c; Value = 0x00000000; PC = 0x80383c8 *)
mov r5 L0x20015b7c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b88; Value = 0xffd68f04; PC = 0x80383cc *)
mov r6 L0x20015b88;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016230; Value = 0xffd68efe; PC = 0x80383d0 *)
mov r7 L0x20016230;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001623c; Value = 0x0014b87b; PC = 0x80383d4 *)
mov r8 L0x2001623c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016248; Value = 0x00000000; PC = 0x80383d8 *)
mov r9 L0x20016248;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b7c; PC = 0x8038480 *)
mov L0x20015b7c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b88; PC = 0x8038484 *)
mov L0x20015b88 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016230; PC = 0x8038488 *)
mov L0x20016230 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001623c; PC = 0x803848c *)
mov L0x2001623c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016248; PC = 0x8038490 *)
mov L0x20016248 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b70; PC = 0x8038494 *)
mov L0x20015b70 r4;




(**************** CUT 183, - *****************)

ecut and [
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x20015b70*x**0*z**12 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x20015b7c*x**0*z**12 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x20015b88*x**0*z**12 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x20016230*x**0*z**12 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x2001623c*x**0*z**12 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x20016248*x**0*z**12 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015b94; Value = 0x00000003; PC = 0x803823c *)
mov r4 L0x20015b94;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015ba0; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x20015ba0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015bac; Value = 0x00000003; PC = 0x8038244 *)
mov r6 L0x20015bac;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016254; Value = 0x00000000; PC = 0x8038248 *)
mov r7 L0x20016254;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016260; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20016260;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001626c; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x2001626c;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015ba0; PC = 0x80382d8 *)
mov L0x20015ba0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015bac; PC = 0x80382dc *)
mov L0x20015bac r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016254; PC = 0x80382e0 *)
mov L0x20016254 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016260; PC = 0x80382e4 *)
mov L0x20016260 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001626c; PC = 0x80382e8 *)
mov L0x2001626c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b94; PC = 0x80382ec *)
mov L0x20015b94 r4;




(**************** CUT 184, - *****************)

ecut and [
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x20015b94*x**0*z**13 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x20015ba0*x**0*z**13 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x20015bac*x**0*z**13 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x20016254*x**0*z**13 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x20016260*x**0*z**13 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x2001626c*x**0*z**13 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015bb8; Value = 0x00000003; PC = 0x80382f0 *)
mov r4 L0x20015bb8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015bc4; Value = 0xffeb477f; PC = 0x80382f4 *)
mov r5 L0x20015bc4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015bd0; Value = 0xfff61641; PC = 0x80382f8 *)
mov r6 L0x20015bd0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016278; Value = 0x00000000; PC = 0x80382fc *)
mov r7 L0x20016278;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016284; Value = 0x00000000; PC = 0x8038300 *)
mov r8 L0x20016284;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016290; Value = 0xffeb4785; PC = 0x8038304 *)
mov r9 L0x20016290;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015bc4; PC = 0x80383ac *)
mov L0x20015bc4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015bd0; PC = 0x80383b0 *)
mov L0x20015bd0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016278; PC = 0x80383b4 *)
mov L0x20016278 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016284; PC = 0x80383b8 *)
mov L0x20016284 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016290; PC = 0x80383bc *)
mov L0x20016290 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015bb8; PC = 0x80383c0 *)
mov L0x20015bb8 r4;




(**************** CUT 185, - *****************)

ecut and [
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20015bb8*x**0*z**13 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20015bc4*x**0*z**13 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20015bd0*x**0*z**13 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20016278*x**0*z**13 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20016284*x**0*z**13 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20016290*x**0*z**13 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015bdc; Value = 0x00000003; PC = 0x80383c4 *)
mov r4 L0x20015bdc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015be8; Value = 0x0014b87e; PC = 0x80383c8 *)
mov r5 L0x20015be8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015bf4; Value = 0x0009e9c5; PC = 0x80383cc *)
mov r6 L0x20015bf4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001629c; Value = 0x00000000; PC = 0x80383d0 *)
mov r7 L0x2001629c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162a8; Value = 0x00000000; PC = 0x80383d4 *)
mov r8 L0x200162a8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162b4; Value = 0x0014b884; PC = 0x80383d8 *)
mov r9 L0x200162b4;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015be8; PC = 0x8038480 *)
mov L0x20015be8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015bf4; PC = 0x8038484 *)
mov L0x20015bf4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001629c; PC = 0x8038488 *)
mov L0x2001629c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162a8; PC = 0x803848c *)
mov L0x200162a8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162b4; PC = 0x8038490 *)
mov L0x200162b4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015bdc; PC = 0x8038494 *)
mov L0x20015bdc r4;




(**************** CUT 186, - *****************)

ecut and [
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x20015bdc*x**0*z**13 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x20015be8*x**0*z**13 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x20015bf4*x**0*z**13 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x2001629c*x**0*z**13 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x200162a8*x**0*z**13 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x200162b4*x**0*z**13 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015c00; Value = 0x00000006; PC = 0x803823c *)
mov r4 L0x20015c00;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c0c; Value = 0x00000006; PC = 0x8038240 *)
mov r5 L0x20015c0c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c18; Value = 0x00000000; PC = 0x8038244 *)
mov r6 L0x20015c18;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162c0; Value = 0x00000000; PC = 0x8038248 *)
mov r7 L0x200162c0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162cc; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x200162cc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162d8; Value = 0x00000003; PC = 0x8038250 *)
mov r9 L0x200162d8;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c0c; PC = 0x80382d8 *)
mov L0x20015c0c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c18; PC = 0x80382dc *)
mov L0x20015c18 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162c0; PC = 0x80382e0 *)
mov L0x200162c0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162cc; PC = 0x80382e4 *)
mov L0x200162cc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162d8; PC = 0x80382e8 *)
mov L0x200162d8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c00; PC = 0x80382ec *)
mov L0x20015c00 r4;




(**************** CUT 187, - *****************)

ecut and [
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x20015c00*x**0*z**14 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x20015c0c*x**0*z**14 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x20015c18*x**0*z**14 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x200162c0*x**0*z**14 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x200162cc*x**0*z**14 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x200162d8*x**0*z**14 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015c24; Value = 0x0014b881; PC = 0x80382f0 *)
mov r4 L0x20015c24;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c30; Value = 0xfffffffd; PC = 0x80382f4 *)
mov r5 L0x20015c30;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c3c; Value = 0xffeb477c; PC = 0x80382f8 *)
mov r6 L0x20015c3c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162e4; Value = 0x0014b884; PC = 0x80382fc *)
mov r7 L0x200162e4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162f0; Value = 0x0014b87b; PC = 0x8038300 *)
mov r8 L0x200162f0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162fc; Value = 0xffeb477f; PC = 0x8038304 *)
mov r9 L0x200162fc;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c30; PC = 0x80383ac *)
mov L0x20015c30 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c3c; PC = 0x80383b0 *)
mov L0x20015c3c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162e4; PC = 0x80383b4 *)
mov L0x200162e4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162f0; PC = 0x80383b8 *)
mov L0x200162f0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162fc; PC = 0x80383bc *)
mov L0x200162fc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c24; PC = 0x80383c0 *)
mov L0x20015c24 r4;




(**************** CUT 188, - *****************)

ecut and [
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x20015c24*x**0*z**14 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x20015c30*x**0*z**14 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x20015c3c*x**0*z**14 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x200162e4*x**0*z**14 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x200162f0*x**0*z**14 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x200162fc*x**0*z**14 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015c48; Value = 0xffeb4782; PC = 0x80383c4 *)
mov r4 L0x20015c48;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c54; Value = 0xfffffffd; PC = 0x80383c8 *)
mov r5 L0x20015c54;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c60; Value = 0x0014b87b; PC = 0x80383cc *)
mov r6 L0x20015c60;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016308; Value = 0xffeb4785; PC = 0x80383d0 *)
mov r7 L0x20016308;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016314; Value = 0xffeb477c; PC = 0x80383d4 *)
mov r8 L0x20016314;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016320; Value = 0x0014b87e; PC = 0x80383d8 *)
mov r9 L0x20016320;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c54; PC = 0x8038480 *)
mov L0x20015c54 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c60; PC = 0x8038484 *)
mov L0x20015c60 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016308; PC = 0x8038488 *)
mov L0x20016308 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016314; PC = 0x803848c *)
mov L0x20016314 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016320; PC = 0x8038490 *)
mov L0x20016320 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c48; PC = 0x8038494 *)
mov L0x20015c48 r4;




(**************** CUT 189, - *****************)

ecut and [
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20015c48*x**0*z**14 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20015c54*x**0*z**14 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20015c60*x**0*z**14 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20016308*x**0*z**14 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20016314*x**0*z**14 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20016320*x**0*z**14 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015c6c; Value = 0x00000006; PC = 0x803823c *)
mov r4 L0x20015c6c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c78; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x20015c78;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c84; Value = 0x00000000; PC = 0x8038244 *)
mov r6 L0x20015c84;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001632c; Value = 0x00000000; PC = 0x8038248 *)
mov r7 L0x2001632c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016338; Value = 0x00000003; PC = 0x803824c *)
mov r8 L0x20016338;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016344; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x20016344;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c78; PC = 0x80382d8 *)
mov L0x20015c78 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c84; PC = 0x80382dc *)
mov L0x20015c84 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001632c; PC = 0x80382e0 *)
mov L0x2001632c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016338; PC = 0x80382e4 *)
mov L0x20016338 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016344; PC = 0x80382e8 *)
mov L0x20016344 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c6c; PC = 0x80382ec *)
mov L0x20015c6c r4;




(**************** CUT 190, - *****************)

ecut and [
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x20015c6c*x**0*z**15 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x20015c78*x**0*z**15 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x20015c84*x**0*z**15 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x2001632c*x**0*z**15 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x20016338*x**0*z**15 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x20016344*x**0*z**15 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015c90; Value = 0xffeb4782; PC = 0x80382f0 *)
mov r4 L0x20015c90;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c9c; Value = 0xffd68f04; PC = 0x80382f4 *)
mov r5 L0x20015c9c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015ca8; Value = 0xffeb477c; PC = 0x80382f8 *)
mov r6 L0x20015ca8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016350; Value = 0xffeb477c; PC = 0x80382fc *)
mov r7 L0x20016350;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001635c; Value = 0x00000003; PC = 0x8038300 *)
mov r8 L0x2001635c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016368; Value = 0xffeb4782; PC = 0x8038304 *)
mov r9 L0x20016368;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c9c; PC = 0x80383ac *)
mov L0x20015c9c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015ca8; PC = 0x80383b0 *)
mov L0x20015ca8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016350; PC = 0x80383b4 *)
mov L0x20016350 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001635c; PC = 0x80383b8 *)
mov L0x2001635c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016368; PC = 0x80383bc *)
mov L0x20016368 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c90; PC = 0x80383c0 *)
mov L0x20015c90 r4;




(**************** CUT 191, - *****************)

ecut and [
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x20015c90*x**0*z**15 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x20015c9c*x**0*z**15 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x20015ca8*x**0*z**15 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x20016350*x**0*z**15 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x2001635c*x**0*z**15 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x20016368*x**0*z**15 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015cb4; Value = 0x0014b881; PC = 0x80383c4 *)
mov r4 L0x20015cb4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015cc0; Value = 0x00297102; PC = 0x80383c8 *)
mov r5 L0x20015cc0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015ccc; Value = 0x0014b87b; PC = 0x80383cc *)
mov r6 L0x20015ccc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016374; Value = 0x0014b87b; PC = 0x80383d0 *)
mov r7 L0x20016374;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016380; Value = 0x00000003; PC = 0x80383d4 *)
mov r8 L0x20016380;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001638c; Value = 0x0014b881; PC = 0x80383d8 *)
mov r9 L0x2001638c;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015cc0; PC = 0x8038480 *)
mov L0x20015cc0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015ccc; PC = 0x8038484 *)
mov L0x20015ccc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016374; PC = 0x8038488 *)
mov L0x20016374 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016380; PC = 0x803848c *)
mov L0x20016380 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001638c; PC = 0x8038490 *)
mov L0x2001638c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015cb4; PC = 0x8038494 *)
mov L0x20015cb4 r4;




(**************** CUT 192, - *****************)

ecut and [
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x20015cb4*x**0*z**15 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x20015cc0*x**0*z**15 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x20015ccc*x**0*z**15 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x20016374*x**0*z**15 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x20016380*x**0*z**15 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x2001638c*x**0*z**15 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* sub.w	r0, r0, #1728	; 0x6c0                     #! PC = 0x80384a4 *)
subs dc r0 r0 1728@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x80384a8 *)
adds dc r0 r0 4@uint32;
(* vmov	r12, s2                                    #! PC = 0x80384ac *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80384b0 *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8038234 <_3x2>                         #! PC = 0x80384b4 *)
#bne.w	0x8038234 <_3x2>                         #! 0x80384b4 = 0x80384b4;
(* add.w	lr, r0, #1728	; 0x6c0                     #! PC = 0x8038234 *)
adds dc lr r0 1728@uint32;
(* vmov	s3, lr                                     #! PC = 0x8038238 *)
mov s3 lr;
(* ldr.w	r4, [r0]                                  #! EA = L0x2001561c; Value = 0x00000003; PC = 0x803823c *)
mov r4 L0x2001561c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015628; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x20015628;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015634; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x20015634;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015cdc; Value = 0xfffffff7; PC = 0x8038248 *)
mov r7 L0x20015cdc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ce8; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20015ce8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015cf4; Value = 0x00000006; PC = 0x8038250 *)
mov r9 L0x20015cf4;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015628; PC = 0x80382d8 *)
mov L0x20015628 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015634; PC = 0x80382dc *)
mov L0x20015634 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015cdc; PC = 0x80382e0 *)
mov L0x20015cdc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ce8; PC = 0x80382e4 *)
mov L0x20015ce8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015cf4; PC = 0x80382e8 *)
mov L0x20015cf4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001561c; PC = 0x80382ec *)
mov L0x2001561c r4;




(**************** CUT 193, - *****************)

ecut and [
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x2001561c*x**1*z** 0 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x20015628*x**1*z** 0 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x20015634*x**1*z** 0 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x20015cdc*x**1*z** 0 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x20015ce8*x**1*z** 0 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x20015cf4*x**1*z** 0 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015640; Value = 0xffd68f04; PC = 0x80382f0 *)
mov r4 L0x20015640;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001564c; Value = 0x00000000; PC = 0x80382f4 *)
mov r5 L0x2001564c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015658; Value = 0xffeb4782; PC = 0x80382f8 *)
mov r6 L0x20015658;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d00; Value = 0x00000000; PC = 0x80382fc *)
mov r7 L0x20015d00;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d0c; Value = 0xffeb477c; PC = 0x8038300 *)
mov r8 L0x20015d0c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d18; Value = 0x0014b881; PC = 0x8038304 *)
mov r9 L0x20015d18;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001564c; PC = 0x80383ac *)
mov L0x2001564c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015658; PC = 0x80383b0 *)
mov L0x20015658 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d00; PC = 0x80383b4 *)
mov L0x20015d00 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d0c; PC = 0x80383b8 *)
mov L0x20015d0c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d18; PC = 0x80383bc *)
mov L0x20015d18 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015640; PC = 0x80383c0 *)
mov L0x20015640 r4;




(**************** CUT 194, - *****************)

ecut and [
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x20015640*x**1*z** 0 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x2001564c*x**1*z** 0 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x20015658*x**1*z** 0 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x20015d00*x**1*z** 0 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x20015d0c*x**1*z** 0 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x20015d18*x**1*z** 0 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015664; Value = 0x00297102; PC = 0x80383c4 *)
mov r4 L0x20015664;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015670; Value = 0x00000000; PC = 0x80383c8 *)
mov r5 L0x20015670;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001567c; Value = 0x0014b881; PC = 0x80383cc *)
mov r6 L0x2001567c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d24; Value = 0x00000000; PC = 0x80383d0 *)
mov r7 L0x20015d24;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d30; Value = 0x0014b87b; PC = 0x80383d4 *)
mov r8 L0x20015d30;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d3c; Value = 0xffeb4782; PC = 0x80383d8 *)
mov r9 L0x20015d3c;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015670; PC = 0x8038480 *)
mov L0x20015670 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001567c; PC = 0x8038484 *)
mov L0x2001567c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d24; PC = 0x8038488 *)
mov L0x20015d24 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d30; PC = 0x803848c *)
mov L0x20015d30 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d3c; PC = 0x8038490 *)
mov L0x20015d3c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015664; PC = 0x8038494 *)
mov L0x20015664 r4;




(**************** CUT 195, - *****************)

ecut and [
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x20015664*x**1*z** 0 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x20015670*x**1*z** 0 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x2001567c*x**1*z** 0 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x20015d24*x**1*z** 0 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x20015d30*x**1*z** 0 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x20015d3c*x**1*z** 0 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015688; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x20015688;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015694; Value = 0xfffffffd; PC = 0x8038240 *)
mov r5 L0x20015694;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200156a0; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x200156a0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d48; Value = 0xfffffffa; PC = 0x8038248 *)
mov r7 L0x20015d48;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d54; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20015d54;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d60; Value = 0x00000009; PC = 0x8038250 *)
mov r9 L0x20015d60;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015694; PC = 0x80382d8 *)
mov L0x20015694 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200156a0; PC = 0x80382dc *)
mov L0x200156a0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d48; PC = 0x80382e0 *)
mov L0x20015d48 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d54; PC = 0x80382e4 *)
mov L0x20015d54 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d60; PC = 0x80382e8 *)
mov L0x20015d60 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015688; PC = 0x80382ec *)
mov L0x20015688 r4;




(**************** CUT 196, - *****************)

ecut and [
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x20015688*x**1*z** 1 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x20015694*x**1*z** 1 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x200156a0*x**1*z** 1 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x20015d48*x**1*z** 1 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x20015d54*x**1*z** 1 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x20015d60*x**1*z** 1 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200156ac; Value = 0xffeb4782; PC = 0x80382f0 *)
mov r4 L0x200156ac;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200156b8; Value = 0xffd68efe; PC = 0x80382f4 *)
mov r5 L0x200156b8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200156c4; Value = 0xfffffffd; PC = 0x80382f8 *)
mov r6 L0x200156c4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d6c; Value = 0x0014b87e; PC = 0x80382fc *)
mov r7 L0x20015d6c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d78; Value = 0x0014b884; PC = 0x8038300 *)
mov r8 L0x20015d78;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d84; Value = 0x00000000; PC = 0x8038304 *)
mov r9 L0x20015d84;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200156b8; PC = 0x80383ac *)
mov L0x200156b8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200156c4; PC = 0x80383b0 *)
mov L0x200156c4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d6c; PC = 0x80383b4 *)
mov L0x20015d6c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d78; PC = 0x80383b8 *)
mov L0x20015d78 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d84; PC = 0x80383bc *)
mov L0x20015d84 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200156ac; PC = 0x80383c0 *)
mov L0x200156ac r4;




(**************** CUT 197, - *****************)

ecut and [
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x200156ac*x**1*z** 1 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x200156b8*x**1*z** 1 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x200156c4*x**1*z** 1 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x20015d6c*x**1*z** 1 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x20015d78*x**1*z** 1 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x20015d84*x**1*z** 1 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200156d0; Value = 0x0014b881; PC = 0x80383c4 *)
mov r4 L0x200156d0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200156dc; Value = 0x002970fc; PC = 0x80383c8 *)
mov r5 L0x200156dc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200156e8; Value = 0xfffffffd; PC = 0x80383cc *)
mov r6 L0x200156e8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d90; Value = 0xffeb477f; PC = 0x80383d0 *)
mov r7 L0x20015d90;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d9c; Value = 0xffeb4785; PC = 0x80383d4 *)
mov r8 L0x20015d9c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015da8; Value = 0x00000000; PC = 0x80383d8 *)
mov r9 L0x20015da8;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200156dc; PC = 0x8038480 *)
mov L0x200156dc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200156e8; PC = 0x8038484 *)
mov L0x200156e8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d90; PC = 0x8038488 *)
mov L0x20015d90 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d9c; PC = 0x803848c *)
mov L0x20015d9c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015da8; PC = 0x8038490 *)
mov L0x20015da8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200156d0; PC = 0x8038494 *)
mov L0x200156d0 r4;




(**************** CUT 198, - *****************)

ecut and [
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x200156d0*x**1*z** 1 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x200156dc*x**1*z** 1 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x200156e8*x**1*z** 1 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x20015d90*x**1*z** 1 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x20015d9c*x**1*z** 1 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x20015da8*x**1*z** 1 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200156f4; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x200156f4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015700; Value = 0x00000006; PC = 0x8038240 *)
mov r5 L0x20015700;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001570c; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x2001570c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015db4; Value = 0x00000006; PC = 0x8038248 *)
mov r7 L0x20015db4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015dc0; Value = 0xfffffffd; PC = 0x803824c *)
mov r8 L0x20015dc0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015dcc; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x20015dcc;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015700; PC = 0x80382d8 *)
mov L0x20015700 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001570c; PC = 0x80382dc *)
mov L0x2001570c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015db4; PC = 0x80382e0 *)
mov L0x20015db4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015dc0; PC = 0x80382e4 *)
mov L0x20015dc0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015dcc; PC = 0x80382e8 *)
mov L0x20015dcc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200156f4; PC = 0x80382ec *)
mov L0x200156f4 r4;




(**************** CUT 199, - *****************)

ecut and [
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x200156f4*x**1*z** 2 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x20015700*x**1*z** 2 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x2001570c*x**1*z** 2 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x20015db4*x**1*z** 2 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x20015dc0*x**1*z** 2 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x20015dcc*x**1*z** 2 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015718; Value = 0x00000000; PC = 0x80382f0 *)
mov r4 L0x20015718;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015724; Value = 0xffeb4782; PC = 0x80382f4 *)
mov r5 L0x20015724;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015730; Value = 0xfffffffd; PC = 0x80382f8 *)
mov r6 L0x20015730;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015dd8; Value = 0x0014b881; PC = 0x80382fc *)
mov r7 L0x20015dd8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015de4; Value = 0x0014b881; PC = 0x8038300 *)
mov r8 L0x20015de4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015df0; Value = 0xffeb477c; PC = 0x8038304 *)
mov r9 L0x20015df0;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015724; PC = 0x80383ac *)
mov L0x20015724 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015730; PC = 0x80383b0 *)
mov L0x20015730 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015dd8; PC = 0x80383b4 *)
mov L0x20015dd8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015de4; PC = 0x80383b8 *)
mov L0x20015de4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015df0; PC = 0x80383bc *)
mov L0x20015df0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015718; PC = 0x80383c0 *)
mov L0x20015718 r4;




(**************** CUT 200, - *****************)

ecut and [
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015718*x**1*z** 2 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015724*x**1*z** 2 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015730*x**1*z** 2 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015dd8*x**1*z** 2 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015de4*x**1*z** 2 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015df0*x**1*z** 2 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x2001573c; Value = 0x00000000; PC = 0x80383c4 *)
mov r4 L0x2001573c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015748; Value = 0x0014b881; PC = 0x80383c8 *)
mov r5 L0x20015748;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015754; Value = 0xfffffffd; PC = 0x80383cc *)
mov r6 L0x20015754;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015dfc; Value = 0xffeb4782; PC = 0x80383d0 *)
mov r7 L0x20015dfc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e08; Value = 0xffeb4782; PC = 0x80383d4 *)
mov r8 L0x20015e08;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e14; Value = 0x0014b87b; PC = 0x80383d8 *)
mov r9 L0x20015e14;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015748; PC = 0x8038480 *)
mov L0x20015748 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015754; PC = 0x8038484 *)
mov L0x20015754 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015dfc; PC = 0x8038488 *)
mov L0x20015dfc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e08; PC = 0x803848c *)
mov L0x20015e08 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e14; PC = 0x8038490 *)
mov L0x20015e14 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001573c; PC = 0x8038494 *)
mov L0x2001573c r4;




(**************** CUT 201, - *****************)

ecut and [
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x2001573c*x**1*z** 2 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x20015748*x**1*z** 2 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x20015754*x**1*z** 2 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x20015dfc*x**1*z** 2 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x20015e08*x**1*z** 2 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x20015e14*x**1*z** 2 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015760; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x20015760;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001576c; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x2001576c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015778; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x20015778;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e20; Value = 0x00000006; PC = 0x8038248 *)
mov r7 L0x20015e20;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e2c; Value = 0x00000006; PC = 0x803824c *)
mov r8 L0x20015e2c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e38; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x20015e38;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001576c; PC = 0x80382d8 *)
mov L0x2001576c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015778; PC = 0x80382dc *)
mov L0x20015778 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e20; PC = 0x80382e0 *)
mov L0x20015e20 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e2c; PC = 0x80382e4 *)
mov L0x20015e2c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e38; PC = 0x80382e8 *)
mov L0x20015e38 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015760; PC = 0x80382ec *)
mov L0x20015760 r4;




(**************** CUT 202, - *****************)

ecut and [
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x20015760*x**1*z** 3 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x2001576c*x**1*z** 3 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x20015778*x**1*z** 3 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x20015e20*x**1*z** 3 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x20015e2c*x**1*z** 3 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x20015e38*x**1*z** 3 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015784; Value = 0xfffffffd; PC = 0x80382f0 *)
mov r4 L0x20015784;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015790; Value = 0x0009e9c5; PC = 0x80382f4 *)
mov r5 L0x20015790;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001579c; Value = 0x0014b881; PC = 0x80382f8 *)
mov r6 L0x2001579c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e44; Value = 0x0014b881; PC = 0x80382fc *)
mov r7 L0x20015e44;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e50; Value = 0xffeb4782; PC = 0x8038300 *)
mov r8 L0x20015e50;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e5c; Value = 0x0014b881; PC = 0x8038304 *)
mov r9 L0x20015e5c;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015790; PC = 0x80383ac *)
mov L0x20015790 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001579c; PC = 0x80383b0 *)
mov L0x2001579c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e44; PC = 0x80383b4 *)
mov L0x20015e44 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e50; PC = 0x80383b8 *)
mov L0x20015e50 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e5c; PC = 0x80383bc *)
mov L0x20015e5c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015784; PC = 0x80383c0 *)
mov L0x20015784 r4;




(**************** CUT 203, - *****************)

ecut and [
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x20015784*x**1*z** 3 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x20015790*x**1*z** 3 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x2001579c*x**1*z** 3 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x20015e44*x**1*z** 3 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x20015e50*x**1*z** 3 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x20015e5c*x**1*z** 3 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200157a8; Value = 0xfffffffd; PC = 0x80383c4 *)
mov r4 L0x200157a8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200157b4; Value = 0xfff61641; PC = 0x80383c8 *)
mov r5 L0x200157b4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200157c0; Value = 0xffeb4782; PC = 0x80383cc *)
mov r6 L0x200157c0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e68; Value = 0xffeb4782; PC = 0x80383d0 *)
mov r7 L0x20015e68;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e74; Value = 0x0014b881; PC = 0x80383d4 *)
mov r8 L0x20015e74;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e80; Value = 0xffeb4782; PC = 0x80383d8 *)
mov r9 L0x20015e80;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200157b4; PC = 0x8038480 *)
mov L0x200157b4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200157c0; PC = 0x8038484 *)
mov L0x200157c0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e68; PC = 0x8038488 *)
mov L0x20015e68 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e74; PC = 0x803848c *)
mov L0x20015e74 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e80; PC = 0x8038490 *)
mov L0x20015e80 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200157a8; PC = 0x8038494 *)
mov L0x200157a8 r4;



(**************** CUT 204, - *****************)

ecut and [
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x200157a8*x**1*z** 3 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x200157b4*x**1*z** 3 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x200157c0*x**1*z** 3 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x20015e68*x**1*z** 3 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x20015e74*x**1*z** 3 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x20015e80*x**1*z** 3 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200157cc; Value = 0x00000003; PC = 0x803823c *)
mov r4 L0x200157cc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200157d8; Value = 0x00001000; PC = 0x8038240 *)
mov r5 L0x200157d8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200157e4; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x200157e4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e8c; Value = 0x00000000; PC = 0x8038248 *)
mov r7 L0x20015e8c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e98; Value = 0x00000003; PC = 0x803824c *)
mov r8 L0x20015e98;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ea4; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x20015ea4;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200157d8; PC = 0x80382d8 *)
mov L0x200157d8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200157e4; PC = 0x80382dc *)
mov L0x200157e4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e8c; PC = 0x80382e0 *)
mov L0x20015e8c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e98; PC = 0x80382e4 *)
mov L0x20015e98 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ea4; PC = 0x80382e8 *)
mov L0x20015ea4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200157cc; PC = 0x80382ec *)
mov L0x200157cc r4;




(**************** CUT 205, - *****************)

ecut and [
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x200157cc*x**1*z** 4 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x200157d8*x**1*z** 4 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x200157e4*x**1*z** 4 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x20015e8c*x**1*z** 4 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x20015e98*x**1*z** 4 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x20015ea4*x**1*z** 4 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200157f0; Value = 0x0014b87e; PC = 0x80382f0 *)
mov r4 L0x200157f0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200157fc; Value = 0xfffbbba5; PC = 0x80382f4 *)
mov r5 L0x200157fc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015808; Value = 0xffeb4782; PC = 0x80382f8 *)
mov r6 L0x20015808;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015eb0; Value = 0x0014b87b; PC = 0x80382fc *)
mov r7 L0x20015eb0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ebc; Value = 0xffeb477f; PC = 0x8038300 *)
mov r8 L0x20015ebc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ec8; Value = 0xffd68efe; PC = 0x8038304 *)
mov r9 L0x20015ec8;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200157fc; PC = 0x80383ac *)
mov L0x200157fc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015808; PC = 0x80383b0 *)
mov L0x20015808 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015eb0; PC = 0x80383b4 *)
mov L0x20015eb0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ebc; PC = 0x80383b8 *)
mov L0x20015ebc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ec8; PC = 0x80383bc *)
mov L0x20015ec8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200157f0; PC = 0x80383c0 *)
mov L0x200157f0 r4;




(**************** CUT 206, - *****************)

ecut and [
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x200157f0*x**1*z** 4 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x200157fc*x**1*z** 4 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x20015808*x**1*z** 4 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x20015eb0*x**1*z** 4 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x20015ebc*x**1*z** 4 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x20015ec8*x**1*z** 4 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015814; Value = 0xffeb477f; PC = 0x80383c4 *)
mov r4 L0x20015814;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015820; Value = 0x0004345b; PC = 0x80383c8 *)
mov r5 L0x20015820;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001582c; Value = 0x0014b881; PC = 0x80383cc *)
mov r6 L0x2001582c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ed4; Value = 0xffeb477c; PC = 0x80383d0 *)
mov r7 L0x20015ed4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ee0; Value = 0x0014b87e; PC = 0x80383d4 *)
mov r8 L0x20015ee0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015eec; Value = 0x002970fc; PC = 0x80383d8 *)
mov r9 L0x20015eec;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015820; PC = 0x8038480 *)
mov L0x20015820 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001582c; PC = 0x8038484 *)
mov L0x2001582c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ed4; PC = 0x8038488 *)
mov L0x20015ed4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ee0; PC = 0x803848c *)
mov L0x20015ee0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015eec; PC = 0x8038490 *)
mov L0x20015eec r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015814; PC = 0x8038494 *)
mov L0x20015814 r4;




(**************** CUT 207, - *****************)

ecut and [
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x20015814*x**1*z** 4 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x20015820*x**1*z** 4 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x2001582c*x**1*z** 4 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x20015ed4*x**1*z** 4 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x20015ee0*x**1*z** 4 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x20015eec*x**1*z** 4 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015838; Value = 0xfffffff7; PC = 0x803823c *)
mov r4 L0x20015838;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015844; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x20015844;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015850; Value = 0x00000006; PC = 0x8038244 *)
mov r6 L0x20015850;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ef8; Value = 0xfffffff7; PC = 0x8038248 *)
mov r7 L0x20015ef8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f04; Value = 0x00000006; PC = 0x803824c *)
mov r8 L0x20015f04;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f10; Value = 0x00000006; PC = 0x8038250 *)
mov r9 L0x20015f10;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015844; PC = 0x80382d8 *)
mov L0x20015844 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015850; PC = 0x80382dc *)
mov L0x20015850 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ef8; PC = 0x80382e0 *)
mov L0x20015ef8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f04; PC = 0x80382e4 *)
mov L0x20015f04 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f10; PC = 0x80382e8 *)
mov L0x20015f10 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015838; PC = 0x80382ec *)
mov L0x20015838 r4;




(**************** CUT 208, - *****************)

ecut and [
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015838*x**1*z** 5 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015844*x**1*z** 5 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015850*x**1*z** 5 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015ef8*x**1*z** 5 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015f04*x**1*z** 5 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015f10*x**1*z** 5 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x2001585c; Value = 0x00000000; PC = 0x80382f0 *)
mov r4 L0x2001585c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015868; Value = 0xffeb4785; PC = 0x80382f4 *)
mov r5 L0x20015868;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015874; Value = 0xfffffffd; PC = 0x80382f8 *)
mov r6 L0x20015874;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f1c; Value = 0x00000000; PC = 0x80382fc *)
mov r7 L0x20015f1c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f28; Value = 0x0014b881; PC = 0x8038300 *)
mov r8 L0x20015f28;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f34; Value = 0xfffffffd; PC = 0x8038304 *)
mov r9 L0x20015f34;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015868; PC = 0x80383ac *)
mov L0x20015868 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015874; PC = 0x80383b0 *)
mov L0x20015874 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f1c; PC = 0x80383b4 *)
mov L0x20015f1c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f28; PC = 0x80383b8 *)
mov L0x20015f28 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f34; PC = 0x80383bc *)
mov L0x20015f34 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001585c; PC = 0x80383c0 *)
mov L0x2001585c r4;




(**************** CUT 209, - *****************)

ecut and [
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x2001585c*x**1*z** 5 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x20015868*x**1*z** 5 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x20015874*x**1*z** 5 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x20015f1c*x**1*z** 5 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x20015f28*x**1*z** 5 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x20015f34*x**1*z** 5 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015880; Value = 0x00000000; PC = 0x80383c4 *)
mov r4 L0x20015880;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001588c; Value = 0x0014b884; PC = 0x80383c8 *)
mov r5 L0x2001588c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015898; Value = 0xfffffffd; PC = 0x80383cc *)
mov r6 L0x20015898;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f40; Value = 0x00000000; PC = 0x80383d0 *)
mov r7 L0x20015f40;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f4c; Value = 0xffeb4782; PC = 0x80383d4 *)
mov r8 L0x20015f4c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f58; Value = 0xfffffffd; PC = 0x80383d8 *)
mov r9 L0x20015f58;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001588c; PC = 0x8038480 *)
mov L0x2001588c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015898; PC = 0x8038484 *)
mov L0x20015898 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f40; PC = 0x8038488 *)
mov L0x20015f40 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f4c; PC = 0x803848c *)
mov L0x20015f4c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f58; PC = 0x8038490 *)
mov L0x20015f58 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015880; PC = 0x8038494 *)
mov L0x20015880 r4;




(**************** CUT 210, - *****************)

ecut and [
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x20015880*x**1*z** 5 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x2001588c*x**1*z** 5 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x20015898*x**1*z** 5 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x20015f40*x**1*z** 5 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x20015f4c*x**1*z** 5 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x20015f58*x**1*z** 5 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200158a4; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x200158a4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200158b0; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x200158b0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200158bc; Value = 0x00000006; PC = 0x8038244 *)
mov r6 L0x200158bc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f64; Value = 0x00000006; PC = 0x8038248 *)
mov r7 L0x20015f64;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f70; Value = 0x00000ffa; PC = 0x803824c *)
mov r8 L0x20015f70;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f7c; Value = 0x00000003; PC = 0x8038250 *)
mov r9 L0x20015f7c;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200158b0; PC = 0x80382d8 *)
mov L0x200158b0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200158bc; PC = 0x80382dc *)
mov L0x200158bc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f64; PC = 0x80382e0 *)
mov L0x20015f64 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f70; PC = 0x80382e4 *)
mov L0x20015f70 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f7c; PC = 0x80382e8 *)
mov L0x20015f7c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200158a4; PC = 0x80382ec *)
mov L0x200158a4 r4;




(**************** CUT 211, - *****************)

ecut and [
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x200158a4*x**1*z** 6 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x200158b0*x**1*z** 6 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x200158bc*x**1*z** 6 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x20015f64*x**1*z** 6 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x20015f70*x**1*z** 6 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x20015f7c*x**1*z** 6 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200158c8; Value = 0xffeb4785; PC = 0x80382f0 *)
mov r4 L0x200158c8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200158d4; Value = 0x002970ff; PC = 0x80382f4 *)
mov r5 L0x200158d4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200158e0; Value = 0x0014b881; PC = 0x80382f8 *)
mov r6 L0x200158e0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f88; Value = 0xffeb4782; PC = 0x80382fc *)
mov r7 L0x20015f88;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f94; Value = 0x0005a56a; PC = 0x8038300 *)
mov r8 L0x20015f94;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fa0; Value = 0x0014b87e; PC = 0x8038304 *)
mov r9 L0x20015fa0;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200158d4; PC = 0x80383ac *)
mov L0x200158d4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200158e0; PC = 0x80383b0 *)
mov L0x200158e0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f88; PC = 0x80383b4 *)
mov L0x20015f88 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f94; PC = 0x80383b8 *)
mov L0x20015f94 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fa0; PC = 0x80383bc *)
mov L0x20015fa0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200158c8; PC = 0x80383c0 *)
mov L0x200158c8 r4;




(**************** CUT 212, - *****************)

ecut and [
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x200158c8*x**1*z** 6 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x200158d4*x**1*z** 6 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x200158e0*x**1*z** 6 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x20015f88*x**1*z** 6 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x20015f94*x**1*z** 6 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x20015fa0*x**1*z** 6 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200158ec; Value = 0x0014b884; PC = 0x80383c4 *)
mov r4 L0x200158ec;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200158f8; Value = 0xffd68f01; PC = 0x80383c8 *)
mov r5 L0x200158f8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015904; Value = 0xffeb4782; PC = 0x80383cc *)
mov r6 L0x20015904;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fac; Value = 0x0014b881; PC = 0x80383d0 *)
mov r7 L0x20015fac;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fb8; Value = 0xfffa4a9c; PC = 0x80383d4 *)
mov r8 L0x20015fb8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fc4; Value = 0xffeb477f; PC = 0x80383d8 *)
mov r9 L0x20015fc4;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200158f8; PC = 0x8038480 *)
mov L0x200158f8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015904; PC = 0x8038484 *)
mov L0x20015904 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fac; PC = 0x8038488 *)
mov L0x20015fac r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fb8; PC = 0x803848c *)
mov L0x20015fb8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fc4; PC = 0x8038490 *)
mov L0x20015fc4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200158ec; PC = 0x8038494 *)
mov L0x200158ec r4;




(**************** CUT 213, - *****************)

ecut and [
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x200158ec*x**1*z** 6 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x200158f8*x**1*z** 6 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x20015904*x**1*z** 6 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x20015fac*x**1*z** 6 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x20015fb8*x**1*z** 6 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x20015fc4*x**1*z** 6 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015910; Value = 0xfffffff7; PC = 0x803823c *)
mov r4 L0x20015910;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001591c; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x2001591c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015928; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x20015928;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fd0; Value = 0xfffffffd; PC = 0x8038248 *)
mov r7 L0x20015fd0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fdc; Value = 0x00000003; PC = 0x803824c *)
mov r8 L0x20015fdc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fe8; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x20015fe8;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001591c; PC = 0x80382d8 *)
mov L0x2001591c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015928; PC = 0x80382dc *)
mov L0x20015928 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fd0; PC = 0x80382e0 *)
mov L0x20015fd0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fdc; PC = 0x80382e4 *)
mov L0x20015fdc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fe8; PC = 0x80382e8 *)
mov L0x20015fe8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015910; PC = 0x80382ec *)
mov L0x20015910 r4;




(**************** CUT 214, - *****************)

ecut and [
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x20015910*x**1*z** 7 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x2001591c*x**1*z** 7 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x20015928*x**1*z** 7 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x20015fd0*x**1*z** 7 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x20015fdc*x**1*z** 7 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x20015fe8*x**1*z** 7 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015934; Value = 0x00000000; PC = 0x80382f0 *)
mov r4 L0x20015934;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015940; Value = 0xffd68f01; PC = 0x80382f4 *)
mov r5 L0x20015940;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001594c; Value = 0xffeb4782; PC = 0x80382f8 *)
mov r6 L0x2001594c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ff4; Value = 0xffeb4782; PC = 0x80382fc *)
mov r7 L0x20015ff4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016000; Value = 0x0014b87e; PC = 0x8038300 *)
mov r8 L0x20016000;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001600c; Value = 0xffeb4782; PC = 0x8038304 *)
mov r9 L0x2001600c;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015940; PC = 0x80383ac *)
mov L0x20015940 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001594c; PC = 0x80383b0 *)
mov L0x2001594c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ff4; PC = 0x80383b4 *)
mov L0x20015ff4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016000; PC = 0x80383b8 *)
mov L0x20016000 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001600c; PC = 0x80383bc *)
mov L0x2001600c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015934; PC = 0x80383c0 *)
mov L0x20015934 r4;




(**************** CUT 215, - *****************)

ecut and [
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x20015934*x**1*z** 7 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x20015940*x**1*z** 7 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x2001594c*x**1*z** 7 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x20015ff4*x**1*z** 7 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x20016000*x**1*z** 7 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x2001600c*x**1*z** 7 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015958; Value = 0x00000000; PC = 0x80383c4 *)
mov r4 L0x20015958;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015964; Value = 0x002970ff; PC = 0x80383c8 *)
mov r5 L0x20015964;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015970; Value = 0x0014b881; PC = 0x80383cc *)
mov r6 L0x20015970;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016018; Value = 0x0014b881; PC = 0x80383d0 *)
mov r7 L0x20016018;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016024; Value = 0xffeb477f; PC = 0x80383d4 *)
mov r8 L0x20016024;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016030; Value = 0x0014b881; PC = 0x80383d8 *)
mov r9 L0x20016030;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015964; PC = 0x8038480 *)
mov L0x20015964 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015970; PC = 0x8038484 *)
mov L0x20015970 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016018; PC = 0x8038488 *)
mov L0x20016018 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016024; PC = 0x803848c *)
mov L0x20016024 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016030; PC = 0x8038490 *)
mov L0x20016030 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015958; PC = 0x8038494 *)
mov L0x20015958 r4;




(**************** CUT 216, - *****************)

ecut and [
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20015958*x**1*z** 7 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20015964*x**1*z** 7 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20015970*x**1*z** 7 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20016018*x**1*z** 7 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20016024*x**1*z** 7 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20016030*x**1*z** 7 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x2001597c; Value = 0x00000003; PC = 0x803823c *)
mov r4 L0x2001597c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015988; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x20015988;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015994; Value = 0x00000000; PC = 0x8038244 *)
mov r6 L0x20015994;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001603c; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x2001603c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016048; Value = 0x00000003; PC = 0x803824c *)
mov r8 L0x20016048;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016054; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x20016054;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015988; PC = 0x80382d8 *)
mov L0x20015988 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015994; PC = 0x80382dc *)
mov L0x20015994 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001603c; PC = 0x80382e0 *)
mov L0x2001603c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016048; PC = 0x80382e4 *)
mov L0x20016048 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016054; PC = 0x80382e8 *)
mov L0x20016054 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001597c; PC = 0x80382ec *)
mov L0x2001597c r4;




(**************** CUT 217, - *****************)

ecut and [
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x2001597c*x**1*z** 8 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x20015988*x**1*z** 8 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x20015994*x**1*z** 8 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x2001603c*x**1*z** 8 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x20016048*x**1*z** 8 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x20016054*x**1*z** 8 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200159a0; Value = 0xfffffffa; PC = 0x80382f0 *)
mov r4 L0x200159a0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200159ac; Value = 0x00000000; PC = 0x80382f4 *)
mov r5 L0x200159ac;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200159b8; Value = 0x00000000; PC = 0x80382f8 *)
mov r6 L0x200159b8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016060; Value = 0xffeb477f; PC = 0x80382fc *)
mov r7 L0x20016060;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001606c; Value = 0xffeb477f; PC = 0x8038300 *)
mov r8 L0x2001606c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016078; Value = 0x002970fc; PC = 0x8038304 *)
mov r9 L0x20016078;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200159ac; PC = 0x80383ac *)
mov L0x200159ac r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200159b8; PC = 0x80383b0 *)
mov L0x200159b8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016060; PC = 0x80383b4 *)
mov L0x20016060 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001606c; PC = 0x80383b8 *)
mov L0x2001606c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016078; PC = 0x80383bc *)
mov L0x20016078 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200159a0; PC = 0x80383c0 *)
mov L0x200159a0 r4;




(**************** CUT 218, - *****************)

ecut and [
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x200159a0*x**1*z** 8 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x200159ac*x**1*z** 8 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x200159b8*x**1*z** 8 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x20016060*x**1*z** 8 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x2001606c*x**1*z** 8 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x20016078*x**1*z** 8 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200159c4; Value = 0xfffffffa; PC = 0x80383c4 *)
mov r4 L0x200159c4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200159d0; Value = 0x00000000; PC = 0x80383c8 *)
mov r5 L0x200159d0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200159dc; Value = 0x00000000; PC = 0x80383cc *)
mov r6 L0x200159dc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016084; Value = 0x0014b87e; PC = 0x80383d0 *)
mov r7 L0x20016084;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016090; Value = 0x0014b87e; PC = 0x80383d4 *)
mov r8 L0x20016090;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001609c; Value = 0xffd68efe; PC = 0x80383d8 *)
mov r9 L0x2001609c;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200159d0; PC = 0x8038480 *)
mov L0x200159d0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200159dc; PC = 0x8038484 *)
mov L0x200159dc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016084; PC = 0x8038488 *)
mov L0x20016084 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016090; PC = 0x803848c *)
mov L0x20016090 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001609c; PC = 0x8038490 *)
mov L0x2001609c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200159c4; PC = 0x8038494 *)
mov L0x200159c4 r4;




(**************** CUT 219, - *****************)

ecut and [
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x200159c4*x**1*z** 8 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x200159d0*x**1*z** 8 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x200159dc*x**1*z** 8 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x20016084*x**1*z** 8 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x20016090*x**1*z** 8 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x2001609c*x**1*z** 8 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200159e8; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x200159e8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200159f4; Value = 0xfffffffa; PC = 0x8038240 *)
mov r5 L0x200159f4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a00; Value = 0x00000006; PC = 0x8038244 *)
mov r6 L0x20015a00;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160a8; Value = 0x00000006; PC = 0x8038248 *)
mov r7 L0x200160a8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160b4; Value = 0x00000003; PC = 0x803824c *)
mov r8 L0x200160b4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160c0; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x200160c0;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200159f4; PC = 0x80382d8 *)
mov L0x200159f4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a00; PC = 0x80382dc *)
mov L0x20015a00 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160a8; PC = 0x80382e0 *)
mov L0x200160a8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160b4; PC = 0x80382e4 *)
mov L0x200160b4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160c0; PC = 0x80382e8 *)
mov L0x200160c0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200159e8; PC = 0x80382ec *)
mov L0x200159e8 r4;




(**************** CUT 220, - *****************)

ecut and [
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x200159e8*x**1*z** 9 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x200159f4*x**1*z** 9 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x20015a00*x**1*z** 9 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x200160a8*x**1*z** 9 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x200160b4*x**1*z** 9 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x200160c0*x**1*z** 9 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015a0c; Value = 0x00000006; PC = 0x80382f0 *)
mov r4 L0x20015a0c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a18; Value = 0xffeb477f; PC = 0x80382f4 *)
mov r5 L0x20015a18;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a24; Value = 0xffeb4782; PC = 0x80382f8 *)
mov r6 L0x20015a24;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160cc; Value = 0xffeb4782; PC = 0x80382fc *)
mov r7 L0x200160cc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160d8; Value = 0x00297102; PC = 0x8038300 *)
mov r8 L0x200160d8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160e4; Value = 0xffd68f01; PC = 0x8038304 *)
mov r9 L0x200160e4;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a18; PC = 0x80383ac *)
mov L0x20015a18 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a24; PC = 0x80383b0 *)
mov L0x20015a24 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160cc; PC = 0x80383b4 *)
mov L0x200160cc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160d8; PC = 0x80383b8 *)
mov L0x200160d8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160e4; PC = 0x80383bc *)
mov L0x200160e4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a0c; PC = 0x80383c0 *)
mov L0x20015a0c r4;




(**************** CUT 221, - *****************)

ecut and [
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x20015a0c*x**1*z** 9 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x20015a18*x**1*z** 9 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x20015a24*x**1*z** 9 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x200160cc*x**1*z** 9 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x200160d8*x**1*z** 9 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x200160e4*x**1*z** 9 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015a30; Value = 0x00000006; PC = 0x80383c4 *)
mov r4 L0x20015a30;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a3c; Value = 0x0014b87e; PC = 0x80383c8 *)
mov r5 L0x20015a3c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a48; Value = 0x0014b881; PC = 0x80383cc *)
mov r6 L0x20015a48;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160f0; Value = 0x0014b881; PC = 0x80383d0 *)
mov r7 L0x200160f0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160fc; Value = 0xffd68f04; PC = 0x80383d4 *)
mov r8 L0x200160fc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016108; Value = 0x002970ff; PC = 0x80383d8 *)
mov r9 L0x20016108;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a3c; PC = 0x8038480 *)
mov L0x20015a3c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a48; PC = 0x8038484 *)
mov L0x20015a48 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160f0; PC = 0x8038488 *)
mov L0x200160f0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160fc; PC = 0x803848c *)
mov L0x200160fc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016108; PC = 0x8038490 *)
mov L0x20016108 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a30; PC = 0x8038494 *)
mov L0x20015a30 r4;




(**************** CUT 222, - *****************)

ecut and [
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x20015a30*x**1*z** 9 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x20015a3c*x**1*z** 9 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x20015a48*x**1*z** 9 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x200160f0*x**1*z** 9 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x200160fc*x**1*z** 9 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x20016108*x**1*z** 9 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015a54; Value = 0x00000003; PC = 0x803823c *)
mov r4 L0x20015a54;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a60; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x20015a60;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a6c; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x20015a6c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016114; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x20016114;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016120; Value = 0x00000003; PC = 0x803824c *)
mov r8 L0x20016120;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001612c; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x2001612c;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a60; PC = 0x80382d8 *)
mov L0x20015a60 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a6c; PC = 0x80382dc *)
mov L0x20015a6c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016114; PC = 0x80382e0 *)
mov L0x20016114 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016120; PC = 0x80382e4 *)
mov L0x20016120 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001612c; PC = 0x80382e8 *)
mov L0x2001612c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a54; PC = 0x80382ec *)
mov L0x20015a54 r4;




(**************** CUT 223, - *****************)

ecut and [
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x20015a54*x**1*z**10 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x20015a60*x**1*z**10 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x20015a6c*x**1*z**10 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x20016114*x**1*z**10 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x20016120*x**1*z**10 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x2001612c*x**1*z**10 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015a78; Value = 0xffeb477f; PC = 0x80382f0 *)
mov r4 L0x20015a78;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a84; Value = 0x00000000; PC = 0x80382f4 *)
mov r5 L0x20015a84;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a90; Value = 0x002970fc; PC = 0x80382f8 *)
mov r6 L0x20015a90;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016138; Value = 0xffeb477f; PC = 0x80382fc *)
mov r7 L0x20016138;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016144; Value = 0xfff61641; PC = 0x8038300 *)
mov r8 L0x20016144;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016150; Value = 0x0014b884; PC = 0x8038304 *)
mov r9 L0x20016150;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a84; PC = 0x80383ac *)
mov L0x20015a84 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a90; PC = 0x80383b0 *)
mov L0x20015a90 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016138; PC = 0x80383b4 *)
mov L0x20016138 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016144; PC = 0x80383b8 *)
mov L0x20016144 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016150; PC = 0x80383bc *)
mov L0x20016150 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a78; PC = 0x80383c0 *)
mov L0x20015a78 r4;




(**************** CUT 224, - *****************)

ecut and [
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20015a78*x**1*z**10 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20015a84*x**1*z**10 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20015a90*x**1*z**10 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20016138*x**1*z**10 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20016144*x**1*z**10 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20016150*x**1*z**10 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015a9c; Value = 0x0014b87e; PC = 0x80383c4 *)
mov r4 L0x20015a9c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015aa8; Value = 0x00000000; PC = 0x80383c8 *)
mov r5 L0x20015aa8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015ab4; Value = 0xffd68efe; PC = 0x80383cc *)
mov r6 L0x20015ab4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001615c; Value = 0x0014b87e; PC = 0x80383d0 *)
mov r7 L0x2001615c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016168; Value = 0x0009e9c5; PC = 0x80383d4 *)
mov r8 L0x20016168;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016174; Value = 0xffeb4785; PC = 0x80383d8 *)
mov r9 L0x20016174;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015aa8; PC = 0x8038480 *)
mov L0x20015aa8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015ab4; PC = 0x8038484 *)
mov L0x20015ab4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001615c; PC = 0x8038488 *)
mov L0x2001615c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016168; PC = 0x803848c *)
mov L0x20016168 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016174; PC = 0x8038490 *)
mov L0x20016174 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a9c; PC = 0x8038494 *)
mov L0x20015a9c r4;




(**************** CUT 225, - *****************)

ecut and [
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x20015a9c*x**1*z**10 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x20015aa8*x**1*z**10 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x20015ab4*x**1*z**10 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x2001615c*x**1*z**10 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x20016168*x**1*z**10 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x20016174*x**1*z**10 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015ac0; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x20015ac0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015acc; Value = 0x00000006; PC = 0x8038240 *)
mov r5 L0x20015acc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015ad8; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x20015ad8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016180; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x20016180;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001618c; Value = 0xfffffffd; PC = 0x803824c *)
mov r8 L0x2001618c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016198; Value = 0x00000009; PC = 0x8038250 *)
mov r9 L0x20016198;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015acc; PC = 0x80382d8 *)
mov L0x20015acc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015ad8; PC = 0x80382dc *)
mov L0x20015ad8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016180; PC = 0x80382e0 *)
mov L0x20016180 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001618c; PC = 0x80382e4 *)
mov L0x2001618c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016198; PC = 0x80382e8 *)
mov L0x20016198 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015ac0; PC = 0x80382ec *)
mov L0x20015ac0 r4;




(**************** CUT 226, - *****************)

ecut and [
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x20015ac0*x**1*z**11 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x20015acc*x**1*z**11 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x20015ad8*x**1*z**11 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x20016180*x**1*z**11 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x2001618c*x**1*z**11 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x20016198*x**1*z**11 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015ae4; Value = 0x0014b881; PC = 0x80382f0 *)
mov r4 L0x20015ae4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015af0; Value = 0x0014b881; PC = 0x80382f4 *)
mov r5 L0x20015af0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015afc; Value = 0x002970fc; PC = 0x80382f8 *)
mov r6 L0x20015afc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161a4; Value = 0x00297102; PC = 0x80382fc *)
mov r7 L0x200161a4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161b0; Value = 0x0014b881; PC = 0x8038300 *)
mov r8 L0x200161b0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161bc; Value = 0x00000000; PC = 0x8038304 *)
mov r9 L0x200161bc;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015af0; PC = 0x80383ac *)
mov L0x20015af0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015afc; PC = 0x80383b0 *)
mov L0x20015afc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161a4; PC = 0x80383b4 *)
mov L0x200161a4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161b0; PC = 0x80383b8 *)
mov L0x200161b0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161bc; PC = 0x80383bc *)
mov L0x200161bc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015ae4; PC = 0x80383c0 *)
mov L0x20015ae4 r4;




(**************** CUT 227, - *****************)

ecut and [
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x20015ae4*x**1*z**11 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x20015af0*x**1*z**11 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x20015afc*x**1*z**11 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x200161a4*x**1*z**11 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x200161b0*x**1*z**11 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x200161bc*x**1*z**11 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015b08; Value = 0xffeb4782; PC = 0x80383c4 *)
mov r4 L0x20015b08;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b14; Value = 0xffeb4782; PC = 0x80383c8 *)
mov r5 L0x20015b14;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b20; Value = 0xffd68efe; PC = 0x80383cc *)
mov r6 L0x20015b20;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161c8; Value = 0xffd68f04; PC = 0x80383d0 *)
mov r7 L0x200161c8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161d4; Value = 0xffeb4782; PC = 0x80383d4 *)
mov r8 L0x200161d4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161e0; Value = 0x00000000; PC = 0x80383d8 *)
mov r9 L0x200161e0;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b14; PC = 0x8038480 *)
mov L0x20015b14 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b20; PC = 0x8038484 *)
mov L0x20015b20 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161c8; PC = 0x8038488 *)
mov L0x200161c8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161d4; PC = 0x803848c *)
mov L0x200161d4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161e0; PC = 0x8038490 *)
mov L0x200161e0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b08; PC = 0x8038494 *)
mov L0x20015b08 r4;




(**************** CUT 228, - *****************)

ecut and [
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x20015b08*x**1*z**11 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x20015b14*x**1*z**11 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x20015b20*x**1*z**11 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x200161c8*x**1*z**11 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x200161d4*x**1*z**11 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x200161e0*x**1*z**11 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015b2c; Value = 0xfffffffa; PC = 0x803823c *)
mov r4 L0x20015b2c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b38; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x20015b38;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b44; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x20015b44;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161ec; Value = 0x00000000; PC = 0x8038248 *)
mov r7 L0x200161ec;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161f8; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x200161f8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016204; Value = 0xfffffffa; PC = 0x8038250 *)
mov r9 L0x20016204;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b38; PC = 0x80382d8 *)
mov L0x20015b38 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b44; PC = 0x80382dc *)
mov L0x20015b44 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161ec; PC = 0x80382e0 *)
mov L0x200161ec r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161f8; PC = 0x80382e4 *)
mov L0x200161f8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016204; PC = 0x80382e8 *)
mov L0x20016204 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b2c; PC = 0x80382ec *)
mov L0x20015b2c r4;




(**************** CUT 229, - *****************)

ecut and [
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x20015b2c*x**1*z**12 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x20015b38*x**1*z**12 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x20015b44*x**1*z**12 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x200161ec*x**1*z**12 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x200161f8*x**1*z**12 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x20016204*x**1*z**12 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015b50; Value = 0xffeb477f; PC = 0x80382f0 *)
mov r4 L0x20015b50;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b5c; Value = 0xfffffffa; PC = 0x80382f4 *)
mov r5 L0x20015b5c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b68; Value = 0x00000006; PC = 0x80382f8 *)
mov r6 L0x20015b68;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016210; Value = 0x0014b884; PC = 0x80382fc *)
mov r7 L0x20016210;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001621c; Value = 0x00000000; PC = 0x8038300 *)
mov r8 L0x2001621c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016228; Value = 0x0014b87e; PC = 0x8038304 *)
mov r9 L0x20016228;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b5c; PC = 0x80383ac *)
mov L0x20015b5c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b68; PC = 0x80383b0 *)
mov L0x20015b68 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016210; PC = 0x80383b4 *)
mov L0x20016210 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001621c; PC = 0x80383b8 *)
mov L0x2001621c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016228; PC = 0x80383bc *)
mov L0x20016228 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b50; PC = 0x80383c0 *)
mov L0x20015b50 r4;




(**************** CUT 230, - *****************)

ecut and [
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x20015b50*x**1*z**12 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x20015b5c*x**1*z**12 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x20015b68*x**1*z**12 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x20016210*x**1*z**12 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x2001621c*x**1*z**12 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x20016228*x**1*z**12 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015b74; Value = 0x0014b87e; PC = 0x80383c4 *)
mov r4 L0x20015b74;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b80; Value = 0xfffffffa; PC = 0x80383c8 *)
mov r5 L0x20015b80;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b8c; Value = 0x00000006; PC = 0x80383cc *)
mov r6 L0x20015b8c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016234; Value = 0xffeb4785; PC = 0x80383d0 *)
mov r7 L0x20016234;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016240; Value = 0x00000000; PC = 0x80383d4 *)
mov r8 L0x20016240;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001624c; Value = 0xffeb477f; PC = 0x80383d8 *)
mov r9 L0x2001624c;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b80; PC = 0x8038480 *)
mov L0x20015b80 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b8c; PC = 0x8038484 *)
mov L0x20015b8c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016234; PC = 0x8038488 *)
mov L0x20016234 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016240; PC = 0x803848c *)
mov L0x20016240 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001624c; PC = 0x8038490 *)
mov L0x2001624c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b74; PC = 0x8038494 *)
mov L0x20015b74 r4;




(**************** CUT 231, - *****************)

ecut and [
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x20015b74*x**1*z**12 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x20015b80*x**1*z**12 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x20015b8c*x**1*z**12 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x20016234*x**1*z**12 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x20016240*x**1*z**12 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x2001624c*x**1*z**12 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015b98; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x20015b98;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015ba4; Value = 0x00000ffd; PC = 0x8038240 *)
mov r5 L0x20015ba4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015bb0; Value = 0xfffffff7; PC = 0x8038244 *)
mov r6 L0x20015bb0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016258; Value = 0xfffffffa; PC = 0x8038248 *)
mov r7 L0x20016258;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016264; Value = 0xfffffffa; PC = 0x803824c *)
mov r8 L0x20016264;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016270; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x20016270;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015ba4; PC = 0x80382d8 *)
mov L0x20015ba4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015bb0; PC = 0x80382dc *)
mov L0x20015bb0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016258; PC = 0x80382e0 *)
mov L0x20016258 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016264; PC = 0x80382e4 *)
mov L0x20016264 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016270; PC = 0x80382e8 *)
mov L0x20016270 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b98; PC = 0x80382ec *)
mov L0x20015b98 r4;




(**************** CUT 232, - *****************)

ecut and [
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20015b98*x**1*z**13 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20015ba4*x**1*z**13 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20015bb0*x**1*z**13 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20016258*x**1*z**13 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20016264*x**1*z**13 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20016270*x**1*z**13 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015bbc; Value = 0xffd68f01; PC = 0x80382f0 *)
mov r4 L0x20015bbc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015bc8; Value = 0x0005a56d; PC = 0x80382f4 *)
mov r5 L0x20015bc8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015bd4; Value = 0x00000000; PC = 0x80382f8 *)
mov r6 L0x20015bd4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001627c; Value = 0xffeb477f; PC = 0x80382fc *)
mov r7 L0x2001627c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016288; Value = 0x0014b87e; PC = 0x8038300 *)
mov r8 L0x20016288;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016294; Value = 0xffd68f01; PC = 0x8038304 *)
mov r9 L0x20016294;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015bc8; PC = 0x80383ac *)
mov L0x20015bc8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015bd4; PC = 0x80383b0 *)
mov L0x20015bd4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001627c; PC = 0x80383b4 *)
mov L0x2001627c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016288; PC = 0x80383b8 *)
mov L0x20016288 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016294; PC = 0x80383bc *)
mov L0x20016294 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015bbc; PC = 0x80383c0 *)
mov L0x20015bbc r4;




(**************** CUT 233, - *****************)

ecut and [
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x20015bbc*x**1*z**13 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x20015bc8*x**1*z**13 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x20015bd4*x**1*z**13 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x2001627c*x**1*z**13 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x20016288*x**1*z**13 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x20016294*x**1*z**13 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015be0; Value = 0x002970ff; PC = 0x80383c4 *)
mov r4 L0x20015be0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015bec; Value = 0xfffa4a9f; PC = 0x80383c8 *)
mov r5 L0x20015bec;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015bf8; Value = 0x00000000; PC = 0x80383cc *)
mov r6 L0x20015bf8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162a0; Value = 0x0014b87e; PC = 0x80383d0 *)
mov r7 L0x200162a0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162ac; Value = 0xffeb477f; PC = 0x80383d4 *)
mov r8 L0x200162ac;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162b8; Value = 0x002970ff; PC = 0x80383d8 *)
mov r9 L0x200162b8;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015bec; PC = 0x8038480 *)
mov L0x20015bec r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015bf8; PC = 0x8038484 *)
mov L0x20015bf8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162a0; PC = 0x8038488 *)
mov L0x200162a0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162ac; PC = 0x803848c *)
mov L0x200162ac r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162b8; PC = 0x8038490 *)
mov L0x200162b8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015be0; PC = 0x8038494 *)
mov L0x20015be0 r4;




(**************** CUT 234, - *****************)

ecut and [
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x20015be0*x**1*z**13 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x20015bec*x**1*z**13 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x20015bf8*x**1*z**13 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x200162a0*x**1*z**13 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x200162ac*x**1*z**13 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x200162b8*x**1*z**13 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015c04; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x20015c04;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c10; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x20015c10;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c1c; Value = 0x00000003; PC = 0x8038244 *)
mov r6 L0x20015c1c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162c4; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x200162c4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162d0; Value = 0x00000003; PC = 0x803824c *)
mov r8 L0x200162d0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162dc; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x200162dc;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c10; PC = 0x80382d8 *)
mov L0x20015c10 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c1c; PC = 0x80382dc *)
mov L0x20015c1c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162c4; PC = 0x80382e0 *)
mov L0x200162c4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162d0; PC = 0x80382e4 *)
mov L0x200162d0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162dc; PC = 0x80382e8 *)
mov L0x200162dc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c04; PC = 0x80382ec *)
mov L0x20015c04 r4;




(**************** CUT 235, - *****************)

ecut and [
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x20015c04*x**1*z**14 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x20015c10*x**1*z**14 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x20015c1c*x**1*z**14 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x200162c4*x**1*z**14 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x200162d0*x**1*z**14 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x200162dc*x**1*z**14 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015c28; Value = 0xffd68f01; PC = 0x80382f0 *)
mov r4 L0x20015c28;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c34; Value = 0xffeb477f; PC = 0x80382f4 *)
mov r5 L0x20015c34;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c40; Value = 0xfffffffa; PC = 0x80382f8 *)
mov r6 L0x20015c40;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162e8; Value = 0xfff61641; PC = 0x80382fc *)
mov r7 L0x200162e8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162f4; Value = 0x0014b87e; PC = 0x8038300 *)
mov r8 L0x200162f4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016300; Value = 0x0014b87b; PC = 0x8038304 *)
mov r9 L0x20016300;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c34; PC = 0x80383ac *)
mov L0x20015c34 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c40; PC = 0x80383b0 *)
mov L0x20015c40 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162e8; PC = 0x80383b4 *)
mov L0x200162e8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162f4; PC = 0x80383b8 *)
mov L0x200162f4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016300; PC = 0x80383bc *)
mov L0x20016300 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c28; PC = 0x80383c0 *)
mov L0x20015c28 r4;




(**************** CUT 236, - *****************)

ecut and [
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x20015c28*x**1*z**14 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x20015c34*x**1*z**14 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x20015c40*x**1*z**14 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x200162e8*x**1*z**14 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x200162f4*x**1*z**14 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x20016300*x**1*z**14 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015c4c; Value = 0x002970ff; PC = 0x80383c4 *)
mov r4 L0x20015c4c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c58; Value = 0x0014b87e; PC = 0x80383c8 *)
mov r5 L0x20015c58;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c64; Value = 0xfffffffa; PC = 0x80383cc *)
mov r6 L0x20015c64;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001630c; Value = 0x0009e9c5; PC = 0x80383d0 *)
mov r7 L0x2001630c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016318; Value = 0xffeb477f; PC = 0x80383d4 *)
mov r8 L0x20016318;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016324; Value = 0xffeb477c; PC = 0x80383d8 *)
mov r9 L0x20016324;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c58; PC = 0x8038480 *)
mov L0x20015c58 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c64; PC = 0x8038484 *)
mov L0x20015c64 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001630c; PC = 0x8038488 *)
mov L0x2001630c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016318; PC = 0x803848c *)
mov L0x20016318 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016324; PC = 0x8038490 *)
mov L0x20016324 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c4c; PC = 0x8038494 *)
mov L0x20015c4c r4;




(**************** CUT 237, - *****************)

ecut and [
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x20015c4c*x**1*z**14 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x20015c58*x**1*z**14 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x20015c64*x**1*z**14 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x2001630c*x**1*z**14 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x20016318*x**1*z**14 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x20016324*x**1*z**14 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015c70; Value = 0x00000003; PC = 0x803823c *)
mov r4 L0x20015c70;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c7c; Value = 0x00000006; PC = 0x8038240 *)
mov r5 L0x20015c7c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c88; Value = 0x00000000; PC = 0x8038244 *)
mov r6 L0x20015c88;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016330; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x20016330;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001633c; Value = 0x00000003; PC = 0x803824c *)
mov r8 L0x2001633c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016348; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x20016348;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c7c; PC = 0x80382d8 *)
mov L0x20015c7c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c88; PC = 0x80382dc *)
mov L0x20015c88 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016330; PC = 0x80382e0 *)
mov L0x20016330 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001633c; PC = 0x80382e4 *)
mov L0x2001633c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016348; PC = 0x80382e8 *)
mov L0x20016348 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c70; PC = 0x80382ec *)
mov L0x20015c70 r4;




(**************** CUT 238, - *****************)

ecut and [
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x20015c70*x**1*z**15 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x20015c7c*x**1*z**15 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x20015c88*x**1*z**15 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x20016330*x**1*z**15 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x2001633c*x**1*z**15 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x20016348*x**1*z**15 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015c94; Value = 0x00000003; PC = 0x80382f0 *)
mov r4 L0x20015c94;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015ca0; Value = 0xffeb4782; PC = 0x80382f4 *)
mov r5 L0x20015ca0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015cac; Value = 0x0014b87b; PC = 0x80382f8 *)
mov r6 L0x20015cac;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016354; Value = 0xffeb477f; PC = 0x80382fc *)
mov r7 L0x20016354;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016360; Value = 0xffeb477f; PC = 0x8038300 *)
mov r8 L0x20016360;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001636c; Value = 0x0014b87b; PC = 0x8038304 *)
mov r9 L0x2001636c;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015ca0; PC = 0x80383ac *)
mov L0x20015ca0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015cac; PC = 0x80383b0 *)
mov L0x20015cac r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016354; PC = 0x80383b4 *)
mov L0x20016354 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016360; PC = 0x80383b8 *)
mov L0x20016360 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001636c; PC = 0x80383bc *)
mov L0x2001636c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c94; PC = 0x80383c0 *)
mov L0x20015c94 r4;




(**************** CUT 239, - *****************)

ecut and [
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x20015c94*x**1*z**15 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x20015ca0*x**1*z**15 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x20015cac*x**1*z**15 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x20016354*x**1*z**15 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x20016360*x**1*z**15 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x2001636c*x**1*z**15 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015cb8; Value = 0x00000003; PC = 0x80383c4 *)
mov r4 L0x20015cb8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015cc4; Value = 0x0014b881; PC = 0x80383c8 *)
mov r5 L0x20015cc4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015cd0; Value = 0xffeb477c; PC = 0x80383cc *)
mov r6 L0x20015cd0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016378; Value = 0x0014b87e; PC = 0x80383d0 *)
mov r7 L0x20016378;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016384; Value = 0x0014b87e; PC = 0x80383d4 *)
mov r8 L0x20016384;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016390; Value = 0xffeb477c; PC = 0x80383d8 *)
mov r9 L0x20016390;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015cc4; PC = 0x8038480 *)
mov L0x20015cc4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015cd0; PC = 0x8038484 *)
mov L0x20015cd0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016378; PC = 0x8038488 *)
mov L0x20016378 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016384; PC = 0x803848c *)
mov L0x20016384 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016390; PC = 0x8038490 *)
mov L0x20016390 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015cb8; PC = 0x8038494 *)
mov L0x20015cb8 r4;




(**************** CUT 240, - *****************)

ecut and [
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20015cb8*x**1*z**15 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20015cc4*x**1*z**15 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20015cd0*x**1*z**15 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20016378*x**1*z**15 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20016384*x**1*z**15 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20016390*x**1*z**15 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* sub.w	r0, r0, #1728	; 0x6c0                     #! PC = 0x80384a4 *)
subs dc r0 r0 1728@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x80384a8 *)
adds dc r0 r0 4@uint32;
(* vmov	r12, s2                                    #! PC = 0x80384ac *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80384b0 *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8038234 <_3x2>                         #! PC = 0x80384b4 *)
#bne.w	0x8038234 <_3x2>                         #! 0x80384b4 = 0x80384b4;
(* add.w	lr, r0, #1728	; 0x6c0                     #! PC = 0x8038234 *)
adds dc lr r0 1728@uint32;
(* vmov	s3, lr                                     #! PC = 0x8038238 *)
mov s3 lr;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015620; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x20015620;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001562c; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x2001562c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015638; Value = 0x00000003; PC = 0x8038244 *)
mov r6 L0x20015638;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ce0; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x20015ce0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015cec; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20015cec;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015cf8; Value = 0xfffffffa; PC = 0x8038250 *)
mov r9 L0x20015cf8;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001562c; PC = 0x80382d8 *)
mov L0x2001562c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015638; PC = 0x80382dc *)
mov L0x20015638 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ce0; PC = 0x80382e0 *)
mov L0x20015ce0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015cec; PC = 0x80382e4 *)
mov L0x20015cec r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015cf8; PC = 0x80382e8 *)
mov L0x20015cf8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015620; PC = 0x80382ec *)
mov L0x20015620 r4;




(**************** CUT 241, - *****************)

ecut and [
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x20015620*x**2*z** 0 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x2001562c*x**2*z** 0 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x20015638*x**2*z** 0 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x20015ce0*x**2*z** 0 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x20015cec*x**2*z** 0 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x20015cf8*x**2*z** 0 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015644; Value = 0xfffffffd; PC = 0x80382f0 *)
mov r4 L0x20015644;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015650; Value = 0xfffffffa; PC = 0x80382f4 *)
mov r5 L0x20015650;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001565c; Value = 0xfff61641; PC = 0x80382f8 *)
mov r6 L0x2001565c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d04; Value = 0xfffffffa; PC = 0x80382fc *)
mov r7 L0x20015d04;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d10; Value = 0xffeb477c; PC = 0x8038300 *)
mov r8 L0x20015d10;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d1c; Value = 0x00000003; PC = 0x8038304 *)
mov r9 L0x20015d1c;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015650; PC = 0x80383ac *)
mov L0x20015650 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001565c; PC = 0x80383b0 *)
mov L0x2001565c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d04; PC = 0x80383b4 *)
mov L0x20015d04 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d10; PC = 0x80383b8 *)
mov L0x20015d10 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d1c; PC = 0x80383bc *)
mov L0x20015d1c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015644; PC = 0x80383c0 *)
mov L0x20015644 r4;




(**************** CUT 242, - *****************)

ecut and [
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x20015644*x**2*z** 0 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x20015650*x**2*z** 0 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x2001565c*x**2*z** 0 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x20015d04*x**2*z** 0 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x20015d10*x**2*z** 0 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x20015d1c*x**2*z** 0 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015668; Value = 0xfffffffd; PC = 0x80383c4 *)
mov r4 L0x20015668;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015674; Value = 0xfffffffa; PC = 0x80383c8 *)
mov r5 L0x20015674;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015680; Value = 0x0009e9c5; PC = 0x80383cc *)
mov r6 L0x20015680;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d28; Value = 0xfffffffa; PC = 0x80383d0 *)
mov r7 L0x20015d28;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d34; Value = 0x0014b87b; PC = 0x80383d4 *)
mov r8 L0x20015d34;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d40; Value = 0x00000003; PC = 0x80383d8 *)
mov r9 L0x20015d40;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015674; PC = 0x8038480 *)
mov L0x20015674 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015680; PC = 0x8038484 *)
mov L0x20015680 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d28; PC = 0x8038488 *)
mov L0x20015d28 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d34; PC = 0x803848c *)
mov L0x20015d34 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d40; PC = 0x8038490 *)
mov L0x20015d40 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015668; PC = 0x8038494 *)
mov L0x20015668 r4;




(**************** CUT 243, - *****************)

ecut and [
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015668*x**2*z** 0 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015674*x**2*z** 0 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015680*x**2*z** 0 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015d28*x**2*z** 0 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015d34*x**2*z** 0 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015d40*x**2*z** 0 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x2001568c; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x2001568c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015698; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x20015698;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200156a4; Value = 0x00000000; PC = 0x8038244 *)
mov r6 L0x200156a4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d4c; Value = 0x00000006; PC = 0x8038248 *)
mov r7 L0x20015d4c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d58; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20015d58;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d64; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x20015d64;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015698; PC = 0x80382d8 *)
mov L0x20015698 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200156a4; PC = 0x80382dc *)
mov L0x200156a4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d4c; PC = 0x80382e0 *)
mov L0x20015d4c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d58; PC = 0x80382e4 *)
mov L0x20015d58 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d64; PC = 0x80382e8 *)
mov L0x20015d64 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001568c; PC = 0x80382ec *)
mov L0x2001568c r4;




(**************** CUT 244, - *****************)

ecut and [
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x2001568c*x**2*z** 1 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x20015698*x**2*z** 1 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x200156a4*x**2*z** 1 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x20015d4c*x**2*z** 1 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x20015d58*x**2*z** 1 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x20015d64*x**2*z** 1 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200156b0; Value = 0xfffffffd; PC = 0x80382f0 *)
mov r4 L0x200156b0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200156bc; Value = 0x002970ff; PC = 0x80382f4 *)
mov r5 L0x200156bc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200156c8; Value = 0x00000000; PC = 0x80382f8 *)
mov r6 L0x200156c8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d70; Value = 0xffeb4782; PC = 0x80382fc *)
mov r7 L0x20015d70;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d7c; Value = 0xfff6163e; PC = 0x8038300 *)
mov r8 L0x20015d7c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d88; Value = 0x0014b881; PC = 0x8038304 *)
mov r9 L0x20015d88;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200156bc; PC = 0x80383ac *)
mov L0x200156bc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200156c8; PC = 0x80383b0 *)
mov L0x200156c8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d70; PC = 0x80383b4 *)
mov L0x20015d70 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015d7c; PC = 0x80383b8 *)
mov L0x20015d7c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015d88; PC = 0x80383bc *)
mov L0x20015d88 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200156b0; PC = 0x80383c0 *)
mov L0x200156b0 r4;




(**************** CUT 245, - *****************)

ecut and [
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x200156b0*x**2*z** 1 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x200156bc*x**2*z** 1 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x200156c8*x**2*z** 1 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x20015d70*x**2*z** 1 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x20015d7c*x**2*z** 1 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x20015d88*x**2*z** 1 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200156d4; Value = 0xfffffffd; PC = 0x80383c4 *)
mov r4 L0x200156d4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200156e0; Value = 0xffd68f01; PC = 0x80383c8 *)
mov r5 L0x200156e0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200156ec; Value = 0x00000000; PC = 0x80383cc *)
mov r6 L0x200156ec;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d94; Value = 0x0014b881; PC = 0x80383d0 *)
mov r7 L0x20015d94;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015da0; Value = 0x0009e9c2; PC = 0x80383d4 *)
mov r8 L0x20015da0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015dac; Value = 0xffeb4782; PC = 0x80383d8 *)
mov r9 L0x20015dac;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200156e0; PC = 0x8038480 *)
mov L0x200156e0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200156ec; PC = 0x8038484 *)
mov L0x200156ec r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015d94; PC = 0x8038488 *)
mov L0x20015d94 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015da0; PC = 0x803848c *)
mov L0x20015da0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015dac; PC = 0x8038490 *)
mov L0x20015dac r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200156d4; PC = 0x8038494 *)
mov L0x200156d4 r4;




(**************** CUT 246, - *****************)

ecut and [
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x200156d4*x**2*z** 1 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x200156e0*x**2*z** 1 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x200156ec*x**2*z** 1 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x20015d94*x**2*z** 1 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x20015da0*x**2*z** 1 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x20015dac*x**2*z** 1 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200156f8; Value = 0x00000003; PC = 0x803823c *)
mov r4 L0x200156f8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015704; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x20015704;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015710; Value = 0x00000009; PC = 0x8038244 *)
mov r6 L0x20015710;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015db8; Value = 0x00000006; PC = 0x8038248 *)
mov r7 L0x20015db8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015dc4; Value = 0xfffffffa; PC = 0x803824c *)
mov r8 L0x20015dc4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015dd0; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x20015dd0;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015704; PC = 0x80382d8 *)
mov L0x20015704 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015710; PC = 0x80382dc *)
mov L0x20015710 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015db8; PC = 0x80382e0 *)
mov L0x20015db8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015dc4; PC = 0x80382e4 *)
mov L0x20015dc4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015dd0; PC = 0x80382e8 *)
mov L0x20015dd0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200156f8; PC = 0x80382ec *)
mov L0x200156f8 r4;




(**************** CUT 247, - *****************)

ecut and [
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x200156f8*x**2*z** 2 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x20015704*x**2*z** 2 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x20015710*x**2*z** 2 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x20015db8*x**2*z** 2 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x20015dc4*x**2*z** 2 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x20015dd0*x**2*z** 2 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x2001571c; Value = 0xfffffffa; PC = 0x80382f0 *)
mov r4 L0x2001571c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015728; Value = 0xffeb477c; PC = 0x80382f4 *)
mov r5 L0x20015728;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015734; Value = 0x00000000; PC = 0x80382f8 *)
mov r6 L0x20015734;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ddc; Value = 0xffeb4782; PC = 0x80382fc *)
mov r7 L0x20015ddc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015de8; Value = 0x0014b87e; PC = 0x8038300 *)
mov r8 L0x20015de8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015df4; Value = 0xffeb4782; PC = 0x8038304 *)
mov r9 L0x20015df4;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015728; PC = 0x80383ac *)
mov L0x20015728 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015734; PC = 0x80383b0 *)
mov L0x20015734 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ddc; PC = 0x80383b4 *)
mov L0x20015ddc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015de8; PC = 0x80383b8 *)
mov L0x20015de8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015df4; PC = 0x80383bc *)
mov L0x20015df4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001571c; PC = 0x80383c0 *)
mov L0x2001571c r4;




(**************** CUT 248, - *****************)

ecut and [
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x2001571c*x**2*z** 2 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x20015728*x**2*z** 2 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x20015734*x**2*z** 2 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x20015ddc*x**2*z** 2 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x20015de8*x**2*z** 2 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x20015df4*x**2*z** 2 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015740; Value = 0xfffffffa; PC = 0x80383c4 *)
mov r4 L0x20015740;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001574c; Value = 0x0014b87b; PC = 0x80383c8 *)
mov r5 L0x2001574c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015758; Value = 0x00000000; PC = 0x80383cc *)
mov r6 L0x20015758;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e00; Value = 0x0014b881; PC = 0x80383d0 *)
mov r7 L0x20015e00;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e0c; Value = 0xffeb477f; PC = 0x80383d4 *)
mov r8 L0x20015e0c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e18; Value = 0x0014b881; PC = 0x80383d8 *)
mov r9 L0x20015e18;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001574c; PC = 0x8038480 *)
mov L0x2001574c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015758; PC = 0x8038484 *)
mov L0x20015758 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e00; PC = 0x8038488 *)
mov L0x20015e00 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e0c; PC = 0x803848c *)
mov L0x20015e0c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e18; PC = 0x8038490 *)
mov L0x20015e18 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015740; PC = 0x8038494 *)
mov L0x20015740 r4;




(**************** CUT 249, - *****************)

ecut and [
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x20015740*x**2*z** 2 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x2001574c*x**2*z** 2 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x20015758*x**2*z** 2 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x20015e00*x**2*z** 2 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x20015e0c*x**2*z** 2 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x20015e18*x**2*z** 2 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015764; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x20015764;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015770; Value = 0xfffffffd; PC = 0x8038240 *)
mov r5 L0x20015770;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001577c; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x2001577c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e24; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x20015e24;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e30; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20015e30;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e3c; Value = 0x00000006; PC = 0x8038250 *)
mov r9 L0x20015e3c;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015770; PC = 0x80382d8 *)
mov L0x20015770 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001577c; PC = 0x80382dc *)
mov L0x2001577c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e24; PC = 0x80382e0 *)
mov L0x20015e24 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e30; PC = 0x80382e4 *)
mov L0x20015e30 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e3c; PC = 0x80382e8 *)
mov L0x20015e3c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015764; PC = 0x80382ec *)
mov L0x20015764 r4;




(**************** CUT 250, - *****************)

ecut and [
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x20015764*x**2*z** 3 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x20015770*x**2*z** 3 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x2001577c*x**2*z** 3 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x20015e24*x**2*z** 3 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x20015e30*x**2*z** 3 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x20015e3c*x**2*z** 3 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015788; Value = 0x00000006; PC = 0x80382f0 *)
mov r4 L0x20015788;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015794; Value = 0x0009e9bf; PC = 0x80382f4 *)
mov r5 L0x20015794;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200157a0; Value = 0x002970fc; PC = 0x80382f8 *)
mov r6 L0x200157a0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e48; Value = 0xffd68f04; PC = 0x80382fc *)
mov r7 L0x20015e48;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e54; Value = 0x0014b884; PC = 0x8038300 *)
mov r8 L0x20015e54;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e60; Value = 0xffeb4782; PC = 0x8038304 *)
mov r9 L0x20015e60;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015794; PC = 0x80383ac *)
mov L0x20015794 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200157a0; PC = 0x80383b0 *)
mov L0x200157a0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e48; PC = 0x80383b4 *)
mov L0x20015e48 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e54; PC = 0x80383b8 *)
mov L0x20015e54 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e60; PC = 0x80383bc *)
mov L0x20015e60 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015788; PC = 0x80383c0 *)
mov L0x20015788 r4;




(**************** CUT 251, - *****************)

ecut and [
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x20015788*x**2*z** 3 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x20015794*x**2*z** 3 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x200157a0*x**2*z** 3 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x20015e48*x**2*z** 3 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x20015e54*x**2*z** 3 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x20015e60*x**2*z** 3 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200157ac; Value = 0x00000006; PC = 0x80383c4 *)
mov r4 L0x200157ac;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200157b8; Value = 0xfff6163b; PC = 0x80383c8 *)
mov r5 L0x200157b8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200157c4; Value = 0xffd68efe; PC = 0x80383cc *)
mov r6 L0x200157c4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e6c; Value = 0x00297102; PC = 0x80383d0 *)
mov r7 L0x20015e6c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e78; Value = 0xffeb4785; PC = 0x80383d4 *)
mov r8 L0x20015e78;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e84; Value = 0x0014b881; PC = 0x80383d8 *)
mov r9 L0x20015e84;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200157b8; PC = 0x8038480 *)
mov L0x200157b8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200157c4; PC = 0x8038484 *)
mov L0x200157c4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e6c; PC = 0x8038488 *)
mov L0x20015e6c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e78; PC = 0x803848c *)
mov L0x20015e78 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015e84; PC = 0x8038490 *)
mov L0x20015e84 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200157ac; PC = 0x8038494 *)
mov L0x200157ac r4;




(**************** CUT 252, - *****************)

ecut and [
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x200157ac*x**2*z** 3 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x200157b8*x**2*z** 3 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x200157c4*x**2*z** 3 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x20015e6c*x**2*z** 3 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x20015e78*x**2*z** 3 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x20015e84*x**2*z** 3 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200157d0; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x200157d0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200157dc; Value = 0xfffffffd; PC = 0x8038240 *)
mov r5 L0x200157dc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200157e8; Value = 0x00000006; PC = 0x8038244 *)
mov r6 L0x200157e8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e90; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x20015e90;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e9c; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20015e9c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ea8; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x20015ea8;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200157dc; PC = 0x80382d8 *)
mov L0x200157dc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200157e8; PC = 0x80382dc *)
mov L0x200157e8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015e90; PC = 0x80382e0 *)
mov L0x20015e90 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015e9c; PC = 0x80382e4 *)
mov L0x20015e9c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ea8; PC = 0x80382e8 *)
mov L0x20015ea8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200157d0; PC = 0x80382ec *)
mov L0x200157d0 r4;




(**************** CUT 253, - *****************)

ecut and [
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x200157d0*x**2*z** 4 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x200157dc*x**2*z** 4 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x200157e8*x**2*z** 4 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x20015e90*x**2*z** 4 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x20015e9c*x**2*z** 4 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x20015ea8*x**2*z** 4 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200157f4; Value = 0x0014b87b; PC = 0x80382f0 *)
mov r4 L0x200157f4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015800; Value = 0xffeb4782; PC = 0x80382f4 *)
mov r5 L0x20015800;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001580c; Value = 0xfffffffd; PC = 0x80382f8 *)
mov r6 L0x2001580c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015eb4; Value = 0x0014b87e; PC = 0x80382fc *)
mov r7 L0x20015eb4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ec0; Value = 0x00000000; PC = 0x8038300 *)
mov r8 L0x20015ec0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ecc; Value = 0x00000000; PC = 0x8038304 *)
mov r9 L0x20015ecc;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015800; PC = 0x80383ac *)
mov L0x20015800 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001580c; PC = 0x80383b0 *)
mov L0x2001580c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015eb4; PC = 0x80383b4 *)
mov L0x20015eb4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ec0; PC = 0x80383b8 *)
mov L0x20015ec0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ecc; PC = 0x80383bc *)
mov L0x20015ecc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200157f4; PC = 0x80383c0 *)
mov L0x200157f4 r4;




(**************** CUT 254, - *****************)

ecut and [
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x200157f4*x**2*z** 4 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x20015800*x**2*z** 4 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x2001580c*x**2*z** 4 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x20015eb4*x**2*z** 4 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x20015ec0*x**2*z** 4 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x20015ecc*x**2*z** 4 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015818; Value = 0xffeb477c; PC = 0x80383c4 *)
mov r4 L0x20015818;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015824; Value = 0x0014b881; PC = 0x80383c8 *)
mov r5 L0x20015824;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015830; Value = 0xfffffffd; PC = 0x80383cc *)
mov r6 L0x20015830;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ed8; Value = 0xffeb477f; PC = 0x80383d0 *)
mov r7 L0x20015ed8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ee4; Value = 0x00000000; PC = 0x80383d4 *)
mov r8 L0x20015ee4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ef0; Value = 0x00000000; PC = 0x80383d8 *)
mov r9 L0x20015ef0;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015824; PC = 0x8038480 *)
mov L0x20015824 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015830; PC = 0x8038484 *)
mov L0x20015830 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ed8; PC = 0x8038488 *)
mov L0x20015ed8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015ee4; PC = 0x803848c *)
mov L0x20015ee4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015ef0; PC = 0x8038490 *)
mov L0x20015ef0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015818; PC = 0x8038494 *)
mov L0x20015818 r4;




(**************** CUT 255, - *****************)

ecut and [
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015818*x**2*z** 4 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015824*x**2*z** 4 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015830*x**2*z** 4 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015ed8*x**2*z** 4 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015ee4*x**2*z** 4 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015ef0*x**2*z** 4 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x2001583c; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x2001583c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015848; Value = 0xfffffffd; PC = 0x8038240 *)
mov r5 L0x20015848;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015854; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x20015854;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015efc; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x20015efc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f08; Value = 0xfffffffd; PC = 0x803824c *)
mov r8 L0x20015f08;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f14; Value = 0x00000003; PC = 0x8038250 *)
mov r9 L0x20015f14;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015848; PC = 0x80382d8 *)
mov L0x20015848 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015854; PC = 0x80382dc *)
mov L0x20015854 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015efc; PC = 0x80382e0 *)
mov L0x20015efc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f08; PC = 0x80382e4 *)
mov L0x20015f08 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f14; PC = 0x80382e8 *)
mov L0x20015f14 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001583c; PC = 0x80382ec *)
mov L0x2001583c r4;




(**************** CUT 256, - *****************)

ecut and [
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x2001583c*x**2*z** 5 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x20015848*x**2*z** 5 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x20015854*x**2*z** 5 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x20015efc*x**2*z** 5 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x20015f08*x**2*z** 5 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x20015f14*x**2*z** 5 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015860; Value = 0x002970ff; PC = 0x80382f0 *)
mov r4 L0x20015860;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001586c; Value = 0xffeb4782; PC = 0x80382f4 *)
mov r5 L0x2001586c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015878; Value = 0x00000006; PC = 0x80382f8 *)
mov r6 L0x20015878;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f20; Value = 0xffeb477f; PC = 0x80382fc *)
mov r7 L0x20015f20;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f2c; Value = 0xffeb4782; PC = 0x8038300 *)
mov r8 L0x20015f2c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f38; Value = 0x0014b87e; PC = 0x8038304 *)
mov r9 L0x20015f38;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001586c; PC = 0x80383ac *)
mov L0x2001586c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015878; PC = 0x80383b0 *)
mov L0x20015878 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f20; PC = 0x80383b4 *)
mov L0x20015f20 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f2c; PC = 0x80383b8 *)
mov L0x20015f2c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f38; PC = 0x80383bc *)
mov L0x20015f38 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015860; PC = 0x80383c0 *)
mov L0x20015860 r4;




(**************** CUT 257, - *****************)

ecut and [
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x20015860*x**2*z** 5 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x2001586c*x**2*z** 5 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x20015878*x**2*z** 5 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x20015f20*x**2*z** 5 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x20015f2c*x**2*z** 5 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x20015f38*x**2*z** 5 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015884; Value = 0xffd68f01; PC = 0x80383c4 *)
mov r4 L0x20015884;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015890; Value = 0x0014b881; PC = 0x80383c8 *)
mov r5 L0x20015890;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001589c; Value = 0x00000006; PC = 0x80383cc *)
mov r6 L0x2001589c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f44; Value = 0x0014b87e; PC = 0x80383d0 *)
mov r7 L0x20015f44;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f50; Value = 0x0014b881; PC = 0x80383d4 *)
mov r8 L0x20015f50;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f5c; Value = 0xffeb477f; PC = 0x80383d8 *)
mov r9 L0x20015f5c;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015890; PC = 0x8038480 *)
mov L0x20015890 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001589c; PC = 0x8038484 *)
mov L0x2001589c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f44; PC = 0x8038488 *)
mov L0x20015f44 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f50; PC = 0x803848c *)
mov L0x20015f50 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f5c; PC = 0x8038490 *)
mov L0x20015f5c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015884; PC = 0x8038494 *)
mov L0x20015884 r4;




(**************** CUT 258, - *****************)

ecut and [
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x20015884*x**2*z** 5 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x20015890*x**2*z** 5 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x2001589c*x**2*z** 5 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x20015f44*x**2*z** 5 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x20015f50*x**2*z** 5 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x20015f5c*x**2*z** 5 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200158a8; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x200158a8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200158b4; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x200158b4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200158c0; Value = 0x00000000; PC = 0x8038244 *)
mov r6 L0x200158c0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f68; Value = 0x00000006; PC = 0x8038248 *)
mov r7 L0x20015f68;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f74; Value = 0x00000003; PC = 0x803824c *)
mov r8 L0x20015f74;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f80; Value = 0x00000003; PC = 0x8038250 *)
mov r9 L0x20015f80;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200158b4; PC = 0x80382d8 *)
mov L0x200158b4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200158c0; PC = 0x80382dc *)
mov L0x200158c0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f68; PC = 0x80382e0 *)
mov L0x20015f68 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f74; PC = 0x80382e4 *)
mov L0x20015f74 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015f80; PC = 0x80382e8 *)
mov L0x20015f80 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200158a8; PC = 0x80382ec *)
mov L0x200158a8 r4;




(**************** CUT 259, - *****************)

ecut and [
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x200158a8*x**2*z** 6 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x200158b4*x**2*z** 6 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x200158c0*x**2*z** 6 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x20015f68*x**2*z** 6 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x20015f74*x**2*z** 6 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x20015f80*x**2*z** 6 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200158cc; Value = 0xffeb4782; PC = 0x80382f0 *)
mov r4 L0x200158cc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200158d8; Value = 0xffeb477c; PC = 0x80382f4 *)
mov r5 L0x200158d8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200158e4; Value = 0x0014b884; PC = 0x80382f8 *)
mov r6 L0x200158e4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f8c; Value = 0xfffffffd; PC = 0x80382fc *)
mov r7 L0x20015f8c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f98; Value = 0x00000003; PC = 0x8038300 *)
mov r8 L0x20015f98;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fa4; Value = 0x00000003; PC = 0x8038304 *)
mov r9 L0x20015fa4;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200158d8; PC = 0x80383ac *)
mov L0x200158d8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200158e4; PC = 0x80383b0 *)
mov L0x200158e4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015f8c; PC = 0x80383b4 *)
mov L0x20015f8c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015f98; PC = 0x80383b8 *)
mov L0x20015f98 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fa4; PC = 0x80383bc *)
mov L0x20015fa4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200158cc; PC = 0x80383c0 *)
mov L0x200158cc r4;




(**************** CUT 260, - *****************)

ecut and [
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x200158cc*x**2*z** 6 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x200158d8*x**2*z** 6 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x200158e4*x**2*z** 6 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x20015f8c*x**2*z** 6 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x20015f98*x**2*z** 6 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x20015fa4*x**2*z** 6 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200158f0; Value = 0x0014b881; PC = 0x80383c4 *)
mov r4 L0x200158f0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200158fc; Value = 0x0014b87b; PC = 0x80383c8 *)
mov r5 L0x200158fc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015908; Value = 0xffeb4785; PC = 0x80383cc *)
mov r6 L0x20015908;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fb0; Value = 0xfffffffd; PC = 0x80383d0 *)
mov r7 L0x20015fb0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fbc; Value = 0x00000003; PC = 0x80383d4 *)
mov r8 L0x20015fbc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fc8; Value = 0x00000003; PC = 0x80383d8 *)
mov r9 L0x20015fc8;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200158fc; PC = 0x8038480 *)
mov L0x200158fc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015908; PC = 0x8038484 *)
mov L0x20015908 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fb0; PC = 0x8038488 *)
mov L0x20015fb0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fbc; PC = 0x803848c *)
mov L0x20015fbc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fc8; PC = 0x8038490 *)
mov L0x20015fc8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200158f0; PC = 0x8038494 *)
mov L0x200158f0 r4;




(**************** CUT 261, - *****************)

ecut and [
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x200158f0*x**2*z** 6 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x200158fc*x**2*z** 6 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x20015908*x**2*z** 6 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x20015fb0*x**2*z** 6 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x20015fbc*x**2*z** 6 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x20015fc8*x**2*z** 6 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015914; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x20015914;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015920; Value = 0x00000006; PC = 0x8038240 *)
mov r5 L0x20015920;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x2001592c; Value = 0x00000003; PC = 0x8038244 *)
mov r6 L0x2001592c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fd4; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x20015fd4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fe0; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20015fe0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fec; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x20015fec;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015920; PC = 0x80382d8 *)
mov L0x20015920 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x2001592c; PC = 0x80382dc *)
mov L0x2001592c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015fd4; PC = 0x80382e0 *)
mov L0x20015fd4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20015fe0; PC = 0x80382e4 *)
mov L0x20015fe0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20015fec; PC = 0x80382e8 *)
mov L0x20015fec r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015914; PC = 0x80382ec *)
mov L0x20015914 r4;




(**************** CUT 262, - *****************)

ecut and [
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x20015914*x**2*z** 7 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x20015920*x**2*z** 7 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x2001592c*x**2*z** 7 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x20015fd4*x**2*z** 7 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x20015fe0*x**2*z** 7 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x20015fec*x**2*z** 7 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015938; Value = 0x0014b884; PC = 0x80382f0 *)
mov r4 L0x20015938;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015944; Value = 0x0014b881; PC = 0x80382f4 *)
mov r5 L0x20015944;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015950; Value = 0xffd68f04; PC = 0x80382f8 *)
mov r6 L0x20015950;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ff8; Value = 0x0014b87e; PC = 0x80382fc *)
mov r7 L0x20015ff8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016004; Value = 0x002970ff; PC = 0x8038300 *)
mov r8 L0x20016004;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016010; Value = 0x0014b87b; PC = 0x8038304 *)
mov r9 L0x20016010;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015944; PC = 0x80383ac *)
mov L0x20015944 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015950; PC = 0x80383b0 *)
mov L0x20015950 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20015ff8; PC = 0x80383b4 *)
mov L0x20015ff8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016004; PC = 0x80383b8 *)
mov L0x20016004 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016010; PC = 0x80383bc *)
mov L0x20016010 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015938; PC = 0x80383c0 *)
mov L0x20015938 r4;




(**************** CUT 263, - *****************)

ecut and [
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20015938*x**2*z** 7 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20015944*x**2*z** 7 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20015950*x**2*z** 7 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20015ff8*x**2*z** 7 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20016004*x**2*z** 7 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20016010*x**2*z** 7 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x2001595c; Value = 0xffeb4785; PC = 0x80383c4 *)
mov r4 L0x2001595c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015968; Value = 0xffeb4782; PC = 0x80383c8 *)
mov r5 L0x20015968;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015974; Value = 0x00297102; PC = 0x80383cc *)
mov r6 L0x20015974;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001601c; Value = 0xffeb477f; PC = 0x80383d0 *)
mov r7 L0x2001601c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016028; Value = 0xffd68f01; PC = 0x80383d4 *)
mov r8 L0x20016028;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016034; Value = 0xffeb477c; PC = 0x80383d8 *)
mov r9 L0x20016034;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015968; PC = 0x8038480 *)
mov L0x20015968 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015974; PC = 0x8038484 *)
mov L0x20015974 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001601c; PC = 0x8038488 *)
mov L0x2001601c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016028; PC = 0x803848c *)
mov L0x20016028 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016034; PC = 0x8038490 *)
mov L0x20016034 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x2001595c; PC = 0x8038494 *)
mov L0x2001595c r4;




(**************** CUT 264, - *****************)

ecut and [
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x2001595c*x**2*z** 7 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x20015968*x**2*z** 7 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x20015974*x**2*z** 7 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x2001601c*x**2*z** 7 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x20016028*x**2*z** 7 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x20016034*x**2*z** 7 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015980; Value = 0xfffffff7; PC = 0x803823c *)
mov r4 L0x20015980;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x2001598c; Value = 0xfffffffd; PC = 0x8038240 *)
mov r5 L0x2001598c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015998; Value = 0xfffffffd; PC = 0x8038244 *)
mov r6 L0x20015998;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016040; Value = 0x00000000; PC = 0x8038248 *)
mov r7 L0x20016040;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001604c; Value = 0xfffffffa; PC = 0x803824c *)
mov r8 L0x2001604c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016058; Value = 0x00000006; PC = 0x8038250 *)
mov r9 L0x20016058;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x2001598c; PC = 0x80382d8 *)
mov L0x2001598c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015998; PC = 0x80382dc *)
mov L0x20015998 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016040; PC = 0x80382e0 *)
mov L0x20016040 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001604c; PC = 0x80382e4 *)
mov L0x2001604c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016058; PC = 0x80382e8 *)
mov L0x20016058 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015980; PC = 0x80382ec *)
mov L0x20015980 r4;




(**************** CUT 265, - *****************)

ecut and [
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x20015980*x**2*z** 8 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x2001598c*x**2*z** 8 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x20015998*x**2*z** 8 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x20016040*x**2*z** 8 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x2001604c*x**2*z** 8 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x20016058*x**2*z** 8 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200159a4; Value = 0x00000000; PC = 0x80382f0 *)
mov r4 L0x200159a4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200159b0; Value = 0x0014b881; PC = 0x80382f4 *)
mov r5 L0x200159b0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200159bc; Value = 0xffeb4782; PC = 0x80382f8 *)
mov r6 L0x200159bc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016064; Value = 0x00000000; PC = 0x80382fc *)
mov r7 L0x20016064;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016070; Value = 0x00000003; PC = 0x8038300 *)
mov r8 L0x20016070;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001607c; Value = 0xfffffffd; PC = 0x8038304 *)
mov r9 L0x2001607c;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200159b0; PC = 0x80383ac *)
mov L0x200159b0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200159bc; PC = 0x80383b0 *)
mov L0x200159bc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016064; PC = 0x80383b4 *)
mov L0x20016064 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016070; PC = 0x80383b8 *)
mov L0x20016070 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001607c; PC = 0x80383bc *)
mov L0x2001607c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200159a4; PC = 0x80383c0 *)
mov L0x200159a4 r4;




(**************** CUT 266, - *****************)

ecut and [
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x200159a4*x**2*z** 8 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x200159b0*x**2*z** 8 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x200159bc*x**2*z** 8 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x20016064*x**2*z** 8 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x20016070*x**2*z** 8 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x2001607c*x**2*z** 8 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x200159c8; Value = 0x00000000; PC = 0x80383c4 *)
mov r4 L0x200159c8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200159d4; Value = 0xffeb4782; PC = 0x80383c8 *)
mov r5 L0x200159d4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x200159e0; Value = 0x0014b881; PC = 0x80383cc *)
mov r6 L0x200159e0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016088; Value = 0x00000000; PC = 0x80383d0 *)
mov r7 L0x20016088;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016094; Value = 0x00000003; PC = 0x80383d4 *)
mov r8 L0x20016094;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160a0; Value = 0xfffffffd; PC = 0x80383d8 *)
mov r9 L0x200160a0;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200159d4; PC = 0x8038480 *)
mov L0x200159d4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x200159e0; PC = 0x8038484 *)
mov L0x200159e0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016088; PC = 0x8038488 *)
mov L0x20016088 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016094; PC = 0x803848c *)
mov L0x20016094 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160a0; PC = 0x8038490 *)
mov L0x200160a0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200159c8; PC = 0x8038494 *)
mov L0x200159c8 r4;




(**************** CUT 267, - *****************)

ecut and [
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x200159c8*x**2*z** 8 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x200159d4*x**2*z** 8 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x200159e0*x**2*z** 8 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x20016088*x**2*z** 8 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x20016094*x**2*z** 8 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x200160a0*x**2*z** 8 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x200159ec; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x200159ec;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x200159f8; Value = 0xfffffffd; PC = 0x8038240 *)
mov r5 L0x200159f8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a04; Value = 0x00000000; PC = 0x8038244 *)
mov r6 L0x20015a04;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160ac; Value = 0x00000006; PC = 0x8038248 *)
mov r7 L0x200160ac;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160b8; Value = 0x00000003; PC = 0x803824c *)
mov r8 L0x200160b8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160c4; Value = 0x00000003; PC = 0x8038250 *)
mov r9 L0x200160c4;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x200159f8; PC = 0x80382d8 *)
mov L0x200159f8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a04; PC = 0x80382dc *)
mov L0x20015a04 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160ac; PC = 0x80382e0 *)
mov L0x200160ac r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160b8; PC = 0x80382e4 *)
mov L0x200160b8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160c4; PC = 0x80382e8 *)
mov L0x200160c4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x200159ec; PC = 0x80382ec *)
mov L0x200159ec r4;




(**************** CUT 268, - *****************)

ecut and [
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x200159ec*x**2*z** 9 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x200159f8*x**2*z** 9 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x20015a04*x**2*z** 9 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x200160ac*x**2*z** 9 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x200160b8*x**2*z** 9 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x200160c4*x**2*z** 9 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015a10; Value = 0x0014b87b; PC = 0x80382f0 *)
mov r4 L0x20015a10;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a1c; Value = 0xfffffffd; PC = 0x80382f4 *)
mov r5 L0x20015a1c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a28; Value = 0x0014b87b; PC = 0x80382f8 *)
mov r6 L0x20015a28;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160d0; Value = 0xffeb4782; PC = 0x80382fc *)
mov r7 L0x200160d0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160dc; Value = 0x00000003; PC = 0x8038300 *)
mov r8 L0x200160dc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160e8; Value = 0x0014b87e; PC = 0x8038304 *)
mov r9 L0x200160e8;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a1c; PC = 0x80383ac *)
mov L0x20015a1c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a28; PC = 0x80383b0 *)
mov L0x20015a28 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160d0; PC = 0x80383b4 *)
mov L0x200160d0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200160dc; PC = 0x80383b8 *)
mov L0x200160dc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200160e8; PC = 0x80383bc *)
mov L0x200160e8 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a10; PC = 0x80383c0 *)
mov L0x20015a10 r4;




(**************** CUT 269, - *****************)

ecut and [
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x20015a10*x**2*z** 9 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x20015a1c*x**2*z** 9 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x20015a28*x**2*z** 9 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x200160d0*x**2*z** 9 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x200160dc*x**2*z** 9 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x200160e8*x**2*z** 9 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015a34; Value = 0xffeb477c; PC = 0x80383c4 *)
mov r4 L0x20015a34;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a40; Value = 0xfffffffd; PC = 0x80383c8 *)
mov r5 L0x20015a40;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a4c; Value = 0xffeb477c; PC = 0x80383cc *)
mov r6 L0x20015a4c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160f4; Value = 0x0014b881; PC = 0x80383d0 *)
mov r7 L0x200160f4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016100; Value = 0x00000003; PC = 0x80383d4 *)
mov r8 L0x20016100;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001610c; Value = 0xffeb477f; PC = 0x80383d8 *)
mov r9 L0x2001610c;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a40; PC = 0x8038480 *)
mov L0x20015a40 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a4c; PC = 0x8038484 *)
mov L0x20015a4c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200160f4; PC = 0x8038488 *)
mov L0x200160f4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016100; PC = 0x803848c *)
mov L0x20016100 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001610c; PC = 0x8038490 *)
mov L0x2001610c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a34; PC = 0x8038494 *)
mov L0x20015a34 r4;




(**************** CUT 270, - *****************)

ecut and [
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x20015a34*x**2*z** 9 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x20015a40*x**2*z** 9 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x20015a4c*x**2*z** 9 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x200160f4*x**2*z** 9 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x20016100*x**2*z** 9 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x2001610c*x**2*z** 9 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015a58; Value = 0x00000000; PC = 0x803823c *)
mov r4 L0x20015a58;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a64; Value = 0xfffffffd; PC = 0x8038240 *)
mov r5 L0x20015a64;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a70; Value = 0xfffffffa; PC = 0x8038244 *)
mov r6 L0x20015a70;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016118; Value = 0xfffffffd; PC = 0x8038248 *)
mov r7 L0x20016118;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016124; Value = 0xfffffffd; PC = 0x803824c *)
mov r8 L0x20016124;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016130; Value = 0x00000003; PC = 0x8038250 *)
mov r9 L0x20016130;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a64; PC = 0x80382d8 *)
mov L0x20015a64 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a70; PC = 0x80382dc *)
mov L0x20015a70 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016118; PC = 0x80382e0 *)
mov L0x20016118 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016124; PC = 0x80382e4 *)
mov L0x20016124 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016130; PC = 0x80382e8 *)
mov L0x20016130 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a58; PC = 0x80382ec *)
mov L0x20015a58 r4;




(**************** CUT 271, - *****************)

ecut and [
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20015a58*x**2*z**10 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20015a64*x**2*z**10 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20015a70*x**2*z**10 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20016118*x**2*z**10 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20016124*x**2*z**10 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20016130*x**2*z**10 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015a7c; Value = 0xffeb4785; PC = 0x80382f0 *)
mov r4 L0x20015a7c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015a88; Value = 0x0014b881; PC = 0x80382f4 *)
mov r5 L0x20015a88;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015a94; Value = 0x00000003; PC = 0x80382f8 *)
mov r6 L0x20015a94;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001613c; Value = 0x0014b881; PC = 0x80382fc *)
mov r7 L0x2001613c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016148; Value = 0x0014b881; PC = 0x8038300 *)
mov r8 L0x20016148;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016154; Value = 0xfffffffa; PC = 0x8038304 *)
mov r9 L0x20016154;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015a88; PC = 0x80383ac *)
mov L0x20015a88 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015a94; PC = 0x80383b0 *)
mov L0x20015a94 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001613c; PC = 0x80383b4 *)
mov L0x2001613c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016148; PC = 0x80383b8 *)
mov L0x20016148 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016154; PC = 0x80383bc *)
mov L0x20016154 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015a7c; PC = 0x80383c0 *)
mov L0x20015a7c r4;




(**************** CUT 272, - *****************)

ecut and [
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x20015a7c*x**2*z**10 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x20015a88*x**2*z**10 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x20015a94*x**2*z**10 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x2001613c*x**2*z**10 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x20016148*x**2*z**10 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x20016154*x**2*z**10 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015aa0; Value = 0x0014b884; PC = 0x80383c4 *)
mov r4 L0x20015aa0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015aac; Value = 0xffeb4782; PC = 0x80383c8 *)
mov r5 L0x20015aac;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015ab8; Value = 0x00000003; PC = 0x80383cc *)
mov r6 L0x20015ab8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016160; Value = 0xffeb4782; PC = 0x80383d0 *)
mov r7 L0x20016160;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001616c; Value = 0xffeb4782; PC = 0x80383d4 *)
mov r8 L0x2001616c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016178; Value = 0xfffffffa; PC = 0x80383d8 *)
mov r9 L0x20016178;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015aac; PC = 0x8038480 *)
mov L0x20015aac r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015ab8; PC = 0x8038484 *)
mov L0x20015ab8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016160; PC = 0x8038488 *)
mov L0x20016160 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001616c; PC = 0x803848c *)
mov L0x2001616c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016178; PC = 0x8038490 *)
mov L0x20016178 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015aa0; PC = 0x8038494 *)
mov L0x20015aa0 r4;




(**************** CUT 273, - *****************)

ecut and [
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x20015aa0*x**2*z**10 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x20015aac*x**2*z**10 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x20015ab8*x**2*z**10 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x20016160*x**2*z**10 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x2001616c*x**2*z**10 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x20016178*x**2*z**10 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015ac4; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x20015ac4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015ad0; Value = 0x00000000; PC = 0x8038240 *)
mov r5 L0x20015ad0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015adc; Value = 0x00000006; PC = 0x8038244 *)
mov r6 L0x20015adc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016184; Value = 0x00000009; PC = 0x8038248 *)
mov r7 L0x20016184;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016190; Value = 0xfffffffa; PC = 0x803824c *)
mov r8 L0x20016190;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001619c; Value = 0xfffffffd; PC = 0x8038250 *)
mov r9 L0x2001619c;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015ad0; PC = 0x80382d8 *)
mov L0x20015ad0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015adc; PC = 0x80382dc *)
mov L0x20015adc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016184; PC = 0x80382e0 *)
mov L0x20016184 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016190; PC = 0x80382e4 *)
mov L0x20016190 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001619c; PC = 0x80382e8 *)
mov L0x2001619c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015ac4; PC = 0x80382ec *)
mov L0x20015ac4 r4;




(**************** CUT 274, - *****************)

ecut and [
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x20015ac4*x**2*z**11 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x20015ad0*x**2*z**11 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x20015adc*x**2*z**11 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x20016184*x**2*z**11 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x20016190*x**2*z**11 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x2001619c*x**2*z**11 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015ae8; Value = 0x00000006; PC = 0x80382f0 *)
mov r4 L0x20015ae8;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015af4; Value = 0xffd68f01; PC = 0x80382f4 *)
mov r5 L0x20015af4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b00; Value = 0x0014b881; PC = 0x80382f8 *)
mov r6 L0x20015b00;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161a8; Value = 0x00000000; PC = 0x80382fc *)
mov r7 L0x200161a8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161b4; Value = 0x0014b87e; PC = 0x8038300 *)
mov r8 L0x200161b4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161c0; Value = 0xffeb4782; PC = 0x8038304 *)
mov r9 L0x200161c0;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015af4; PC = 0x80383ac *)
mov L0x20015af4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b00; PC = 0x80383b0 *)
mov L0x20015b00 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161a8; PC = 0x80383b4 *)
mov L0x200161a8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161b4; PC = 0x80383b8 *)
mov L0x200161b4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161c0; PC = 0x80383bc *)
mov L0x200161c0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015ae8; PC = 0x80383c0 *)
mov L0x20015ae8 r4;




(**************** CUT 275, - *****************)

ecut and [
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x20015ae8*x**2*z**11 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x20015af4*x**2*z**11 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x20015b00*x**2*z**11 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x200161a8*x**2*z**11 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x200161b4*x**2*z**11 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x200161c0*x**2*z**11 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015b0c; Value = 0x00000006; PC = 0x80383c4 *)
mov r4 L0x20015b0c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b18; Value = 0x002970ff; PC = 0x80383c8 *)
mov r5 L0x20015b18;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b24; Value = 0xffeb4782; PC = 0x80383cc *)
mov r6 L0x20015b24;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161cc; Value = 0x00000000; PC = 0x80383d0 *)
mov r7 L0x200161cc;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161d8; Value = 0xffeb477f; PC = 0x80383d4 *)
mov r8 L0x200161d8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161e4; Value = 0x0014b881; PC = 0x80383d8 *)
mov r9 L0x200161e4;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b18; PC = 0x8038480 *)
mov L0x20015b18 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b24; PC = 0x8038484 *)
mov L0x20015b24 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161cc; PC = 0x8038488 *)
mov L0x200161cc r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161d8; PC = 0x803848c *)
mov L0x200161d8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200161e4; PC = 0x8038490 *)
mov L0x200161e4 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b0c; PC = 0x8038494 *)
mov L0x20015b0c r4;




(**************** CUT 276, - *****************)

ecut and [
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x20015b0c*x**2*z**11 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x20015b18*x**2*z**11 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x20015b24*x**2*z**11 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x200161cc*x**2*z**11 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x200161d8*x**2*z**11 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x200161e4*x**2*z**11 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015b30; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x20015b30;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b3c; Value = 0xfffffffd; PC = 0x8038240 *)
mov r5 L0x20015b3c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b48; Value = 0x00000003; PC = 0x8038244 *)
mov r6 L0x20015b48;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161f0; Value = 0x00000003; PC = 0x8038248 *)
mov r7 L0x200161f0;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161fc; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x200161fc;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016208; Value = 0x00000003; PC = 0x8038250 *)
mov r9 L0x20016208;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b3c; PC = 0x80382d8 *)
mov L0x20015b3c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b48; PC = 0x80382dc *)
mov L0x20015b48 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200161f0; PC = 0x80382e0 *)
mov L0x200161f0 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200161fc; PC = 0x80382e4 *)
mov L0x200161fc r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016208; PC = 0x80382e8 *)
mov L0x20016208 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b30; PC = 0x80382ec *)
mov L0x20015b30 r4;




(**************** CUT 277, - *****************)

ecut and [
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x20015b30*x**2*z**12 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x20015b3c*x**2*z**12 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x20015b48*x**2*z**12 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x200161f0*x**2*z**12 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x200161fc*x**2*z**12 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x20016208*x**2*z**12 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015b54; Value = 0xfffffffd; PC = 0x80382f0 *)
mov r4 L0x20015b54;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b60; Value = 0x00000006; PC = 0x80382f4 *)
mov r5 L0x20015b60;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b6c; Value = 0xffd68f04; PC = 0x80382f8 *)
mov r6 L0x20015b6c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016214; Value = 0x00297102; PC = 0x80382fc *)
mov r7 L0x20016214;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016220; Value = 0x002970ff; PC = 0x8038300 *)
mov r8 L0x20016220;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001622c; Value = 0xffeb477f; PC = 0x8038304 *)
mov r9 L0x2001622c;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b60; PC = 0x80383ac *)
mov L0x20015b60 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b6c; PC = 0x80383b0 *)
mov L0x20015b6c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016214; PC = 0x80383b4 *)
mov L0x20016214 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016220; PC = 0x80383b8 *)
mov L0x20016220 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001622c; PC = 0x80383bc *)
mov L0x2001622c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b54; PC = 0x80383c0 *)
mov L0x20015b54 r4;




(**************** CUT 278, - *****************)

ecut and [
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x20015b54*x**2*z**12 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x20015b60*x**2*z**12 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x20015b6c*x**2*z**12 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x20016214*x**2*z**12 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x20016220*x**2*z**12 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x2001622c*x**2*z**12 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015b78; Value = 0xfffffffd; PC = 0x80383c4 *)
mov r4 L0x20015b78;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015b84; Value = 0x00000006; PC = 0x80383c8 *)
mov r5 L0x20015b84;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015b90; Value = 0x00297102; PC = 0x80383cc *)
mov r6 L0x20015b90;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016238; Value = 0xffd68f04; PC = 0x80383d0 *)
mov r7 L0x20016238;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016244; Value = 0xffd68f01; PC = 0x80383d4 *)
mov r8 L0x20016244;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016250; Value = 0x0014b87e; PC = 0x80383d8 *)
mov r9 L0x20016250;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015b84; PC = 0x8038480 *)
mov L0x20015b84 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015b90; PC = 0x8038484 *)
mov L0x20015b90 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016238; PC = 0x8038488 *)
mov L0x20016238 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016244; PC = 0x803848c *)
mov L0x20016244 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016250; PC = 0x8038490 *)
mov L0x20016250 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b78; PC = 0x8038494 *)
mov L0x20015b78 r4;




(**************** CUT 279, - *****************)

ecut and [
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20015b78*x**2*z**12 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20015b84*x**2*z**12 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20015b90*x**2*z**12 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20016238*x**2*z**12 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20016244*x**2*z**12 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20016250*x**2*z**12 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015b9c; Value = 0x00000003; PC = 0x803823c *)
mov r4 L0x20015b9c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015ba8; Value = 0x00000006; PC = 0x8038240 *)
mov r5 L0x20015ba8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015bb4; Value = 0x00000000; PC = 0x8038244 *)
mov r6 L0x20015bb4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001625c; Value = 0xfffffffd; PC = 0x8038248 *)
mov r7 L0x2001625c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016268; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20016268;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016274; Value = 0x00000000; PC = 0x8038250 *)
mov r9 L0x20016274;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015ba8; PC = 0x80382d8 *)
mov L0x20015ba8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015bb4; PC = 0x80382dc *)
mov L0x20015bb4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001625c; PC = 0x80382e0 *)
mov L0x2001625c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016268; PC = 0x80382e4 *)
mov L0x20016268 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016274; PC = 0x80382e8 *)
mov L0x20016274 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015b9c; PC = 0x80382ec *)
mov L0x20015b9c r4;




(**************** CUT 280, - *****************)

ecut and [
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x20015b9c*x**2*z**13 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x20015ba8*x**2*z**13 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x20015bb4*x**2*z**13 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x2001625c*x**2*z**13 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x20016268*x**2*z**13 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x20016274*x**2*z**13 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015bc0; Value = 0x00297102; PC = 0x80382f0 *)
mov r4 L0x20015bc0;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015bcc; Value = 0xfffffffd; PC = 0x80382f4 *)
mov r5 L0x20015bcc;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015bd8; Value = 0xfff6163e; PC = 0x80382f8 *)
mov r6 L0x20015bd8;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016280; Value = 0xfffffffd; PC = 0x80382fc *)
mov r7 L0x20016280;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001628c; Value = 0x00000000; PC = 0x8038300 *)
mov r8 L0x2001628c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016298; Value = 0xffeb4785; PC = 0x8038304 *)
mov r9 L0x20016298;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015bcc; PC = 0x80383ac *)
mov L0x20015bcc r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015bd8; PC = 0x80383b0 *)
mov L0x20015bd8 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016280; PC = 0x80383b4 *)
mov L0x20016280 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001628c; PC = 0x80383b8 *)
mov L0x2001628c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016298; PC = 0x80383bc *)
mov L0x20016298 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015bc0; PC = 0x80383c0 *)
mov L0x20015bc0 r4;




(**************** CUT 281, - *****************)

ecut and [
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x20015bc0*x**2*z**13 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x20015bcc*x**2*z**13 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x20015bd8*x**2*z**13 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x20016280*x**2*z**13 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x2001628c*x**2*z**13 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x20016298*x**2*z**13 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015be4; Value = 0xffd68f04; PC = 0x80383c4 *)
mov r4 L0x20015be4;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015bf0; Value = 0xfffffffd; PC = 0x80383c8 *)
mov r5 L0x20015bf0;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015bfc; Value = 0x0009e9c2; PC = 0x80383cc *)
mov r6 L0x20015bfc;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162a4; Value = 0xfffffffd; PC = 0x80383d0 *)
mov r7 L0x200162a4;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162b0; Value = 0x00000000; PC = 0x80383d4 *)
mov r8 L0x200162b0;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162bc; Value = 0x0014b884; PC = 0x80383d8 *)
mov r9 L0x200162bc;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015bf0; PC = 0x8038480 *)
mov L0x20015bf0 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015bfc; PC = 0x8038484 *)
mov L0x20015bfc r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162a4; PC = 0x8038488 *)
mov L0x200162a4 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162b0; PC = 0x803848c *)
mov L0x200162b0 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162bc; PC = 0x8038490 *)
mov L0x200162bc r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015be4; PC = 0x8038494 *)
mov L0x20015be4 r4;




(**************** CUT 282, - *****************)

ecut and [
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x20015be4*x**2*z**13 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x20015bf0*x**2*z**13 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x20015bfc*x**2*z**13 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x200162a4*x**2*z**13 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x200162b0*x**2*z**13 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x200162bc*x**2*z**13 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015c08; Value = 0xfffffffd; PC = 0x803823c *)
mov r4 L0x20015c08;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c14; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x20015c14;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c20; Value = 0x00000000; PC = 0x8038244 *)
mov r6 L0x20015c20;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162c8; Value = 0xfffffffd; PC = 0x8038248 *)
mov r7 L0x200162c8;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162d4; Value = 0xfffffffa; PC = 0x803824c *)
mov r8 L0x200162d4;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162e0; Value = 0x00000003; PC = 0x8038250 *)
mov r9 L0x200162e0;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c14; PC = 0x80382d8 *)
mov L0x20015c14 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c20; PC = 0x80382dc *)
mov L0x20015c20 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162c8; PC = 0x80382e0 *)
mov L0x200162c8 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162d4; PC = 0x80382e4 *)
mov L0x200162d4 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x200162e0; PC = 0x80382e8 *)
mov L0x200162e0 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c08; PC = 0x80382ec *)
mov L0x20015c08 r4;




(**************** CUT 283, - *****************)

ecut and [
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x20015c08*x**2*z**14 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x20015c14*x**2*z**14 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x20015c20*x**2*z**14 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x200162c8*x**2*z**14 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x200162d4*x**2*z**14 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x200162e0*x**2*z**14 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015c2c; Value = 0x0014b881; PC = 0x80382f0 *)
mov r4 L0x20015c2c;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c38; Value = 0x0014b87e; PC = 0x80382f4 *)
mov r5 L0x20015c38;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c44; Value = 0xffeb477c; PC = 0x80382f8 *)
mov r6 L0x20015c44;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162ec; Value = 0xffeb4782; PC = 0x80382fc *)
mov r7 L0x200162ec;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162f8; Value = 0xffeb477f; PC = 0x8038300 *)
mov r8 L0x200162f8;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016304; Value = 0x0014b87e; PC = 0x8038304 *)
mov r9 L0x20016304;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c38; PC = 0x80383ac *)
mov L0x20015c38 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c44; PC = 0x80383b0 *)
mov L0x20015c44 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x200162ec; PC = 0x80383b4 *)
mov L0x200162ec r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x200162f8; PC = 0x80383b8 *)
mov L0x200162f8 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016304; PC = 0x80383bc *)
mov L0x20016304 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c2c; PC = 0x80383c0 *)
mov L0x20015c2c r4;




(**************** CUT 284, - *****************)

ecut and [
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x20015c2c*x**2*z**14 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x20015c38*x**2*z**14 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x20015c44*x**2*z**14 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x200162ec*x**2*z**14 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x200162f8*x**2*z**14 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x20016304*x**2*z**14 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015c50; Value = 0xffeb4782; PC = 0x80383c4 *)
mov r4 L0x20015c50;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c5c; Value = 0xffeb477f; PC = 0x80383c8 *)
mov r5 L0x20015c5c;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c68; Value = 0x0014b87b; PC = 0x80383cc *)
mov r6 L0x20015c68;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016310; Value = 0x0014b881; PC = 0x80383d0 *)
mov r7 L0x20016310;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001631c; Value = 0x0014b87e; PC = 0x80383d4 *)
mov r8 L0x2001631c;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016328; Value = 0xffeb477f; PC = 0x80383d8 *)
mov r9 L0x20016328;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c5c; PC = 0x8038480 *)
mov L0x20015c5c r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c68; PC = 0x8038484 *)
mov L0x20015c68 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016310; PC = 0x8038488 *)
mov L0x20016310 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x2001631c; PC = 0x803848c *)
mov L0x2001631c r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016328; PC = 0x8038490 *)
mov L0x20016328 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c50; PC = 0x8038494 *)
mov L0x20015c50 r4;




(**************** CUT 285, - *****************)

ecut and [
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x20015c50*x**2*z**14 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x20015c5c*x**2*z**14 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x20015c68*x**2*z**14 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x20016310*x**2*z**14 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x2001631c*x**2*z**14 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x20016328*x**2*z**14 [ 3365569, y - 3070820, z**16 -  735217 ]
];



(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* ldr.w	r4, [r0]                                  #! EA = L0x20015c74; Value = 0x00000006; PC = 0x803823c *)
mov r4 L0x20015c74;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015c80; Value = 0x00000003; PC = 0x8038240 *)
mov r5 L0x20015c80;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015c8c; Value = 0x00000003; PC = 0x8038244 *)
mov r6 L0x20015c8c;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016334; Value = 0x00000006; PC = 0x8038248 *)
mov r7 L0x20016334;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016340; Value = 0x00000000; PC = 0x803824c *)
mov r8 L0x20016340;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001634c; Value = 0x00000006; PC = 0x8038250 *)
mov r9 L0x2001634c;
(* vmov	r1, s13                                    #! PC = 0x8038254 *)
mov r1 s13;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038258 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803825c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038260 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s14                                    #! PC = 0x8038264 *)
mov r1 s14;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038268 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803826c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038270 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s15                                    #! PC = 0x8038274 *)
mov r1 s15;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038278 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x803827c *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038280 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038284 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x8038286 *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038288 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x803828c *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038290 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038294 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038298 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x803829c *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x80382a0 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x80382a4 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x80382a8 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x80382ac *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x80382b0 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x80382b4 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x80382b6 *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x80382b8 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x80382bc *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x80382c0 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x80382c4 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x80382c8 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80382cc *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80382d0 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80382d4 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80382d6 *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015c80; PC = 0x80382d8 *)
mov L0x20015c80 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015c8c; PC = 0x80382dc *)
mov L0x20015c8c r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016334; PC = 0x80382e0 *)
mov L0x20016334 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016340; PC = 0x80382e4 *)
mov L0x20016340 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x2001634c; PC = 0x80382e8 *)
mov L0x2001634c r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c74; PC = 0x80382ec *)
mov L0x20015c74 r4;




(**************** CUT 286, - *****************)

ecut and [
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x20015c74*x**2*z**15 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x20015c80*x**2*z**15 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x20015c8c*x**2*z**15 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x20016334*x**2*z**15 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x20016340*x**2*z**15 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x2001634c*x**2*z**15 [ 3365569, y - 2912918, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015c98; Value = 0xffeb4782; PC = 0x80382f0 *)
mov r4 L0x20015c98;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015ca4; Value = 0x0014b87e; PC = 0x80382f4 *)
mov r5 L0x20015ca4;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015cb0; Value = 0xffeb477f; PC = 0x80382f8 *)
mov r6 L0x20015cb0;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016358; Value = 0xfffffffd; PC = 0x80382fc *)
mov r7 L0x20016358;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016364; Value = 0xffeb4785; PC = 0x8038300 *)
mov r8 L0x20016364;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016370; Value = 0x0014b881; PC = 0x8038304 *)
mov r9 L0x20016370;
(* vmov	r1, s8                                     #! PC = 0x8038308 *)
mov r1 s8;
(* smull	r12, r5, r5, r1                           #! PC = 0x803830c *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038310 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x8038314 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s9                                     #! PC = 0x8038318 *)
mov r1 s9;
(* smull	r12, r6, r6, r1                           #! PC = 0x803831c *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038320 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x8038324 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s16                                    #! PC = 0x8038328 *)
mov r1 s16;
(* smull	r12, r7, r7, r1                           #! PC = 0x803832c *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038330 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038334 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s17                                    #! PC = 0x8038338 *)
mov r1 s17;
(* smull	r12, r8, r8, r1                           #! PC = 0x803833c *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038340 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038344 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s18                                    #! PC = 0x8038348 *)
mov r1 s18;
(* smull	r12, r9, r9, r1                           #! PC = 0x803834c *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038350 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038354 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x8038358 *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803835a *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x803835c *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038360 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038364 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x8038368 *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x803836c *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038370 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038374 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x8038378 *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x803837c *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038380 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038384 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x8038388 *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803838a *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x803838c *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038390 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038394 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x8038398 *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x803839c *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x80383a0 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x80383a4 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x80383a8 *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x80383aa *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015ca4; PC = 0x80383ac *)
mov L0x20015ca4 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015cb0; PC = 0x80383b0 *)
mov L0x20015cb0 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x20016358; PC = 0x80383b4 *)
mov L0x20016358 r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016364; PC = 0x80383b8 *)
mov L0x20016364 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016370; PC = 0x80383bc *)
mov L0x20016370 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015c98; PC = 0x80383c0 *)
mov L0x20015c98 r4;




(**************** CUT 287, - *****************)

ecut and [
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20015c98*x**2*z**15 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20015ca4*x**2*z**15 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20015cb0*x**2*z**15 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20016358*x**2*z**15 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20016364*x**2*z**15 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20016370*x**2*z**15 [ 3365569, y - 1540404, z**16 -  735217 ]
];



(* ldr.w	r4, [r0]                                  #! EA = L0x20015cbc; Value = 0x0014b881; PC = 0x80383c4 *)
mov r4 L0x20015cbc;
(* ldr.w	r5, [r0, #12]                             #! EA = L0x20015cc8; Value = 0xffeb477f; PC = 0x80383c8 *)
mov r5 L0x20015cc8;
(* ldr.w	r6, [r0, #24]                             #! EA = L0x20015cd4; Value = 0x0014b87e; PC = 0x80383cc *)
mov r6 L0x20015cd4;
(* ldr.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001637c; Value = 0xfffffffd; PC = 0x80383d0 *)
mov r7 L0x2001637c;
(* ldr.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016388; Value = 0x0014b884; PC = 0x80383d4 *)
mov r8 L0x20016388;
(* ldr.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016394; Value = 0xffeb4782; PC = 0x80383d8 *)
mov r9 L0x20016394;
(* vmov	r1, s11                                    #! PC = 0x80383dc *)
mov r1 s11;
(* smull	r12, r5, r5, r1                           #! PC = 0x80383e0 *)
smull r5 r12 r5 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383e4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r5, lr, r3                           #! PC = 0x80383e8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s12                                    #! PC = 0x80383ec *)
mov r1 s12;
(* smull	r12, r6, r6, r1                           #! PC = 0x80383f0 *)
smull r6 r12 r6 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x80383f4 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r6, lr, r3                           #! PC = 0x80383f8 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r6 r6 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s19                                    #! PC = 0x80383fc *)
mov r1 s19;
(* smull	r12, r7, r7, r1                           #! PC = 0x8038400 *)
smull r7 r12 r7 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038404 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r7, lr, r3                           #! PC = 0x8038408 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r7 r7 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s20                                    #! PC = 0x803840c *)
mov r1 s20;
(* smull	r12, r8, r8, r1                           #! PC = 0x8038410 *)
smull r8 r12 r8 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038414 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r8, lr, r3                           #! PC = 0x8038418 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* vmov	r1, s21                                    #! PC = 0x803841c *)
mov r1 s21;
(* smull	r12, r9, r9, r1                           #! PC = 0x8038420 *)
smull r9 r12 r9 r1;
(* mul.w	lr, r12, r2                               #! PC = 0x8038424 *)
mull dontcare lr r12 r2;
cast lr@sint32 lr;
(* smlal	r12, r9, lr, r3                           #! PC = 0x8038428 *)
smull tmpml_h tmpml_l lr r3;
adds carry r12 r12 tmpml_l;
adc r9 r9 tmpml_h carry;
assert eqmod r12 0 (2**32) && true;
assume r12 = 0 && true;
(* add	r4, r7                                      #! PC = 0x803842c *)
add r4 r4 r7;
(* add	r5, r8                                      #! PC = 0x803842e *)
add r5 r5 r8;
(* sub.w	r7, r4, r7, lsl #1                        #! PC = 0x8038430 *)
shl tmpx2 r7 1;
sub r7 r4 tmpx2;
(* sub.w	r8, r5, r8, lsl #1                        #! PC = 0x8038434 *)
shl tmpx2 r8 1;
sub r8 r5 tmpx2;
(* add.w	r6, r6, r9                                #! PC = 0x8038438 *)
add r6 r6 r9;
(* sub.w	r9, r6, r9, lsl #1                        #! PC = 0x803843c *)
shl tmpx2 r9 1;
sub r9 r6 tmpx2;
(* add.w	r1, r5, r6                                #! PC = 0x8038440 *)
add r1 r5 r6;
(* smull	lr, r5, r5, r10                           #! PC = 0x8038444 *)
smull r5 lr r5 r10;
(* smlal	lr, r5, r6, r11                           #! PC = 0x8038448 *)
smull tmpml_h tmpml_l r6 r11;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
(* mul.w	r6, lr, r2                                #! PC = 0x803844c *)
mull dontcare r6 lr r2;
cast r6@sint32 r6;
(* smlal	lr, r5, r6, r3                            #! PC = 0x8038450 *)
smull tmpml_h tmpml_l r6 r3;
adds carry lr lr tmpml_l;
adc r5 r5 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r6, r5, r1                                #! PC = 0x8038454 *)
add r6 r5 r1;
(* rsb	r6, r6, r4                                  #! PC = 0x8038458 *)
sub r6 r4 r6;
(* add	r5, r4                                      #! PC = 0x803845c *)
add r5 r5 r4;
(* add	r4, r1                                      #! PC = 0x803845e *)
add r4 r4 r1;
(* add.w	r1, r8, r9                                #! PC = 0x8038460 *)
add r1 r8 r9;
(* smull	lr, r8, r8, r10                           #! PC = 0x8038464 *)
smull r8 lr r8 r10;
(* smlal	lr, r8, r9, r11                           #! PC = 0x8038468 *)
smull tmpml_h tmpml_l r9 r11;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
(* mul.w	r9, lr, r2                                #! PC = 0x803846c *)
mull dontcare r9 lr r2;
cast r9@sint32 r9;
(* smlal	lr, r8, r9, r3                            #! PC = 0x8038470 *)
smull tmpml_h tmpml_l r9 r3;
adds carry lr lr tmpml_l;
adc r8 r8 tmpml_h carry;
assert eqmod lr 0 (2**32) && true;
assume lr = 0 && true;
(* add.w	r9, r8, r1                                #! PC = 0x8038474 *)
add r9 r8 r1;
(* rsb	r9, r9, r7                                  #! PC = 0x8038478 *)
sub r9 r7 r9;
(* add	r8, r7                                      #! PC = 0x803847c *)
add r8 r8 r7;
(* add	r7, r1                                      #! PC = 0x803847e *)
add r7 r7 r1;
(* str.w	r5, [r0, #12]                             #! EA = L0x20015cc8; PC = 0x8038480 *)
mov L0x20015cc8 r5;
(* str.w	r6, [r0, #24]                             #! EA = L0x20015cd4; PC = 0x8038484 *)
mov L0x20015cd4 r6;
(* str.w	r7, [r0, #1728]	; 0x6c0                   #! EA = L0x2001637c; PC = 0x8038488 *)
mov L0x2001637c r7;
(* str.w	r8, [r0, #1740]	; 0x6cc                   #! EA = L0x20016388; PC = 0x803848c *)
mov L0x20016388 r8;
(* str.w	r9, [r0, #1752]	; 0x6d8                   #! EA = L0x20016394; PC = 0x8038490 *)
mov L0x20016394 r9;
(* str.w	r4, [r0], #36                             #! EA = L0x20015cbc; PC = 0x8038494 *)
mov L0x20015cbc r4;




(**************** CUT 288, - *****************)

ecut and [
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x20015cbc*x**2*z**15 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x20015cc8*x**2*z**15 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x20015cd4*x**2*z**15 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x2001637c*x**2*z**15 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x20016388*x**2*z**15 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x20016394*x**2*z**15 [ 3365569, y - 3070820, z**16 -  735217 ]
];

(**************** CUT -,   1 *****************)

rcut and [
(-20207750)@32 <=s L0x20015618, L0x20015618 <=s 20207750@32,
(-16831941)@32 <=s L0x20015624, L0x20015624 <=s 16831941@32,
(-23565127)@32 <=s L0x20015630, L0x20015630 <=s 23565127@32,
(-20207750)@32 <=s L0x20015cd8, L0x20015cd8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ce4, L0x20015ce4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cf0, L0x20015cf0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001563c, L0x2001563c <=s 20207750@32,
(-16831941)@32 <=s L0x20015648, L0x20015648 <=s 16831941@32,
(-23565127)@32 <=s L0x20015654, L0x20015654 <=s 23565127@32,
(-20207750)@32 <=s L0x20015cfc, L0x20015cfc <=s 20207750@32,
(-16831941)@32 <=s L0x20015d08, L0x20015d08 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d14, L0x20015d14 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015660, L0x20015660 <=s 20207750@32,
(-16831941)@32 <=s L0x2001566c, L0x2001566c <=s 16831941@32,
(-23565127)@32 <=s L0x20015678, L0x20015678 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d20, L0x20015d20 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d2c, L0x20015d2c <=s 16831941@32,
(-23565127)@32 <=s L0x20015d38, L0x20015d38 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015684, L0x20015684 <=s 20207750@32,
(-16831941)@32 <=s L0x20015690, L0x20015690 <=s 16831941@32,
(-23565127)@32 <=s L0x2001569c, L0x2001569c <=s 23565127@32,
(-20207750)@32 <=s L0x20015d44, L0x20015d44 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d50, L0x20015d50 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d5c, L0x20015d5c <=s 23565127@32
,
(-20207750)@32 <=s L0x200156a8, L0x200156a8 <=s 20207750@32,
(-16831941)@32 <=s L0x200156b4, L0x200156b4 <=s 16831941@32,
(-23565127)@32 <=s L0x200156c0, L0x200156c0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d68, L0x20015d68 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d74, L0x20015d74 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d80, L0x20015d80 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156cc, L0x200156cc <=s 20207750@32,
(-16831941)@32 <=s L0x200156d8, L0x200156d8 <=s 16831941@32,
(-23565127)@32 <=s L0x200156e4, L0x200156e4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d8c, L0x20015d8c <=s 20207750@32,
(-16831941)@32 <=s L0x20015d98, L0x20015d98 <=s 16831941@32,
(-23565127)@32 <=s L0x20015da4, L0x20015da4 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156f0, L0x200156f0 <=s 20207750@32,
(-16831941)@32 <=s L0x200156fc, L0x200156fc <=s 16831941@32,
(-23565127)@32 <=s L0x20015708, L0x20015708 <=s 23565127@32,
(-20207750)@32 <=s L0x20015db0, L0x20015db0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015dbc, L0x20015dbc <=s 16831941@32,
(-23565127)@32 <=s L0x20015dc8, L0x20015dc8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015714, L0x20015714 <=s 20207750@32,
(-16831941)@32 <=s L0x20015720, L0x20015720 <=s 16831941@32,
(-23565127)@32 <=s L0x2001572c, L0x2001572c <=s 23565127@32,
(-20207750)@32 <=s L0x20015dd4, L0x20015dd4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015de0, L0x20015de0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015dec, L0x20015dec <=s 23565127@32
,
(-20207750)@32 <=s L0x20015738, L0x20015738 <=s 20207750@32,
(-16831941)@32 <=s L0x20015744, L0x20015744 <=s 16831941@32,
(-23565127)@32 <=s L0x20015750, L0x20015750 <=s 23565127@32,
(-20207750)@32 <=s L0x20015df8, L0x20015df8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e04, L0x20015e04 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e10, L0x20015e10 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001575c, L0x2001575c <=s 20207750@32,
(-16831941)@32 <=s L0x20015768, L0x20015768 <=s 16831941@32,
(-23565127)@32 <=s L0x20015774, L0x20015774 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e1c, L0x20015e1c <=s 20207750@32,
(-16831941)@32 <=s L0x20015e28, L0x20015e28 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e34, L0x20015e34 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015780, L0x20015780 <=s 20207750@32,
(-16831941)@32 <=s L0x2001578c, L0x2001578c <=s 16831941@32,
(-23565127)@32 <=s L0x20015798, L0x20015798 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e40, L0x20015e40 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e4c, L0x20015e4c <=s 16831941@32,
(-23565127)@32 <=s L0x20015e58, L0x20015e58 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157a4, L0x200157a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200157b0, L0x200157b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200157bc, L0x200157bc <=s 23565127@32,
(-20207750)@32 <=s L0x20015e64, L0x20015e64 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e70, L0x20015e70 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e7c, L0x20015e7c <=s 23565127@32
,
(-20207750)@32 <=s L0x200157c8, L0x200157c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200157d4, L0x200157d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200157e0, L0x200157e0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e88, L0x20015e88 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e94, L0x20015e94 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ea0, L0x20015ea0 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157ec, L0x200157ec <=s 20207750@32,
(-16831941)@32 <=s L0x200157f8, L0x200157f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015804, L0x20015804 <=s 23565127@32,
(-20207750)@32 <=s L0x20015eac, L0x20015eac <=s 20207750@32,
(-16831941)@32 <=s L0x20015eb8, L0x20015eb8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ec4, L0x20015ec4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015810, L0x20015810 <=s 20207750@32,
(-16831941)@32 <=s L0x2001581c, L0x2001581c <=s 16831941@32,
(-23565127)@32 <=s L0x20015828, L0x20015828 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ed0, L0x20015ed0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015edc, L0x20015edc <=s 16831941@32,
(-23565127)@32 <=s L0x20015ee8, L0x20015ee8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015834, L0x20015834 <=s 20207750@32,
(-16831941)@32 <=s L0x20015840, L0x20015840 <=s 16831941@32,
(-23565127)@32 <=s L0x2001584c, L0x2001584c <=s 23565127@32,
(-20207750)@32 <=s L0x20015ef4, L0x20015ef4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f00, L0x20015f00 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f0c, L0x20015f0c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015858, L0x20015858 <=s 20207750@32,
(-16831941)@32 <=s L0x20015864, L0x20015864 <=s 16831941@32,
(-23565127)@32 <=s L0x20015870, L0x20015870 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f18, L0x20015f18 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f24, L0x20015f24 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f30, L0x20015f30 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001587c, L0x2001587c <=s 20207750@32,
(-16831941)@32 <=s L0x20015888, L0x20015888 <=s 16831941@32,
(-23565127)@32 <=s L0x20015894, L0x20015894 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f3c, L0x20015f3c <=s 20207750@32,
(-16831941)@32 <=s L0x20015f48, L0x20015f48 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f54, L0x20015f54 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158a0, L0x200158a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200158ac, L0x200158ac <=s 16831941@32,
(-23565127)@32 <=s L0x200158b8, L0x200158b8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f60, L0x20015f60 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f6c, L0x20015f6c <=s 16831941@32,
(-23565127)@32 <=s L0x20015f78, L0x20015f78 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158c4, L0x200158c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200158d0, L0x200158d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200158dc, L0x200158dc <=s 23565127@32,
(-20207750)@32 <=s L0x20015f84, L0x20015f84 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f90, L0x20015f90 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f9c, L0x20015f9c <=s 23565127@32
,
(-20207750)@32 <=s L0x200158e8, L0x200158e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200158f4, L0x200158f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015900, L0x20015900 <=s 23565127@32,
(-20207750)@32 <=s L0x20015fa8, L0x20015fa8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015fb4, L0x20015fb4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fc0, L0x20015fc0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001590c, L0x2001590c <=s 20207750@32,
(-16831941)@32 <=s L0x20015918, L0x20015918 <=s 16831941@32,
(-23565127)@32 <=s L0x20015924, L0x20015924 <=s 23565127@32,
(-20207750)@32 <=s L0x20015fcc, L0x20015fcc <=s 20207750@32,
(-16831941)@32 <=s L0x20015fd8, L0x20015fd8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fe4, L0x20015fe4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015930, L0x20015930 <=s 20207750@32,
(-16831941)@32 <=s L0x2001593c, L0x2001593c <=s 16831941@32,
(-23565127)@32 <=s L0x20015948, L0x20015948 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ff0, L0x20015ff0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ffc, L0x20015ffc <=s 16831941@32,
(-23565127)@32 <=s L0x20016008, L0x20016008 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015954, L0x20015954 <=s 20207750@32,
(-16831941)@32 <=s L0x20015960, L0x20015960 <=s 16831941@32,
(-23565127)@32 <=s L0x2001596c, L0x2001596c <=s 23565127@32,
(-20207750)@32 <=s L0x20016014, L0x20016014 <=s 20207750@32,
(-16831941)@32 <=s L0x20016020, L0x20016020 <=s 16831941@32,
(-23565127)@32 <=s L0x2001602c, L0x2001602c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015978, L0x20015978 <=s 20207750@32,
(-16831941)@32 <=s L0x20015984, L0x20015984 <=s 16831941@32,
(-23565127)@32 <=s L0x20015990, L0x20015990 <=s 23565127@32,
(-20207750)@32 <=s L0x20016038, L0x20016038 <=s 20207750@32,
(-16831941)@32 <=s L0x20016044, L0x20016044 <=s 16831941@32,
(-23565127)@32 <=s L0x20016050, L0x20016050 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001599c, L0x2001599c <=s 20207750@32,
(-16831941)@32 <=s L0x200159a8, L0x200159a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200159b4, L0x200159b4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001605c, L0x2001605c <=s 20207750@32,
(-16831941)@32 <=s L0x20016068, L0x20016068 <=s 16831941@32,
(-23565127)@32 <=s L0x20016074, L0x20016074 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159c0, L0x200159c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200159cc, L0x200159cc <=s 16831941@32,
(-23565127)@32 <=s L0x200159d8, L0x200159d8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016080, L0x20016080 <=s 20207750@32,
(-16831941)@32 <=s L0x2001608c, L0x2001608c <=s 16831941@32,
(-23565127)@32 <=s L0x20016098, L0x20016098 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159e4, L0x200159e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200159f0, L0x200159f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200159fc, L0x200159fc <=s 23565127@32,
(-20207750)@32 <=s L0x200160a4, L0x200160a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200160b0, L0x200160b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200160bc, L0x200160bc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a08, L0x20015a08 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a14, L0x20015a14 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a20, L0x20015a20 <=s 23565127@32,
(-20207750)@32 <=s L0x200160c8, L0x200160c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200160d4, L0x200160d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200160e0, L0x200160e0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a2c, L0x20015a2c <=s 20207750@32,
(-16831941)@32 <=s L0x20015a38, L0x20015a38 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a44, L0x20015a44 <=s 23565127@32,
(-20207750)@32 <=s L0x200160ec, L0x200160ec <=s 20207750@32,
(-16831941)@32 <=s L0x200160f8, L0x200160f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20016104, L0x20016104 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a50, L0x20015a50 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a5c, L0x20015a5c <=s 16831941@32,
(-23565127)@32 <=s L0x20015a68, L0x20015a68 <=s 23565127@32,
(-20207750)@32 <=s L0x20016110, L0x20016110 <=s 20207750@32,
(-16831941)@32 <=s L0x2001611c, L0x2001611c <=s 16831941@32,
(-23565127)@32 <=s L0x20016128, L0x20016128 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a74, L0x20015a74 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a80, L0x20015a80 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a8c, L0x20015a8c <=s 23565127@32,
(-20207750)@32 <=s L0x20016134, L0x20016134 <=s 20207750@32,
(-16831941)@32 <=s L0x20016140, L0x20016140 <=s 16831941@32,
(-23565127)@32 <=s L0x2001614c, L0x2001614c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a98, L0x20015a98 <=s 20207750@32,
(-16831941)@32 <=s L0x20015aa4, L0x20015aa4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ab0, L0x20015ab0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016158, L0x20016158 <=s 20207750@32,
(-16831941)@32 <=s L0x20016164, L0x20016164 <=s 16831941@32,
(-23565127)@32 <=s L0x20016170, L0x20016170 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015abc, L0x20015abc <=s 20207750@32,
(-16831941)@32 <=s L0x20015ac8, L0x20015ac8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ad4, L0x20015ad4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001617c, L0x2001617c <=s 20207750@32,
(-16831941)@32 <=s L0x20016188, L0x20016188 <=s 16831941@32,
(-23565127)@32 <=s L0x20016194, L0x20016194 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015ae0, L0x20015ae0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015aec, L0x20015aec <=s 16831941@32,
(-23565127)@32 <=s L0x20015af8, L0x20015af8 <=s 23565127@32,
(-20207750)@32 <=s L0x200161a0, L0x200161a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200161ac, L0x200161ac <=s 16831941@32,
(-23565127)@32 <=s L0x200161b8, L0x200161b8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b04, L0x20015b04 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b10, L0x20015b10 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b1c, L0x20015b1c <=s 23565127@32,
(-20207750)@32 <=s L0x200161c4, L0x200161c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200161d0, L0x200161d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200161dc, L0x200161dc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b28, L0x20015b28 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b34, L0x20015b34 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b40, L0x20015b40 <=s 23565127@32,
(-20207750)@32 <=s L0x200161e8, L0x200161e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200161f4, L0x200161f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20016200, L0x20016200 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b4c, L0x20015b4c <=s 20207750@32,
(-16831941)@32 <=s L0x20015b58, L0x20015b58 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b64, L0x20015b64 <=s 23565127@32,
(-20207750)@32 <=s L0x2001620c, L0x2001620c <=s 20207750@32,
(-16831941)@32 <=s L0x20016218, L0x20016218 <=s 16831941@32,
(-23565127)@32 <=s L0x20016224, L0x20016224 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b70, L0x20015b70 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b7c, L0x20015b7c <=s 16831941@32,
(-23565127)@32 <=s L0x20015b88, L0x20015b88 <=s 23565127@32,
(-20207750)@32 <=s L0x20016230, L0x20016230 <=s 20207750@32,
(-16831941)@32 <=s L0x2001623c, L0x2001623c <=s 16831941@32,
(-23565127)@32 <=s L0x20016248, L0x20016248 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b94, L0x20015b94 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ba0, L0x20015ba0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bac, L0x20015bac <=s 23565127@32,
(-20207750)@32 <=s L0x20016254, L0x20016254 <=s 20207750@32,
(-16831941)@32 <=s L0x20016260, L0x20016260 <=s 16831941@32,
(-23565127)@32 <=s L0x2001626c, L0x2001626c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015bb8, L0x20015bb8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015bc4, L0x20015bc4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bd0, L0x20015bd0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016278, L0x20016278 <=s 20207750@32,
(-16831941)@32 <=s L0x20016284, L0x20016284 <=s 16831941@32,
(-23565127)@32 <=s L0x20016290, L0x20016290 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015bdc, L0x20015bdc <=s 20207750@32,
(-16831941)@32 <=s L0x20015be8, L0x20015be8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bf4, L0x20015bf4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001629c, L0x2001629c <=s 20207750@32,
(-16831941)@32 <=s L0x200162a8, L0x200162a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200162b4, L0x200162b4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c00, L0x20015c00 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c0c, L0x20015c0c <=s 16831941@32,
(-23565127)@32 <=s L0x20015c18, L0x20015c18 <=s 23565127@32,
(-20207750)@32 <=s L0x200162c0, L0x200162c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200162cc, L0x200162cc <=s 16831941@32,
(-23565127)@32 <=s L0x200162d8, L0x200162d8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c24, L0x20015c24 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c30, L0x20015c30 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c3c, L0x20015c3c <=s 23565127@32,
(-20207750)@32 <=s L0x200162e4, L0x200162e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200162f0, L0x200162f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200162fc, L0x200162fc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c48, L0x20015c48 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c54, L0x20015c54 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c60, L0x20015c60 <=s 23565127@32,
(-20207750)@32 <=s L0x20016308, L0x20016308 <=s 20207750@32,
(-16831941)@32 <=s L0x20016314, L0x20016314 <=s 16831941@32,
(-23565127)@32 <=s L0x20016320, L0x20016320 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c6c, L0x20015c6c <=s 20207750@32,
(-16831941)@32 <=s L0x20015c78, L0x20015c78 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c84, L0x20015c84 <=s 23565127@32,
(-20207750)@32 <=s L0x2001632c, L0x2001632c <=s 20207750@32,
(-16831941)@32 <=s L0x20016338, L0x20016338 <=s 16831941@32,
(-23565127)@32 <=s L0x20016344, L0x20016344 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c90, L0x20015c90 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c9c, L0x20015c9c <=s 16831941@32,
(-23565127)@32 <=s L0x20015ca8, L0x20015ca8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016350, L0x20016350 <=s 20207750@32,
(-16831941)@32 <=s L0x2001635c, L0x2001635c <=s 16831941@32,
(-23565127)@32 <=s L0x20016368, L0x20016368 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015cb4, L0x20015cb4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015cc0, L0x20015cc0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ccc, L0x20015ccc <=s 23565127@32,
(-20207750)@32 <=s L0x20016374, L0x20016374 <=s 20207750@32,
(-16831941)@32 <=s L0x20016380, L0x20016380 <=s 16831941@32,
(-23565127)@32 <=s L0x2001638c, L0x2001638c <=s 23565127@32
,
(-20207750)@32 <=s L0x2001561c, L0x2001561c <=s 20207750@32,
(-16831941)@32 <=s L0x20015628, L0x20015628 <=s 16831941@32,
(-23565127)@32 <=s L0x20015634, L0x20015634 <=s 23565127@32,
(-20207750)@32 <=s L0x20015cdc, L0x20015cdc <=s 20207750@32,
(-16831941)@32 <=s L0x20015ce8, L0x20015ce8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cf4, L0x20015cf4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015640, L0x20015640 <=s 20207750@32,
(-16831941)@32 <=s L0x2001564c, L0x2001564c <=s 16831941@32,
(-23565127)@32 <=s L0x20015658, L0x20015658 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d00, L0x20015d00 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d0c, L0x20015d0c <=s 16831941@32,
(-23565127)@32 <=s L0x20015d18, L0x20015d18 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015664, L0x20015664 <=s 20207750@32,
(-16831941)@32 <=s L0x20015670, L0x20015670 <=s 16831941@32,
(-23565127)@32 <=s L0x2001567c, L0x2001567c <=s 23565127@32,
(-20207750)@32 <=s L0x20015d24, L0x20015d24 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d30, L0x20015d30 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d3c, L0x20015d3c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015688, L0x20015688 <=s 20207750@32,
(-16831941)@32 <=s L0x20015694, L0x20015694 <=s 16831941@32,
(-23565127)@32 <=s L0x200156a0, L0x200156a0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d48, L0x20015d48 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d54, L0x20015d54 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d60, L0x20015d60 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156ac, L0x200156ac <=s 20207750@32,
(-16831941)@32 <=s L0x200156b8, L0x200156b8 <=s 16831941@32,
(-23565127)@32 <=s L0x200156c4, L0x200156c4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d6c, L0x20015d6c <=s 20207750@32,
(-16831941)@32 <=s L0x20015d78, L0x20015d78 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d84, L0x20015d84 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156d0, L0x200156d0 <=s 20207750@32,
(-16831941)@32 <=s L0x200156dc, L0x200156dc <=s 16831941@32,
(-23565127)@32 <=s L0x200156e8, L0x200156e8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d90, L0x20015d90 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d9c, L0x20015d9c <=s 16831941@32,
(-23565127)@32 <=s L0x20015da8, L0x20015da8 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156f4, L0x200156f4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015700, L0x20015700 <=s 16831941@32,
(-23565127)@32 <=s L0x2001570c, L0x2001570c <=s 23565127@32,
(-20207750)@32 <=s L0x20015db4, L0x20015db4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015dc0, L0x20015dc0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015dcc, L0x20015dcc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015718, L0x20015718 <=s 20207750@32,
(-16831941)@32 <=s L0x20015724, L0x20015724 <=s 16831941@32,
(-23565127)@32 <=s L0x20015730, L0x20015730 <=s 23565127@32,
(-20207750)@32 <=s L0x20015dd8, L0x20015dd8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015de4, L0x20015de4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015df0, L0x20015df0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001573c, L0x2001573c <=s 20207750@32,
(-16831941)@32 <=s L0x20015748, L0x20015748 <=s 16831941@32,
(-23565127)@32 <=s L0x20015754, L0x20015754 <=s 23565127@32,
(-20207750)@32 <=s L0x20015dfc, L0x20015dfc <=s 20207750@32,
(-16831941)@32 <=s L0x20015e08, L0x20015e08 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e14, L0x20015e14 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015760, L0x20015760 <=s 20207750@32,
(-16831941)@32 <=s L0x2001576c, L0x2001576c <=s 16831941@32,
(-23565127)@32 <=s L0x20015778, L0x20015778 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e20, L0x20015e20 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e2c, L0x20015e2c <=s 16831941@32,
(-23565127)@32 <=s L0x20015e38, L0x20015e38 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015784, L0x20015784 <=s 20207750@32,
(-16831941)@32 <=s L0x20015790, L0x20015790 <=s 16831941@32,
(-23565127)@32 <=s L0x2001579c, L0x2001579c <=s 23565127@32,
(-20207750)@32 <=s L0x20015e44, L0x20015e44 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e50, L0x20015e50 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e5c, L0x20015e5c <=s 23565127@32
,
(-20207750)@32 <=s L0x200157a8, L0x200157a8 <=s 20207750@32,
(-16831941)@32 <=s L0x200157b4, L0x200157b4 <=s 16831941@32,
(-23565127)@32 <=s L0x200157c0, L0x200157c0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e68, L0x20015e68 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e74, L0x20015e74 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e80, L0x20015e80 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157cc, L0x200157cc <=s 20207750@32,
(-16831941)@32 <=s L0x200157d8, L0x200157d8 <=s 16831941@32,
(-23565127)@32 <=s L0x200157e4, L0x200157e4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e8c, L0x20015e8c <=s 20207750@32,
(-16831941)@32 <=s L0x20015e98, L0x20015e98 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ea4, L0x20015ea4 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157f0, L0x200157f0 <=s 20207750@32,
(-16831941)@32 <=s L0x200157fc, L0x200157fc <=s 16831941@32,
(-23565127)@32 <=s L0x20015808, L0x20015808 <=s 23565127@32,
(-20207750)@32 <=s L0x20015eb0, L0x20015eb0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ebc, L0x20015ebc <=s 16831941@32,
(-23565127)@32 <=s L0x20015ec8, L0x20015ec8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015814, L0x20015814 <=s 20207750@32,
(-16831941)@32 <=s L0x20015820, L0x20015820 <=s 16831941@32,
(-23565127)@32 <=s L0x2001582c, L0x2001582c <=s 23565127@32,
(-20207750)@32 <=s L0x20015ed4, L0x20015ed4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ee0, L0x20015ee0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015eec, L0x20015eec <=s 23565127@32
,
(-20207750)@32 <=s L0x20015838, L0x20015838 <=s 20207750@32,
(-16831941)@32 <=s L0x20015844, L0x20015844 <=s 16831941@32,
(-23565127)@32 <=s L0x20015850, L0x20015850 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ef8, L0x20015ef8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f04, L0x20015f04 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f10, L0x20015f10 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001585c, L0x2001585c <=s 20207750@32,
(-16831941)@32 <=s L0x20015868, L0x20015868 <=s 16831941@32,
(-23565127)@32 <=s L0x20015874, L0x20015874 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f1c, L0x20015f1c <=s 20207750@32,
(-16831941)@32 <=s L0x20015f28, L0x20015f28 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f34, L0x20015f34 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015880, L0x20015880 <=s 20207750@32,
(-16831941)@32 <=s L0x2001588c, L0x2001588c <=s 16831941@32,
(-23565127)@32 <=s L0x20015898, L0x20015898 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f40, L0x20015f40 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f4c, L0x20015f4c <=s 16831941@32,
(-23565127)@32 <=s L0x20015f58, L0x20015f58 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158a4, L0x200158a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200158b0, L0x200158b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200158bc, L0x200158bc <=s 23565127@32,
(-20207750)@32 <=s L0x20015f64, L0x20015f64 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f70, L0x20015f70 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f7c, L0x20015f7c <=s 23565127@32
,
(-20207750)@32 <=s L0x200158c8, L0x200158c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200158d4, L0x200158d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200158e0, L0x200158e0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f88, L0x20015f88 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f94, L0x20015f94 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fa0, L0x20015fa0 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158ec, L0x200158ec <=s 20207750@32,
(-16831941)@32 <=s L0x200158f8, L0x200158f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015904, L0x20015904 <=s 23565127@32,
(-20207750)@32 <=s L0x20015fac, L0x20015fac <=s 20207750@32,
(-16831941)@32 <=s L0x20015fb8, L0x20015fb8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fc4, L0x20015fc4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015910, L0x20015910 <=s 20207750@32,
(-16831941)@32 <=s L0x2001591c, L0x2001591c <=s 16831941@32,
(-23565127)@32 <=s L0x20015928, L0x20015928 <=s 23565127@32,
(-20207750)@32 <=s L0x20015fd0, L0x20015fd0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015fdc, L0x20015fdc <=s 16831941@32,
(-23565127)@32 <=s L0x20015fe8, L0x20015fe8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015934, L0x20015934 <=s 20207750@32,
(-16831941)@32 <=s L0x20015940, L0x20015940 <=s 16831941@32,
(-23565127)@32 <=s L0x2001594c, L0x2001594c <=s 23565127@32,
(-20207750)@32 <=s L0x20015ff4, L0x20015ff4 <=s 20207750@32,
(-16831941)@32 <=s L0x20016000, L0x20016000 <=s 16831941@32,
(-23565127)@32 <=s L0x2001600c, L0x2001600c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015958, L0x20015958 <=s 20207750@32,
(-16831941)@32 <=s L0x20015964, L0x20015964 <=s 16831941@32,
(-23565127)@32 <=s L0x20015970, L0x20015970 <=s 23565127@32,
(-20207750)@32 <=s L0x20016018, L0x20016018 <=s 20207750@32,
(-16831941)@32 <=s L0x20016024, L0x20016024 <=s 16831941@32,
(-23565127)@32 <=s L0x20016030, L0x20016030 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001597c, L0x2001597c <=s 20207750@32,
(-16831941)@32 <=s L0x20015988, L0x20015988 <=s 16831941@32,
(-23565127)@32 <=s L0x20015994, L0x20015994 <=s 23565127@32,
(-20207750)@32 <=s L0x2001603c, L0x2001603c <=s 20207750@32,
(-16831941)@32 <=s L0x20016048, L0x20016048 <=s 16831941@32,
(-23565127)@32 <=s L0x20016054, L0x20016054 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159a0, L0x200159a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200159ac, L0x200159ac <=s 16831941@32,
(-23565127)@32 <=s L0x200159b8, L0x200159b8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016060, L0x20016060 <=s 20207750@32,
(-16831941)@32 <=s L0x2001606c, L0x2001606c <=s 16831941@32,
(-23565127)@32 <=s L0x20016078, L0x20016078 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159c4, L0x200159c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200159d0, L0x200159d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200159dc, L0x200159dc <=s 23565127@32,
(-20207750)@32 <=s L0x20016084, L0x20016084 <=s 20207750@32,
(-16831941)@32 <=s L0x20016090, L0x20016090 <=s 16831941@32,
(-23565127)@32 <=s L0x2001609c, L0x2001609c <=s 23565127@32
,
(-20207750)@32 <=s L0x200159e8, L0x200159e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200159f4, L0x200159f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a00, L0x20015a00 <=s 23565127@32,
(-20207750)@32 <=s L0x200160a8, L0x200160a8 <=s 20207750@32,
(-16831941)@32 <=s L0x200160b4, L0x200160b4 <=s 16831941@32,
(-23565127)@32 <=s L0x200160c0, L0x200160c0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a0c, L0x20015a0c <=s 20207750@32,
(-16831941)@32 <=s L0x20015a18, L0x20015a18 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a24, L0x20015a24 <=s 23565127@32,
(-20207750)@32 <=s L0x200160cc, L0x200160cc <=s 20207750@32,
(-16831941)@32 <=s L0x200160d8, L0x200160d8 <=s 16831941@32,
(-23565127)@32 <=s L0x200160e4, L0x200160e4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a30, L0x20015a30 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a3c, L0x20015a3c <=s 16831941@32,
(-23565127)@32 <=s L0x20015a48, L0x20015a48 <=s 23565127@32,
(-20207750)@32 <=s L0x200160f0, L0x200160f0 <=s 20207750@32,
(-16831941)@32 <=s L0x200160fc, L0x200160fc <=s 16831941@32,
(-23565127)@32 <=s L0x20016108, L0x20016108 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a54, L0x20015a54 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a60, L0x20015a60 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a6c, L0x20015a6c <=s 23565127@32,
(-20207750)@32 <=s L0x20016114, L0x20016114 <=s 20207750@32,
(-16831941)@32 <=s L0x20016120, L0x20016120 <=s 16831941@32,
(-23565127)@32 <=s L0x2001612c, L0x2001612c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a78, L0x20015a78 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a84, L0x20015a84 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a90, L0x20015a90 <=s 23565127@32,
(-20207750)@32 <=s L0x20016138, L0x20016138 <=s 20207750@32,
(-16831941)@32 <=s L0x20016144, L0x20016144 <=s 16831941@32,
(-23565127)@32 <=s L0x20016150, L0x20016150 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a9c, L0x20015a9c <=s 20207750@32,
(-16831941)@32 <=s L0x20015aa8, L0x20015aa8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ab4, L0x20015ab4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001615c, L0x2001615c <=s 20207750@32,
(-16831941)@32 <=s L0x20016168, L0x20016168 <=s 16831941@32,
(-23565127)@32 <=s L0x20016174, L0x20016174 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015ac0, L0x20015ac0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015acc, L0x20015acc <=s 16831941@32,
(-23565127)@32 <=s L0x20015ad8, L0x20015ad8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016180, L0x20016180 <=s 20207750@32,
(-16831941)@32 <=s L0x2001618c, L0x2001618c <=s 16831941@32,
(-23565127)@32 <=s L0x20016198, L0x20016198 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015ae4, L0x20015ae4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015af0, L0x20015af0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015afc, L0x20015afc <=s 23565127@32,
(-20207750)@32 <=s L0x200161a4, L0x200161a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200161b0, L0x200161b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200161bc, L0x200161bc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b08, L0x20015b08 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b14, L0x20015b14 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b20, L0x20015b20 <=s 23565127@32,
(-20207750)@32 <=s L0x200161c8, L0x200161c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200161d4, L0x200161d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200161e0, L0x200161e0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b2c, L0x20015b2c <=s 20207750@32,
(-16831941)@32 <=s L0x20015b38, L0x20015b38 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b44, L0x20015b44 <=s 23565127@32,
(-20207750)@32 <=s L0x200161ec, L0x200161ec <=s 20207750@32,
(-16831941)@32 <=s L0x200161f8, L0x200161f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20016204, L0x20016204 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b50, L0x20015b50 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b5c, L0x20015b5c <=s 16831941@32,
(-23565127)@32 <=s L0x20015b68, L0x20015b68 <=s 23565127@32,
(-20207750)@32 <=s L0x20016210, L0x20016210 <=s 20207750@32,
(-16831941)@32 <=s L0x2001621c, L0x2001621c <=s 16831941@32,
(-23565127)@32 <=s L0x20016228, L0x20016228 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b74, L0x20015b74 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b80, L0x20015b80 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b8c, L0x20015b8c <=s 23565127@32,
(-20207750)@32 <=s L0x20016234, L0x20016234 <=s 20207750@32,
(-16831941)@32 <=s L0x20016240, L0x20016240 <=s 16831941@32,
(-23565127)@32 <=s L0x2001624c, L0x2001624c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b98, L0x20015b98 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ba4, L0x20015ba4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bb0, L0x20015bb0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016258, L0x20016258 <=s 20207750@32,
(-16831941)@32 <=s L0x20016264, L0x20016264 <=s 16831941@32,
(-23565127)@32 <=s L0x20016270, L0x20016270 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015bbc, L0x20015bbc <=s 20207750@32,
(-16831941)@32 <=s L0x20015bc8, L0x20015bc8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bd4, L0x20015bd4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001627c, L0x2001627c <=s 20207750@32,
(-16831941)@32 <=s L0x20016288, L0x20016288 <=s 16831941@32,
(-23565127)@32 <=s L0x20016294, L0x20016294 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015be0, L0x20015be0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015bec, L0x20015bec <=s 16831941@32,
(-23565127)@32 <=s L0x20015bf8, L0x20015bf8 <=s 23565127@32,
(-20207750)@32 <=s L0x200162a0, L0x200162a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200162ac, L0x200162ac <=s 16831941@32,
(-23565127)@32 <=s L0x200162b8, L0x200162b8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c04, L0x20015c04 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c10, L0x20015c10 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c1c, L0x20015c1c <=s 23565127@32,
(-20207750)@32 <=s L0x200162c4, L0x200162c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200162d0, L0x200162d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200162dc, L0x200162dc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c28, L0x20015c28 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c34, L0x20015c34 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c40, L0x20015c40 <=s 23565127@32,
(-20207750)@32 <=s L0x200162e8, L0x200162e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200162f4, L0x200162f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20016300, L0x20016300 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c4c, L0x20015c4c <=s 20207750@32,
(-16831941)@32 <=s L0x20015c58, L0x20015c58 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c64, L0x20015c64 <=s 23565127@32,
(-20207750)@32 <=s L0x2001630c, L0x2001630c <=s 20207750@32,
(-16831941)@32 <=s L0x20016318, L0x20016318 <=s 16831941@32,
(-23565127)@32 <=s L0x20016324, L0x20016324 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c70, L0x20015c70 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c7c, L0x20015c7c <=s 16831941@32,
(-23565127)@32 <=s L0x20015c88, L0x20015c88 <=s 23565127@32,
(-20207750)@32 <=s L0x20016330, L0x20016330 <=s 20207750@32,
(-16831941)@32 <=s L0x2001633c, L0x2001633c <=s 16831941@32,
(-23565127)@32 <=s L0x20016348, L0x20016348 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c94, L0x20015c94 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ca0, L0x20015ca0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cac, L0x20015cac <=s 23565127@32,
(-20207750)@32 <=s L0x20016354, L0x20016354 <=s 20207750@32,
(-16831941)@32 <=s L0x20016360, L0x20016360 <=s 16831941@32,
(-23565127)@32 <=s L0x2001636c, L0x2001636c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015cb8, L0x20015cb8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015cc4, L0x20015cc4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cd0, L0x20015cd0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016378, L0x20016378 <=s 20207750@32,
(-16831941)@32 <=s L0x20016384, L0x20016384 <=s 16831941@32,
(-23565127)@32 <=s L0x20016390, L0x20016390 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015620, L0x20015620 <=s 20207750@32,
(-16831941)@32 <=s L0x2001562c, L0x2001562c <=s 16831941@32,
(-23565127)@32 <=s L0x20015638, L0x20015638 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ce0, L0x20015ce0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015cec, L0x20015cec <=s 16831941@32,
(-23565127)@32 <=s L0x20015cf8, L0x20015cf8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015644, L0x20015644 <=s 20207750@32,
(-16831941)@32 <=s L0x20015650, L0x20015650 <=s 16831941@32,
(-23565127)@32 <=s L0x2001565c, L0x2001565c <=s 23565127@32,
(-20207750)@32 <=s L0x20015d04, L0x20015d04 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d10, L0x20015d10 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d1c, L0x20015d1c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015668, L0x20015668 <=s 20207750@32,
(-16831941)@32 <=s L0x20015674, L0x20015674 <=s 16831941@32,
(-23565127)@32 <=s L0x20015680, L0x20015680 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d28, L0x20015d28 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d34, L0x20015d34 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d40, L0x20015d40 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001568c, L0x2001568c <=s 20207750@32,
(-16831941)@32 <=s L0x20015698, L0x20015698 <=s 16831941@32,
(-23565127)@32 <=s L0x200156a4, L0x200156a4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d4c, L0x20015d4c <=s 20207750@32,
(-16831941)@32 <=s L0x20015d58, L0x20015d58 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d64, L0x20015d64 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156b0, L0x200156b0 <=s 20207750@32,
(-16831941)@32 <=s L0x200156bc, L0x200156bc <=s 16831941@32,
(-23565127)@32 <=s L0x200156c8, L0x200156c8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d70, L0x20015d70 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d7c, L0x20015d7c <=s 16831941@32,
(-23565127)@32 <=s L0x20015d88, L0x20015d88 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156d4, L0x200156d4 <=s 20207750@32,
(-16831941)@32 <=s L0x200156e0, L0x200156e0 <=s 16831941@32,
(-23565127)@32 <=s L0x200156ec, L0x200156ec <=s 23565127@32,
(-20207750)@32 <=s L0x20015d94, L0x20015d94 <=s 20207750@32,
(-16831941)@32 <=s L0x20015da0, L0x20015da0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015dac, L0x20015dac <=s 23565127@32
,
(-20207750)@32 <=s L0x200156f8, L0x200156f8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015704, L0x20015704 <=s 16831941@32,
(-23565127)@32 <=s L0x20015710, L0x20015710 <=s 23565127@32,
(-20207750)@32 <=s L0x20015db8, L0x20015db8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015dc4, L0x20015dc4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015dd0, L0x20015dd0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001571c, L0x2001571c <=s 20207750@32,
(-16831941)@32 <=s L0x20015728, L0x20015728 <=s 16831941@32,
(-23565127)@32 <=s L0x20015734, L0x20015734 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ddc, L0x20015ddc <=s 20207750@32,
(-16831941)@32 <=s L0x20015de8, L0x20015de8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015df4, L0x20015df4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015740, L0x20015740 <=s 20207750@32,
(-16831941)@32 <=s L0x2001574c, L0x2001574c <=s 16831941@32,
(-23565127)@32 <=s L0x20015758, L0x20015758 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e00, L0x20015e00 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e0c, L0x20015e0c <=s 16831941@32,
(-23565127)@32 <=s L0x20015e18, L0x20015e18 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015764, L0x20015764 <=s 20207750@32,
(-16831941)@32 <=s L0x20015770, L0x20015770 <=s 16831941@32,
(-23565127)@32 <=s L0x2001577c, L0x2001577c <=s 23565127@32,
(-20207750)@32 <=s L0x20015e24, L0x20015e24 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e30, L0x20015e30 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e3c, L0x20015e3c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015788, L0x20015788 <=s 20207750@32,
(-16831941)@32 <=s L0x20015794, L0x20015794 <=s 16831941@32,
(-23565127)@32 <=s L0x200157a0, L0x200157a0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e48, L0x20015e48 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e54, L0x20015e54 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e60, L0x20015e60 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157ac, L0x200157ac <=s 20207750@32,
(-16831941)@32 <=s L0x200157b8, L0x200157b8 <=s 16831941@32,
(-23565127)@32 <=s L0x200157c4, L0x200157c4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e6c, L0x20015e6c <=s 20207750@32,
(-16831941)@32 <=s L0x20015e78, L0x20015e78 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e84, L0x20015e84 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157d0, L0x200157d0 <=s 20207750@32,
(-16831941)@32 <=s L0x200157dc, L0x200157dc <=s 16831941@32,
(-23565127)@32 <=s L0x200157e8, L0x200157e8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e90, L0x20015e90 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e9c, L0x20015e9c <=s 16831941@32,
(-23565127)@32 <=s L0x20015ea8, L0x20015ea8 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157f4, L0x200157f4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015800, L0x20015800 <=s 16831941@32,
(-23565127)@32 <=s L0x2001580c, L0x2001580c <=s 23565127@32,
(-20207750)@32 <=s L0x20015eb4, L0x20015eb4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ec0, L0x20015ec0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ecc, L0x20015ecc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015818, L0x20015818 <=s 20207750@32,
(-16831941)@32 <=s L0x20015824, L0x20015824 <=s 16831941@32,
(-23565127)@32 <=s L0x20015830, L0x20015830 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ed8, L0x20015ed8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ee4, L0x20015ee4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ef0, L0x20015ef0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001583c, L0x2001583c <=s 20207750@32,
(-16831941)@32 <=s L0x20015848, L0x20015848 <=s 16831941@32,
(-23565127)@32 <=s L0x20015854, L0x20015854 <=s 23565127@32,
(-20207750)@32 <=s L0x20015efc, L0x20015efc <=s 20207750@32,
(-16831941)@32 <=s L0x20015f08, L0x20015f08 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f14, L0x20015f14 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015860, L0x20015860 <=s 20207750@32,
(-16831941)@32 <=s L0x2001586c, L0x2001586c <=s 16831941@32,
(-23565127)@32 <=s L0x20015878, L0x20015878 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f20, L0x20015f20 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f2c, L0x20015f2c <=s 16831941@32,
(-23565127)@32 <=s L0x20015f38, L0x20015f38 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015884, L0x20015884 <=s 20207750@32,
(-16831941)@32 <=s L0x20015890, L0x20015890 <=s 16831941@32,
(-23565127)@32 <=s L0x2001589c, L0x2001589c <=s 23565127@32,
(-20207750)@32 <=s L0x20015f44, L0x20015f44 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f50, L0x20015f50 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f5c, L0x20015f5c <=s 23565127@32
,
(-20207750)@32 <=s L0x200158a8, L0x200158a8 <=s 20207750@32,
(-16831941)@32 <=s L0x200158b4, L0x200158b4 <=s 16831941@32,
(-23565127)@32 <=s L0x200158c0, L0x200158c0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f68, L0x20015f68 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f74, L0x20015f74 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f80, L0x20015f80 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158cc, L0x200158cc <=s 20207750@32,
(-16831941)@32 <=s L0x200158d8, L0x200158d8 <=s 16831941@32,
(-23565127)@32 <=s L0x200158e4, L0x200158e4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f8c, L0x20015f8c <=s 20207750@32,
(-16831941)@32 <=s L0x20015f98, L0x20015f98 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fa4, L0x20015fa4 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158f0, L0x200158f0 <=s 20207750@32,
(-16831941)@32 <=s L0x200158fc, L0x200158fc <=s 16831941@32,
(-23565127)@32 <=s L0x20015908, L0x20015908 <=s 23565127@32,
(-20207750)@32 <=s L0x20015fb0, L0x20015fb0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015fbc, L0x20015fbc <=s 16831941@32,
(-23565127)@32 <=s L0x20015fc8, L0x20015fc8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015914, L0x20015914 <=s 20207750@32,
(-16831941)@32 <=s L0x20015920, L0x20015920 <=s 16831941@32,
(-23565127)@32 <=s L0x2001592c, L0x2001592c <=s 23565127@32,
(-20207750)@32 <=s L0x20015fd4, L0x20015fd4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015fe0, L0x20015fe0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fec, L0x20015fec <=s 23565127@32
,
(-20207750)@32 <=s L0x20015938, L0x20015938 <=s 20207750@32,
(-16831941)@32 <=s L0x20015944, L0x20015944 <=s 16831941@32,
(-23565127)@32 <=s L0x20015950, L0x20015950 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ff8, L0x20015ff8 <=s 20207750@32,
(-16831941)@32 <=s L0x20016004, L0x20016004 <=s 16831941@32,
(-23565127)@32 <=s L0x20016010, L0x20016010 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001595c, L0x2001595c <=s 20207750@32,
(-16831941)@32 <=s L0x20015968, L0x20015968 <=s 16831941@32,
(-23565127)@32 <=s L0x20015974, L0x20015974 <=s 23565127@32,
(-20207750)@32 <=s L0x2001601c, L0x2001601c <=s 20207750@32,
(-16831941)@32 <=s L0x20016028, L0x20016028 <=s 16831941@32,
(-23565127)@32 <=s L0x20016034, L0x20016034 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015980, L0x20015980 <=s 20207750@32,
(-16831941)@32 <=s L0x2001598c, L0x2001598c <=s 16831941@32,
(-23565127)@32 <=s L0x20015998, L0x20015998 <=s 23565127@32,
(-20207750)@32 <=s L0x20016040, L0x20016040 <=s 20207750@32,
(-16831941)@32 <=s L0x2001604c, L0x2001604c <=s 16831941@32,
(-23565127)@32 <=s L0x20016058, L0x20016058 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159a4, L0x200159a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200159b0, L0x200159b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200159bc, L0x200159bc <=s 23565127@32,
(-20207750)@32 <=s L0x20016064, L0x20016064 <=s 20207750@32,
(-16831941)@32 <=s L0x20016070, L0x20016070 <=s 16831941@32,
(-23565127)@32 <=s L0x2001607c, L0x2001607c <=s 23565127@32
,
(-20207750)@32 <=s L0x200159c8, L0x200159c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200159d4, L0x200159d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200159e0, L0x200159e0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016088, L0x20016088 <=s 20207750@32,
(-16831941)@32 <=s L0x20016094, L0x20016094 <=s 16831941@32,
(-23565127)@32 <=s L0x200160a0, L0x200160a0 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159ec, L0x200159ec <=s 20207750@32,
(-16831941)@32 <=s L0x200159f8, L0x200159f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a04, L0x20015a04 <=s 23565127@32,
(-20207750)@32 <=s L0x200160ac, L0x200160ac <=s 20207750@32,
(-16831941)@32 <=s L0x200160b8, L0x200160b8 <=s 16831941@32,
(-23565127)@32 <=s L0x200160c4, L0x200160c4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a10, L0x20015a10 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a1c, L0x20015a1c <=s 16831941@32,
(-23565127)@32 <=s L0x20015a28, L0x20015a28 <=s 23565127@32,
(-20207750)@32 <=s L0x200160d0, L0x200160d0 <=s 20207750@32,
(-16831941)@32 <=s L0x200160dc, L0x200160dc <=s 16831941@32,
(-23565127)@32 <=s L0x200160e8, L0x200160e8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a34, L0x20015a34 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a40, L0x20015a40 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a4c, L0x20015a4c <=s 23565127@32,
(-20207750)@32 <=s L0x200160f4, L0x200160f4 <=s 20207750@32,
(-16831941)@32 <=s L0x20016100, L0x20016100 <=s 16831941@32,
(-23565127)@32 <=s L0x2001610c, L0x2001610c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a58, L0x20015a58 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a64, L0x20015a64 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a70, L0x20015a70 <=s 23565127@32,
(-20207750)@32 <=s L0x20016118, L0x20016118 <=s 20207750@32,
(-16831941)@32 <=s L0x20016124, L0x20016124 <=s 16831941@32,
(-23565127)@32 <=s L0x20016130, L0x20016130 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a7c, L0x20015a7c <=s 20207750@32,
(-16831941)@32 <=s L0x20015a88, L0x20015a88 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a94, L0x20015a94 <=s 23565127@32,
(-20207750)@32 <=s L0x2001613c, L0x2001613c <=s 20207750@32,
(-16831941)@32 <=s L0x20016148, L0x20016148 <=s 16831941@32,
(-23565127)@32 <=s L0x20016154, L0x20016154 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015aa0, L0x20015aa0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015aac, L0x20015aac <=s 16831941@32,
(-23565127)@32 <=s L0x20015ab8, L0x20015ab8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016160, L0x20016160 <=s 20207750@32,
(-16831941)@32 <=s L0x2001616c, L0x2001616c <=s 16831941@32,
(-23565127)@32 <=s L0x20016178, L0x20016178 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015ac4, L0x20015ac4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ad0, L0x20015ad0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015adc, L0x20015adc <=s 23565127@32,
(-20207750)@32 <=s L0x20016184, L0x20016184 <=s 20207750@32,
(-16831941)@32 <=s L0x20016190, L0x20016190 <=s 16831941@32,
(-23565127)@32 <=s L0x2001619c, L0x2001619c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015ae8, L0x20015ae8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015af4, L0x20015af4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b00, L0x20015b00 <=s 23565127@32,
(-20207750)@32 <=s L0x200161a8, L0x200161a8 <=s 20207750@32,
(-16831941)@32 <=s L0x200161b4, L0x200161b4 <=s 16831941@32,
(-23565127)@32 <=s L0x200161c0, L0x200161c0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b0c, L0x20015b0c <=s 20207750@32,
(-16831941)@32 <=s L0x20015b18, L0x20015b18 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b24, L0x20015b24 <=s 23565127@32,
(-20207750)@32 <=s L0x200161cc, L0x200161cc <=s 20207750@32,
(-16831941)@32 <=s L0x200161d8, L0x200161d8 <=s 16831941@32,
(-23565127)@32 <=s L0x200161e4, L0x200161e4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b30, L0x20015b30 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b3c, L0x20015b3c <=s 16831941@32,
(-23565127)@32 <=s L0x20015b48, L0x20015b48 <=s 23565127@32,
(-20207750)@32 <=s L0x200161f0, L0x200161f0 <=s 20207750@32,
(-16831941)@32 <=s L0x200161fc, L0x200161fc <=s 16831941@32,
(-23565127)@32 <=s L0x20016208, L0x20016208 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b54, L0x20015b54 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b60, L0x20015b60 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b6c, L0x20015b6c <=s 23565127@32,
(-20207750)@32 <=s L0x20016214, L0x20016214 <=s 20207750@32,
(-16831941)@32 <=s L0x20016220, L0x20016220 <=s 16831941@32,
(-23565127)@32 <=s L0x2001622c, L0x2001622c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b78, L0x20015b78 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b84, L0x20015b84 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b90, L0x20015b90 <=s 23565127@32,
(-20207750)@32 <=s L0x20016238, L0x20016238 <=s 20207750@32,
(-16831941)@32 <=s L0x20016244, L0x20016244 <=s 16831941@32,
(-23565127)@32 <=s L0x20016250, L0x20016250 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b9c, L0x20015b9c <=s 20207750@32,
(-16831941)@32 <=s L0x20015ba8, L0x20015ba8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bb4, L0x20015bb4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001625c, L0x2001625c <=s 20207750@32,
(-16831941)@32 <=s L0x20016268, L0x20016268 <=s 16831941@32,
(-23565127)@32 <=s L0x20016274, L0x20016274 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015bc0, L0x20015bc0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015bcc, L0x20015bcc <=s 16831941@32,
(-23565127)@32 <=s L0x20015bd8, L0x20015bd8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016280, L0x20016280 <=s 20207750@32,
(-16831941)@32 <=s L0x2001628c, L0x2001628c <=s 16831941@32,
(-23565127)@32 <=s L0x20016298, L0x20016298 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015be4, L0x20015be4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015bf0, L0x20015bf0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bfc, L0x20015bfc <=s 23565127@32,
(-20207750)@32 <=s L0x200162a4, L0x200162a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200162b0, L0x200162b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200162bc, L0x200162bc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c08, L0x20015c08 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c14, L0x20015c14 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c20, L0x20015c20 <=s 23565127@32,
(-20207750)@32 <=s L0x200162c8, L0x200162c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200162d4, L0x200162d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200162e0, L0x200162e0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c2c, L0x20015c2c <=s 20207750@32,
(-16831941)@32 <=s L0x20015c38, L0x20015c38 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c44, L0x20015c44 <=s 23565127@32,
(-20207750)@32 <=s L0x200162ec, L0x200162ec <=s 20207750@32,
(-16831941)@32 <=s L0x200162f8, L0x200162f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20016304, L0x20016304 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c50, L0x20015c50 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c5c, L0x20015c5c <=s 16831941@32,
(-23565127)@32 <=s L0x20015c68, L0x20015c68 <=s 23565127@32,
(-20207750)@32 <=s L0x20016310, L0x20016310 <=s 20207750@32,
(-16831941)@32 <=s L0x2001631c, L0x2001631c <=s 16831941@32,
(-23565127)@32 <=s L0x20016328, L0x20016328 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c74, L0x20015c74 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c80, L0x20015c80 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c8c, L0x20015c8c <=s 23565127@32,
(-20207750)@32 <=s L0x20016334, L0x20016334 <=s 20207750@32,
(-16831941)@32 <=s L0x20016340, L0x20016340 <=s 16831941@32,
(-23565127)@32 <=s L0x2001634c, L0x2001634c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c98, L0x20015c98 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ca4, L0x20015ca4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cb0, L0x20015cb0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016358, L0x20016358 <=s 20207750@32,
(-16831941)@32 <=s L0x20016364, L0x20016364 <=s 16831941@32,
(-23565127)@32 <=s L0x20016370, L0x20016370 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015cbc, L0x20015cbc <=s 20207750@32,
(-16831941)@32 <=s L0x20015cc8, L0x20015cc8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cd4, L0x20015cd4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001637c, L0x2001637c <=s 20207750@32,
(-16831941)@32 <=s L0x20016388, L0x20016388 <=s 16831941@32,
(-23565127)@32 <=s L0x20016394, L0x20016394 <=s 23565127@32
] prove with [ precondition ];


(******************** collect eqmods ********************)


(**************** CUT 289, - *****************)

ecut and [
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x20014898*x**0*z** 0 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x20014904*x**0*z** 1 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x20014970*x**0*z** 2 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x200149dc*x**0*z** 3 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20014a48*x**0*z** 4 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x20014ab4*x**0*z** 5 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x20014b20*x**0*z** 6 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x20014b8c*x**0*z** 7 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x20014bf8*x**0*z** 8 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x20014c64*x**0*z** 9 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x20014cd0*x**0*z**10 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x20014d3c*x**0*z**11 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20014da8*x**0*z**12 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x20014e14*x**0*z**13 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x20014e80*x**0*z**14 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x20014eec*x**0*z**15 [ 3365569, y -       1, z**16 -       1 ]
] prove with [ cuts [   1,   4,   7,  10,  13,  16,  19,  22,
                       25,  28,  31,  34,  37,  40,  43,  46 ] ];


(**************** CUT 290, - *****************)

ecut and [
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x200148a4*x**0*z** 0 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x20014910*x**0*z** 1 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x2001497c*x**0*z** 2 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x200149e8*x**0*z** 3 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20014a54*x**0*z** 4 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x20014ac0*x**0*z** 5 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x20014b2c*x**0*z** 6 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x20014b98*x**0*z** 7 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x20014c04*x**0*z** 8 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x20014c70*x**0*z** 9 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x20014cdc*x**0*z**10 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x20014d48*x**0*z**11 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20014db4*x**0*z**12 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x20014e20*x**0*z**13 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x20014e8c*x**0*z**14 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x20014ef8*x**0*z**15 [ 3365569, y -  452650, z**16 -       1 ]
] prove with [ cuts [   1,   4,   7,  10,  13,  16,  19,  22,
                       25,  28,  31,  34,  37,  40,  43,  46 ] ];


(**************** CUT 291, - *****************)

ecut and [
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x200148b0*x**0*z** 0 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x2001491c*x**0*z** 1 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x20014988*x**0*z** 2 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x200149f4*x**0*z** 3 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20014a60*x**0*z** 4 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x20014acc*x**0*z** 5 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x20014b38*x**0*z** 6 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x20014ba4*x**0*z** 7 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x20014c10*x**0*z** 8 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x20014c7c*x**0*z** 9 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x20014ce8*x**0*z**10 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x20014d54*x**0*z**11 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20014dc0*x**0*z**12 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x20014e2c*x**0*z**13 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x20014e98*x**0*z**14 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x20014f04*x**0*z**15 [ 3365569, y - 2912918, z**16 -       1 ]
] prove with [ cuts [   1,   4,   7,  10,  13,  16,  19,  22,
                       25,  28,  31,  34,  37,  40,  43,  46 ] ];


(**************** CUT 292, - *****************)

ecut and [
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x200148bc*x**0*z** 0 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20014928*x**0*z** 1 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x20014994*x**0*z** 2 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x20014a00*x**0*z** 3 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x20014a6c*x**0*z** 4 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x20014ad8*x**0*z** 5 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x20014b44*x**0*z** 6 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x20014bb0*x**0*z** 7 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x20014c1c*x**0*z** 8 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20014c88*x**0*z** 9 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x20014cf4*x**0*z**10 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x20014d60*x**0*z**11 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x20014dcc*x**0*z**12 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x20014e38*x**0*z**13 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x20014ea4*x**0*z**14 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x20014f10*x**0*z**15 [ 3365569, y - 2113025, z**16 -       1 ]
] prove with [ cuts [   2,   5,   8,  11,  14,  17,  20,  23,
                       26,  29,  32,  35,  38,  41,  44,  47 ] ];


(**************** CUT 293, - *****************)

ecut and [
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x200148c8*x**0*z** 0 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20014934*x**0*z** 1 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x200149a0*x**0*z** 2 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x20014a0c*x**0*z** 3 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x20014a78*x**0*z** 4 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x20014ae4*x**0*z** 5 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x20014b50*x**0*z** 6 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x20014bbc*x**0*z** 7 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x20014c28*x**0*z** 8 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20014c94*x**0*z** 9 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x20014d00*x**0*z**10 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x20014d6c*x**0*z**11 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x20014dd8*x**0*z**12 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x20014e44*x**0*z**13 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x20014eb0*x**0*z**14 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x20014f1c*x**0*z**15 [ 3365569, y - 3077709, z**16 -       1 ]
] prove with [ cuts [   2,   5,   8,  11,  14,  17,  20,  23,
                       26,  29,  32,  35,  38,  41,  44,  47 ] ];


(**************** CUT 294, - *****************)

ecut and [
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x200148d4*x**0*z** 0 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20014940*x**0*z** 1 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x200149ac*x**0*z** 2 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x20014a18*x**0*z** 3 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x20014a84*x**0*z** 4 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x20014af0*x**0*z** 5 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x20014b5c*x**0*z** 6 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x20014bc8*x**0*z** 7 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x20014c34*x**0*z** 8 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20014ca0*x**0*z** 9 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x20014d0c*x**0*z**10 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x20014d78*x**0*z**11 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x20014de4*x**0*z**12 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x20014e50*x**0*z**13 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x20014ebc*x**0*z**14 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x20014f28*x**0*z**15 [ 3365569, y - 1540404, z**16 -       1 ]
] prove with [ cuts [   2,   5,   8,  11,  14,  17,  20,  23,
                       26,  29,  32,  35,  38,  41,  44,  47 ] ];


(**************** CUT 295, - *****************)

ecut and [
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x200148e0*x**0*z** 0 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x2001494c*x**0*z** 1 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x200149b8*x**0*z** 2 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x20014a24*x**0*z** 3 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x20014a90*x**0*z** 4 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x20014afc*x**0*z** 5 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20014b68*x**0*z** 6 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x20014bd4*x**0*z** 7 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x20014c40*x**0*z** 8 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x20014cac*x**0*z** 9 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x20014d18*x**0*z**10 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x20014d84*x**0*z**11 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x20014df0*x**0*z**12 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x20014e5c*x**0*z**13 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x20014ec8*x**0*z**14 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x20014f34*x**0*z**15 [ 3365569, y - 3117017, z**16 -       1 ]
] prove with [ cuts [   3,   6,   9,  12,  15,  18,  21,  24,
                       27,  30,  33,  36,  39,  42,  45,  48 ] ];


(**************** CUT 296, - *****************)

ecut and [
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x200148ec*x**0*z** 0 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x20014958*x**0*z** 1 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x200149c4*x**0*z** 2 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x20014a30*x**0*z** 3 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x20014a9c*x**0*z** 4 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x20014b08*x**0*z** 5 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20014b74*x**0*z** 6 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x20014be0*x**0*z** 7 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x20014c4c*x**0*z** 8 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x20014cb8*x**0*z** 9 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x20014d24*x**0*z**10 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x20014d90*x**0*z**11 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x20014dfc*x**0*z**12 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x20014e68*x**0*z**13 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x20014ed4*x**0*z**14 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x20014f40*x**0*z**15 [ 3365569, y -  543301, z**16 -       1 ]
] prove with [ cuts [   3,   6,   9,  12,  15,  18,  21,  24,
                       27,  30,  33,  36,  39,  42,  45,  48 ] ];


(**************** CUT 297, - *****************)

ecut and [
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x200148f8*x**0*z** 0 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x20014964*x**0*z** 1 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x200149d0*x**0*z** 2 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x20014a3c*x**0*z** 3 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x20014aa8*x**0*z** 4 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x20014b14*x**0*z** 5 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20014b80*x**0*z** 6 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x20014bec*x**0*z** 7 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x20014c58*x**0*z** 8 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x20014cc4*x**0*z** 9 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x20014d30*x**0*z**10 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x20014d9c*x**0*z**11 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x20014e08*x**0*z**12 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x20014e74*x**0*z**13 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x20014ee0*x**0*z**14 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x20014f4c*x**0*z**15 [ 3365569, y - 3070820, z**16 -       1 ]
] prove with [ cuts [   3,   6,   9,  12,  15,  18,  21,  24,
                       27,  30,  33,  36,  39,  42,  45,  48 ] ];


(**************** CUT 298, - *****************)

ecut and [
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x20014f58*x**0*z** 0 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x20014fc4*x**0*z** 1 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x20015030*x**0*z** 2 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x2001509c*x**0*z** 3 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20015108*x**0*z** 4 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x20015174*x**0*z** 5 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x200151e0*x**0*z** 6 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x2001524c*x**0*z** 7 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x200152b8*x**0*z** 8 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x20015324*x**0*z** 9 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x20015390*x**0*z**10 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x200153fc*x**0*z**11 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20015468*x**0*z**12 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x200154d4*x**0*z**13 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x20015540*x**0*z**14 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x200155ac*x**0*z**15 [ 3365569, y -       1, z**16 - 3365568 ]
] prove with [ cuts [   1,   4,   7,  10,  13,  16,  19,  22,
                       25,  28,  31,  34,  37,  40,  43,  46 ] ];


(**************** CUT 299, - *****************)

ecut and [
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x20014f64*x**0*z** 0 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x20014fd0*x**0*z** 1 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x2001503c*x**0*z** 2 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x200150a8*x**0*z** 3 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20015114*x**0*z** 4 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x20015180*x**0*z** 5 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x200151ec*x**0*z** 6 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x20015258*x**0*z** 7 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x200152c4*x**0*z** 8 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x20015330*x**0*z** 9 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x2001539c*x**0*z**10 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x20015408*x**0*z**11 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20015474*x**0*z**12 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x200154e0*x**0*z**13 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x2001554c*x**0*z**14 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x200155b8*x**0*z**15 [ 3365569, y -  452650, z**16 - 3365568 ]
] prove with [ cuts [   1,   4,   7,  10,  13,  16,  19,  22,
                       25,  28,  31,  34,  37,  40,  43,  46 ] ];


(**************** CUT 300, - *****************)

ecut and [
eqmod (cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
       cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16)
      L0x20014f70*x**0*z** 0 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
       cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17)
      L0x20014fdc*x**0*z** 1 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
       cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18)
      L0x20015048*x**0*z** 2 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
       cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19)
      L0x200150b4*x**0*z** 3 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
       cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20)
      L0x20015120*x**0*z** 4 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
       cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21)
      L0x2001518c*x**0*z** 5 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
       cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22)
      L0x200151f8*x**0*z** 6 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
       cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23)
      L0x20015264*x**0*z** 7 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
       cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24)
      L0x200152d0*x**0*z** 8 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
       cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25)
      L0x2001533c*x**0*z** 9 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
       cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26)
      L0x200153a8*x**0*z**10 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
       cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27)
      L0x20015414*x**0*z**11 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
       cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28)
      L0x20015480*x**0*z**12 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
       cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29)
      L0x200154ec*x**0*z**13 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
       cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30)
      L0x20015558*x**0*z**14 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
       cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31)
      L0x200155c4*x**0*z**15 [ 3365569, y - 2912918, z**16 - 3365568 ]
] prove with [ cuts [   1,   4,   7,  10,  13,  16,  19,  22,
                       25,  28,  31,  34,  37,  40,  43,  46 ] ];


(**************** CUT 301, - *****************)

ecut and [
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x20014f7c*x**0*z** 0 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20014fe8*x**0*z** 1 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x20015054*x**0*z** 2 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x200150c0*x**0*z** 3 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x2001512c*x**0*z** 4 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x20015198*x**0*z** 5 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x20015204*x**0*z** 6 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x20015270*x**0*z** 7 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x200152dc*x**0*z** 8 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20015348*x**0*z** 9 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x200153b4*x**0*z**10 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x20015420*x**0*z**11 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x2001548c*x**0*z**12 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x200154f8*x**0*z**13 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x20015564*x**0*z**14 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x200155d0*x**0*z**15 [ 3365569, y - 2113025, z**16 - 3365568 ]
] prove with [ cuts [   2,   5,   8,  11,  14,  17,  20,  23,
                       26,  29,  32,  35,  38,  41,  44,  47 ] ];


(**************** CUT 302, - *****************)

ecut and [
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x20014f88*x**0*z** 0 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20014ff4*x**0*z** 1 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x20015060*x**0*z** 2 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x200150cc*x**0*z** 3 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x20015138*x**0*z** 4 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x200151a4*x**0*z** 5 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x20015210*x**0*z** 6 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x2001527c*x**0*z** 7 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x200152e8*x**0*z** 8 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20015354*x**0*z** 9 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x200153c0*x**0*z**10 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x2001542c*x**0*z**11 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x20015498*x**0*z**12 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x20015504*x**0*z**13 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x20015570*x**0*z**14 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x200155dc*x**0*z**15 [ 3365569, y - 3077709, z**16 - 3365568 ]
] prove with [ cuts [   2,   5,   8,  11,  14,  17,  20,  23,
                       26,  29,  32,  35,  38,  41,  44,  47 ] ];


(**************** CUT 303, - *****************)

ecut and [
eqmod (cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
       cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16)
      L0x20014f94*x**0*z** 0 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
       cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17)
      L0x20015000*x**0*z** 1 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
       cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18)
      L0x2001506c*x**0*z** 2 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
       cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19)
      L0x200150d8*x**0*z** 3 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
       cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20)
      L0x20015144*x**0*z** 4 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
       cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21)
      L0x200151b0*x**0*z** 5 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
       cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22)
      L0x2001521c*x**0*z** 6 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
       cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23)
      L0x20015288*x**0*z** 7 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
       cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24)
      L0x200152f4*x**0*z** 8 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
       cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25)
      L0x20015360*x**0*z** 9 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
       cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26)
      L0x200153cc*x**0*z**10 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
       cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27)
      L0x20015438*x**0*z**11 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
       cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28)
      L0x200154a4*x**0*z**12 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
       cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29)
      L0x20015510*x**0*z**13 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
       cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30)
      L0x2001557c*x**0*z**14 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
       cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31)
      L0x200155e8*x**0*z**15 [ 3365569, y - 1540404, z**16 - 3365568 ]
] prove with [ cuts [   2,   5,   8,  11,  14,  17,  20,  23,
                       26,  29,  32,  35,  38,  41,  44,  47 ] ];


(**************** CUT 304, - *****************)

ecut and [
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x20014fa0*x**0*z** 0 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x2001500c*x**0*z** 1 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x20015078*x**0*z** 2 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x200150e4*x**0*z** 3 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x20015150*x**0*z** 4 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x200151bc*x**0*z** 5 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20015228*x**0*z** 6 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x20015294*x**0*z** 7 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x20015300*x**0*z** 8 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x2001536c*x**0*z** 9 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x200153d8*x**0*z**10 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x20015444*x**0*z**11 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x200154b0*x**0*z**12 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x2001551c*x**0*z**13 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x20015588*x**0*z**14 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x200155f4*x**0*z**15 [ 3365569, y - 3117017, z**16 - 3365568 ]
] prove with [ cuts [   3,   6,   9,  12,  15,  18,  21,  24,
                       27,  30,  33,  36,  39,  42,  45,  48 ] ];


(**************** CUT 305, - *****************)

ecut and [
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x20014fac*x**0*z** 0 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x20015018*x**0*z** 1 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x20015084*x**0*z** 2 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x200150f0*x**0*z** 3 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x2001515c*x**0*z** 4 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x200151c8*x**0*z** 5 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20015234*x**0*z** 6 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x200152a0*x**0*z** 7 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x2001530c*x**0*z** 8 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x20015378*x**0*z** 9 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x200153e4*x**0*z**10 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x20015450*x**0*z**11 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x200154bc*x**0*z**12 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x20015528*x**0*z**13 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x20015594*x**0*z**14 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x20015600*x**0*z**15 [ 3365569, y -  543301, z**16 - 3365568 ]
] prove with [ cuts [   3,   6,   9,  12,  15,  18,  21,  24,
                       27,  30,  33,  36,  39,  42,  45,  48 ] ];


(**************** CUT 306, - *****************)

ecut and [
eqmod (cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
       cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16)
      L0x20014fb8*x**0*z** 0 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
       cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17)
      L0x20015024*x**0*z** 1 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
       cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18)
      L0x20015090*x**0*z** 2 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
       cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19)
      L0x200150fc*x**0*z** 3 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
       cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20)
      L0x20015168*x**0*z** 4 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
       cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21)
      L0x200151d4*x**0*z** 5 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
       cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22)
      L0x20015240*x**0*z** 6 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
       cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23)
      L0x200152ac*x**0*z** 7 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
       cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24)
      L0x20015318*x**0*z** 8 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
       cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25)
      L0x20015384*x**0*z** 9 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
       cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26)
      L0x200153f0*x**0*z**10 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
       cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27)
      L0x2001545c*x**0*z**11 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
       cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28)
      L0x200154c8*x**0*z**12 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
       cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29)
      L0x20015534*x**0*z**13 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
       cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30)
      L0x200155a0*x**0*z**14 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
       cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31)
      L0x2001560c*x**0*z**15 [ 3365569, y - 3070820, z**16 - 3365568 ]
] prove with [ cuts [   3,   6,   9,  12,  15,  18,  21,  24,
                       27,  30,  33,  36,  39,  42,  45,  48 ] ];


(**************** CUT 307, - *****************)

ecut and [
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015618*x**0*z** 0 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x20015684*x**0*z** 1 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x200156f0*x**0*z** 2 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x2001575c*x**0*z** 3 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x200157c8*x**0*z** 4 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x20015834*x**0*z** 5 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x200158a0*x**0*z** 6 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x2001590c*x**0*z** 7 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20015978*x**0*z** 8 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200159e4*x**0*z** 9 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x20015a50*x**0*z**10 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x20015abc*x**0*z**11 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x20015b28*x**0*z**12 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x20015b94*x**0*z**13 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x20015c00*x**0*z**14 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x20015c6c*x**0*z**15 [ 3365569, y -       1, z**16 - 2630352 ]
] prove with [ cuts [ 145, 148, 151, 154, 157, 160, 163, 166,
                      169, 172, 175, 178, 181, 184, 187, 190 ] ];


(**************** CUT 308, - *****************)

ecut and [
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015624*x**0*z** 0 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x20015690*x**0*z** 1 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x200156fc*x**0*z** 2 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x20015768*x**0*z** 3 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x200157d4*x**0*z** 4 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x20015840*x**0*z** 5 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x200158ac*x**0*z** 6 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x20015918*x**0*z** 7 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20015984*x**0*z** 8 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200159f0*x**0*z** 9 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x20015a5c*x**0*z**10 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x20015ac8*x**0*z**11 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x20015b34*x**0*z**12 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x20015ba0*x**0*z**13 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x20015c0c*x**0*z**14 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x20015c78*x**0*z**15 [ 3365569, y -  452650, z**16 - 2630352 ]
] prove with [ cuts [ 145, 148, 151, 154, 157, 160, 163, 166,
                      169, 172, 175, 178, 181, 184, 187, 190 ] ];


(**************** CUT 309, - *****************)

ecut and [
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015630*x**0*z** 0 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x2001569c*x**0*z** 1 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x20015708*x**0*z** 2 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x20015774*x**0*z** 3 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x200157e0*x**0*z** 4 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x2001584c*x**0*z** 5 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x200158b8*x**0*z** 6 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x20015924*x**0*z** 7 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20015990*x**0*z** 8 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200159fc*x**0*z** 9 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x20015a68*x**0*z**10 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x20015ad4*x**0*z**11 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x20015b40*x**0*z**12 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x20015bac*x**0*z**13 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x20015c18*x**0*z**14 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x20015c84*x**0*z**15 [ 3365569, y - 2912918, z**16 - 2630352 ]
] prove with [ cuts [ 145, 148, 151, 154, 157, 160, 163, 166,
                      169, 172, 175, 178, 181, 184, 187, 190 ] ];


(**************** CUT 310, - *****************)

ecut and [
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x2001563c*x**0*z** 0 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x200156a8*x**0*z** 1 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x20015714*x**0*z** 2 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x20015780*x**0*z** 3 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x200157ec*x**0*z** 4 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015858*x**0*z** 5 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x200158c4*x**0*z** 6 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x20015930*x**0*z** 7 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x2001599c*x**0*z** 8 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x20015a08*x**0*z** 9 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x20015a74*x**0*z**10 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x20015ae0*x**0*z**11 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x20015b4c*x**0*z**12 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20015bb8*x**0*z**13 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x20015c24*x**0*z**14 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x20015c90*x**0*z**15 [ 3365569, y - 2113025, z**16 - 2630352 ]
] prove with [ cuts [ 146, 149, 152, 155, 158, 161, 164, 167,
                      170, 173, 176, 179, 182, 185, 188, 191 ] ];


(**************** CUT 311, - *****************)

ecut and [
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x20015648*x**0*z** 0 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x200156b4*x**0*z** 1 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x20015720*x**0*z** 2 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x2001578c*x**0*z** 3 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x200157f8*x**0*z** 4 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015864*x**0*z** 5 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x200158d0*x**0*z** 6 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x2001593c*x**0*z** 7 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x200159a8*x**0*z** 8 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x20015a14*x**0*z** 9 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x20015a80*x**0*z**10 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x20015aec*x**0*z**11 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x20015b58*x**0*z**12 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20015bc4*x**0*z**13 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x20015c30*x**0*z**14 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x20015c9c*x**0*z**15 [ 3365569, y - 3077709, z**16 - 2630352 ]
] prove with [ cuts [ 146, 149, 152, 155, 158, 161, 164, 167,
                      170, 173, 176, 179, 182, 185, 188, 191 ] ];


(**************** CUT 312, - *****************)

ecut and [
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x20015654*x**0*z** 0 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x200156c0*x**0*z** 1 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x2001572c*x**0*z** 2 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x20015798*x**0*z** 3 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x20015804*x**0*z** 4 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015870*x**0*z** 5 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x200158dc*x**0*z** 6 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x20015948*x**0*z** 7 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x200159b4*x**0*z** 8 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x20015a20*x**0*z** 9 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x20015a8c*x**0*z**10 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x20015af8*x**0*z**11 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x20015b64*x**0*z**12 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20015bd0*x**0*z**13 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x20015c3c*x**0*z**14 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x20015ca8*x**0*z**15 [ 3365569, y - 1540404, z**16 - 2630352 ]
] prove with [ cuts [ 146, 149, 152, 155, 158, 161, 164, 167,
                      170, 173, 176, 179, 182, 185, 188, 191 ] ];


(**************** CUT 313, - *****************)

ecut and [
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x20015660*x**0*z** 0 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x200156cc*x**0*z** 1 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015738*x**0*z** 2 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x200157a4*x**0*z** 3 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x20015810*x**0*z** 4 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x2001587c*x**0*z** 5 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x200158e8*x**0*z** 6 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x20015954*x**0*z** 7 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x200159c0*x**0*z** 8 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x20015a2c*x**0*z** 9 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20015a98*x**0*z**10 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x20015b04*x**0*z**11 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x20015b70*x**0*z**12 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x20015bdc*x**0*z**13 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20015c48*x**0*z**14 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x20015cb4*x**0*z**15 [ 3365569, y - 3117017, z**16 - 2630352 ]
] prove with [ cuts [ 147, 150, 153, 156, 159, 162, 165, 168,
                      171, 174, 177, 180, 183, 186, 189, 192 ] ];


(**************** CUT 314, - *****************)

ecut and [
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x2001566c*x**0*z** 0 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x200156d8*x**0*z** 1 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015744*x**0*z** 2 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x200157b0*x**0*z** 3 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x2001581c*x**0*z** 4 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x20015888*x**0*z** 5 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x200158f4*x**0*z** 6 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x20015960*x**0*z** 7 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x200159cc*x**0*z** 8 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x20015a38*x**0*z** 9 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20015aa4*x**0*z**10 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x20015b10*x**0*z**11 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x20015b7c*x**0*z**12 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x20015be8*x**0*z**13 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20015c54*x**0*z**14 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x20015cc0*x**0*z**15 [ 3365569, y -  543301, z**16 - 2630352 ]
] prove with [ cuts [ 147, 150, 153, 156, 159, 162, 165, 168,
                      171, 174, 177, 180, 183, 186, 189, 192 ] ];


(**************** CUT 315, - *****************)

ecut and [
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x20015678*x**0*z** 0 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x200156e4*x**0*z** 1 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015750*x**0*z** 2 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x200157bc*x**0*z** 3 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x20015828*x**0*z** 4 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x20015894*x**0*z** 5 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x20015900*x**0*z** 6 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x2001596c*x**0*z** 7 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x200159d8*x**0*z** 8 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x20015a44*x**0*z** 9 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20015ab0*x**0*z**10 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x20015b1c*x**0*z**11 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x20015b88*x**0*z**12 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x20015bf4*x**0*z**13 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20015c60*x**0*z**14 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x20015ccc*x**0*z**15 [ 3365569, y - 3070820, z**16 - 2630352 ]
] prove with [ cuts [ 147, 150, 153, 156, 159, 162, 165, 168,
                      171, 174, 177, 180, 183, 186, 189, 192 ] ];


(**************** CUT 316, - *****************)

ecut and [
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015cd8*x**0*z** 0 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x20015d44*x**0*z** 1 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x20015db0*x**0*z** 2 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x20015e1c*x**0*z** 3 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x20015e88*x**0*z** 4 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x20015ef4*x**0*z** 5 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x20015f60*x**0*z** 6 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x20015fcc*x**0*z** 7 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20016038*x**0*z** 8 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200160a4*x**0*z** 9 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x20016110*x**0*z**10 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x2001617c*x**0*z**11 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x200161e8*x**0*z**12 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x20016254*x**0*z**13 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x200162c0*x**0*z**14 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x2001632c*x**0*z**15 [ 3365569, y -       1, z**16 -  735217 ]
] prove with [ cuts [ 145, 148, 151, 154, 157, 160, 163, 166,
                      169, 172, 175, 178, 181, 184, 187, 190 ] ];


(**************** CUT 317, - *****************)

ecut and [
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015ce4*x**0*z** 0 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x20015d50*x**0*z** 1 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x20015dbc*x**0*z** 2 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x20015e28*x**0*z** 3 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x20015e94*x**0*z** 4 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x20015f00*x**0*z** 5 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x20015f6c*x**0*z** 6 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x20015fd8*x**0*z** 7 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20016044*x**0*z** 8 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200160b0*x**0*z** 9 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x2001611c*x**0*z**10 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x20016188*x**0*z**11 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x200161f4*x**0*z**12 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x20016260*x**0*z**13 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x200162cc*x**0*z**14 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x20016338*x**0*z**15 [ 3365569, y -  452650, z**16 -  735217 ]
] prove with [ cuts [ 145, 148, 151, 154, 157, 160, 163, 166,
                      169, 172, 175, 178, 181, 184, 187, 190 ] ];


(**************** CUT 318, - *****************)

ecut and [
eqmod (cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
       cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16)
      L0x20015cf0*x**0*z** 0 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
       cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17)
      L0x20015d5c*x**0*z** 1 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
       cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18)
      L0x20015dc8*x**0*z** 2 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
       cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19)
      L0x20015e34*x**0*z** 3 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
       cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20)
      L0x20015ea0*x**0*z** 4 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
       cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21)
      L0x20015f0c*x**0*z** 5 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
       cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22)
      L0x20015f78*x**0*z** 6 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
       cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23)
      L0x20015fe4*x**0*z** 7 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
       cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24)
      L0x20016050*x**0*z** 8 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
       cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25)
      L0x200160bc*x**0*z** 9 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
       cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26)
      L0x20016128*x**0*z**10 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
       cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27)
      L0x20016194*x**0*z**11 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
       cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28)
      L0x20016200*x**0*z**12 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
       cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29)
      L0x2001626c*x**0*z**13 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
       cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30)
      L0x200162d8*x**0*z**14 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
       cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31)
      L0x20016344*x**0*z**15 [ 3365569, y - 2912918, z**16 -  735217 ]
] prove with [ cuts [ 145, 148, 151, 154, 157, 160, 163, 166,
                      169, 172, 175, 178, 181, 184, 187, 190 ] ];


(**************** CUT 319, - *****************)

ecut and [
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x20015cfc*x**0*z** 0 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x20015d68*x**0*z** 1 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x20015dd4*x**0*z** 2 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x20015e40*x**0*z** 3 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x20015eac*x**0*z** 4 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015f18*x**0*z** 5 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x20015f84*x**0*z** 6 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x20015ff0*x**0*z** 7 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x2001605c*x**0*z** 8 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x200160c8*x**0*z** 9 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x20016134*x**0*z**10 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x200161a0*x**0*z**11 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x2001620c*x**0*z**12 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20016278*x**0*z**13 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x200162e4*x**0*z**14 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x20016350*x**0*z**15 [ 3365569, y - 2113025, z**16 -  735217 ]
] prove with [ cuts [ 146, 149, 152, 155, 158, 161, 164, 167,
                      170, 173, 176, 179, 182, 185, 188, 191 ] ];


(**************** CUT 320, - *****************)

ecut and [
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x20015d08*x**0*z** 0 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x20015d74*x**0*z** 1 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x20015de0*x**0*z** 2 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x20015e4c*x**0*z** 3 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x20015eb8*x**0*z** 4 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015f24*x**0*z** 5 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x20015f90*x**0*z** 6 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x20015ffc*x**0*z** 7 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x20016068*x**0*z** 8 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x200160d4*x**0*z** 9 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x20016140*x**0*z**10 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x200161ac*x**0*z**11 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x20016218*x**0*z**12 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20016284*x**0*z**13 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x200162f0*x**0*z**14 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x2001635c*x**0*z**15 [ 3365569, y - 3077709, z**16 -  735217 ]
] prove with [ cuts [ 146, 149, 152, 155, 158, 161, 164, 167,
                      170, 173, 176, 179, 182, 185, 188, 191 ] ];


(**************** CUT 321, - *****************)

ecut and [
eqmod (cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
       cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16)
      L0x20015d14*x**0*z** 0 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
       cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17)
      L0x20015d80*x**0*z** 1 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
       cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18)
      L0x20015dec*x**0*z** 2 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
       cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19)
      L0x20015e58*x**0*z** 3 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
       cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20)
      L0x20015ec4*x**0*z** 4 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
       cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21)
      L0x20015f30*x**0*z** 5 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
       cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22)
      L0x20015f9c*x**0*z** 6 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
       cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23)
      L0x20016008*x**0*z** 7 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
       cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24)
      L0x20016074*x**0*z** 8 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
       cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25)
      L0x200160e0*x**0*z** 9 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
       cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26)
      L0x2001614c*x**0*z**10 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
       cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27)
      L0x200161b8*x**0*z**11 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
       cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28)
      L0x20016224*x**0*z**12 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
       cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29)
      L0x20016290*x**0*z**13 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
       cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30)
      L0x200162fc*x**0*z**14 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
       cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31)
      L0x20016368*x**0*z**15 [ 3365569, y - 1540404, z**16 -  735217 ]
] prove with [ cuts [ 146, 149, 152, 155, 158, 161, 164, 167,
                      170, 173, 176, 179, 182, 185, 188, 191 ] ];


(**************** CUT 322, - *****************)

ecut and [
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x20015d20*x**0*z** 0 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x20015d8c*x**0*z** 1 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015df8*x**0*z** 2 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x20015e64*x**0*z** 3 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x20015ed0*x**0*z** 4 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x20015f3c*x**0*z** 5 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x20015fa8*x**0*z** 6 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x20016014*x**0*z** 7 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x20016080*x**0*z** 8 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x200160ec*x**0*z** 9 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20016158*x**0*z**10 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x200161c4*x**0*z**11 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x20016230*x**0*z**12 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x2001629c*x**0*z**13 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20016308*x**0*z**14 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x20016374*x**0*z**15 [ 3365569, y - 3117017, z**16 -  735217 ]
] prove with [ cuts [ 147, 150, 153, 156, 159, 162, 165, 168,
                      171, 174, 177, 180, 183, 186, 189, 192 ] ];


(**************** CUT 323, - *****************)

ecut and [
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x20015d2c*x**0*z** 0 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x20015d98*x**0*z** 1 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015e04*x**0*z** 2 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x20015e70*x**0*z** 3 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x20015edc*x**0*z** 4 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x20015f48*x**0*z** 5 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x20015fb4*x**0*z** 6 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x20016020*x**0*z** 7 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x2001608c*x**0*z** 8 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x200160f8*x**0*z** 9 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20016164*x**0*z**10 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x200161d0*x**0*z**11 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x2001623c*x**0*z**12 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x200162a8*x**0*z**13 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20016314*x**0*z**14 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x20016380*x**0*z**15 [ 3365569, y -  543301, z**16 -  735217 ]
] prove with [ cuts [ 147, 150, 153, 156, 159, 162, 165, 168,
                      171, 174, 177, 180, 183, 186, 189, 192 ] ];


(**************** CUT 324, - *****************)

ecut and [
eqmod (cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
       cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16)
      L0x20015d38*x**0*z** 0 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
       cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17)
      L0x20015da4*x**0*z** 1 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
       cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18)
      L0x20015e10*x**0*z** 2 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
       cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19)
      L0x20015e7c*x**0*z** 3 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
       cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20)
      L0x20015ee8*x**0*z** 4 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
       cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21)
      L0x20015f54*x**0*z** 5 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
       cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22)
      L0x20015fc0*x**0*z** 6 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
       cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23)
      L0x2001602c*x**0*z** 7 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
       cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24)
      L0x20016098*x**0*z** 8 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
       cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25)
      L0x20016104*x**0*z** 9 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
       cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26)
      L0x20016170*x**0*z**10 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
       cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27)
      L0x200161dc*x**0*z**11 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
       cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28)
      L0x20016248*x**0*z**12 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
       cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29)
      L0x200162b4*x**0*z**13 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
       cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30)
      L0x20016320*x**0*z**14 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
       cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31)
      L0x2001638c*x**0*z**15 [ 3365569, y - 3070820, z**16 -  735217 ]
] prove with [ cuts [ 147, 150, 153, 156, 159, 162, 165, 168,
                      171, 174, 177, 180, 183, 186, 189, 192 ] ];


(**************** CUT 325, - *****************)

ecut and [
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x2001489c*x**1*z** 0 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014908*x**1*z** 1 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x20014974*x**1*z** 2 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200149e0*x**1*z** 3 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x20014a4c*x**1*z** 4 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20014ab8*x**1*z** 5 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x20014b24*x**1*z** 6 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x20014b90*x**1*z** 7 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x20014bfc*x**1*z** 8 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20014c68*x**1*z** 9 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x20014cd4*x**1*z**10 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x20014d40*x**1*z**11 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x20014dac*x**1*z**12 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x20014e18*x**1*z**13 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x20014e84*x**1*z**14 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x20014ef0*x**1*z**15 [ 3365569, y -       1, z**16 -       1 ]
] prove with [ cuts [  49,  52,  55,  58,  61,  64,  67,  70,
                       73,  76,  79,  82,  85,  88,  91,  94 ] ];


(**************** CUT 326, - *****************)

ecut and [
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x200148a8*x**1*z** 0 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014914*x**1*z** 1 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x20014980*x**1*z** 2 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200149ec*x**1*z** 3 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x20014a58*x**1*z** 4 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20014ac4*x**1*z** 5 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x20014b30*x**1*z** 6 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x20014b9c*x**1*z** 7 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x20014c08*x**1*z** 8 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20014c74*x**1*z** 9 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x20014ce0*x**1*z**10 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x20014d4c*x**1*z**11 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x20014db8*x**1*z**12 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x20014e24*x**1*z**13 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x20014e90*x**1*z**14 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x20014efc*x**1*z**15 [ 3365569, y -  452650, z**16 -       1 ]
] prove with [ cuts [  49,  52,  55,  58,  61,  64,  67,  70,
                       73,  76,  79,  82,  85,  88,  91,  94 ] ];


(**************** CUT 327, - *****************)

ecut and [
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x200148b4*x**1*z** 0 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014920*x**1*z** 1 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x2001498c*x**1*z** 2 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200149f8*x**1*z** 3 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x20014a64*x**1*z** 4 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20014ad0*x**1*z** 5 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x20014b3c*x**1*z** 6 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x20014ba8*x**1*z** 7 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x20014c14*x**1*z** 8 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20014c80*x**1*z** 9 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x20014cec*x**1*z**10 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x20014d58*x**1*z**11 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x20014dc4*x**1*z**12 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x20014e30*x**1*z**13 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x20014e9c*x**1*z**14 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x20014f08*x**1*z**15 [ 3365569, y - 2912918, z**16 -       1 ]
] prove with [ cuts [  49,  52,  55,  58,  61,  64,  67,  70,
                       73,  76,  79,  82,  85,  88,  91,  94 ] ];


(**************** CUT 328, - *****************)

ecut and [
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x200148c0*x**1*z** 0 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x2001492c*x**1*z** 1 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x20014998*x**1*z** 2 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x20014a04*x**1*z** 3 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x20014a70*x**1*z** 4 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x20014adc*x**1*z** 5 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20014b48*x**1*z** 6 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x20014bb4*x**1*z** 7 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x20014c20*x**1*z** 8 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x20014c8c*x**1*z** 9 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x20014cf8*x**1*z**10 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x20014d64*x**1*z**11 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x20014dd0*x**1*z**12 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x20014e3c*x**1*z**13 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20014ea8*x**1*z**14 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x20014f14*x**1*z**15 [ 3365569, y - 2113025, z**16 -       1 ]
] prove with [ cuts [  50,  53,  56,  59,  62,  65,  68,  71,
                       74,  77,  80,  83,  86,  89,  92,  95 ] ];


(**************** CUT 329, - *****************)

ecut and [
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x200148cc*x**1*z** 0 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x20014938*x**1*z** 1 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x200149a4*x**1*z** 2 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x20014a10*x**1*z** 3 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x20014a7c*x**1*z** 4 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x20014ae8*x**1*z** 5 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20014b54*x**1*z** 6 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x20014bc0*x**1*z** 7 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x20014c2c*x**1*z** 8 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x20014c98*x**1*z** 9 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x20014d04*x**1*z**10 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x20014d70*x**1*z**11 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x20014ddc*x**1*z**12 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x20014e48*x**1*z**13 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20014eb4*x**1*z**14 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x20014f20*x**1*z**15 [ 3365569, y - 3077709, z**16 -       1 ]
] prove with [ cuts [  50,  53,  56,  59,  62,  65,  68,  71,
                       74,  77,  80,  83,  86,  89,  92,  95 ] ];


(**************** CUT 330, - *****************)

ecut and [
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x200148d8*x**1*z** 0 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x20014944*x**1*z** 1 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x200149b0*x**1*z** 2 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x20014a1c*x**1*z** 3 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x20014a88*x**1*z** 4 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x20014af4*x**1*z** 5 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20014b60*x**1*z** 6 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x20014bcc*x**1*z** 7 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x20014c38*x**1*z** 8 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x20014ca4*x**1*z** 9 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x20014d10*x**1*z**10 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x20014d7c*x**1*z**11 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x20014de8*x**1*z**12 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x20014e54*x**1*z**13 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20014ec0*x**1*z**14 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x20014f2c*x**1*z**15 [ 3365569, y - 1540404, z**16 -       1 ]
] prove with [ cuts [  50,  53,  56,  59,  62,  65,  68,  71,
                       74,  77,  80,  83,  86,  89,  92,  95 ] ];


(**************** CUT 331, - *****************)

ecut and [
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x200148e4*x**1*z** 0 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x20014950*x**1*z** 1 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x200149bc*x**1*z** 2 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x20014a28*x**1*z** 3 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x20014a94*x**1*z** 4 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x20014b00*x**1*z** 5 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x20014b6c*x**1*z** 6 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x20014bd8*x**1*z** 7 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x20014c44*x**1*z** 8 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x20014cb0*x**1*z** 9 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x20014d1c*x**1*z**10 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20014d88*x**1*z**11 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x20014df4*x**1*z**12 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x20014e60*x**1*z**13 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x20014ecc*x**1*z**14 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x20014f38*x**1*z**15 [ 3365569, y - 3117017, z**16 -       1 ]
] prove with [ cuts [  51,  54,  57,  60,  63,  66,  69,  72,
                       75,  78,  81,  84,  87,  90,  93,  96 ] ];


(**************** CUT 332, - *****************)

ecut and [
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x200148f0*x**1*z** 0 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x2001495c*x**1*z** 1 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x200149c8*x**1*z** 2 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x20014a34*x**1*z** 3 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x20014aa0*x**1*z** 4 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x20014b0c*x**1*z** 5 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x20014b78*x**1*z** 6 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x20014be4*x**1*z** 7 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x20014c50*x**1*z** 8 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x20014cbc*x**1*z** 9 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x20014d28*x**1*z**10 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20014d94*x**1*z**11 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x20014e00*x**1*z**12 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x20014e6c*x**1*z**13 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x20014ed8*x**1*z**14 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x20014f44*x**1*z**15 [ 3365569, y -  543301, z**16 -       1 ]
] prove with [ cuts [  51,  54,  57,  60,  63,  66,  69,  72,
                       75,  78,  81,  84,  87,  90,  93,  96 ] ];


(**************** CUT 333, - *****************)

ecut and [
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x200148fc*x**1*z** 0 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x20014968*x**1*z** 1 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x200149d4*x**1*z** 2 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x20014a40*x**1*z** 3 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x20014aac*x**1*z** 4 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x20014b18*x**1*z** 5 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x20014b84*x**1*z** 6 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x20014bf0*x**1*z** 7 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x20014c5c*x**1*z** 8 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x20014cc8*x**1*z** 9 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x20014d34*x**1*z**10 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20014da0*x**1*z**11 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x20014e0c*x**1*z**12 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x20014e78*x**1*z**13 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x20014ee4*x**1*z**14 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x20014f50*x**1*z**15 [ 3365569, y - 3070820, z**16 -       1 ]
] prove with [ cuts [  51,  54,  57,  60,  63,  66,  69,  72,
                       75,  78,  81,  84,  87,  90,  93,  96 ] ];


(**************** CUT 334, - *****************)

ecut and [
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x20014f5c*x**1*z** 0 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014fc8*x**1*z** 1 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x20015034*x**1*z** 2 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200150a0*x**1*z** 3 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x2001510c*x**1*z** 4 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20015178*x**1*z** 5 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x200151e4*x**1*z** 6 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x20015250*x**1*z** 7 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x200152bc*x**1*z** 8 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20015328*x**1*z** 9 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x20015394*x**1*z**10 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x20015400*x**1*z**11 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x2001546c*x**1*z**12 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x200154d8*x**1*z**13 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x20015544*x**1*z**14 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x200155b0*x**1*z**15 [ 3365569, y -       1, z**16 - 3365568 ]
] prove with [ cuts [  49,  52,  55,  58,  61,  64,  67,  70,
                       73,  76,  79,  82,  85,  88,  91,  94 ] ];


(**************** CUT 335, - *****************)

ecut and [
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x20014f68*x**1*z** 0 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014fd4*x**1*z** 1 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x20015040*x**1*z** 2 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200150ac*x**1*z** 3 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x20015118*x**1*z** 4 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20015184*x**1*z** 5 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x200151f0*x**1*z** 6 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x2001525c*x**1*z** 7 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x200152c8*x**1*z** 8 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20015334*x**1*z** 9 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x200153a0*x**1*z**10 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x2001540c*x**1*z**11 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x20015478*x**1*z**12 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x200154e4*x**1*z**13 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x20015550*x**1*z**14 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x200155bc*x**1*z**15 [ 3365569, y -  452650, z**16 - 3365568 ]
] prove with [ cuts [  49,  52,  55,  58,  61,  64,  67,  70,
                       73,  76,  79,  82,  85,  88,  91,  94 ] ];


(**************** CUT 336, - *****************)

ecut and [
eqmod (cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
       cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16)
      L0x20014f74*x**1*z** 0 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
       cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17)
      L0x20014fe0*x**1*z** 1 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
       cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18)
      L0x2001504c*x**1*z** 2 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
       cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19)
      L0x200150b8*x**1*z** 3 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
       cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20)
      L0x20015124*x**1*z** 4 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
       cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21)
      L0x20015190*x**1*z** 5 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
       cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22)
      L0x200151fc*x**1*z** 6 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
       cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23)
      L0x20015268*x**1*z** 7 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
       cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24)
      L0x200152d4*x**1*z** 8 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
       cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25)
      L0x20015340*x**1*z** 9 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
       cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26)
      L0x200153ac*x**1*z**10 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
       cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27)
      L0x20015418*x**1*z**11 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
       cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28)
      L0x20015484*x**1*z**12 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
       cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29)
      L0x200154f0*x**1*z**13 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
       cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30)
      L0x2001555c*x**1*z**14 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
       cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31)
      L0x200155c8*x**1*z**15 [ 3365569, y - 2912918, z**16 - 3365568 ]
] prove with [ cuts [  49,  52,  55,  58,  61,  64,  67,  70,
                       73,  76,  79,  82,  85,  88,  91,  94 ] ];


(**************** CUT 337, - *****************)

ecut and [
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x20014f80*x**1*z** 0 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x20014fec*x**1*z** 1 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x20015058*x**1*z** 2 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x200150c4*x**1*z** 3 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x20015130*x**1*z** 4 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x2001519c*x**1*z** 5 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20015208*x**1*z** 6 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x20015274*x**1*z** 7 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x200152e0*x**1*z** 8 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x2001534c*x**1*z** 9 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x200153b8*x**1*z**10 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x20015424*x**1*z**11 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x20015490*x**1*z**12 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x200154fc*x**1*z**13 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20015568*x**1*z**14 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x200155d4*x**1*z**15 [ 3365569, y - 2113025, z**16 - 3365568 ]
] prove with [ cuts [  50,  53,  56,  59,  62,  65,  68,  71,
                       74,  77,  80,  83,  86,  89,  92,  95 ] ];


(**************** CUT 338, - *****************)

ecut and [
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x20014f8c*x**1*z** 0 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x20014ff8*x**1*z** 1 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x20015064*x**1*z** 2 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x200150d0*x**1*z** 3 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x2001513c*x**1*z** 4 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x200151a8*x**1*z** 5 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20015214*x**1*z** 6 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x20015280*x**1*z** 7 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x200152ec*x**1*z** 8 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x20015358*x**1*z** 9 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x200153c4*x**1*z**10 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x20015430*x**1*z**11 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x2001549c*x**1*z**12 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x20015508*x**1*z**13 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20015574*x**1*z**14 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x200155e0*x**1*z**15 [ 3365569, y - 3077709, z**16 - 3365568 ]
] prove with [ cuts [  50,  53,  56,  59,  62,  65,  68,  71,
                       74,  77,  80,  83,  86,  89,  92,  95 ] ];


(**************** CUT 339, - *****************)

ecut and [
eqmod (cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
       cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16)
      L0x20014f98*x**1*z** 0 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
       cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17)
      L0x20015004*x**1*z** 1 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
       cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18)
      L0x20015070*x**1*z** 2 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
       cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19)
      L0x200150dc*x**1*z** 3 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
       cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20)
      L0x20015148*x**1*z** 4 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
       cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21)
      L0x200151b4*x**1*z** 5 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
       cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22)
      L0x20015220*x**1*z** 6 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
       cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23)
      L0x2001528c*x**1*z** 7 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
       cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24)
      L0x200152f8*x**1*z** 8 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
       cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25)
      L0x20015364*x**1*z** 9 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
       cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26)
      L0x200153d0*x**1*z**10 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
       cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27)
      L0x2001543c*x**1*z**11 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
       cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28)
      L0x200154a8*x**1*z**12 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
       cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29)
      L0x20015514*x**1*z**13 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
       cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30)
      L0x20015580*x**1*z**14 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
       cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31)
      L0x200155ec*x**1*z**15 [ 3365569, y - 1540404, z**16 - 3365568 ]
] prove with [ cuts [  50,  53,  56,  59,  62,  65,  68,  71,
                       74,  77,  80,  83,  86,  89,  92,  95 ] ];


(**************** CUT 340, - *****************)

ecut and [
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x20014fa4*x**1*z** 0 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x20015010*x**1*z** 1 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x2001507c*x**1*z** 2 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x200150e8*x**1*z** 3 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x20015154*x**1*z** 4 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x200151c0*x**1*z** 5 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x2001522c*x**1*z** 6 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x20015298*x**1*z** 7 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x20015304*x**1*z** 8 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x20015370*x**1*z** 9 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x200153dc*x**1*z**10 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20015448*x**1*z**11 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x200154b4*x**1*z**12 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x20015520*x**1*z**13 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x2001558c*x**1*z**14 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x200155f8*x**1*z**15 [ 3365569, y - 3117017, z**16 - 3365568 ]
] prove with [ cuts [  51,  54,  57,  60,  63,  66,  69,  72,
                       75,  78,  81,  84,  87,  90,  93,  96 ] ];


(**************** CUT 341, - *****************)

ecut and [
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x20014fb0*x**1*z** 0 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x2001501c*x**1*z** 1 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x20015088*x**1*z** 2 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x200150f4*x**1*z** 3 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x20015160*x**1*z** 4 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x200151cc*x**1*z** 5 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x20015238*x**1*z** 6 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x200152a4*x**1*z** 7 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x20015310*x**1*z** 8 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x2001537c*x**1*z** 9 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x200153e8*x**1*z**10 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20015454*x**1*z**11 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x200154c0*x**1*z**12 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x2001552c*x**1*z**13 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x20015598*x**1*z**14 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x20015604*x**1*z**15 [ 3365569, y -  543301, z**16 - 3365568 ]
] prove with [ cuts [  51,  54,  57,  60,  63,  66,  69,  72,
                       75,  78,  81,  84,  87,  90,  93,  96 ] ];


(**************** CUT 342, - *****************)

ecut and [
eqmod (cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
       cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16)
      L0x20014fbc*x**1*z** 0 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
       cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17)
      L0x20015028*x**1*z** 1 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
       cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18)
      L0x20015094*x**1*z** 2 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
       cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19)
      L0x20015100*x**1*z** 3 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
       cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20)
      L0x2001516c*x**1*z** 4 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
       cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21)
      L0x200151d8*x**1*z** 5 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
       cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22)
      L0x20015244*x**1*z** 6 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
       cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23)
      L0x200152b0*x**1*z** 7 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
       cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24)
      L0x2001531c*x**1*z** 8 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
       cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25)
      L0x20015388*x**1*z** 9 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
       cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26)
      L0x200153f4*x**1*z**10 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
       cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27)
      L0x20015460*x**1*z**11 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
       cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28)
      L0x200154cc*x**1*z**12 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
       cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29)
      L0x20015538*x**1*z**13 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
       cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30)
      L0x200155a4*x**1*z**14 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
       cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31)
      L0x20015610*x**1*z**15 [ 3365569, y - 3070820, z**16 - 3365568 ]
] prove with [ cuts [  51,  54,  57,  60,  63,  66,  69,  72,
                       75,  78,  81,  84,  87,  90,  93,  96 ] ];


(**************** CUT 343, - *****************)

ecut and [
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x2001561c*x**1*z** 0 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x20015688*x**1*z** 1 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x200156f4*x**1*z** 2 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x20015760*x**1*z** 3 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x200157cc*x**1*z** 4 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015838*x**1*z** 5 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x200158a4*x**1*z** 6 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x20015910*x**1*z** 7 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x2001597c*x**1*z** 8 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x200159e8*x**1*z** 9 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x20015a54*x**1*z**10 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x20015ac0*x**1*z**11 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x20015b2c*x**1*z**12 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20015b98*x**1*z**13 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x20015c04*x**1*z**14 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x20015c70*x**1*z**15 [ 3365569, y -       1, z**16 - 2630352 ]
] prove with [ cuts [ 193, 196, 199, 202, 205, 208, 211, 214,
                      217, 220, 223, 226, 229, 232, 235, 238 ] ];


(**************** CUT 344, - *****************)

ecut and [
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x20015628*x**1*z** 0 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x20015694*x**1*z** 1 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x20015700*x**1*z** 2 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x2001576c*x**1*z** 3 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x200157d8*x**1*z** 4 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015844*x**1*z** 5 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x200158b0*x**1*z** 6 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x2001591c*x**1*z** 7 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x20015988*x**1*z** 8 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x200159f4*x**1*z** 9 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x20015a60*x**1*z**10 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x20015acc*x**1*z**11 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x20015b38*x**1*z**12 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20015ba4*x**1*z**13 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x20015c10*x**1*z**14 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x20015c7c*x**1*z**15 [ 3365569, y -  452650, z**16 - 2630352 ]
] prove with [ cuts [ 193, 196, 199, 202, 205, 208, 211, 214,
                      217, 220, 223, 226, 229, 232, 235, 238 ] ];


(**************** CUT 345, - *****************)

ecut and [
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x20015634*x**1*z** 0 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x200156a0*x**1*z** 1 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x2001570c*x**1*z** 2 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x20015778*x**1*z** 3 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x200157e4*x**1*z** 4 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015850*x**1*z** 5 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x200158bc*x**1*z** 6 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x20015928*x**1*z** 7 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x20015994*x**1*z** 8 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x20015a00*x**1*z** 9 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x20015a6c*x**1*z**10 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x20015ad8*x**1*z**11 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x20015b44*x**1*z**12 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20015bb0*x**1*z**13 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x20015c1c*x**1*z**14 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x20015c88*x**1*z**15 [ 3365569, y - 2912918, z**16 - 2630352 ]
] prove with [ cuts [ 193, 196, 199, 202, 205, 208, 211, 214,
                      217, 220, 223, 226, 229, 232, 235, 238 ] ];


(**************** CUT 346, - *****************)

ecut and [
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x20015640*x**1*z** 0 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x200156ac*x**1*z** 1 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015718*x**1*z** 2 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x20015784*x**1*z** 3 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x200157f0*x**1*z** 4 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x2001585c*x**1*z** 5 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x200158c8*x**1*z** 6 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x20015934*x**1*z** 7 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x200159a0*x**1*z** 8 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x20015a0c*x**1*z** 9 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20015a78*x**1*z**10 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x20015ae4*x**1*z**11 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x20015b50*x**1*z**12 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x20015bbc*x**1*z**13 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x20015c28*x**1*z**14 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x20015c94*x**1*z**15 [ 3365569, y - 2113025, z**16 - 2630352 ]
] prove with [ cuts [ 194, 197, 200, 203, 206, 209, 212, 215,
                      218, 221, 224, 227, 230, 233, 236, 239 ] ];


(**************** CUT 347, - *****************)

ecut and [
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x2001564c*x**1*z** 0 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x200156b8*x**1*z** 1 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015724*x**1*z** 2 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x20015790*x**1*z** 3 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x200157fc*x**1*z** 4 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x20015868*x**1*z** 5 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x200158d4*x**1*z** 6 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x20015940*x**1*z** 7 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x200159ac*x**1*z** 8 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x20015a18*x**1*z** 9 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20015a84*x**1*z**10 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x20015af0*x**1*z**11 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x20015b5c*x**1*z**12 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x20015bc8*x**1*z**13 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x20015c34*x**1*z**14 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x20015ca0*x**1*z**15 [ 3365569, y - 3077709, z**16 - 2630352 ]
] prove with [ cuts [ 194, 197, 200, 203, 206, 209, 212, 215,
                      218, 221, 224, 227, 230, 233, 236, 239 ] ];


(**************** CUT 348, - *****************)

ecut and [
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x20015658*x**1*z** 0 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x200156c4*x**1*z** 1 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015730*x**1*z** 2 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x2001579c*x**1*z** 3 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x20015808*x**1*z** 4 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x20015874*x**1*z** 5 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x200158e0*x**1*z** 6 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x2001594c*x**1*z** 7 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x200159b8*x**1*z** 8 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x20015a24*x**1*z** 9 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20015a90*x**1*z**10 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x20015afc*x**1*z**11 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x20015b68*x**1*z**12 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x20015bd4*x**1*z**13 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x20015c40*x**1*z**14 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x20015cac*x**1*z**15 [ 3365569, y - 1540404, z**16 - 2630352 ]
] prove with [ cuts [ 194, 197, 200, 203, 206, 209, 212, 215,
                      218, 221, 224, 227, 230, 233, 236, 239 ] ];


(**************** CUT 349, - *****************)

ecut and [
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x20015664*x**1*z** 0 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x200156d0*x**1*z** 1 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x2001573c*x**1*z** 2 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x200157a8*x**1*z** 3 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x20015814*x**1*z** 4 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x20015880*x**1*z** 5 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x200158ec*x**1*z** 6 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20015958*x**1*z** 7 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x200159c4*x**1*z** 8 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x20015a30*x**1*z** 9 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x20015a9c*x**1*z**10 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x20015b08*x**1*z**11 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x20015b74*x**1*z**12 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x20015be0*x**1*z**13 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x20015c4c*x**1*z**14 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20015cb8*x**1*z**15 [ 3365569, y - 3117017, z**16 - 2630352 ]
] prove with [ cuts [ 195, 198, 201, 204, 207, 210, 213, 216,
                      219, 222, 225, 228, 231, 234, 237, 240 ] ];


(**************** CUT 350, - *****************)

ecut and [
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x20015670*x**1*z** 0 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x200156dc*x**1*z** 1 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x20015748*x**1*z** 2 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x200157b4*x**1*z** 3 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x20015820*x**1*z** 4 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x2001588c*x**1*z** 5 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x200158f8*x**1*z** 6 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20015964*x**1*z** 7 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x200159d0*x**1*z** 8 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x20015a3c*x**1*z** 9 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x20015aa8*x**1*z**10 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x20015b14*x**1*z**11 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x20015b80*x**1*z**12 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x20015bec*x**1*z**13 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x20015c58*x**1*z**14 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20015cc4*x**1*z**15 [ 3365569, y -  543301, z**16 - 2630352 ]
] prove with [ cuts [ 195, 198, 201, 204, 207, 210, 213, 216,
                      219, 222, 225, 228, 231, 234, 237, 240 ] ];


(**************** CUT 351, - *****************)

ecut and [
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x2001567c*x**1*z** 0 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x200156e8*x**1*z** 1 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x20015754*x**1*z** 2 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x200157c0*x**1*z** 3 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x2001582c*x**1*z** 4 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x20015898*x**1*z** 5 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x20015904*x**1*z** 6 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20015970*x**1*z** 7 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x200159dc*x**1*z** 8 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x20015a48*x**1*z** 9 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x20015ab4*x**1*z**10 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x20015b20*x**1*z**11 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x20015b8c*x**1*z**12 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x20015bf8*x**1*z**13 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x20015c64*x**1*z**14 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20015cd0*x**1*z**15 [ 3365569, y - 3070820, z**16 - 2630352 ]
] prove with [ cuts [ 195, 198, 201, 204, 207, 210, 213, 216,
                      219, 222, 225, 228, 231, 234, 237, 240 ] ];


(**************** CUT 352, - *****************)

ecut and [
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x20015cdc*x**1*z** 0 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x20015d48*x**1*z** 1 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x20015db4*x**1*z** 2 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x20015e20*x**1*z** 3 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x20015e8c*x**1*z** 4 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015ef8*x**1*z** 5 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x20015f64*x**1*z** 6 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x20015fd0*x**1*z** 7 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x2001603c*x**1*z** 8 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x200160a8*x**1*z** 9 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x20016114*x**1*z**10 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x20016180*x**1*z**11 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x200161ec*x**1*z**12 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20016258*x**1*z**13 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x200162c4*x**1*z**14 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x20016330*x**1*z**15 [ 3365569, y -       1, z**16 -  735217 ]
] prove with [ cuts [ 193, 196, 199, 202, 205, 208, 211, 214,
                      217, 220, 223, 226, 229, 232, 235, 238 ] ];


(**************** CUT 353, - *****************)

ecut and [
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x20015ce8*x**1*z** 0 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x20015d54*x**1*z** 1 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x20015dc0*x**1*z** 2 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x20015e2c*x**1*z** 3 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x20015e98*x**1*z** 4 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015f04*x**1*z** 5 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x20015f70*x**1*z** 6 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x20015fdc*x**1*z** 7 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x20016048*x**1*z** 8 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x200160b4*x**1*z** 9 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x20016120*x**1*z**10 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x2001618c*x**1*z**11 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x200161f8*x**1*z**12 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20016264*x**1*z**13 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x200162d0*x**1*z**14 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x2001633c*x**1*z**15 [ 3365569, y -  452650, z**16 -  735217 ]
] prove with [ cuts [ 193, 196, 199, 202, 205, 208, 211, 214,
                      217, 220, 223, 226, 229, 232, 235, 238 ] ];


(**************** CUT 354, - *****************)

ecut and [
eqmod (cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
       cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16)
      L0x20015cf4*x**1*z** 0 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
       cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17)
      L0x20015d60*x**1*z** 1 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
       cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18)
      L0x20015dcc*x**1*z** 2 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
       cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19)
      L0x20015e38*x**1*z** 3 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
       cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20)
      L0x20015ea4*x**1*z** 4 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
       cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21)
      L0x20015f10*x**1*z** 5 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
       cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22)
      L0x20015f7c*x**1*z** 6 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
       cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23)
      L0x20015fe8*x**1*z** 7 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
       cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24)
      L0x20016054*x**1*z** 8 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
       cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25)
      L0x200160c0*x**1*z** 9 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
       cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26)
      L0x2001612c*x**1*z**10 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
       cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27)
      L0x20016198*x**1*z**11 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
       cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28)
      L0x20016204*x**1*z**12 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
       cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29)
      L0x20016270*x**1*z**13 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
       cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30)
      L0x200162dc*x**1*z**14 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
       cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31)
      L0x20016348*x**1*z**15 [ 3365569, y - 2912918, z**16 -  735217 ]
] prove with [ cuts [ 193, 196, 199, 202, 205, 208, 211, 214,
                      217, 220, 223, 226, 229, 232, 235, 238 ] ];


(**************** CUT 355, - *****************)

ecut and [
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x20015d00*x**1*z** 0 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x20015d6c*x**1*z** 1 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015dd8*x**1*z** 2 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x20015e44*x**1*z** 3 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x20015eb0*x**1*z** 4 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x20015f1c*x**1*z** 5 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x20015f88*x**1*z** 6 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x20015ff4*x**1*z** 7 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x20016060*x**1*z** 8 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x200160cc*x**1*z** 9 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20016138*x**1*z**10 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x200161a4*x**1*z**11 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x20016210*x**1*z**12 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x2001627c*x**1*z**13 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x200162e8*x**1*z**14 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x20016354*x**1*z**15 [ 3365569, y - 2113025, z**16 -  735217 ]
] prove with [ cuts [ 194, 197, 200, 203, 206, 209, 212, 215,
                      218, 221, 224, 227, 230, 233, 236, 239 ] ];


(**************** CUT 356, - *****************)

ecut and [
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x20015d0c*x**1*z** 0 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x20015d78*x**1*z** 1 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015de4*x**1*z** 2 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x20015e50*x**1*z** 3 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x20015ebc*x**1*z** 4 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x20015f28*x**1*z** 5 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x20015f94*x**1*z** 6 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x20016000*x**1*z** 7 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x2001606c*x**1*z** 8 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x200160d8*x**1*z** 9 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20016144*x**1*z**10 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x200161b0*x**1*z**11 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x2001621c*x**1*z**12 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x20016288*x**1*z**13 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x200162f4*x**1*z**14 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x20016360*x**1*z**15 [ 3365569, y - 3077709, z**16 -  735217 ]
] prove with [ cuts [ 194, 197, 200, 203, 206, 209, 212, 215,
                      218, 221, 224, 227, 230, 233, 236, 239 ] ];


(**************** CUT 357, - *****************)

ecut and [
eqmod (cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
       cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16)
      L0x20015d18*x**1*z** 0 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
       cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17)
      L0x20015d84*x**1*z** 1 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
       cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18)
      L0x20015df0*x**1*z** 2 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
       cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19)
      L0x20015e5c*x**1*z** 3 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
       cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20)
      L0x20015ec8*x**1*z** 4 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
       cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21)
      L0x20015f34*x**1*z** 5 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
       cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22)
      L0x20015fa0*x**1*z** 6 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
       cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23)
      L0x2001600c*x**1*z** 7 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
       cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24)
      L0x20016078*x**1*z** 8 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
       cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25)
      L0x200160e4*x**1*z** 9 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
       cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26)
      L0x20016150*x**1*z**10 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
       cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27)
      L0x200161bc*x**1*z**11 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
       cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28)
      L0x20016228*x**1*z**12 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
       cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29)
      L0x20016294*x**1*z**13 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
       cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30)
      L0x20016300*x**1*z**14 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
       cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31)
      L0x2001636c*x**1*z**15 [ 3365569, y - 1540404, z**16 -  735217 ]
] prove with [ cuts [ 194, 197, 200, 203, 206, 209, 212, 215,
                      218, 221, 224, 227, 230, 233, 236, 239 ] ];


(**************** CUT 358, - *****************)

ecut and [
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x20015d24*x**1*z** 0 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x20015d90*x**1*z** 1 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x20015dfc*x**1*z** 2 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x20015e68*x**1*z** 3 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x20015ed4*x**1*z** 4 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x20015f40*x**1*z** 5 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x20015fac*x**1*z** 6 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20016018*x**1*z** 7 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x20016084*x**1*z** 8 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x200160f0*x**1*z** 9 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x2001615c*x**1*z**10 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x200161c8*x**1*z**11 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x20016234*x**1*z**12 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x200162a0*x**1*z**13 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x2001630c*x**1*z**14 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20016378*x**1*z**15 [ 3365569, y - 3117017, z**16 -  735217 ]
] prove with [ cuts [ 195, 198, 201, 204, 207, 210, 213, 216,
                      219, 222, 225, 228, 231, 234, 237, 240 ] ];


(**************** CUT 359, - *****************)

ecut and [
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x20015d30*x**1*z** 0 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x20015d9c*x**1*z** 1 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x20015e08*x**1*z** 2 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x20015e74*x**1*z** 3 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x20015ee0*x**1*z** 4 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x20015f4c*x**1*z** 5 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x20015fb8*x**1*z** 6 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20016024*x**1*z** 7 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x20016090*x**1*z** 8 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x200160fc*x**1*z** 9 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x20016168*x**1*z**10 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x200161d4*x**1*z**11 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x20016240*x**1*z**12 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x200162ac*x**1*z**13 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x20016318*x**1*z**14 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20016384*x**1*z**15 [ 3365569, y -  543301, z**16 -  735217 ]
] prove with [ cuts [ 195, 198, 201, 204, 207, 210, 213, 216,
                      219, 222, 225, 228, 231, 234, 237, 240 ] ];


(**************** CUT 360, - *****************)

ecut and [
eqmod (cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
       cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16)
      L0x20015d3c*x**1*z** 0 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
       cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17)
      L0x20015da8*x**1*z** 1 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
       cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18)
      L0x20015e14*x**1*z** 2 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
       cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19)
      L0x20015e80*x**1*z** 3 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
       cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20)
      L0x20015eec*x**1*z** 4 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
       cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21)
      L0x20015f58*x**1*z** 5 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
       cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22)
      L0x20015fc4*x**1*z** 6 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
       cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23)
      L0x20016030*x**1*z** 7 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
       cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24)
      L0x2001609c*x**1*z** 8 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
       cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25)
      L0x20016108*x**1*z** 9 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
       cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26)
      L0x20016174*x**1*z**10 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
       cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27)
      L0x200161e0*x**1*z**11 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
       cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28)
      L0x2001624c*x**1*z**12 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
       cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29)
      L0x200162b8*x**1*z**13 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
       cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30)
      L0x20016324*x**1*z**14 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
       cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31)
      L0x20016390*x**1*z**15 [ 3365569, y - 3070820, z**16 -  735217 ]
] prove with [ cuts [ 195, 198, 201, 204, 207, 210, 213, 216,
                      219, 222, 225, 228, 231, 234, 237, 240 ] ];


(**************** CUT 361, - *****************)

ecut and [
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x200148a0*x**2*z** 0 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x2001490c*x**2*z** 1 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20014978*x**2*z** 2 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200149e4*x**2*z** 3 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x20014a50*x**2*z** 4 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x20014abc*x**2*z** 5 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x20014b28*x**2*z** 6 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x20014b94*x**2*z** 7 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x20014c00*x**2*z** 8 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x20014c6c*x**2*z** 9 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x20014cd8*x**2*z**10 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x20014d44*x**2*z**11 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x20014db0*x**2*z**12 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x20014e1c*x**2*z**13 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20014e88*x**2*z**14 [ 3365569, y -       1, z**16 -       1 ],
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x20014ef4*x**2*z**15 [ 3365569, y -       1, z**16 -       1 ]
] prove with [ cuts [  97, 100, 103, 106, 109, 112, 115, 118,
                      121, 124, 127, 130, 133, 136, 139, 142 ] ];


(**************** CUT 362, - *****************)

ecut and [
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x200148ac*x**2*z** 0 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x20014918*x**2*z** 1 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20014984*x**2*z** 2 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200149f0*x**2*z** 3 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x20014a5c*x**2*z** 4 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x20014ac8*x**2*z** 5 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x20014b34*x**2*z** 6 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x20014ba0*x**2*z** 7 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x20014c0c*x**2*z** 8 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x20014c78*x**2*z** 9 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x20014ce4*x**2*z**10 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x20014d50*x**2*z**11 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x20014dbc*x**2*z**12 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x20014e28*x**2*z**13 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20014e94*x**2*z**14 [ 3365569, y -  452650, z**16 -       1 ],
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x20014f00*x**2*z**15 [ 3365569, y -  452650, z**16 -       1 ]
] prove with [ cuts [  97, 100, 103, 106, 109, 112, 115, 118,
                      121, 124, 127, 130, 133, 136, 139, 142 ] ];


(**************** CUT 363, - *****************)

ecut and [
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x200148b8*x**2*z** 0 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x20014924*x**2*z** 1 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20014990*x**2*z** 2 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200149fc*x**2*z** 3 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x20014a68*x**2*z** 4 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x20014ad4*x**2*z** 5 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x20014b40*x**2*z** 6 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x20014bac*x**2*z** 7 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x20014c18*x**2*z** 8 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x20014c84*x**2*z** 9 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x20014cf0*x**2*z**10 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x20014d5c*x**2*z**11 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x20014dc8*x**2*z**12 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x20014e34*x**2*z**13 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20014ea0*x**2*z**14 [ 3365569, y - 2912918, z**16 -       1 ],
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x20014f0c*x**2*z**15 [ 3365569, y - 2912918, z**16 -       1 ]
] prove with [ cuts [  97, 100, 103, 106, 109, 112, 115, 118,
                      121, 124, 127, 130, 133, 136, 139, 142 ] ];


(**************** CUT 364, - *****************)

ecut and [
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x200148c4*x**2*z** 0 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x20014930*x**2*z** 1 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x2001499c*x**2*z** 2 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x20014a08*x**2*z** 3 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x20014a74*x**2*z** 4 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x20014ae0*x**2*z** 5 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x20014b4c*x**2*z** 6 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20014bb8*x**2*z** 7 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x20014c24*x**2*z** 8 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x20014c90*x**2*z** 9 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x20014cfc*x**2*z**10 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20014d68*x**2*z**11 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x20014dd4*x**2*z**12 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x20014e40*x**2*z**13 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x20014eac*x**2*z**14 [ 3365569, y - 2113025, z**16 -       1 ],
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x20014f18*x**2*z**15 [ 3365569, y - 2113025, z**16 -       1 ]
] prove with [ cuts [  98, 101, 104, 107, 110, 113, 116, 119,
                      122, 125, 128, 131, 134, 137, 140, 143 ] ];


(**************** CUT 365, - *****************)

ecut and [
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x200148d0*x**2*z** 0 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x2001493c*x**2*z** 1 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x200149a8*x**2*z** 2 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x20014a14*x**2*z** 3 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x20014a80*x**2*z** 4 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x20014aec*x**2*z** 5 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x20014b58*x**2*z** 6 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20014bc4*x**2*z** 7 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x20014c30*x**2*z** 8 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x20014c9c*x**2*z** 9 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x20014d08*x**2*z**10 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20014d74*x**2*z**11 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x20014de0*x**2*z**12 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x20014e4c*x**2*z**13 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x20014eb8*x**2*z**14 [ 3365569, y - 3077709, z**16 -       1 ],
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x20014f24*x**2*z**15 [ 3365569, y - 3077709, z**16 -       1 ]
] prove with [ cuts [  98, 101, 104, 107, 110, 113, 116, 119,
                      122, 125, 128, 131, 134, 137, 140, 143 ] ];


(**************** CUT 366, - *****************)

ecut and [
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x200148dc*x**2*z** 0 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x20014948*x**2*z** 1 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x200149b4*x**2*z** 2 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x20014a20*x**2*z** 3 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x20014a8c*x**2*z** 4 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x20014af8*x**2*z** 5 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x20014b64*x**2*z** 6 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20014bd0*x**2*z** 7 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x20014c3c*x**2*z** 8 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x20014ca8*x**2*z** 9 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x20014d14*x**2*z**10 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20014d80*x**2*z**11 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x20014dec*x**2*z**12 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x20014e58*x**2*z**13 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x20014ec4*x**2*z**14 [ 3365569, y - 1540404, z**16 -       1 ],
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x20014f30*x**2*z**15 [ 3365569, y - 1540404, z**16 -       1 ]
] prove with [ cuts [  98, 101, 104, 107, 110, 113, 116, 119,
                      122, 125, 128, 131, 134, 137, 140, 143 ] ];


(**************** CUT 367, - *****************)

ecut and [
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x200148e8*x**2*z** 0 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x20014954*x**2*z** 1 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x200149c0*x**2*z** 2 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x20014a2c*x**2*z** 3 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20014a98*x**2*z** 4 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x20014b04*x**2*z** 5 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x20014b70*x**2*z** 6 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x20014bdc*x**2*z** 7 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20014c48*x**2*z** 8 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x20014cb4*x**2*z** 9 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x20014d20*x**2*z**10 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x20014d8c*x**2*z**11 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x20014df8*x**2*z**12 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x20014e64*x**2*z**13 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x20014ed0*x**2*z**14 [ 3365569, y - 3117017, z**16 -       1 ],
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x20014f3c*x**2*z**15 [ 3365569, y - 3117017, z**16 -       1 ]
] prove with [ cuts [  99, 102, 105, 108, 111, 114, 117, 120,
                      123, 126, 129, 132, 135, 138, 141, 144 ] ];


(**************** CUT 368, - *****************)

ecut and [
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x200148f4*x**2*z** 0 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x20014960*x**2*z** 1 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x200149cc*x**2*z** 2 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x20014a38*x**2*z** 3 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20014aa4*x**2*z** 4 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x20014b10*x**2*z** 5 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x20014b7c*x**2*z** 6 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x20014be8*x**2*z** 7 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20014c54*x**2*z** 8 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x20014cc0*x**2*z** 9 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x20014d2c*x**2*z**10 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x20014d98*x**2*z**11 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x20014e04*x**2*z**12 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x20014e70*x**2*z**13 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x20014edc*x**2*z**14 [ 3365569, y -  543301, z**16 -       1 ],
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x20014f48*x**2*z**15 [ 3365569, y -  543301, z**16 -       1 ]
] prove with [ cuts [  99, 102, 105, 108, 111, 114, 117, 120,
                      123, 126, 129, 132, 135, 138, 141, 144 ] ];


(**************** CUT 369, - *****************)

ecut and [
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x20014900*x**2*z** 0 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x2001496c*x**2*z** 1 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x200149d8*x**2*z** 2 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x20014a44*x**2*z** 3 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20014ab0*x**2*z** 4 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x20014b1c*x**2*z** 5 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x20014b88*x**2*z** 6 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x20014bf4*x**2*z** 7 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20014c60*x**2*z** 8 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x20014ccc*x**2*z** 9 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x20014d38*x**2*z**10 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x20014da4*x**2*z**11 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x20014e10*x**2*z**12 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x20014e7c*x**2*z**13 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x20014ee8*x**2*z**14 [ 3365569, y - 3070820, z**16 -       1 ],
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x20014f54*x**2*z**15 [ 3365569, y - 3070820, z**16 -       1 ]
] prove with [ cuts [  99, 102, 105, 108, 111, 114, 117, 120,
                      123, 126, 129, 132, 135, 138, 141, 144 ] ];


(**************** CUT 370, - *****************)

ecut and [
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x20014f60*x**2*z** 0 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x20014fcc*x**2*z** 1 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20015038*x**2*z** 2 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200150a4*x**2*z** 3 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x20015110*x**2*z** 4 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x2001517c*x**2*z** 5 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x200151e8*x**2*z** 6 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x20015254*x**2*z** 7 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x200152c0*x**2*z** 8 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x2001532c*x**2*z** 9 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x20015398*x**2*z**10 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x20015404*x**2*z**11 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x20015470*x**2*z**12 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x200154dc*x**2*z**13 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20015548*x**2*z**14 [ 3365569, y -       1, z**16 - 3365568 ],
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x200155b4*x**2*z**15 [ 3365569, y -       1, z**16 - 3365568 ]
] prove with [ cuts [  97, 100, 103, 106, 109, 112, 115, 118,
                      121, 124, 127, 130, 133, 136, 139, 142 ] ];


(**************** CUT 371, - *****************)

ecut and [
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x20014f6c*x**2*z** 0 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x20014fd8*x**2*z** 1 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20015044*x**2*z** 2 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200150b0*x**2*z** 3 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x2001511c*x**2*z** 4 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x20015188*x**2*z** 5 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x200151f4*x**2*z** 6 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x20015260*x**2*z** 7 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x200152cc*x**2*z** 8 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x20015338*x**2*z** 9 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x200153a4*x**2*z**10 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x20015410*x**2*z**11 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x2001547c*x**2*z**12 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x200154e8*x**2*z**13 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20015554*x**2*z**14 [ 3365569, y -  452650, z**16 - 3365568 ],
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x200155c0*x**2*z**15 [ 3365569, y -  452650, z**16 - 3365568 ]
] prove with [ cuts [  97, 100, 103, 106, 109, 112, 115, 118,
                      121, 124, 127, 130, 133, 136, 139, 142 ] ];


(**************** CUT 372, - *****************)

ecut and [
eqmod (cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
       cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16)
      L0x20014f78*x**2*z** 0 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
       cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17)
      L0x20014fe4*x**2*z** 1 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
       cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18)
      L0x20015050*x**2*z** 2 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
       cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19)
      L0x200150bc*x**2*z** 3 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
       cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20)
      L0x20015128*x**2*z** 4 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
       cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21)
      L0x20015194*x**2*z** 5 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
       cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22)
      L0x20015200*x**2*z** 6 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
       cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23)
      L0x2001526c*x**2*z** 7 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
       cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24)
      L0x200152d8*x**2*z** 8 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
       cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25)
      L0x20015344*x**2*z** 9 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
       cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26)
      L0x200153b0*x**2*z**10 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
       cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27)
      L0x2001541c*x**2*z**11 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
       cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28)
      L0x20015488*x**2*z**12 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
       cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29)
      L0x200154f4*x**2*z**13 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
       cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30)
      L0x20015560*x**2*z**14 [ 3365569, y - 2912918, z**16 - 3365568 ],
eqmod (cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
       cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31)
      L0x200155cc*x**2*z**15 [ 3365569, y - 2912918, z**16 - 3365568 ]
] prove with [ cuts [  97, 100, 103, 106, 109, 112, 115, 118,
                      121, 124, 127, 130, 133, 136, 139, 142 ] ];


(**************** CUT 373, - *****************)

ecut and [
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x20014f84*x**2*z** 0 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x20014ff0*x**2*z** 1 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x2001505c*x**2*z** 2 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x200150c8*x**2*z** 3 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x20015134*x**2*z** 4 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x200151a0*x**2*z** 5 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x2001520c*x**2*z** 6 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20015278*x**2*z** 7 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x200152e4*x**2*z** 8 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x20015350*x**2*z** 9 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x200153bc*x**2*z**10 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20015428*x**2*z**11 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x20015494*x**2*z**12 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x20015500*x**2*z**13 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x2001556c*x**2*z**14 [ 3365569, y - 2113025, z**16 - 3365568 ],
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x200155d8*x**2*z**15 [ 3365569, y - 2113025, z**16 - 3365568 ]
] prove with [ cuts [  98, 101, 104, 107, 110, 113, 116, 119,
                      122, 125, 128, 131, 134, 137, 140, 143 ] ];


(**************** CUT 374, - *****************)

ecut and [
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x20014f90*x**2*z** 0 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x20014ffc*x**2*z** 1 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x20015068*x**2*z** 2 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x200150d4*x**2*z** 3 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x20015140*x**2*z** 4 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x200151ac*x**2*z** 5 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x20015218*x**2*z** 6 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20015284*x**2*z** 7 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x200152f0*x**2*z** 8 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x2001535c*x**2*z** 9 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x200153c8*x**2*z**10 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20015434*x**2*z**11 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x200154a0*x**2*z**12 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x2001550c*x**2*z**13 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x20015578*x**2*z**14 [ 3365569, y - 3077709, z**16 - 3365568 ],
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x200155e4*x**2*z**15 [ 3365569, y - 3077709, z**16 - 3365568 ]
] prove with [ cuts [  98, 101, 104, 107, 110, 113, 116, 119,
                      122, 125, 128, 131, 134, 137, 140, 143 ] ];


(**************** CUT 375, - *****************)

ecut and [
eqmod (cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
       cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16)
      L0x20014f9c*x**2*z** 0 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
       cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17)
      L0x20015008*x**2*z** 1 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
       cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18)
      L0x20015074*x**2*z** 2 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
       cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19)
      L0x200150e0*x**2*z** 3 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
       cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20)
      L0x2001514c*x**2*z** 4 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
       cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21)
      L0x200151b8*x**2*z** 5 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
       cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22)
      L0x20015224*x**2*z** 6 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
       cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23)
      L0x20015290*x**2*z** 7 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
       cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24)
      L0x200152fc*x**2*z** 8 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
       cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25)
      L0x20015368*x**2*z** 9 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
       cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26)
      L0x200153d4*x**2*z**10 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
       cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27)
      L0x20015440*x**2*z**11 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
       cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28)
      L0x200154ac*x**2*z**12 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
       cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29)
      L0x20015518*x**2*z**13 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
       cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30)
      L0x20015584*x**2*z**14 [ 3365569, y - 1540404, z**16 - 3365568 ],
eqmod (cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
       cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31)
      L0x200155f0*x**2*z**15 [ 3365569, y - 1540404, z**16 - 3365568 ]
] prove with [ cuts [  98, 101, 104, 107, 110, 113, 116, 119,
                      122, 125, 128, 131, 134, 137, 140, 143 ] ];


(**************** CUT 376, - *****************)

ecut and [
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x20014fa8*x**2*z** 0 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x20015014*x**2*z** 1 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x20015080*x**2*z** 2 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x200150ec*x**2*z** 3 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20015158*x**2*z** 4 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x200151c4*x**2*z** 5 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x20015230*x**2*z** 6 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x2001529c*x**2*z** 7 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20015308*x**2*z** 8 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x20015374*x**2*z** 9 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x200153e0*x**2*z**10 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x2001544c*x**2*z**11 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x200154b8*x**2*z**12 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x20015524*x**2*z**13 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x20015590*x**2*z**14 [ 3365569, y - 3117017, z**16 - 3365568 ],
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x200155fc*x**2*z**15 [ 3365569, y - 3117017, z**16 - 3365568 ]
] prove with [ cuts [  99, 102, 105, 108, 111, 114, 117, 120,
                      123, 126, 129, 132, 135, 138, 141, 144 ] ];


(**************** CUT 377, - *****************)

ecut and [
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x20014fb4*x**2*z** 0 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x20015020*x**2*z** 1 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x2001508c*x**2*z** 2 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x200150f8*x**2*z** 3 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20015164*x**2*z** 4 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x200151d0*x**2*z** 5 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x2001523c*x**2*z** 6 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x200152a8*x**2*z** 7 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20015314*x**2*z** 8 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x20015380*x**2*z** 9 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x200153ec*x**2*z**10 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x20015458*x**2*z**11 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x200154c4*x**2*z**12 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x20015530*x**2*z**13 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x2001559c*x**2*z**14 [ 3365569, y -  543301, z**16 - 3365568 ],
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x20015608*x**2*z**15 [ 3365569, y -  543301, z**16 - 3365568 ]
] prove with [ cuts [  99, 102, 105, 108, 111, 114, 117, 120,
                      123, 126, 129, 132, 135, 138, 141, 144 ] ];


(**************** CUT 378, - *****************)

ecut and [
eqmod (cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
       cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16)
      L0x20014fc0*x**2*z** 0 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
       cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17)
      L0x2001502c*x**2*z** 1 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
       cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18)
      L0x20015098*x**2*z** 2 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
       cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19)
      L0x20015104*x**2*z** 3 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
       cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20)
      L0x20015170*x**2*z** 4 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
       cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21)
      L0x200151dc*x**2*z** 5 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
       cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22)
      L0x20015248*x**2*z** 6 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
       cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23)
      L0x200152b4*x**2*z** 7 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
       cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24)
      L0x20015320*x**2*z** 8 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
       cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25)
      L0x2001538c*x**2*z** 9 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
       cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26)
      L0x200153f8*x**2*z**10 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
       cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27)
      L0x20015464*x**2*z**11 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
       cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28)
      L0x200154d0*x**2*z**12 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
       cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29)
      L0x2001553c*x**2*z**13 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
       cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30)
      L0x200155a8*x**2*z**14 [ 3365569, y - 3070820, z**16 - 3365568 ],
eqmod (cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
       cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31)
      L0x20015614*x**2*z**15 [ 3365569, y - 3070820, z**16 - 3365568 ]
] prove with [ cuts [  99, 102, 105, 108, 111, 114, 117, 120,
                      123, 126, 129, 132, 135, 138, 141, 144 ] ];


(**************** CUT 379, - *****************)

ecut and [
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x20015620*x**2*z** 0 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x2001568c*x**2*z** 1 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x200156f8*x**2*z** 2 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x20015764*x**2*z** 3 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x200157d0*x**2*z** 4 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x2001583c*x**2*z** 5 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x200158a8*x**2*z** 6 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x20015914*x**2*z** 7 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x20015980*x**2*z** 8 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x200159ec*x**2*z** 9 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20015a58*x**2*z**10 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x20015ac4*x**2*z**11 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x20015b30*x**2*z**12 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x20015b9c*x**2*z**13 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x20015c08*x**2*z**14 [ 3365569, y -       1, z**16 - 2630352 ],
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x20015c74*x**2*z**15 [ 3365569, y -       1, z**16 - 2630352 ]
] prove with [ cuts [ 241, 244, 247, 250, 253, 256, 259, 262,
                      265, 268, 271, 274, 277, 280, 283, 286 ] ];


(**************** CUT 380, - *****************)

ecut and [
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x2001562c*x**2*z** 0 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x20015698*x**2*z** 1 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x20015704*x**2*z** 2 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x20015770*x**2*z** 3 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x200157dc*x**2*z** 4 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x20015848*x**2*z** 5 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x200158b4*x**2*z** 6 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x20015920*x**2*z** 7 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x2001598c*x**2*z** 8 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x200159f8*x**2*z** 9 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20015a64*x**2*z**10 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x20015ad0*x**2*z**11 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x20015b3c*x**2*z**12 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x20015ba8*x**2*z**13 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x20015c14*x**2*z**14 [ 3365569, y -  452650, z**16 - 2630352 ],
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x20015c80*x**2*z**15 [ 3365569, y -  452650, z**16 - 2630352 ]
] prove with [ cuts [ 241, 244, 247, 250, 253, 256, 259, 262,
                      265, 268, 271, 274, 277, 280, 283, 286 ] ];


(**************** CUT 381, - *****************)

ecut and [
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x20015638*x**2*z** 0 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x200156a4*x**2*z** 1 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x20015710*x**2*z** 2 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x2001577c*x**2*z** 3 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x200157e8*x**2*z** 4 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x20015854*x**2*z** 5 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x200158c0*x**2*z** 6 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x2001592c*x**2*z** 7 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x20015998*x**2*z** 8 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x20015a04*x**2*z** 9 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20015a70*x**2*z**10 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x20015adc*x**2*z**11 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x20015b48*x**2*z**12 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x20015bb4*x**2*z**13 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x20015c20*x**2*z**14 [ 3365569, y - 2912918, z**16 - 2630352 ],
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x20015c8c*x**2*z**15 [ 3365569, y - 2912918, z**16 - 2630352 ]
] prove with [ cuts [ 241, 244, 247, 250, 253, 256, 259, 262,
                      265, 268, 271, 274, 277, 280, 283, 286 ] ];


(**************** CUT 382, - *****************)

ecut and [
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x20015644*x**2*z** 0 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x200156b0*x**2*z** 1 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x2001571c*x**2*z** 2 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x20015788*x**2*z** 3 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x200157f4*x**2*z** 4 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x20015860*x**2*z** 5 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x200158cc*x**2*z** 6 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20015938*x**2*z** 7 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x200159a4*x**2*z** 8 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x20015a10*x**2*z** 9 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x20015a7c*x**2*z**10 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x20015ae8*x**2*z**11 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x20015b54*x**2*z**12 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x20015bc0*x**2*z**13 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x20015c2c*x**2*z**14 [ 3365569, y - 2113025, z**16 - 2630352 ],
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20015c98*x**2*z**15 [ 3365569, y - 2113025, z**16 - 2630352 ]
] prove with [ cuts [ 242, 245, 248, 251, 254, 257, 260, 263,
                      266, 269, 272, 275, 278, 281, 284, 287 ] ];


(**************** CUT 383, - *****************)

ecut and [
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x20015650*x**2*z** 0 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x200156bc*x**2*z** 1 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x20015728*x**2*z** 2 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x20015794*x**2*z** 3 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x20015800*x**2*z** 4 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x2001586c*x**2*z** 5 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x200158d8*x**2*z** 6 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20015944*x**2*z** 7 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x200159b0*x**2*z** 8 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x20015a1c*x**2*z** 9 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x20015a88*x**2*z**10 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x20015af4*x**2*z**11 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x20015b60*x**2*z**12 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x20015bcc*x**2*z**13 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x20015c38*x**2*z**14 [ 3365569, y - 3077709, z**16 - 2630352 ],
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20015ca4*x**2*z**15 [ 3365569, y - 3077709, z**16 - 2630352 ]
] prove with [ cuts [ 242, 245, 248, 251, 254, 257, 260, 263,
                      266, 269, 272, 275, 278, 281, 284, 287 ] ];


(**************** CUT 384, - *****************)

ecut and [
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x2001565c*x**2*z** 0 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x200156c8*x**2*z** 1 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x20015734*x**2*z** 2 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x200157a0*x**2*z** 3 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x2001580c*x**2*z** 4 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x20015878*x**2*z** 5 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x200158e4*x**2*z** 6 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20015950*x**2*z** 7 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x200159bc*x**2*z** 8 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x20015a28*x**2*z** 9 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x20015a94*x**2*z**10 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x20015b00*x**2*z**11 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x20015b6c*x**2*z**12 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x20015bd8*x**2*z**13 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x20015c44*x**2*z**14 [ 3365569, y - 1540404, z**16 - 2630352 ],
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20015cb0*x**2*z**15 [ 3365569, y - 1540404, z**16 - 2630352 ]
] prove with [ cuts [ 242, 245, 248, 251, 254, 257, 260, 263,
                      266, 269, 272, 275, 278, 281, 284, 287 ] ];


(**************** CUT 385, - *****************)

ecut and [
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015668*x**2*z** 0 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x200156d4*x**2*z** 1 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x20015740*x**2*z** 2 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x200157ac*x**2*z** 3 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015818*x**2*z** 4 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x20015884*x**2*z** 5 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x200158f0*x**2*z** 6 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x2001595c*x**2*z** 7 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x200159c8*x**2*z** 8 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x20015a34*x**2*z** 9 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x20015aa0*x**2*z**10 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x20015b0c*x**2*z**11 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20015b78*x**2*z**12 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x20015be4*x**2*z**13 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x20015c50*x**2*z**14 [ 3365569, y - 3117017, z**16 - 2630352 ],
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x20015cbc*x**2*z**15 [ 3365569, y - 3117017, z**16 - 2630352 ]
] prove with [ cuts [ 243, 246, 249, 252, 255, 258, 261, 264,
                      267, 270, 273, 276, 279, 282, 285, 288 ] ];


(**************** CUT 386, - *****************)

ecut and [
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015674*x**2*z** 0 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x200156e0*x**2*z** 1 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x2001574c*x**2*z** 2 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x200157b8*x**2*z** 3 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015824*x**2*z** 4 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x20015890*x**2*z** 5 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x200158fc*x**2*z** 6 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x20015968*x**2*z** 7 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x200159d4*x**2*z** 8 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x20015a40*x**2*z** 9 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x20015aac*x**2*z**10 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x20015b18*x**2*z**11 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20015b84*x**2*z**12 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x20015bf0*x**2*z**13 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x20015c5c*x**2*z**14 [ 3365569, y -  543301, z**16 - 2630352 ],
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x20015cc8*x**2*z**15 [ 3365569, y -  543301, z**16 - 2630352 ]
] prove with [ cuts [ 243, 246, 249, 252, 255, 258, 261, 264,
                      267, 270, 273, 276, 279, 282, 285, 288 ] ];


(**************** CUT 387, - *****************)

ecut and [
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015680*x**2*z** 0 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x200156ec*x**2*z** 1 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x20015758*x**2*z** 2 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x200157c4*x**2*z** 3 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015830*x**2*z** 4 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x2001589c*x**2*z** 5 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x20015908*x**2*z** 6 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x20015974*x**2*z** 7 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x200159e0*x**2*z** 8 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x20015a4c*x**2*z** 9 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x20015ab8*x**2*z**10 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x20015b24*x**2*z**11 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20015b90*x**2*z**12 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x20015bfc*x**2*z**13 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x20015c68*x**2*z**14 [ 3365569, y - 3070820, z**16 - 2630352 ],
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x20015cd4*x**2*z**15 [ 3365569, y - 3070820, z**16 - 2630352 ]
] prove with [ cuts [ 243, 246, 249, 252, 255, 258, 261, 264,
                      267, 270, 273, 276, 279, 282, 285, 288 ] ];


(**************** CUT 388, - *****************)

ecut and [
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x20015ce0*x**2*z** 0 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x20015d4c*x**2*z** 1 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x20015db8*x**2*z** 2 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x20015e24*x**2*z** 3 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x20015e90*x**2*z** 4 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x20015efc*x**2*z** 5 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x20015f68*x**2*z** 6 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x20015fd4*x**2*z** 7 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x20016040*x**2*z** 8 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x200160ac*x**2*z** 9 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20016118*x**2*z**10 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x20016184*x**2*z**11 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x200161f0*x**2*z**12 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x2001625c*x**2*z**13 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x200162c8*x**2*z**14 [ 3365569, y -       1, z**16 -  735217 ],
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x20016334*x**2*z**15 [ 3365569, y -       1, z**16 -  735217 ]
] prove with [ cuts [ 241, 244, 247, 250, 253, 256, 259, 262,
                      265, 268, 271, 274, 277, 280, 283, 286 ] ];


(**************** CUT 389, - *****************)

ecut and [
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x20015cec*x**2*z** 0 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x20015d58*x**2*z** 1 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x20015dc4*x**2*z** 2 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x20015e30*x**2*z** 3 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x20015e9c*x**2*z** 4 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x20015f08*x**2*z** 5 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x20015f74*x**2*z** 6 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x20015fe0*x**2*z** 7 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x2001604c*x**2*z** 8 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x200160b8*x**2*z** 9 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20016124*x**2*z**10 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x20016190*x**2*z**11 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x200161fc*x**2*z**12 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x20016268*x**2*z**13 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x200162d4*x**2*z**14 [ 3365569, y -  452650, z**16 -  735217 ],
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x20016340*x**2*z**15 [ 3365569, y -  452650, z**16 -  735217 ]
] prove with [ cuts [ 241, 244, 247, 250, 253, 256, 259, 262,
                      265, 268, 271, 274, 277, 280, 283, 286 ] ];


(**************** CUT 390, - *****************)

ecut and [
eqmod (cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
       cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16)
      L0x20015cf8*x**2*z** 0 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
       cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17)
      L0x20015d64*x**2*z** 1 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
       cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18)
      L0x20015dd0*x**2*z** 2 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
       cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19)
      L0x20015e3c*x**2*z** 3 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
       cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20)
      L0x20015ea8*x**2*z** 4 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
       cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21)
      L0x20015f14*x**2*z** 5 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
       cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22)
      L0x20015f80*x**2*z** 6 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
       cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23)
      L0x20015fec*x**2*z** 7 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
       cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24)
      L0x20016058*x**2*z** 8 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
       cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25)
      L0x200160c4*x**2*z** 9 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
       cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26)
      L0x20016130*x**2*z**10 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
       cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27)
      L0x2001619c*x**2*z**11 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
       cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28)
      L0x20016208*x**2*z**12 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
       cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29)
      L0x20016274*x**2*z**13 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
       cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30)
      L0x200162e0*x**2*z**14 [ 3365569, y - 2912918, z**16 -  735217 ],
eqmod (cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
       cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31)
      L0x2001634c*x**2*z**15 [ 3365569, y - 2912918, z**16 -  735217 ]
] prove with [ cuts [ 241, 244, 247, 250, 253, 256, 259, 262,
                      265, 268, 271, 274, 277, 280, 283, 286 ] ];


(**************** CUT 391, - *****************)

ecut and [
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x20015d04*x**2*z** 0 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x20015d70*x**2*z** 1 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x20015ddc*x**2*z** 2 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x20015e48*x**2*z** 3 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x20015eb4*x**2*z** 4 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x20015f20*x**2*z** 5 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x20015f8c*x**2*z** 6 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20015ff8*x**2*z** 7 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x20016064*x**2*z** 8 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x200160d0*x**2*z** 9 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x2001613c*x**2*z**10 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x200161a8*x**2*z**11 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x20016214*x**2*z**12 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x20016280*x**2*z**13 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x200162ec*x**2*z**14 [ 3365569, y - 2113025, z**16 -  735217 ],
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20016358*x**2*z**15 [ 3365569, y - 2113025, z**16 -  735217 ]
] prove with [ cuts [ 242, 245, 248, 251, 254, 257, 260, 263,
                      266, 269, 272, 275, 278, 281, 284, 287 ] ];


(**************** CUT 392, - *****************)

ecut and [
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x20015d10*x**2*z** 0 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x20015d7c*x**2*z** 1 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x20015de8*x**2*z** 2 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x20015e54*x**2*z** 3 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x20015ec0*x**2*z** 4 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x20015f2c*x**2*z** 5 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x20015f98*x**2*z** 6 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20016004*x**2*z** 7 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x20016070*x**2*z** 8 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x200160dc*x**2*z** 9 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x20016148*x**2*z**10 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x200161b4*x**2*z**11 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x20016220*x**2*z**12 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x2001628c*x**2*z**13 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x200162f8*x**2*z**14 [ 3365569, y - 3077709, z**16 -  735217 ],
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20016364*x**2*z**15 [ 3365569, y - 3077709, z**16 -  735217 ]
] prove with [ cuts [ 242, 245, 248, 251, 254, 257, 260, 263,
                      266, 269, 272, 275, 278, 281, 284, 287 ] ];


(**************** CUT 393, - *****************)

ecut and [
eqmod (cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
       cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16)
      L0x20015d1c*x**2*z** 0 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
       cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17)
      L0x20015d88*x**2*z** 1 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
       cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18)
      L0x20015df4*x**2*z** 2 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
       cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19)
      L0x20015e60*x**2*z** 3 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
       cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20)
      L0x20015ecc*x**2*z** 4 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
       cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21)
      L0x20015f38*x**2*z** 5 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
       cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22)
      L0x20015fa4*x**2*z** 6 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
       cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23)
      L0x20016010*x**2*z** 7 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
       cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24)
      L0x2001607c*x**2*z** 8 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
       cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25)
      L0x200160e8*x**2*z** 9 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
       cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26)
      L0x20016154*x**2*z**10 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
       cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27)
      L0x200161c0*x**2*z**11 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
       cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28)
      L0x2001622c*x**2*z**12 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
       cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29)
      L0x20016298*x**2*z**13 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
       cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30)
      L0x20016304*x**2*z**14 [ 3365569, y - 1540404, z**16 -  735217 ],
eqmod (cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
       cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31)
      L0x20016370*x**2*z**15 [ 3365569, y - 1540404, z**16 -  735217 ]
] prove with [ cuts [ 242, 245, 248, 251, 254, 257, 260, 263,
                      266, 269, 272, 275, 278, 281, 284, 287 ] ];


(**************** CUT 394, - *****************)

ecut and [
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015d28*x**2*z** 0 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x20015d94*x**2*z** 1 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x20015e00*x**2*z** 2 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x20015e6c*x**2*z** 3 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015ed8*x**2*z** 4 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x20015f44*x**2*z** 5 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x20015fb0*x**2*z** 6 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x2001601c*x**2*z** 7 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x20016088*x**2*z** 8 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x200160f4*x**2*z** 9 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x20016160*x**2*z**10 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x200161cc*x**2*z**11 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20016238*x**2*z**12 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x200162a4*x**2*z**13 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x20016310*x**2*z**14 [ 3365569, y - 3117017, z**16 -  735217 ],
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x2001637c*x**2*z**15 [ 3365569, y - 3117017, z**16 -  735217 ]
] prove with [ cuts [ 243, 246, 249, 252, 255, 258, 261, 264,
                      267, 270, 273, 276, 279, 282, 285, 288 ] ];


(**************** CUT 395, - *****************)

ecut and [
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015d34*x**2*z** 0 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x20015da0*x**2*z** 1 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x20015e0c*x**2*z** 2 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x20015e78*x**2*z** 3 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015ee4*x**2*z** 4 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x20015f50*x**2*z** 5 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x20015fbc*x**2*z** 6 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x20016028*x**2*z** 7 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x20016094*x**2*z** 8 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x20016100*x**2*z** 9 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x2001616c*x**2*z**10 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x200161d8*x**2*z**11 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20016244*x**2*z**12 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x200162b0*x**2*z**13 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x2001631c*x**2*z**14 [ 3365569, y -  543301, z**16 -  735217 ],
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x20016388*x**2*z**15 [ 3365569, y -  543301, z**16 -  735217 ]
] prove with [ cuts [ 243, 246, 249, 252, 255, 258, 261, 264,
                      267, 270, 273, 276, 279, 282, 285, 288 ] ];


(**************** CUT 396, - *****************)

ecut and [
eqmod (cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
       cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16)
      L0x20015d40*x**2*z** 0 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
       cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17)
      L0x20015dac*x**2*z** 1 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
       cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18)
      L0x20015e18*x**2*z** 2 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
       cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19)
      L0x20015e84*x**2*z** 3 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
       cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20)
      L0x20015ef0*x**2*z** 4 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
       cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21)
      L0x20015f5c*x**2*z** 5 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
       cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22)
      L0x20015fc8*x**2*z** 6 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
       cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23)
      L0x20016034*x**2*z** 7 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
       cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24)
      L0x200160a0*x**2*z** 8 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
       cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25)
      L0x2001610c*x**2*z** 9 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
       cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26)
      L0x20016178*x**2*z**10 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
       cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27)
      L0x200161e4*x**2*z**11 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
       cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28)
      L0x20016250*x**2*z**12 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
       cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29)
      L0x200162bc*x**2*z**13 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
       cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30)
      L0x20016328*x**2*z**14 [ 3365569, y - 3070820, z**16 -  735217 ],
eqmod (cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
       cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31)
      L0x20016394*x**2*z**15 [ 3365569, y - 3070820, z**16 -  735217 ]
] prove with [ cuts [ 243, 246, 249, 252, 255, 258, 261, 264,
                      267, 270, 273, 276, 279, 282, 285, 288 ] ];


(* vmov	lr, s3                                     #! PC = 0x8038498 *)
mov lr s3;
(* cmp.w	r0, lr                                    #! PC = 0x803849c *)
(* cmp.w r0, lr *)nop;
(* #bne.w	0x803823c <_3x2_inner>                   #! PC = 0x80384a0 *)
#bne.w	0x803823c <_3x2_inner>                   #! 0x80384a0 = 0x80384a0;
(* sub.w	r0, r0, #1728	; 0x6c0                     #! PC = 0x80384a4 *)
subs dc r0 r0 1728@uint32;
(* add.w	r0, r0, #4                                #! PC = 0x80384a8 *)
adds dc r0 r0 4@uint32;
(* vmov	r12, s2                                    #! PC = 0x80384ac *)
mov r12 s2;
(* cmp.w	r0, r12                                   #! PC = 0x80384b0 *)
(* cmp.w r0, r12 *)nop;
(* #bne.w	0x8038234 <_3x2>                         #! PC = 0x80384b4 *)
#bne.w	0x8038234 <_3x2>                         #! 0x80384b4 = 0x80384b4;
(* #vpop	{s16-s22}                                  #! PC = 0x80384b8 *)
#vpop	{%%s16-%%s22}                                  #! 0x80384b8 = 0x80384b8;
(* #ldmia.w	sp!, {r4, r5, r6, r7, r8, r9, r10, r11, r12, pc}#! EA = L0x20014868; Value = 0x20014898; PC = 0x80384bc *)
#ldmia.w	sp!, {%%r4, %%r5, %%r6, %%r7, %%r8, %%r9, %%r10, %%r11, %%r12, pc}#! L0x20014868 = L0x20014868; 0x20014898 = 0x20014898; 0x80384bc = 0x80384bc;



(******************** output polynomials ********************)

ghost CH000@sint32, CH001@sint32, CH002@sint32,
      ch0000@sint32, ch0001@sint32, ch0002@sint32, ch0003@sint32,
      ch0004@sint32, ch0005@sint32, ch0006@sint32, ch0007@sint32,
      ch0008@sint32, ch0009@sint32, ch000a@sint32, ch000b@sint32,
      ch000c@sint32, ch000d@sint32, ch000e@sint32, ch000f@sint32,
      ch0010@sint32, ch0011@sint32, ch0012@sint32, ch0013@sint32,
      ch0014@sint32, ch0015@sint32, ch0016@sint32, ch0017@sint32,
      ch0018@sint32, ch0019@sint32, ch001a@sint32, ch001b@sint32,
      ch001c@sint32, ch001d@sint32, ch001e@sint32, ch001f@sint32,
      ch0020@sint32, ch0021@sint32, ch0022@sint32, ch0023@sint32,
      ch0024@sint32, ch0025@sint32, ch0026@sint32, ch0027@sint32,
      ch0028@sint32, ch0029@sint32, ch002a@sint32, ch002b@sint32,
      ch002c@sint32, ch002d@sint32, ch002e@sint32, ch002f@sint32: and [
CH000**2 =
ch0000*x**0*z** 0 + ch0001*x**0*z** 1 + ch0002*x**0*z** 2 + ch0003*x**0*z** 3 +
ch0004*x**0*z** 4 + ch0005*x**0*z** 5 + ch0006*x**0*z** 6 + ch0007*x**0*z** 7 +
ch0008*x**0*z** 8 + ch0009*x**0*z** 9 + ch000a*x**0*z**10 + ch000b*x**0*z**11 +
ch000c*x**0*z**12 + ch000d*x**0*z**13 + ch000e*x**0*z**14 + ch000f*x**0*z**15,
ch0000=L0x20014898, ch0001=L0x20014904, ch0002=L0x20014970, ch0003=L0x200149dc,
ch0004=L0x20014a48, ch0005=L0x20014ab4, ch0006=L0x20014b20, ch0007=L0x20014b8c,
ch0008=L0x20014bf8, ch0009=L0x20014c64, ch000a=L0x20014cd0, ch000b=L0x20014d3c,
ch000c=L0x20014da8, ch000d=L0x20014e14, ch000e=L0x20014e80, ch000f=L0x20014eec,
CH001**2 =
ch0010*x**1*z** 0 + ch0011*x**1*z** 1 + ch0012*x**1*z** 2 + ch0013*x**1*z** 3 +
ch0014*x**1*z** 4 + ch0015*x**1*z** 5 + ch0016*x**1*z** 6 + ch0017*x**1*z** 7 +
ch0018*x**1*z** 8 + ch0019*x**1*z** 9 + ch001a*x**1*z**10 + ch001b*x**1*z**11 +
ch001c*x**1*z**12 + ch001d*x**1*z**13 + ch001e*x**1*z**14 + ch001f*x**1*z**15,
ch0010=L0x2001489c, ch0011=L0x20014908, ch0012=L0x20014974, ch0013=L0x200149e0,
ch0014=L0x20014a4c, ch0015=L0x20014ab8, ch0016=L0x20014b24, ch0017=L0x20014b90,
ch0018=L0x20014bfc, ch0019=L0x20014c68, ch001a=L0x20014cd4, ch001b=L0x20014d40,
ch001c=L0x20014dac, ch001d=L0x20014e18, ch001e=L0x20014e84, ch001f=L0x20014ef0,
CH002**2 =
ch0020*x**2*z** 0 + ch0021*x**2*z** 1 + ch0022*x**2*z** 2 + ch0023*x**2*z** 3 +
ch0024*x**2*z** 4 + ch0025*x**2*z** 5 + ch0026*x**2*z** 6 + ch0027*x**2*z** 7 +
ch0028*x**2*z** 8 + ch0029*x**2*z** 9 + ch002a*x**2*z**10 + ch002b*x**2*z**11 +
ch002c*x**2*z**12 + ch002d*x**2*z**13 + ch002e*x**2*z**14 + ch002f*x**2*z**15,
ch0020=L0x200148a0, ch0021=L0x2001490c, ch0022=L0x20014978, ch0023=L0x200149e4,
ch0024=L0x20014a50, ch0025=L0x20014abc, ch0026=L0x20014b28, ch0027=L0x20014b94,
ch0028=L0x20014c00, ch0029=L0x20014c6c, ch002a=L0x20014cd8, ch002b=L0x20014d44,
ch002c=L0x20014db0, ch002d=L0x20014e1c, ch002e=L0x20014e88, ch002f=L0x20014ef4
] && true;


ghost CH100@sint32, CH101@sint32, CH102@sint32,
      ch1000@sint32, ch1001@sint32, ch1002@sint32, ch1003@sint32,
      ch1004@sint32, ch1005@sint32, ch1006@sint32, ch1007@sint32,
      ch1008@sint32, ch1009@sint32, ch100a@sint32, ch100b@sint32,
      ch100c@sint32, ch100d@sint32, ch100e@sint32, ch100f@sint32,
      ch1010@sint32, ch1011@sint32, ch1012@sint32, ch1013@sint32,
      ch1014@sint32, ch1015@sint32, ch1016@sint32, ch1017@sint32,
      ch1018@sint32, ch1019@sint32, ch101a@sint32, ch101b@sint32,
      ch101c@sint32, ch101d@sint32, ch101e@sint32, ch101f@sint32,
      ch1020@sint32, ch1021@sint32, ch1022@sint32, ch1023@sint32,
      ch1024@sint32, ch1025@sint32, ch1026@sint32, ch1027@sint32,
      ch1028@sint32, ch1029@sint32, ch102a@sint32, ch102b@sint32,
      ch102c@sint32, ch102d@sint32, ch102e@sint32, ch102f@sint32: and [
CH100**2 =
ch1000*x**0*z** 0 + ch1001*x**0*z** 1 + ch1002*x**0*z** 2 + ch1003*x**0*z** 3 +
ch1004*x**0*z** 4 + ch1005*x**0*z** 5 + ch1006*x**0*z** 6 + ch1007*x**0*z** 7 +
ch1008*x**0*z** 8 + ch1009*x**0*z** 9 + ch100a*x**0*z**10 + ch100b*x**0*z**11 +
ch100c*x**0*z**12 + ch100d*x**0*z**13 + ch100e*x**0*z**14 + ch100f*x**0*z**15,
ch1000=L0x200148a4, ch1001=L0x20014910, ch1002=L0x2001497c, ch1003=L0x200149e8,
ch1004=L0x20014a54, ch1005=L0x20014ac0, ch1006=L0x20014b2c, ch1007=L0x20014b98,
ch1008=L0x20014c04, ch1009=L0x20014c70, ch100a=L0x20014cdc, ch100b=L0x20014d48,
ch100c=L0x20014db4, ch100d=L0x20014e20, ch100e=L0x20014e8c, ch100f=L0x20014ef8,
CH101**2 =
ch1010*x**1*z** 0 + ch1011*x**1*z** 1 + ch1012*x**1*z** 2 + ch1013*x**1*z** 3 +
ch1014*x**1*z** 4 + ch1015*x**1*z** 5 + ch1016*x**1*z** 6 + ch1017*x**1*z** 7 +
ch1018*x**1*z** 8 + ch1019*x**1*z** 9 + ch101a*x**1*z**10 + ch101b*x**1*z**11 +
ch101c*x**1*z**12 + ch101d*x**1*z**13 + ch101e*x**1*z**14 + ch101f*x**1*z**15,
ch1010=L0x200148a8, ch1011=L0x20014914, ch1012=L0x20014980, ch1013=L0x200149ec,
ch1014=L0x20014a58, ch1015=L0x20014ac4, ch1016=L0x20014b30, ch1017=L0x20014b9c,
ch1018=L0x20014c08, ch1019=L0x20014c74, ch101a=L0x20014ce0, ch101b=L0x20014d4c,
ch101c=L0x20014db8, ch101d=L0x20014e24, ch101e=L0x20014e90, ch101f=L0x20014efc,
CH102**2 =
ch1020*x**2*z** 0 + ch1021*x**2*z** 1 + ch1022*x**2*z** 2 + ch1023*x**2*z** 3 +
ch1024*x**2*z** 4 + ch1025*x**2*z** 5 + ch1026*x**2*z** 6 + ch1027*x**2*z** 7 +
ch1028*x**2*z** 8 + ch1029*x**2*z** 9 + ch102a*x**2*z**10 + ch102b*x**2*z**11 +
ch102c*x**2*z**12 + ch102d*x**2*z**13 + ch102e*x**2*z**14 + ch102f*x**2*z**15,
ch1020=L0x200148ac, ch1021=L0x20014918, ch1022=L0x20014984, ch1023=L0x200149f0,
ch1024=L0x20014a5c, ch1025=L0x20014ac8, ch1026=L0x20014b34, ch1027=L0x20014ba0,
ch1028=L0x20014c0c, ch1029=L0x20014c78, ch102a=L0x20014ce4, ch102b=L0x20014d50,
ch102c=L0x20014dbc, ch102d=L0x20014e28, ch102e=L0x20014e94, ch102f=L0x20014f00
] && true;


ghost CH200@sint32, CH201@sint32, CH202@sint32,
      ch2000@sint32, ch2001@sint32, ch2002@sint32, ch2003@sint32,
      ch2004@sint32, ch2005@sint32, ch2006@sint32, ch2007@sint32,
      ch2008@sint32, ch2009@sint32, ch200a@sint32, ch200b@sint32,
      ch200c@sint32, ch200d@sint32, ch200e@sint32, ch200f@sint32,
      ch2010@sint32, ch2011@sint32, ch2012@sint32, ch2013@sint32,
      ch2014@sint32, ch2015@sint32, ch2016@sint32, ch2017@sint32,
      ch2018@sint32, ch2019@sint32, ch201a@sint32, ch201b@sint32,
      ch201c@sint32, ch201d@sint32, ch201e@sint32, ch201f@sint32,
      ch2020@sint32, ch2021@sint32, ch2022@sint32, ch2023@sint32,
      ch2024@sint32, ch2025@sint32, ch2026@sint32, ch2027@sint32,
      ch2028@sint32, ch2029@sint32, ch202a@sint32, ch202b@sint32,
      ch202c@sint32, ch202d@sint32, ch202e@sint32, ch202f@sint32: and [
CH200**2 =
ch2000*x**0*z** 0 + ch2001*x**0*z** 1 + ch2002*x**0*z** 2 + ch2003*x**0*z** 3 +
ch2004*x**0*z** 4 + ch2005*x**0*z** 5 + ch2006*x**0*z** 6 + ch2007*x**0*z** 7 +
ch2008*x**0*z** 8 + ch2009*x**0*z** 9 + ch200a*x**0*z**10 + ch200b*x**0*z**11 +
ch200c*x**0*z**12 + ch200d*x**0*z**13 + ch200e*x**0*z**14 + ch200f*x**0*z**15,
ch2000=L0x200148b0, ch2001=L0x2001491c, ch2002=L0x20014988, ch2003=L0x200149f4,
ch2004=L0x20014a60, ch2005=L0x20014acc, ch2006=L0x20014b38, ch2007=L0x20014ba4,
ch2008=L0x20014c10, ch2009=L0x20014c7c, ch200a=L0x20014ce8, ch200b=L0x20014d54,
ch200c=L0x20014dc0, ch200d=L0x20014e2c, ch200e=L0x20014e98, ch200f=L0x20014f04,
CH201**2 =
ch2010*x**1*z** 0 + ch2011*x**1*z** 1 + ch2012*x**1*z** 2 + ch2013*x**1*z** 3 +
ch2014*x**1*z** 4 + ch2015*x**1*z** 5 + ch2016*x**1*z** 6 + ch2017*x**1*z** 7 +
ch2018*x**1*z** 8 + ch2019*x**1*z** 9 + ch201a*x**1*z**10 + ch201b*x**1*z**11 +
ch201c*x**1*z**12 + ch201d*x**1*z**13 + ch201e*x**1*z**14 + ch201f*x**1*z**15,
ch2010=L0x200148b4, ch2011=L0x20014920, ch2012=L0x2001498c, ch2013=L0x200149f8,
ch2014=L0x20014a64, ch2015=L0x20014ad0, ch2016=L0x20014b3c, ch2017=L0x20014ba8,
ch2018=L0x20014c14, ch2019=L0x20014c80, ch201a=L0x20014cec, ch201b=L0x20014d58,
ch201c=L0x20014dc4, ch201d=L0x20014e30, ch201e=L0x20014e9c, ch201f=L0x20014f08,
CH202**2 =
ch2020*x**2*z** 0 + ch2021*x**2*z** 1 + ch2022*x**2*z** 2 + ch2023*x**2*z** 3 +
ch2024*x**2*z** 4 + ch2025*x**2*z** 5 + ch2026*x**2*z** 6 + ch2027*x**2*z** 7 +
ch2028*x**2*z** 8 + ch2029*x**2*z** 9 + ch202a*x**2*z**10 + ch202b*x**2*z**11 +
ch202c*x**2*z**12 + ch202d*x**2*z**13 + ch202e*x**2*z**14 + ch202f*x**2*z**15,
ch2020=L0x200148b8, ch2021=L0x20014924, ch2022=L0x20014990, ch2023=L0x200149fc,
ch2024=L0x20014a68, ch2025=L0x20014ad4, ch2026=L0x20014b40, ch2027=L0x20014bac,
ch2028=L0x20014c18, ch2029=L0x20014c84, ch202a=L0x20014cf0, ch202b=L0x20014d5c,
ch202c=L0x20014dc8, ch202d=L0x20014e34, ch202e=L0x20014ea0, ch202f=L0x20014f0c
] && true;


ghost CH300@sint32, CH301@sint32, CH302@sint32,
      ch3000@sint32, ch3001@sint32, ch3002@sint32, ch3003@sint32,
      ch3004@sint32, ch3005@sint32, ch3006@sint32, ch3007@sint32,
      ch3008@sint32, ch3009@sint32, ch300a@sint32, ch300b@sint32,
      ch300c@sint32, ch300d@sint32, ch300e@sint32, ch300f@sint32,
      ch3010@sint32, ch3011@sint32, ch3012@sint32, ch3013@sint32,
      ch3014@sint32, ch3015@sint32, ch3016@sint32, ch3017@sint32,
      ch3018@sint32, ch3019@sint32, ch301a@sint32, ch301b@sint32,
      ch301c@sint32, ch301d@sint32, ch301e@sint32, ch301f@sint32,
      ch3020@sint32, ch3021@sint32, ch3022@sint32, ch3023@sint32,
      ch3024@sint32, ch3025@sint32, ch3026@sint32, ch3027@sint32,
      ch3028@sint32, ch3029@sint32, ch302a@sint32, ch302b@sint32,
      ch302c@sint32, ch302d@sint32, ch302e@sint32, ch302f@sint32: and [
CH300**2 =
ch3000*x**0*z** 0 + ch3001*x**0*z** 1 + ch3002*x**0*z** 2 + ch3003*x**0*z** 3 +
ch3004*x**0*z** 4 + ch3005*x**0*z** 5 + ch3006*x**0*z** 6 + ch3007*x**0*z** 7 +
ch3008*x**0*z** 8 + ch3009*x**0*z** 9 + ch300a*x**0*z**10 + ch300b*x**0*z**11 +
ch300c*x**0*z**12 + ch300d*x**0*z**13 + ch300e*x**0*z**14 + ch300f*x**0*z**15,
ch3000=L0x200148bc, ch3001=L0x20014928, ch3002=L0x20014994, ch3003=L0x20014a00,
ch3004=L0x20014a6c, ch3005=L0x20014ad8, ch3006=L0x20014b44, ch3007=L0x20014bb0,
ch3008=L0x20014c1c, ch3009=L0x20014c88, ch300a=L0x20014cf4, ch300b=L0x20014d60,
ch300c=L0x20014dcc, ch300d=L0x20014e38, ch300e=L0x20014ea4, ch300f=L0x20014f10,
CH301**2 =
ch3010*x**1*z** 0 + ch3011*x**1*z** 1 + ch3012*x**1*z** 2 + ch3013*x**1*z** 3 +
ch3014*x**1*z** 4 + ch3015*x**1*z** 5 + ch3016*x**1*z** 6 + ch3017*x**1*z** 7 +
ch3018*x**1*z** 8 + ch3019*x**1*z** 9 + ch301a*x**1*z**10 + ch301b*x**1*z**11 +
ch301c*x**1*z**12 + ch301d*x**1*z**13 + ch301e*x**1*z**14 + ch301f*x**1*z**15,
ch3010=L0x200148c0, ch3011=L0x2001492c, ch3012=L0x20014998, ch3013=L0x20014a04,
ch3014=L0x20014a70, ch3015=L0x20014adc, ch3016=L0x20014b48, ch3017=L0x20014bb4,
ch3018=L0x20014c20, ch3019=L0x20014c8c, ch301a=L0x20014cf8, ch301b=L0x20014d64,
ch301c=L0x20014dd0, ch301d=L0x20014e3c, ch301e=L0x20014ea8, ch301f=L0x20014f14,
CH302**2 =
ch3020*x**2*z** 0 + ch3021*x**2*z** 1 + ch3022*x**2*z** 2 + ch3023*x**2*z** 3 +
ch3024*x**2*z** 4 + ch3025*x**2*z** 5 + ch3026*x**2*z** 6 + ch3027*x**2*z** 7 +
ch3028*x**2*z** 8 + ch3029*x**2*z** 9 + ch302a*x**2*z**10 + ch302b*x**2*z**11 +
ch302c*x**2*z**12 + ch302d*x**2*z**13 + ch302e*x**2*z**14 + ch302f*x**2*z**15,
ch3020=L0x200148c4, ch3021=L0x20014930, ch3022=L0x2001499c, ch3023=L0x20014a08,
ch3024=L0x20014a74, ch3025=L0x20014ae0, ch3026=L0x20014b4c, ch3027=L0x20014bb8,
ch3028=L0x20014c24, ch3029=L0x20014c90, ch302a=L0x20014cfc, ch302b=L0x20014d68,
ch302c=L0x20014dd4, ch302d=L0x20014e40, ch302e=L0x20014eac, ch302f=L0x20014f18
] && true;


ghost CH400@sint32, CH401@sint32, CH402@sint32,
      ch4000@sint32, ch4001@sint32, ch4002@sint32, ch4003@sint32,
      ch4004@sint32, ch4005@sint32, ch4006@sint32, ch4007@sint32,
      ch4008@sint32, ch4009@sint32, ch400a@sint32, ch400b@sint32,
      ch400c@sint32, ch400d@sint32, ch400e@sint32, ch400f@sint32,
      ch4010@sint32, ch4011@sint32, ch4012@sint32, ch4013@sint32,
      ch4014@sint32, ch4015@sint32, ch4016@sint32, ch4017@sint32,
      ch4018@sint32, ch4019@sint32, ch401a@sint32, ch401b@sint32,
      ch401c@sint32, ch401d@sint32, ch401e@sint32, ch401f@sint32,
      ch4020@sint32, ch4021@sint32, ch4022@sint32, ch4023@sint32,
      ch4024@sint32, ch4025@sint32, ch4026@sint32, ch4027@sint32,
      ch4028@sint32, ch4029@sint32, ch402a@sint32, ch402b@sint32,
      ch402c@sint32, ch402d@sint32, ch402e@sint32, ch402f@sint32: and [
CH400**2 =
ch4000*x**0*z** 0 + ch4001*x**0*z** 1 + ch4002*x**0*z** 2 + ch4003*x**0*z** 3 +
ch4004*x**0*z** 4 + ch4005*x**0*z** 5 + ch4006*x**0*z** 6 + ch4007*x**0*z** 7 +
ch4008*x**0*z** 8 + ch4009*x**0*z** 9 + ch400a*x**0*z**10 + ch400b*x**0*z**11 +
ch400c*x**0*z**12 + ch400d*x**0*z**13 + ch400e*x**0*z**14 + ch400f*x**0*z**15,
ch4000=L0x200148c8, ch4001=L0x20014934, ch4002=L0x200149a0, ch4003=L0x20014a0c,
ch4004=L0x20014a78, ch4005=L0x20014ae4, ch4006=L0x20014b50, ch4007=L0x20014bbc,
ch4008=L0x20014c28, ch4009=L0x20014c94, ch400a=L0x20014d00, ch400b=L0x20014d6c,
ch400c=L0x20014dd8, ch400d=L0x20014e44, ch400e=L0x20014eb0, ch400f=L0x20014f1c,
CH401**2 =
ch4010*x**1*z** 0 + ch4011*x**1*z** 1 + ch4012*x**1*z** 2 + ch4013*x**1*z** 3 +
ch4014*x**1*z** 4 + ch4015*x**1*z** 5 + ch4016*x**1*z** 6 + ch4017*x**1*z** 7 +
ch4018*x**1*z** 8 + ch4019*x**1*z** 9 + ch401a*x**1*z**10 + ch401b*x**1*z**11 +
ch401c*x**1*z**12 + ch401d*x**1*z**13 + ch401e*x**1*z**14 + ch401f*x**1*z**15,
ch4010=L0x200148cc, ch4011=L0x20014938, ch4012=L0x200149a4, ch4013=L0x20014a10,
ch4014=L0x20014a7c, ch4015=L0x20014ae8, ch4016=L0x20014b54, ch4017=L0x20014bc0,
ch4018=L0x20014c2c, ch4019=L0x20014c98, ch401a=L0x20014d04, ch401b=L0x20014d70,
ch401c=L0x20014ddc, ch401d=L0x20014e48, ch401e=L0x20014eb4, ch401f=L0x20014f20,
CH402**2 =
ch4020*x**2*z** 0 + ch4021*x**2*z** 1 + ch4022*x**2*z** 2 + ch4023*x**2*z** 3 +
ch4024*x**2*z** 4 + ch4025*x**2*z** 5 + ch4026*x**2*z** 6 + ch4027*x**2*z** 7 +
ch4028*x**2*z** 8 + ch4029*x**2*z** 9 + ch402a*x**2*z**10 + ch402b*x**2*z**11 +
ch402c*x**2*z**12 + ch402d*x**2*z**13 + ch402e*x**2*z**14 + ch402f*x**2*z**15,
ch4020=L0x200148d0, ch4021=L0x2001493c, ch4022=L0x200149a8, ch4023=L0x20014a14,
ch4024=L0x20014a80, ch4025=L0x20014aec, ch4026=L0x20014b58, ch4027=L0x20014bc4,
ch4028=L0x20014c30, ch4029=L0x20014c9c, ch402a=L0x20014d08, ch402b=L0x20014d74,
ch402c=L0x20014de0, ch402d=L0x20014e4c, ch402e=L0x20014eb8, ch402f=L0x20014f24
] && true;


ghost CH500@sint32, CH501@sint32, CH502@sint32,
      ch5000@sint32, ch5001@sint32, ch5002@sint32, ch5003@sint32,
      ch5004@sint32, ch5005@sint32, ch5006@sint32, ch5007@sint32,
      ch5008@sint32, ch5009@sint32, ch500a@sint32, ch500b@sint32,
      ch500c@sint32, ch500d@sint32, ch500e@sint32, ch500f@sint32,
      ch5010@sint32, ch5011@sint32, ch5012@sint32, ch5013@sint32,
      ch5014@sint32, ch5015@sint32, ch5016@sint32, ch5017@sint32,
      ch5018@sint32, ch5019@sint32, ch501a@sint32, ch501b@sint32,
      ch501c@sint32, ch501d@sint32, ch501e@sint32, ch501f@sint32,
      ch5020@sint32, ch5021@sint32, ch5022@sint32, ch5023@sint32,
      ch5024@sint32, ch5025@sint32, ch5026@sint32, ch5027@sint32,
      ch5028@sint32, ch5029@sint32, ch502a@sint32, ch502b@sint32,
      ch502c@sint32, ch502d@sint32, ch502e@sint32, ch502f@sint32: and [
CH500**2 =
ch5000*x**0*z** 0 + ch5001*x**0*z** 1 + ch5002*x**0*z** 2 + ch5003*x**0*z** 3 +
ch5004*x**0*z** 4 + ch5005*x**0*z** 5 + ch5006*x**0*z** 6 + ch5007*x**0*z** 7 +
ch5008*x**0*z** 8 + ch5009*x**0*z** 9 + ch500a*x**0*z**10 + ch500b*x**0*z**11 +
ch500c*x**0*z**12 + ch500d*x**0*z**13 + ch500e*x**0*z**14 + ch500f*x**0*z**15,
ch5000=L0x200148d4, ch5001=L0x20014940, ch5002=L0x200149ac, ch5003=L0x20014a18,
ch5004=L0x20014a84, ch5005=L0x20014af0, ch5006=L0x20014b5c, ch5007=L0x20014bc8,
ch5008=L0x20014c34, ch5009=L0x20014ca0, ch500a=L0x20014d0c, ch500b=L0x20014d78,
ch500c=L0x20014de4, ch500d=L0x20014e50, ch500e=L0x20014ebc, ch500f=L0x20014f28,
CH501**2 =
ch5010*x**1*z** 0 + ch5011*x**1*z** 1 + ch5012*x**1*z** 2 + ch5013*x**1*z** 3 +
ch5014*x**1*z** 4 + ch5015*x**1*z** 5 + ch5016*x**1*z** 6 + ch5017*x**1*z** 7 +
ch5018*x**1*z** 8 + ch5019*x**1*z** 9 + ch501a*x**1*z**10 + ch501b*x**1*z**11 +
ch501c*x**1*z**12 + ch501d*x**1*z**13 + ch501e*x**1*z**14 + ch501f*x**1*z**15,
ch5010=L0x200148d8, ch5011=L0x20014944, ch5012=L0x200149b0, ch5013=L0x20014a1c,
ch5014=L0x20014a88, ch5015=L0x20014af4, ch5016=L0x20014b60, ch5017=L0x20014bcc,
ch5018=L0x20014c38, ch5019=L0x20014ca4, ch501a=L0x20014d10, ch501b=L0x20014d7c,
ch501c=L0x20014de8, ch501d=L0x20014e54, ch501e=L0x20014ec0, ch501f=L0x20014f2c,
CH502**2 =
ch5020*x**2*z** 0 + ch5021*x**2*z** 1 + ch5022*x**2*z** 2 + ch5023*x**2*z** 3 +
ch5024*x**2*z** 4 + ch5025*x**2*z** 5 + ch5026*x**2*z** 6 + ch5027*x**2*z** 7 +
ch5028*x**2*z** 8 + ch5029*x**2*z** 9 + ch502a*x**2*z**10 + ch502b*x**2*z**11 +
ch502c*x**2*z**12 + ch502d*x**2*z**13 + ch502e*x**2*z**14 + ch502f*x**2*z**15,
ch5020=L0x200148dc, ch5021=L0x20014948, ch5022=L0x200149b4, ch5023=L0x20014a20,
ch5024=L0x20014a8c, ch5025=L0x20014af8, ch5026=L0x20014b64, ch5027=L0x20014bd0,
ch5028=L0x20014c3c, ch5029=L0x20014ca8, ch502a=L0x20014d14, ch502b=L0x20014d80,
ch502c=L0x20014dec, ch502d=L0x20014e58, ch502e=L0x20014ec4, ch502f=L0x20014f30
] && true;


ghost CH600@sint32, CH601@sint32, CH602@sint32,
      ch6000@sint32, ch6001@sint32, ch6002@sint32, ch6003@sint32,
      ch6004@sint32, ch6005@sint32, ch6006@sint32, ch6007@sint32,
      ch6008@sint32, ch6009@sint32, ch600a@sint32, ch600b@sint32,
      ch600c@sint32, ch600d@sint32, ch600e@sint32, ch600f@sint32,
      ch6010@sint32, ch6011@sint32, ch6012@sint32, ch6013@sint32,
      ch6014@sint32, ch6015@sint32, ch6016@sint32, ch6017@sint32,
      ch6018@sint32, ch6019@sint32, ch601a@sint32, ch601b@sint32,
      ch601c@sint32, ch601d@sint32, ch601e@sint32, ch601f@sint32,
      ch6020@sint32, ch6021@sint32, ch6022@sint32, ch6023@sint32,
      ch6024@sint32, ch6025@sint32, ch6026@sint32, ch6027@sint32,
      ch6028@sint32, ch6029@sint32, ch602a@sint32, ch602b@sint32,
      ch602c@sint32, ch602d@sint32, ch602e@sint32, ch602f@sint32: and [
CH600**2 =
ch6000*x**0*z** 0 + ch6001*x**0*z** 1 + ch6002*x**0*z** 2 + ch6003*x**0*z** 3 +
ch6004*x**0*z** 4 + ch6005*x**0*z** 5 + ch6006*x**0*z** 6 + ch6007*x**0*z** 7 +
ch6008*x**0*z** 8 + ch6009*x**0*z** 9 + ch600a*x**0*z**10 + ch600b*x**0*z**11 +
ch600c*x**0*z**12 + ch600d*x**0*z**13 + ch600e*x**0*z**14 + ch600f*x**0*z**15,
ch6000=L0x200148e0, ch6001=L0x2001494c, ch6002=L0x200149b8, ch6003=L0x20014a24,
ch6004=L0x20014a90, ch6005=L0x20014afc, ch6006=L0x20014b68, ch6007=L0x20014bd4,
ch6008=L0x20014c40, ch6009=L0x20014cac, ch600a=L0x20014d18, ch600b=L0x20014d84,
ch600c=L0x20014df0, ch600d=L0x20014e5c, ch600e=L0x20014ec8, ch600f=L0x20014f34,
CH601**2 =
ch6010*x**1*z** 0 + ch6011*x**1*z** 1 + ch6012*x**1*z** 2 + ch6013*x**1*z** 3 +
ch6014*x**1*z** 4 + ch6015*x**1*z** 5 + ch6016*x**1*z** 6 + ch6017*x**1*z** 7 +
ch6018*x**1*z** 8 + ch6019*x**1*z** 9 + ch601a*x**1*z**10 + ch601b*x**1*z**11 +
ch601c*x**1*z**12 + ch601d*x**1*z**13 + ch601e*x**1*z**14 + ch601f*x**1*z**15,
ch6010=L0x200148e4, ch6011=L0x20014950, ch6012=L0x200149bc, ch6013=L0x20014a28,
ch6014=L0x20014a94, ch6015=L0x20014b00, ch6016=L0x20014b6c, ch6017=L0x20014bd8,
ch6018=L0x20014c44, ch6019=L0x20014cb0, ch601a=L0x20014d1c, ch601b=L0x20014d88,
ch601c=L0x20014df4, ch601d=L0x20014e60, ch601e=L0x20014ecc, ch601f=L0x20014f38,
CH602**2 =
ch6020*x**2*z** 0 + ch6021*x**2*z** 1 + ch6022*x**2*z** 2 + ch6023*x**2*z** 3 +
ch6024*x**2*z** 4 + ch6025*x**2*z** 5 + ch6026*x**2*z** 6 + ch6027*x**2*z** 7 +
ch6028*x**2*z** 8 + ch6029*x**2*z** 9 + ch602a*x**2*z**10 + ch602b*x**2*z**11 +
ch602c*x**2*z**12 + ch602d*x**2*z**13 + ch602e*x**2*z**14 + ch602f*x**2*z**15,
ch6020=L0x200148e8, ch6021=L0x20014954, ch6022=L0x200149c0, ch6023=L0x20014a2c,
ch6024=L0x20014a98, ch6025=L0x20014b04, ch6026=L0x20014b70, ch6027=L0x20014bdc,
ch6028=L0x20014c48, ch6029=L0x20014cb4, ch602a=L0x20014d20, ch602b=L0x20014d8c,
ch602c=L0x20014df8, ch602d=L0x20014e64, ch602e=L0x20014ed0, ch602f=L0x20014f3c
] && true;


ghost CH700@sint32, CH701@sint32, CH702@sint32,
      ch7000@sint32, ch7001@sint32, ch7002@sint32, ch7003@sint32,
      ch7004@sint32, ch7005@sint32, ch7006@sint32, ch7007@sint32,
      ch7008@sint32, ch7009@sint32, ch700a@sint32, ch700b@sint32,
      ch700c@sint32, ch700d@sint32, ch700e@sint32, ch700f@sint32,
      ch7010@sint32, ch7011@sint32, ch7012@sint32, ch7013@sint32,
      ch7014@sint32, ch7015@sint32, ch7016@sint32, ch7017@sint32,
      ch7018@sint32, ch7019@sint32, ch701a@sint32, ch701b@sint32,
      ch701c@sint32, ch701d@sint32, ch701e@sint32, ch701f@sint32,
      ch7020@sint32, ch7021@sint32, ch7022@sint32, ch7023@sint32,
      ch7024@sint32, ch7025@sint32, ch7026@sint32, ch7027@sint32,
      ch7028@sint32, ch7029@sint32, ch702a@sint32, ch702b@sint32,
      ch702c@sint32, ch702d@sint32, ch702e@sint32, ch702f@sint32: and [
CH700**2 =
ch7000*x**0*z** 0 + ch7001*x**0*z** 1 + ch7002*x**0*z** 2 + ch7003*x**0*z** 3 +
ch7004*x**0*z** 4 + ch7005*x**0*z** 5 + ch7006*x**0*z** 6 + ch7007*x**0*z** 7 +
ch7008*x**0*z** 8 + ch7009*x**0*z** 9 + ch700a*x**0*z**10 + ch700b*x**0*z**11 +
ch700c*x**0*z**12 + ch700d*x**0*z**13 + ch700e*x**0*z**14 + ch700f*x**0*z**15,
ch7000=L0x200148ec, ch7001=L0x20014958, ch7002=L0x200149c4, ch7003=L0x20014a30,
ch7004=L0x20014a9c, ch7005=L0x20014b08, ch7006=L0x20014b74, ch7007=L0x20014be0,
ch7008=L0x20014c4c, ch7009=L0x20014cb8, ch700a=L0x20014d24, ch700b=L0x20014d90,
ch700c=L0x20014dfc, ch700d=L0x20014e68, ch700e=L0x20014ed4, ch700f=L0x20014f40,
CH701**2 =
ch7010*x**1*z** 0 + ch7011*x**1*z** 1 + ch7012*x**1*z** 2 + ch7013*x**1*z** 3 +
ch7014*x**1*z** 4 + ch7015*x**1*z** 5 + ch7016*x**1*z** 6 + ch7017*x**1*z** 7 +
ch7018*x**1*z** 8 + ch7019*x**1*z** 9 + ch701a*x**1*z**10 + ch701b*x**1*z**11 +
ch701c*x**1*z**12 + ch701d*x**1*z**13 + ch701e*x**1*z**14 + ch701f*x**1*z**15,
ch7010=L0x200148f0, ch7011=L0x2001495c, ch7012=L0x200149c8, ch7013=L0x20014a34,
ch7014=L0x20014aa0, ch7015=L0x20014b0c, ch7016=L0x20014b78, ch7017=L0x20014be4,
ch7018=L0x20014c50, ch7019=L0x20014cbc, ch701a=L0x20014d28, ch701b=L0x20014d94,
ch701c=L0x20014e00, ch701d=L0x20014e6c, ch701e=L0x20014ed8, ch701f=L0x20014f44,
CH702**2 =
ch7020*x**2*z** 0 + ch7021*x**2*z** 1 + ch7022*x**2*z** 2 + ch7023*x**2*z** 3 +
ch7024*x**2*z** 4 + ch7025*x**2*z** 5 + ch7026*x**2*z** 6 + ch7027*x**2*z** 7 +
ch7028*x**2*z** 8 + ch7029*x**2*z** 9 + ch702a*x**2*z**10 + ch702b*x**2*z**11 +
ch702c*x**2*z**12 + ch702d*x**2*z**13 + ch702e*x**2*z**14 + ch702f*x**2*z**15,
ch7020=L0x200148f4, ch7021=L0x20014960, ch7022=L0x200149cc, ch7023=L0x20014a38,
ch7024=L0x20014aa4, ch7025=L0x20014b10, ch7026=L0x20014b7c, ch7027=L0x20014be8,
ch7028=L0x20014c54, ch7029=L0x20014cc0, ch702a=L0x20014d2c, ch702b=L0x20014d98,
ch702c=L0x20014e04, ch702d=L0x20014e70, ch702e=L0x20014edc, ch702f=L0x20014f48
] && true;


ghost CH800@sint32, CH801@sint32, CH802@sint32,
      ch8000@sint32, ch8001@sint32, ch8002@sint32, ch8003@sint32,
      ch8004@sint32, ch8005@sint32, ch8006@sint32, ch8007@sint32,
      ch8008@sint32, ch8009@sint32, ch800a@sint32, ch800b@sint32,
      ch800c@sint32, ch800d@sint32, ch800e@sint32, ch800f@sint32,
      ch8010@sint32, ch8011@sint32, ch8012@sint32, ch8013@sint32,
      ch8014@sint32, ch8015@sint32, ch8016@sint32, ch8017@sint32,
      ch8018@sint32, ch8019@sint32, ch801a@sint32, ch801b@sint32,
      ch801c@sint32, ch801d@sint32, ch801e@sint32, ch801f@sint32,
      ch8020@sint32, ch8021@sint32, ch8022@sint32, ch8023@sint32,
      ch8024@sint32, ch8025@sint32, ch8026@sint32, ch8027@sint32,
      ch8028@sint32, ch8029@sint32, ch802a@sint32, ch802b@sint32,
      ch802c@sint32, ch802d@sint32, ch802e@sint32, ch802f@sint32: and [
CH800**2 =
ch8000*x**0*z** 0 + ch8001*x**0*z** 1 + ch8002*x**0*z** 2 + ch8003*x**0*z** 3 +
ch8004*x**0*z** 4 + ch8005*x**0*z** 5 + ch8006*x**0*z** 6 + ch8007*x**0*z** 7 +
ch8008*x**0*z** 8 + ch8009*x**0*z** 9 + ch800a*x**0*z**10 + ch800b*x**0*z**11 +
ch800c*x**0*z**12 + ch800d*x**0*z**13 + ch800e*x**0*z**14 + ch800f*x**0*z**15,
ch8000=L0x200148f8, ch8001=L0x20014964, ch8002=L0x200149d0, ch8003=L0x20014a3c,
ch8004=L0x20014aa8, ch8005=L0x20014b14, ch8006=L0x20014b80, ch8007=L0x20014bec,
ch8008=L0x20014c58, ch8009=L0x20014cc4, ch800a=L0x20014d30, ch800b=L0x20014d9c,
ch800c=L0x20014e08, ch800d=L0x20014e74, ch800e=L0x20014ee0, ch800f=L0x20014f4c,
CH801**2 =
ch8010*x**1*z** 0 + ch8011*x**1*z** 1 + ch8012*x**1*z** 2 + ch8013*x**1*z** 3 +
ch8014*x**1*z** 4 + ch8015*x**1*z** 5 + ch8016*x**1*z** 6 + ch8017*x**1*z** 7 +
ch8018*x**1*z** 8 + ch8019*x**1*z** 9 + ch801a*x**1*z**10 + ch801b*x**1*z**11 +
ch801c*x**1*z**12 + ch801d*x**1*z**13 + ch801e*x**1*z**14 + ch801f*x**1*z**15,
ch8010=L0x200148fc, ch8011=L0x20014968, ch8012=L0x200149d4, ch8013=L0x20014a40,
ch8014=L0x20014aac, ch8015=L0x20014b18, ch8016=L0x20014b84, ch8017=L0x20014bf0,
ch8018=L0x20014c5c, ch8019=L0x20014cc8, ch801a=L0x20014d34, ch801b=L0x20014da0,
ch801c=L0x20014e0c, ch801d=L0x20014e78, ch801e=L0x20014ee4, ch801f=L0x20014f50,
CH802**2 =
ch8020*x**2*z** 0 + ch8021*x**2*z** 1 + ch8022*x**2*z** 2 + ch8023*x**2*z** 3 +
ch8024*x**2*z** 4 + ch8025*x**2*z** 5 + ch8026*x**2*z** 6 + ch8027*x**2*z** 7 +
ch8028*x**2*z** 8 + ch8029*x**2*z** 9 + ch802a*x**2*z**10 + ch802b*x**2*z**11 +
ch802c*x**2*z**12 + ch802d*x**2*z**13 + ch802e*x**2*z**14 + ch802f*x**2*z**15,
ch8020=L0x20014900, ch8021=L0x2001496c, ch8022=L0x200149d8, ch8023=L0x20014a44,
ch8024=L0x20014ab0, ch8025=L0x20014b1c, ch8026=L0x20014b88, ch8027=L0x20014bf4,
ch8028=L0x20014c60, ch8029=L0x20014ccc, ch802a=L0x20014d38, ch802b=L0x20014da4,
ch802c=L0x20014e10, ch802d=L0x20014e7c, ch802e=L0x20014ee8, ch802f=L0x20014f54
] && true;


ghost CH010@sint32, CH011@sint32, CH012@sint32,
      ch0100@sint32, ch0101@sint32, ch0102@sint32, ch0103@sint32,
      ch0104@sint32, ch0105@sint32, ch0106@sint32, ch0107@sint32,
      ch0108@sint32, ch0109@sint32, ch010a@sint32, ch010b@sint32,
      ch010c@sint32, ch010d@sint32, ch010e@sint32, ch010f@sint32,
      ch0110@sint32, ch0111@sint32, ch0112@sint32, ch0113@sint32,
      ch0114@sint32, ch0115@sint32, ch0116@sint32, ch0117@sint32,
      ch0118@sint32, ch0119@sint32, ch011a@sint32, ch011b@sint32,
      ch011c@sint32, ch011d@sint32, ch011e@sint32, ch011f@sint32,
      ch0120@sint32, ch0121@sint32, ch0122@sint32, ch0123@sint32,
      ch0124@sint32, ch0125@sint32, ch0126@sint32, ch0127@sint32,
      ch0128@sint32, ch0129@sint32, ch012a@sint32, ch012b@sint32,
      ch012c@sint32, ch012d@sint32, ch012e@sint32, ch012f@sint32: and [
CH010**2 =
ch0100*x**0*z** 0 + ch0101*x**0*z** 1 + ch0102*x**0*z** 2 + ch0103*x**0*z** 3 +
ch0104*x**0*z** 4 + ch0105*x**0*z** 5 + ch0106*x**0*z** 6 + ch0107*x**0*z** 7 +
ch0108*x**0*z** 8 + ch0109*x**0*z** 9 + ch010a*x**0*z**10 + ch010b*x**0*z**11 +
ch010c*x**0*z**12 + ch010d*x**0*z**13 + ch010e*x**0*z**14 + ch010f*x**0*z**15,
ch0100=L0x20014f58, ch0101=L0x20014fc4, ch0102=L0x20015030, ch0103=L0x2001509c,
ch0104=L0x20015108, ch0105=L0x20015174, ch0106=L0x200151e0, ch0107=L0x2001524c,
ch0108=L0x200152b8, ch0109=L0x20015324, ch010a=L0x20015390, ch010b=L0x200153fc,
ch010c=L0x20015468, ch010d=L0x200154d4, ch010e=L0x20015540, ch010f=L0x200155ac,
CH011**2 =
ch0110*x**1*z** 0 + ch0111*x**1*z** 1 + ch0112*x**1*z** 2 + ch0113*x**1*z** 3 +
ch0114*x**1*z** 4 + ch0115*x**1*z** 5 + ch0116*x**1*z** 6 + ch0117*x**1*z** 7 +
ch0118*x**1*z** 8 + ch0119*x**1*z** 9 + ch011a*x**1*z**10 + ch011b*x**1*z**11 +
ch011c*x**1*z**12 + ch011d*x**1*z**13 + ch011e*x**1*z**14 + ch011f*x**1*z**15,
ch0110=L0x20014f5c, ch0111=L0x20014fc8, ch0112=L0x20015034, ch0113=L0x200150a0,
ch0114=L0x2001510c, ch0115=L0x20015178, ch0116=L0x200151e4, ch0117=L0x20015250,
ch0118=L0x200152bc, ch0119=L0x20015328, ch011a=L0x20015394, ch011b=L0x20015400,
ch011c=L0x2001546c, ch011d=L0x200154d8, ch011e=L0x20015544, ch011f=L0x200155b0,
CH012**2 =
ch0120*x**2*z** 0 + ch0121*x**2*z** 1 + ch0122*x**2*z** 2 + ch0123*x**2*z** 3 +
ch0124*x**2*z** 4 + ch0125*x**2*z** 5 + ch0126*x**2*z** 6 + ch0127*x**2*z** 7 +
ch0128*x**2*z** 8 + ch0129*x**2*z** 9 + ch012a*x**2*z**10 + ch012b*x**2*z**11 +
ch012c*x**2*z**12 + ch012d*x**2*z**13 + ch012e*x**2*z**14 + ch012f*x**2*z**15,
ch0120=L0x20014f60, ch0121=L0x20014fcc, ch0122=L0x20015038, ch0123=L0x200150a4,
ch0124=L0x20015110, ch0125=L0x2001517c, ch0126=L0x200151e8, ch0127=L0x20015254,
ch0128=L0x200152c0, ch0129=L0x2001532c, ch012a=L0x20015398, ch012b=L0x20015404,
ch012c=L0x20015470, ch012d=L0x200154dc, ch012e=L0x20015548, ch012f=L0x200155b4
] && true;


ghost CH110@sint32, CH111@sint32, CH112@sint32,
      ch1100@sint32, ch1101@sint32, ch1102@sint32, ch1103@sint32,
      ch1104@sint32, ch1105@sint32, ch1106@sint32, ch1107@sint32,
      ch1108@sint32, ch1109@sint32, ch110a@sint32, ch110b@sint32,
      ch110c@sint32, ch110d@sint32, ch110e@sint32, ch110f@sint32,
      ch1110@sint32, ch1111@sint32, ch1112@sint32, ch1113@sint32,
      ch1114@sint32, ch1115@sint32, ch1116@sint32, ch1117@sint32,
      ch1118@sint32, ch1119@sint32, ch111a@sint32, ch111b@sint32,
      ch111c@sint32, ch111d@sint32, ch111e@sint32, ch111f@sint32,
      ch1120@sint32, ch1121@sint32, ch1122@sint32, ch1123@sint32,
      ch1124@sint32, ch1125@sint32, ch1126@sint32, ch1127@sint32,
      ch1128@sint32, ch1129@sint32, ch112a@sint32, ch112b@sint32,
      ch112c@sint32, ch112d@sint32, ch112e@sint32, ch112f@sint32: and [
CH110**2 =
ch1100*x**0*z** 0 + ch1101*x**0*z** 1 + ch1102*x**0*z** 2 + ch1103*x**0*z** 3 +
ch1104*x**0*z** 4 + ch1105*x**0*z** 5 + ch1106*x**0*z** 6 + ch1107*x**0*z** 7 +
ch1108*x**0*z** 8 + ch1109*x**0*z** 9 + ch110a*x**0*z**10 + ch110b*x**0*z**11 +
ch110c*x**0*z**12 + ch110d*x**0*z**13 + ch110e*x**0*z**14 + ch110f*x**0*z**15,
ch1100=L0x20014f64, ch1101=L0x20014fd0, ch1102=L0x2001503c, ch1103=L0x200150a8,
ch1104=L0x20015114, ch1105=L0x20015180, ch1106=L0x200151ec, ch1107=L0x20015258,
ch1108=L0x200152c4, ch1109=L0x20015330, ch110a=L0x2001539c, ch110b=L0x20015408,
ch110c=L0x20015474, ch110d=L0x200154e0, ch110e=L0x2001554c, ch110f=L0x200155b8,
CH111**2 =
ch1110*x**1*z** 0 + ch1111*x**1*z** 1 + ch1112*x**1*z** 2 + ch1113*x**1*z** 3 +
ch1114*x**1*z** 4 + ch1115*x**1*z** 5 + ch1116*x**1*z** 6 + ch1117*x**1*z** 7 +
ch1118*x**1*z** 8 + ch1119*x**1*z** 9 + ch111a*x**1*z**10 + ch111b*x**1*z**11 +
ch111c*x**1*z**12 + ch111d*x**1*z**13 + ch111e*x**1*z**14 + ch111f*x**1*z**15,
ch1110=L0x20014f68, ch1111=L0x20014fd4, ch1112=L0x20015040, ch1113=L0x200150ac,
ch1114=L0x20015118, ch1115=L0x20015184, ch1116=L0x200151f0, ch1117=L0x2001525c,
ch1118=L0x200152c8, ch1119=L0x20015334, ch111a=L0x200153a0, ch111b=L0x2001540c,
ch111c=L0x20015478, ch111d=L0x200154e4, ch111e=L0x20015550, ch111f=L0x200155bc,
CH112**2 =
ch1120*x**2*z** 0 + ch1121*x**2*z** 1 + ch1122*x**2*z** 2 + ch1123*x**2*z** 3 +
ch1124*x**2*z** 4 + ch1125*x**2*z** 5 + ch1126*x**2*z** 6 + ch1127*x**2*z** 7 +
ch1128*x**2*z** 8 + ch1129*x**2*z** 9 + ch112a*x**2*z**10 + ch112b*x**2*z**11 +
ch112c*x**2*z**12 + ch112d*x**2*z**13 + ch112e*x**2*z**14 + ch112f*x**2*z**15,
ch1120=L0x20014f6c, ch1121=L0x20014fd8, ch1122=L0x20015044, ch1123=L0x200150b0,
ch1124=L0x2001511c, ch1125=L0x20015188, ch1126=L0x200151f4, ch1127=L0x20015260,
ch1128=L0x200152cc, ch1129=L0x20015338, ch112a=L0x200153a4, ch112b=L0x20015410,
ch112c=L0x2001547c, ch112d=L0x200154e8, ch112e=L0x20015554, ch112f=L0x200155c0
] && true;


ghost CH210@sint32, CH211@sint32, CH212@sint32,
      ch2100@sint32, ch2101@sint32, ch2102@sint32, ch2103@sint32,
      ch2104@sint32, ch2105@sint32, ch2106@sint32, ch2107@sint32,
      ch2108@sint32, ch2109@sint32, ch210a@sint32, ch210b@sint32,
      ch210c@sint32, ch210d@sint32, ch210e@sint32, ch210f@sint32,
      ch2110@sint32, ch2111@sint32, ch2112@sint32, ch2113@sint32,
      ch2114@sint32, ch2115@sint32, ch2116@sint32, ch2117@sint32,
      ch2118@sint32, ch2119@sint32, ch211a@sint32, ch211b@sint32,
      ch211c@sint32, ch211d@sint32, ch211e@sint32, ch211f@sint32,
      ch2120@sint32, ch2121@sint32, ch2122@sint32, ch2123@sint32,
      ch2124@sint32, ch2125@sint32, ch2126@sint32, ch2127@sint32,
      ch2128@sint32, ch2129@sint32, ch212a@sint32, ch212b@sint32,
      ch212c@sint32, ch212d@sint32, ch212e@sint32, ch212f@sint32: and [
CH210**2 =
ch2100*x**0*z** 0 + ch2101*x**0*z** 1 + ch2102*x**0*z** 2 + ch2103*x**0*z** 3 +
ch2104*x**0*z** 4 + ch2105*x**0*z** 5 + ch2106*x**0*z** 6 + ch2107*x**0*z** 7 +
ch2108*x**0*z** 8 + ch2109*x**0*z** 9 + ch210a*x**0*z**10 + ch210b*x**0*z**11 +
ch210c*x**0*z**12 + ch210d*x**0*z**13 + ch210e*x**0*z**14 + ch210f*x**0*z**15,
ch2100=L0x20014f70, ch2101=L0x20014fdc, ch2102=L0x20015048, ch2103=L0x200150b4,
ch2104=L0x20015120, ch2105=L0x2001518c, ch2106=L0x200151f8, ch2107=L0x20015264,
ch2108=L0x200152d0, ch2109=L0x2001533c, ch210a=L0x200153a8, ch210b=L0x20015414,
ch210c=L0x20015480, ch210d=L0x200154ec, ch210e=L0x20015558, ch210f=L0x200155c4,
CH211**2 =
ch2110*x**1*z** 0 + ch2111*x**1*z** 1 + ch2112*x**1*z** 2 + ch2113*x**1*z** 3 +
ch2114*x**1*z** 4 + ch2115*x**1*z** 5 + ch2116*x**1*z** 6 + ch2117*x**1*z** 7 +
ch2118*x**1*z** 8 + ch2119*x**1*z** 9 + ch211a*x**1*z**10 + ch211b*x**1*z**11 +
ch211c*x**1*z**12 + ch211d*x**1*z**13 + ch211e*x**1*z**14 + ch211f*x**1*z**15,
ch2110=L0x20014f74, ch2111=L0x20014fe0, ch2112=L0x2001504c, ch2113=L0x200150b8,
ch2114=L0x20015124, ch2115=L0x20015190, ch2116=L0x200151fc, ch2117=L0x20015268,
ch2118=L0x200152d4, ch2119=L0x20015340, ch211a=L0x200153ac, ch211b=L0x20015418,
ch211c=L0x20015484, ch211d=L0x200154f0, ch211e=L0x2001555c, ch211f=L0x200155c8,
CH212**2 =
ch2120*x**2*z** 0 + ch2121*x**2*z** 1 + ch2122*x**2*z** 2 + ch2123*x**2*z** 3 +
ch2124*x**2*z** 4 + ch2125*x**2*z** 5 + ch2126*x**2*z** 6 + ch2127*x**2*z** 7 +
ch2128*x**2*z** 8 + ch2129*x**2*z** 9 + ch212a*x**2*z**10 + ch212b*x**2*z**11 +
ch212c*x**2*z**12 + ch212d*x**2*z**13 + ch212e*x**2*z**14 + ch212f*x**2*z**15,
ch2120=L0x20014f78, ch2121=L0x20014fe4, ch2122=L0x20015050, ch2123=L0x200150bc,
ch2124=L0x20015128, ch2125=L0x20015194, ch2126=L0x20015200, ch2127=L0x2001526c,
ch2128=L0x200152d8, ch2129=L0x20015344, ch212a=L0x200153b0, ch212b=L0x2001541c,
ch212c=L0x20015488, ch212d=L0x200154f4, ch212e=L0x20015560, ch212f=L0x200155cc
] && true;


ghost CH310@sint32, CH311@sint32, CH312@sint32,
      ch3100@sint32, ch3101@sint32, ch3102@sint32, ch3103@sint32,
      ch3104@sint32, ch3105@sint32, ch3106@sint32, ch3107@sint32,
      ch3108@sint32, ch3109@sint32, ch310a@sint32, ch310b@sint32,
      ch310c@sint32, ch310d@sint32, ch310e@sint32, ch310f@sint32,
      ch3110@sint32, ch3111@sint32, ch3112@sint32, ch3113@sint32,
      ch3114@sint32, ch3115@sint32, ch3116@sint32, ch3117@sint32,
      ch3118@sint32, ch3119@sint32, ch311a@sint32, ch311b@sint32,
      ch311c@sint32, ch311d@sint32, ch311e@sint32, ch311f@sint32,
      ch3120@sint32, ch3121@sint32, ch3122@sint32, ch3123@sint32,
      ch3124@sint32, ch3125@sint32, ch3126@sint32, ch3127@sint32,
      ch3128@sint32, ch3129@sint32, ch312a@sint32, ch312b@sint32,
      ch312c@sint32, ch312d@sint32, ch312e@sint32, ch312f@sint32: and [
CH310**2 =
ch3100*x**0*z** 0 + ch3101*x**0*z** 1 + ch3102*x**0*z** 2 + ch3103*x**0*z** 3 +
ch3104*x**0*z** 4 + ch3105*x**0*z** 5 + ch3106*x**0*z** 6 + ch3107*x**0*z** 7 +
ch3108*x**0*z** 8 + ch3109*x**0*z** 9 + ch310a*x**0*z**10 + ch310b*x**0*z**11 +
ch310c*x**0*z**12 + ch310d*x**0*z**13 + ch310e*x**0*z**14 + ch310f*x**0*z**15,
ch3100=L0x20014f7c, ch3101=L0x20014fe8, ch3102=L0x20015054, ch3103=L0x200150c0,
ch3104=L0x2001512c, ch3105=L0x20015198, ch3106=L0x20015204, ch3107=L0x20015270,
ch3108=L0x200152dc, ch3109=L0x20015348, ch310a=L0x200153b4, ch310b=L0x20015420,
ch310c=L0x2001548c, ch310d=L0x200154f8, ch310e=L0x20015564, ch310f=L0x200155d0,
CH311**2 =
ch3110*x**1*z** 0 + ch3111*x**1*z** 1 + ch3112*x**1*z** 2 + ch3113*x**1*z** 3 +
ch3114*x**1*z** 4 + ch3115*x**1*z** 5 + ch3116*x**1*z** 6 + ch3117*x**1*z** 7 +
ch3118*x**1*z** 8 + ch3119*x**1*z** 9 + ch311a*x**1*z**10 + ch311b*x**1*z**11 +
ch311c*x**1*z**12 + ch311d*x**1*z**13 + ch311e*x**1*z**14 + ch311f*x**1*z**15,
ch3110=L0x20014f80, ch3111=L0x20014fec, ch3112=L0x20015058, ch3113=L0x200150c4,
ch3114=L0x20015130, ch3115=L0x2001519c, ch3116=L0x20015208, ch3117=L0x20015274,
ch3118=L0x200152e0, ch3119=L0x2001534c, ch311a=L0x200153b8, ch311b=L0x20015424,
ch311c=L0x20015490, ch311d=L0x200154fc, ch311e=L0x20015568, ch311f=L0x200155d4,
CH312**2 =
ch3120*x**2*z** 0 + ch3121*x**2*z** 1 + ch3122*x**2*z** 2 + ch3123*x**2*z** 3 +
ch3124*x**2*z** 4 + ch3125*x**2*z** 5 + ch3126*x**2*z** 6 + ch3127*x**2*z** 7 +
ch3128*x**2*z** 8 + ch3129*x**2*z** 9 + ch312a*x**2*z**10 + ch312b*x**2*z**11 +
ch312c*x**2*z**12 + ch312d*x**2*z**13 + ch312e*x**2*z**14 + ch312f*x**2*z**15,
ch3120=L0x20014f84, ch3121=L0x20014ff0, ch3122=L0x2001505c, ch3123=L0x200150c8,
ch3124=L0x20015134, ch3125=L0x200151a0, ch3126=L0x2001520c, ch3127=L0x20015278,
ch3128=L0x200152e4, ch3129=L0x20015350, ch312a=L0x200153bc, ch312b=L0x20015428,
ch312c=L0x20015494, ch312d=L0x20015500, ch312e=L0x2001556c, ch312f=L0x200155d8
] && true;


ghost CH410@sint32, CH411@sint32, CH412@sint32,
      ch4100@sint32, ch4101@sint32, ch4102@sint32, ch4103@sint32,
      ch4104@sint32, ch4105@sint32, ch4106@sint32, ch4107@sint32,
      ch4108@sint32, ch4109@sint32, ch410a@sint32, ch410b@sint32,
      ch410c@sint32, ch410d@sint32, ch410e@sint32, ch410f@sint32,
      ch4110@sint32, ch4111@sint32, ch4112@sint32, ch4113@sint32,
      ch4114@sint32, ch4115@sint32, ch4116@sint32, ch4117@sint32,
      ch4118@sint32, ch4119@sint32, ch411a@sint32, ch411b@sint32,
      ch411c@sint32, ch411d@sint32, ch411e@sint32, ch411f@sint32,
      ch4120@sint32, ch4121@sint32, ch4122@sint32, ch4123@sint32,
      ch4124@sint32, ch4125@sint32, ch4126@sint32, ch4127@sint32,
      ch4128@sint32, ch4129@sint32, ch412a@sint32, ch412b@sint32,
      ch412c@sint32, ch412d@sint32, ch412e@sint32, ch412f@sint32: and [
CH410**2 =
ch4100*x**0*z** 0 + ch4101*x**0*z** 1 + ch4102*x**0*z** 2 + ch4103*x**0*z** 3 +
ch4104*x**0*z** 4 + ch4105*x**0*z** 5 + ch4106*x**0*z** 6 + ch4107*x**0*z** 7 +
ch4108*x**0*z** 8 + ch4109*x**0*z** 9 + ch410a*x**0*z**10 + ch410b*x**0*z**11 +
ch410c*x**0*z**12 + ch410d*x**0*z**13 + ch410e*x**0*z**14 + ch410f*x**0*z**15,
ch4100=L0x20014f88, ch4101=L0x20014ff4, ch4102=L0x20015060, ch4103=L0x200150cc,
ch4104=L0x20015138, ch4105=L0x200151a4, ch4106=L0x20015210, ch4107=L0x2001527c,
ch4108=L0x200152e8, ch4109=L0x20015354, ch410a=L0x200153c0, ch410b=L0x2001542c,
ch410c=L0x20015498, ch410d=L0x20015504, ch410e=L0x20015570, ch410f=L0x200155dc,
CH411**2 =
ch4110*x**1*z** 0 + ch4111*x**1*z** 1 + ch4112*x**1*z** 2 + ch4113*x**1*z** 3 +
ch4114*x**1*z** 4 + ch4115*x**1*z** 5 + ch4116*x**1*z** 6 + ch4117*x**1*z** 7 +
ch4118*x**1*z** 8 + ch4119*x**1*z** 9 + ch411a*x**1*z**10 + ch411b*x**1*z**11 +
ch411c*x**1*z**12 + ch411d*x**1*z**13 + ch411e*x**1*z**14 + ch411f*x**1*z**15,
ch4110=L0x20014f8c, ch4111=L0x20014ff8, ch4112=L0x20015064, ch4113=L0x200150d0,
ch4114=L0x2001513c, ch4115=L0x200151a8, ch4116=L0x20015214, ch4117=L0x20015280,
ch4118=L0x200152ec, ch4119=L0x20015358, ch411a=L0x200153c4, ch411b=L0x20015430,
ch411c=L0x2001549c, ch411d=L0x20015508, ch411e=L0x20015574, ch411f=L0x200155e0,
CH412**2 =
ch4120*x**2*z** 0 + ch4121*x**2*z** 1 + ch4122*x**2*z** 2 + ch4123*x**2*z** 3 +
ch4124*x**2*z** 4 + ch4125*x**2*z** 5 + ch4126*x**2*z** 6 + ch4127*x**2*z** 7 +
ch4128*x**2*z** 8 + ch4129*x**2*z** 9 + ch412a*x**2*z**10 + ch412b*x**2*z**11 +
ch412c*x**2*z**12 + ch412d*x**2*z**13 + ch412e*x**2*z**14 + ch412f*x**2*z**15,
ch4120=L0x20014f90, ch4121=L0x20014ffc, ch4122=L0x20015068, ch4123=L0x200150d4,
ch4124=L0x20015140, ch4125=L0x200151ac, ch4126=L0x20015218, ch4127=L0x20015284,
ch4128=L0x200152f0, ch4129=L0x2001535c, ch412a=L0x200153c8, ch412b=L0x20015434,
ch412c=L0x200154a0, ch412d=L0x2001550c, ch412e=L0x20015578, ch412f=L0x200155e4
] && true;


ghost CH510@sint32, CH511@sint32, CH512@sint32,
      ch5100@sint32, ch5101@sint32, ch5102@sint32, ch5103@sint32,
      ch5104@sint32, ch5105@sint32, ch5106@sint32, ch5107@sint32,
      ch5108@sint32, ch5109@sint32, ch510a@sint32, ch510b@sint32,
      ch510c@sint32, ch510d@sint32, ch510e@sint32, ch510f@sint32,
      ch5110@sint32, ch5111@sint32, ch5112@sint32, ch5113@sint32,
      ch5114@sint32, ch5115@sint32, ch5116@sint32, ch5117@sint32,
      ch5118@sint32, ch5119@sint32, ch511a@sint32, ch511b@sint32,
      ch511c@sint32, ch511d@sint32, ch511e@sint32, ch511f@sint32,
      ch5120@sint32, ch5121@sint32, ch5122@sint32, ch5123@sint32,
      ch5124@sint32, ch5125@sint32, ch5126@sint32, ch5127@sint32,
      ch5128@sint32, ch5129@sint32, ch512a@sint32, ch512b@sint32,
      ch512c@sint32, ch512d@sint32, ch512e@sint32, ch512f@sint32: and [
CH510**2 =
ch5100*x**0*z** 0 + ch5101*x**0*z** 1 + ch5102*x**0*z** 2 + ch5103*x**0*z** 3 +
ch5104*x**0*z** 4 + ch5105*x**0*z** 5 + ch5106*x**0*z** 6 + ch5107*x**0*z** 7 +
ch5108*x**0*z** 8 + ch5109*x**0*z** 9 + ch510a*x**0*z**10 + ch510b*x**0*z**11 +
ch510c*x**0*z**12 + ch510d*x**0*z**13 + ch510e*x**0*z**14 + ch510f*x**0*z**15,
ch5100=L0x20014f94, ch5101=L0x20015000, ch5102=L0x2001506c, ch5103=L0x200150d8,
ch5104=L0x20015144, ch5105=L0x200151b0, ch5106=L0x2001521c, ch5107=L0x20015288,
ch5108=L0x200152f4, ch5109=L0x20015360, ch510a=L0x200153cc, ch510b=L0x20015438,
ch510c=L0x200154a4, ch510d=L0x20015510, ch510e=L0x2001557c, ch510f=L0x200155e8,
CH511**2 =
ch5110*x**1*z** 0 + ch5111*x**1*z** 1 + ch5112*x**1*z** 2 + ch5113*x**1*z** 3 +
ch5114*x**1*z** 4 + ch5115*x**1*z** 5 + ch5116*x**1*z** 6 + ch5117*x**1*z** 7 +
ch5118*x**1*z** 8 + ch5119*x**1*z** 9 + ch511a*x**1*z**10 + ch511b*x**1*z**11 +
ch511c*x**1*z**12 + ch511d*x**1*z**13 + ch511e*x**1*z**14 + ch511f*x**1*z**15,
ch5110=L0x20014f98, ch5111=L0x20015004, ch5112=L0x20015070, ch5113=L0x200150dc,
ch5114=L0x20015148, ch5115=L0x200151b4, ch5116=L0x20015220, ch5117=L0x2001528c,
ch5118=L0x200152f8, ch5119=L0x20015364, ch511a=L0x200153d0, ch511b=L0x2001543c,
ch511c=L0x200154a8, ch511d=L0x20015514, ch511e=L0x20015580, ch511f=L0x200155ec,
CH512**2 =
ch5120*x**2*z** 0 + ch5121*x**2*z** 1 + ch5122*x**2*z** 2 + ch5123*x**2*z** 3 +
ch5124*x**2*z** 4 + ch5125*x**2*z** 5 + ch5126*x**2*z** 6 + ch5127*x**2*z** 7 +
ch5128*x**2*z** 8 + ch5129*x**2*z** 9 + ch512a*x**2*z**10 + ch512b*x**2*z**11 +
ch512c*x**2*z**12 + ch512d*x**2*z**13 + ch512e*x**2*z**14 + ch512f*x**2*z**15,
ch5120=L0x20014f9c, ch5121=L0x20015008, ch5122=L0x20015074, ch5123=L0x200150e0,
ch5124=L0x2001514c, ch5125=L0x200151b8, ch5126=L0x20015224, ch5127=L0x20015290,
ch5128=L0x200152fc, ch5129=L0x20015368, ch512a=L0x200153d4, ch512b=L0x20015440,
ch512c=L0x200154ac, ch512d=L0x20015518, ch512e=L0x20015584, ch512f=L0x200155f0
] && true;


ghost CH610@sint32, CH611@sint32, CH612@sint32,
      ch6100@sint32, ch6101@sint32, ch6102@sint32, ch6103@sint32,
      ch6104@sint32, ch6105@sint32, ch6106@sint32, ch6107@sint32,
      ch6108@sint32, ch6109@sint32, ch610a@sint32, ch610b@sint32,
      ch610c@sint32, ch610d@sint32, ch610e@sint32, ch610f@sint32,
      ch6110@sint32, ch6111@sint32, ch6112@sint32, ch6113@sint32,
      ch6114@sint32, ch6115@sint32, ch6116@sint32, ch6117@sint32,
      ch6118@sint32, ch6119@sint32, ch611a@sint32, ch611b@sint32,
      ch611c@sint32, ch611d@sint32, ch611e@sint32, ch611f@sint32,
      ch6120@sint32, ch6121@sint32, ch6122@sint32, ch6123@sint32,
      ch6124@sint32, ch6125@sint32, ch6126@sint32, ch6127@sint32,
      ch6128@sint32, ch6129@sint32, ch612a@sint32, ch612b@sint32,
      ch612c@sint32, ch612d@sint32, ch612e@sint32, ch612f@sint32: and [
CH610**2 =
ch6100*x**0*z** 0 + ch6101*x**0*z** 1 + ch6102*x**0*z** 2 + ch6103*x**0*z** 3 +
ch6104*x**0*z** 4 + ch6105*x**0*z** 5 + ch6106*x**0*z** 6 + ch6107*x**0*z** 7 +
ch6108*x**0*z** 8 + ch6109*x**0*z** 9 + ch610a*x**0*z**10 + ch610b*x**0*z**11 +
ch610c*x**0*z**12 + ch610d*x**0*z**13 + ch610e*x**0*z**14 + ch610f*x**0*z**15,
ch6100=L0x20014fa0, ch6101=L0x2001500c, ch6102=L0x20015078, ch6103=L0x200150e4,
ch6104=L0x20015150, ch6105=L0x200151bc, ch6106=L0x20015228, ch6107=L0x20015294,
ch6108=L0x20015300, ch6109=L0x2001536c, ch610a=L0x200153d8, ch610b=L0x20015444,
ch610c=L0x200154b0, ch610d=L0x2001551c, ch610e=L0x20015588, ch610f=L0x200155f4,
CH611**2 =
ch6110*x**1*z** 0 + ch6111*x**1*z** 1 + ch6112*x**1*z** 2 + ch6113*x**1*z** 3 +
ch6114*x**1*z** 4 + ch6115*x**1*z** 5 + ch6116*x**1*z** 6 + ch6117*x**1*z** 7 +
ch6118*x**1*z** 8 + ch6119*x**1*z** 9 + ch611a*x**1*z**10 + ch611b*x**1*z**11 +
ch611c*x**1*z**12 + ch611d*x**1*z**13 + ch611e*x**1*z**14 + ch611f*x**1*z**15,
ch6110=L0x20014fa4, ch6111=L0x20015010, ch6112=L0x2001507c, ch6113=L0x200150e8,
ch6114=L0x20015154, ch6115=L0x200151c0, ch6116=L0x2001522c, ch6117=L0x20015298,
ch6118=L0x20015304, ch6119=L0x20015370, ch611a=L0x200153dc, ch611b=L0x20015448,
ch611c=L0x200154b4, ch611d=L0x20015520, ch611e=L0x2001558c, ch611f=L0x200155f8,
CH612**2 =
ch6120*x**2*z** 0 + ch6121*x**2*z** 1 + ch6122*x**2*z** 2 + ch6123*x**2*z** 3 +
ch6124*x**2*z** 4 + ch6125*x**2*z** 5 + ch6126*x**2*z** 6 + ch6127*x**2*z** 7 +
ch6128*x**2*z** 8 + ch6129*x**2*z** 9 + ch612a*x**2*z**10 + ch612b*x**2*z**11 +
ch612c*x**2*z**12 + ch612d*x**2*z**13 + ch612e*x**2*z**14 + ch612f*x**2*z**15,
ch6120=L0x20014fa8, ch6121=L0x20015014, ch6122=L0x20015080, ch6123=L0x200150ec,
ch6124=L0x20015158, ch6125=L0x200151c4, ch6126=L0x20015230, ch6127=L0x2001529c,
ch6128=L0x20015308, ch6129=L0x20015374, ch612a=L0x200153e0, ch612b=L0x2001544c,
ch612c=L0x200154b8, ch612d=L0x20015524, ch612e=L0x20015590, ch612f=L0x200155fc
] && true;


ghost CH710@sint32, CH711@sint32, CH712@sint32,
      ch7100@sint32, ch7101@sint32, ch7102@sint32, ch7103@sint32,
      ch7104@sint32, ch7105@sint32, ch7106@sint32, ch7107@sint32,
      ch7108@sint32, ch7109@sint32, ch710a@sint32, ch710b@sint32,
      ch710c@sint32, ch710d@sint32, ch710e@sint32, ch710f@sint32,
      ch7110@sint32, ch7111@sint32, ch7112@sint32, ch7113@sint32,
      ch7114@sint32, ch7115@sint32, ch7116@sint32, ch7117@sint32,
      ch7118@sint32, ch7119@sint32, ch711a@sint32, ch711b@sint32,
      ch711c@sint32, ch711d@sint32, ch711e@sint32, ch711f@sint32,
      ch7120@sint32, ch7121@sint32, ch7122@sint32, ch7123@sint32,
      ch7124@sint32, ch7125@sint32, ch7126@sint32, ch7127@sint32,
      ch7128@sint32, ch7129@sint32, ch712a@sint32, ch712b@sint32,
      ch712c@sint32, ch712d@sint32, ch712e@sint32, ch712f@sint32: and [
CH710**2 =
ch7100*x**0*z** 0 + ch7101*x**0*z** 1 + ch7102*x**0*z** 2 + ch7103*x**0*z** 3 +
ch7104*x**0*z** 4 + ch7105*x**0*z** 5 + ch7106*x**0*z** 6 + ch7107*x**0*z** 7 +
ch7108*x**0*z** 8 + ch7109*x**0*z** 9 + ch710a*x**0*z**10 + ch710b*x**0*z**11 +
ch710c*x**0*z**12 + ch710d*x**0*z**13 + ch710e*x**0*z**14 + ch710f*x**0*z**15,
ch7100=L0x20014fac, ch7101=L0x20015018, ch7102=L0x20015084, ch7103=L0x200150f0,
ch7104=L0x2001515c, ch7105=L0x200151c8, ch7106=L0x20015234, ch7107=L0x200152a0,
ch7108=L0x2001530c, ch7109=L0x20015378, ch710a=L0x200153e4, ch710b=L0x20015450,
ch710c=L0x200154bc, ch710d=L0x20015528, ch710e=L0x20015594, ch710f=L0x20015600,
CH711**2 =
ch7110*x**1*z** 0 + ch7111*x**1*z** 1 + ch7112*x**1*z** 2 + ch7113*x**1*z** 3 +
ch7114*x**1*z** 4 + ch7115*x**1*z** 5 + ch7116*x**1*z** 6 + ch7117*x**1*z** 7 +
ch7118*x**1*z** 8 + ch7119*x**1*z** 9 + ch711a*x**1*z**10 + ch711b*x**1*z**11 +
ch711c*x**1*z**12 + ch711d*x**1*z**13 + ch711e*x**1*z**14 + ch711f*x**1*z**15,
ch7110=L0x20014fb0, ch7111=L0x2001501c, ch7112=L0x20015088, ch7113=L0x200150f4,
ch7114=L0x20015160, ch7115=L0x200151cc, ch7116=L0x20015238, ch7117=L0x200152a4,
ch7118=L0x20015310, ch7119=L0x2001537c, ch711a=L0x200153e8, ch711b=L0x20015454,
ch711c=L0x200154c0, ch711d=L0x2001552c, ch711e=L0x20015598, ch711f=L0x20015604,
CH712**2 =
ch7120*x**2*z** 0 + ch7121*x**2*z** 1 + ch7122*x**2*z** 2 + ch7123*x**2*z** 3 +
ch7124*x**2*z** 4 + ch7125*x**2*z** 5 + ch7126*x**2*z** 6 + ch7127*x**2*z** 7 +
ch7128*x**2*z** 8 + ch7129*x**2*z** 9 + ch712a*x**2*z**10 + ch712b*x**2*z**11 +
ch712c*x**2*z**12 + ch712d*x**2*z**13 + ch712e*x**2*z**14 + ch712f*x**2*z**15,
ch7120=L0x20014fb4, ch7121=L0x20015020, ch7122=L0x2001508c, ch7123=L0x200150f8,
ch7124=L0x20015164, ch7125=L0x200151d0, ch7126=L0x2001523c, ch7127=L0x200152a8,
ch7128=L0x20015314, ch7129=L0x20015380, ch712a=L0x200153ec, ch712b=L0x20015458,
ch712c=L0x200154c4, ch712d=L0x20015530, ch712e=L0x2001559c, ch712f=L0x20015608
] && true;


ghost CH810@sint32, CH811@sint32, CH812@sint32,
      ch8100@sint32, ch8101@sint32, ch8102@sint32, ch8103@sint32,
      ch8104@sint32, ch8105@sint32, ch8106@sint32, ch8107@sint32,
      ch8108@sint32, ch8109@sint32, ch810a@sint32, ch810b@sint32,
      ch810c@sint32, ch810d@sint32, ch810e@sint32, ch810f@sint32,
      ch8110@sint32, ch8111@sint32, ch8112@sint32, ch8113@sint32,
      ch8114@sint32, ch8115@sint32, ch8116@sint32, ch8117@sint32,
      ch8118@sint32, ch8119@sint32, ch811a@sint32, ch811b@sint32,
      ch811c@sint32, ch811d@sint32, ch811e@sint32, ch811f@sint32,
      ch8120@sint32, ch8121@sint32, ch8122@sint32, ch8123@sint32,
      ch8124@sint32, ch8125@sint32, ch8126@sint32, ch8127@sint32,
      ch8128@sint32, ch8129@sint32, ch812a@sint32, ch812b@sint32,
      ch812c@sint32, ch812d@sint32, ch812e@sint32, ch812f@sint32: and [
CH810**2 =
ch8100*x**0*z** 0 + ch8101*x**0*z** 1 + ch8102*x**0*z** 2 + ch8103*x**0*z** 3 +
ch8104*x**0*z** 4 + ch8105*x**0*z** 5 + ch8106*x**0*z** 6 + ch8107*x**0*z** 7 +
ch8108*x**0*z** 8 + ch8109*x**0*z** 9 + ch810a*x**0*z**10 + ch810b*x**0*z**11 +
ch810c*x**0*z**12 + ch810d*x**0*z**13 + ch810e*x**0*z**14 + ch810f*x**0*z**15,
ch8100=L0x20014fb8, ch8101=L0x20015024, ch8102=L0x20015090, ch8103=L0x200150fc,
ch8104=L0x20015168, ch8105=L0x200151d4, ch8106=L0x20015240, ch8107=L0x200152ac,
ch8108=L0x20015318, ch8109=L0x20015384, ch810a=L0x200153f0, ch810b=L0x2001545c,
ch810c=L0x200154c8, ch810d=L0x20015534, ch810e=L0x200155a0, ch810f=L0x2001560c,
CH811**2 =
ch8110*x**1*z** 0 + ch8111*x**1*z** 1 + ch8112*x**1*z** 2 + ch8113*x**1*z** 3 +
ch8114*x**1*z** 4 + ch8115*x**1*z** 5 + ch8116*x**1*z** 6 + ch8117*x**1*z** 7 +
ch8118*x**1*z** 8 + ch8119*x**1*z** 9 + ch811a*x**1*z**10 + ch811b*x**1*z**11 +
ch811c*x**1*z**12 + ch811d*x**1*z**13 + ch811e*x**1*z**14 + ch811f*x**1*z**15,
ch8110=L0x20014fbc, ch8111=L0x20015028, ch8112=L0x20015094, ch8113=L0x20015100,
ch8114=L0x2001516c, ch8115=L0x200151d8, ch8116=L0x20015244, ch8117=L0x200152b0,
ch8118=L0x2001531c, ch8119=L0x20015388, ch811a=L0x200153f4, ch811b=L0x20015460,
ch811c=L0x200154cc, ch811d=L0x20015538, ch811e=L0x200155a4, ch811f=L0x20015610,
CH812**2 =
ch8120*x**2*z** 0 + ch8121*x**2*z** 1 + ch8122*x**2*z** 2 + ch8123*x**2*z** 3 +
ch8124*x**2*z** 4 + ch8125*x**2*z** 5 + ch8126*x**2*z** 6 + ch8127*x**2*z** 7 +
ch8128*x**2*z** 8 + ch8129*x**2*z** 9 + ch812a*x**2*z**10 + ch812b*x**2*z**11 +
ch812c*x**2*z**12 + ch812d*x**2*z**13 + ch812e*x**2*z**14 + ch812f*x**2*z**15,
ch8120=L0x20014fc0, ch8121=L0x2001502c, ch8122=L0x20015098, ch8123=L0x20015104,
ch8124=L0x20015170, ch8125=L0x200151dc, ch8126=L0x20015248, ch8127=L0x200152b4,
ch8128=L0x20015320, ch8129=L0x2001538c, ch812a=L0x200153f8, ch812b=L0x20015464,
ch812c=L0x200154d0, ch812d=L0x2001553c, ch812e=L0x200155a8, ch812f=L0x20015614
] && true;


ghost CH020@sint32, CH021@sint32, CH022@sint32,
      ch0200@sint32, ch0201@sint32, ch0202@sint32, ch0203@sint32,
      ch0204@sint32, ch0205@sint32, ch0206@sint32, ch0207@sint32,
      ch0208@sint32, ch0209@sint32, ch020a@sint32, ch020b@sint32,
      ch020c@sint32, ch020d@sint32, ch020e@sint32, ch020f@sint32,
      ch0210@sint32, ch0211@sint32, ch0212@sint32, ch0213@sint32,
      ch0214@sint32, ch0215@sint32, ch0216@sint32, ch0217@sint32,
      ch0218@sint32, ch0219@sint32, ch021a@sint32, ch021b@sint32,
      ch021c@sint32, ch021d@sint32, ch021e@sint32, ch021f@sint32,
      ch0220@sint32, ch0221@sint32, ch0222@sint32, ch0223@sint32,
      ch0224@sint32, ch0225@sint32, ch0226@sint32, ch0227@sint32,
      ch0228@sint32, ch0229@sint32, ch022a@sint32, ch022b@sint32,
      ch022c@sint32, ch022d@sint32, ch022e@sint32, ch022f@sint32: and [
CH020**2 =
ch0200*x**0*z** 0 + ch0201*x**0*z** 1 + ch0202*x**0*z** 2 + ch0203*x**0*z** 3 +
ch0204*x**0*z** 4 + ch0205*x**0*z** 5 + ch0206*x**0*z** 6 + ch0207*x**0*z** 7 +
ch0208*x**0*z** 8 + ch0209*x**0*z** 9 + ch020a*x**0*z**10 + ch020b*x**0*z**11 +
ch020c*x**0*z**12 + ch020d*x**0*z**13 + ch020e*x**0*z**14 + ch020f*x**0*z**15,
ch0200=L0x20015618, ch0201=L0x20015684, ch0202=L0x200156f0, ch0203=L0x2001575c,
ch0204=L0x200157c8, ch0205=L0x20015834, ch0206=L0x200158a0, ch0207=L0x2001590c,
ch0208=L0x20015978, ch0209=L0x200159e4, ch020a=L0x20015a50, ch020b=L0x20015abc,
ch020c=L0x20015b28, ch020d=L0x20015b94, ch020e=L0x20015c00, ch020f=L0x20015c6c,
CH021**2 =
ch0210*x**1*z** 0 + ch0211*x**1*z** 1 + ch0212*x**1*z** 2 + ch0213*x**1*z** 3 +
ch0214*x**1*z** 4 + ch0215*x**1*z** 5 + ch0216*x**1*z** 6 + ch0217*x**1*z** 7 +
ch0218*x**1*z** 8 + ch0219*x**1*z** 9 + ch021a*x**1*z**10 + ch021b*x**1*z**11 +
ch021c*x**1*z**12 + ch021d*x**1*z**13 + ch021e*x**1*z**14 + ch021f*x**1*z**15,
ch0210=L0x2001561c, ch0211=L0x20015688, ch0212=L0x200156f4, ch0213=L0x20015760,
ch0214=L0x200157cc, ch0215=L0x20015838, ch0216=L0x200158a4, ch0217=L0x20015910,
ch0218=L0x2001597c, ch0219=L0x200159e8, ch021a=L0x20015a54, ch021b=L0x20015ac0,
ch021c=L0x20015b2c, ch021d=L0x20015b98, ch021e=L0x20015c04, ch021f=L0x20015c70,
CH022**2 =
ch0220*x**2*z** 0 + ch0221*x**2*z** 1 + ch0222*x**2*z** 2 + ch0223*x**2*z** 3 +
ch0224*x**2*z** 4 + ch0225*x**2*z** 5 + ch0226*x**2*z** 6 + ch0227*x**2*z** 7 +
ch0228*x**2*z** 8 + ch0229*x**2*z** 9 + ch022a*x**2*z**10 + ch022b*x**2*z**11 +
ch022c*x**2*z**12 + ch022d*x**2*z**13 + ch022e*x**2*z**14 + ch022f*x**2*z**15,
ch0220=L0x20015620, ch0221=L0x2001568c, ch0222=L0x200156f8, ch0223=L0x20015764,
ch0224=L0x200157d0, ch0225=L0x2001583c, ch0226=L0x200158a8, ch0227=L0x20015914,
ch0228=L0x20015980, ch0229=L0x200159ec, ch022a=L0x20015a58, ch022b=L0x20015ac4,
ch022c=L0x20015b30, ch022d=L0x20015b9c, ch022e=L0x20015c08, ch022f=L0x20015c74
] && true;


ghost CH120@sint32, CH121@sint32, CH122@sint32,
      ch1200@sint32, ch1201@sint32, ch1202@sint32, ch1203@sint32,
      ch1204@sint32, ch1205@sint32, ch1206@sint32, ch1207@sint32,
      ch1208@sint32, ch1209@sint32, ch120a@sint32, ch120b@sint32,
      ch120c@sint32, ch120d@sint32, ch120e@sint32, ch120f@sint32,
      ch1210@sint32, ch1211@sint32, ch1212@sint32, ch1213@sint32,
      ch1214@sint32, ch1215@sint32, ch1216@sint32, ch1217@sint32,
      ch1218@sint32, ch1219@sint32, ch121a@sint32, ch121b@sint32,
      ch121c@sint32, ch121d@sint32, ch121e@sint32, ch121f@sint32,
      ch1220@sint32, ch1221@sint32, ch1222@sint32, ch1223@sint32,
      ch1224@sint32, ch1225@sint32, ch1226@sint32, ch1227@sint32,
      ch1228@sint32, ch1229@sint32, ch122a@sint32, ch122b@sint32,
      ch122c@sint32, ch122d@sint32, ch122e@sint32, ch122f@sint32: and [
CH120**2 =
ch1200*x**0*z** 0 + ch1201*x**0*z** 1 + ch1202*x**0*z** 2 + ch1203*x**0*z** 3 +
ch1204*x**0*z** 4 + ch1205*x**0*z** 5 + ch1206*x**0*z** 6 + ch1207*x**0*z** 7 +
ch1208*x**0*z** 8 + ch1209*x**0*z** 9 + ch120a*x**0*z**10 + ch120b*x**0*z**11 +
ch120c*x**0*z**12 + ch120d*x**0*z**13 + ch120e*x**0*z**14 + ch120f*x**0*z**15,
ch1200=L0x20015624, ch1201=L0x20015690, ch1202=L0x200156fc, ch1203=L0x20015768,
ch1204=L0x200157d4, ch1205=L0x20015840, ch1206=L0x200158ac, ch1207=L0x20015918,
ch1208=L0x20015984, ch1209=L0x200159f0, ch120a=L0x20015a5c, ch120b=L0x20015ac8,
ch120c=L0x20015b34, ch120d=L0x20015ba0, ch120e=L0x20015c0c, ch120f=L0x20015c78,
CH121**2 =
ch1210*x**1*z** 0 + ch1211*x**1*z** 1 + ch1212*x**1*z** 2 + ch1213*x**1*z** 3 +
ch1214*x**1*z** 4 + ch1215*x**1*z** 5 + ch1216*x**1*z** 6 + ch1217*x**1*z** 7 +
ch1218*x**1*z** 8 + ch1219*x**1*z** 9 + ch121a*x**1*z**10 + ch121b*x**1*z**11 +
ch121c*x**1*z**12 + ch121d*x**1*z**13 + ch121e*x**1*z**14 + ch121f*x**1*z**15,
ch1210=L0x20015628, ch1211=L0x20015694, ch1212=L0x20015700, ch1213=L0x2001576c,
ch1214=L0x200157d8, ch1215=L0x20015844, ch1216=L0x200158b0, ch1217=L0x2001591c,
ch1218=L0x20015988, ch1219=L0x200159f4, ch121a=L0x20015a60, ch121b=L0x20015acc,
ch121c=L0x20015b38, ch121d=L0x20015ba4, ch121e=L0x20015c10, ch121f=L0x20015c7c,
CH122**2 =
ch1220*x**2*z** 0 + ch1221*x**2*z** 1 + ch1222*x**2*z** 2 + ch1223*x**2*z** 3 +
ch1224*x**2*z** 4 + ch1225*x**2*z** 5 + ch1226*x**2*z** 6 + ch1227*x**2*z** 7 +
ch1228*x**2*z** 8 + ch1229*x**2*z** 9 + ch122a*x**2*z**10 + ch122b*x**2*z**11 +
ch122c*x**2*z**12 + ch122d*x**2*z**13 + ch122e*x**2*z**14 + ch122f*x**2*z**15,
ch1220=L0x2001562c, ch1221=L0x20015698, ch1222=L0x20015704, ch1223=L0x20015770,
ch1224=L0x200157dc, ch1225=L0x20015848, ch1226=L0x200158b4, ch1227=L0x20015920,
ch1228=L0x2001598c, ch1229=L0x200159f8, ch122a=L0x20015a64, ch122b=L0x20015ad0,
ch122c=L0x20015b3c, ch122d=L0x20015ba8, ch122e=L0x20015c14, ch122f=L0x20015c80
] && true;


ghost CH220@sint32, CH221@sint32, CH222@sint32,
      ch2200@sint32, ch2201@sint32, ch2202@sint32, ch2203@sint32,
      ch2204@sint32, ch2205@sint32, ch2206@sint32, ch2207@sint32,
      ch2208@sint32, ch2209@sint32, ch220a@sint32, ch220b@sint32,
      ch220c@sint32, ch220d@sint32, ch220e@sint32, ch220f@sint32,
      ch2210@sint32, ch2211@sint32, ch2212@sint32, ch2213@sint32,
      ch2214@sint32, ch2215@sint32, ch2216@sint32, ch2217@sint32,
      ch2218@sint32, ch2219@sint32, ch221a@sint32, ch221b@sint32,
      ch221c@sint32, ch221d@sint32, ch221e@sint32, ch221f@sint32,
      ch2220@sint32, ch2221@sint32, ch2222@sint32, ch2223@sint32,
      ch2224@sint32, ch2225@sint32, ch2226@sint32, ch2227@sint32,
      ch2228@sint32, ch2229@sint32, ch222a@sint32, ch222b@sint32,
      ch222c@sint32, ch222d@sint32, ch222e@sint32, ch222f@sint32: and [
CH220**2 =
ch2200*x**0*z** 0 + ch2201*x**0*z** 1 + ch2202*x**0*z** 2 + ch2203*x**0*z** 3 +
ch2204*x**0*z** 4 + ch2205*x**0*z** 5 + ch2206*x**0*z** 6 + ch2207*x**0*z** 7 +
ch2208*x**0*z** 8 + ch2209*x**0*z** 9 + ch220a*x**0*z**10 + ch220b*x**0*z**11 +
ch220c*x**0*z**12 + ch220d*x**0*z**13 + ch220e*x**0*z**14 + ch220f*x**0*z**15,
ch2200=L0x20015630, ch2201=L0x2001569c, ch2202=L0x20015708, ch2203=L0x20015774,
ch2204=L0x200157e0, ch2205=L0x2001584c, ch2206=L0x200158b8, ch2207=L0x20015924,
ch2208=L0x20015990, ch2209=L0x200159fc, ch220a=L0x20015a68, ch220b=L0x20015ad4,
ch220c=L0x20015b40, ch220d=L0x20015bac, ch220e=L0x20015c18, ch220f=L0x20015c84,
CH221**2 =
ch2210*x**1*z** 0 + ch2211*x**1*z** 1 + ch2212*x**1*z** 2 + ch2213*x**1*z** 3 +
ch2214*x**1*z** 4 + ch2215*x**1*z** 5 + ch2216*x**1*z** 6 + ch2217*x**1*z** 7 +
ch2218*x**1*z** 8 + ch2219*x**1*z** 9 + ch221a*x**1*z**10 + ch221b*x**1*z**11 +
ch221c*x**1*z**12 + ch221d*x**1*z**13 + ch221e*x**1*z**14 + ch221f*x**1*z**15,
ch2210=L0x20015634, ch2211=L0x200156a0, ch2212=L0x2001570c, ch2213=L0x20015778,
ch2214=L0x200157e4, ch2215=L0x20015850, ch2216=L0x200158bc, ch2217=L0x20015928,
ch2218=L0x20015994, ch2219=L0x20015a00, ch221a=L0x20015a6c, ch221b=L0x20015ad8,
ch221c=L0x20015b44, ch221d=L0x20015bb0, ch221e=L0x20015c1c, ch221f=L0x20015c88,
CH222**2 =
ch2220*x**2*z** 0 + ch2221*x**2*z** 1 + ch2222*x**2*z** 2 + ch2223*x**2*z** 3 +
ch2224*x**2*z** 4 + ch2225*x**2*z** 5 + ch2226*x**2*z** 6 + ch2227*x**2*z** 7 +
ch2228*x**2*z** 8 + ch2229*x**2*z** 9 + ch222a*x**2*z**10 + ch222b*x**2*z**11 +
ch222c*x**2*z**12 + ch222d*x**2*z**13 + ch222e*x**2*z**14 + ch222f*x**2*z**15,
ch2220=L0x20015638, ch2221=L0x200156a4, ch2222=L0x20015710, ch2223=L0x2001577c,
ch2224=L0x200157e8, ch2225=L0x20015854, ch2226=L0x200158c0, ch2227=L0x2001592c,
ch2228=L0x20015998, ch2229=L0x20015a04, ch222a=L0x20015a70, ch222b=L0x20015adc,
ch222c=L0x20015b48, ch222d=L0x20015bb4, ch222e=L0x20015c20, ch222f=L0x20015c8c
] && true;


ghost CH320@sint32, CH321@sint32, CH322@sint32,
      ch3200@sint32, ch3201@sint32, ch3202@sint32, ch3203@sint32,
      ch3204@sint32, ch3205@sint32, ch3206@sint32, ch3207@sint32,
      ch3208@sint32, ch3209@sint32, ch320a@sint32, ch320b@sint32,
      ch320c@sint32, ch320d@sint32, ch320e@sint32, ch320f@sint32,
      ch3210@sint32, ch3211@sint32, ch3212@sint32, ch3213@sint32,
      ch3214@sint32, ch3215@sint32, ch3216@sint32, ch3217@sint32,
      ch3218@sint32, ch3219@sint32, ch321a@sint32, ch321b@sint32,
      ch321c@sint32, ch321d@sint32, ch321e@sint32, ch321f@sint32,
      ch3220@sint32, ch3221@sint32, ch3222@sint32, ch3223@sint32,
      ch3224@sint32, ch3225@sint32, ch3226@sint32, ch3227@sint32,
      ch3228@sint32, ch3229@sint32, ch322a@sint32, ch322b@sint32,
      ch322c@sint32, ch322d@sint32, ch322e@sint32, ch322f@sint32: and [
CH320**2 =
ch3200*x**0*z** 0 + ch3201*x**0*z** 1 + ch3202*x**0*z** 2 + ch3203*x**0*z** 3 +
ch3204*x**0*z** 4 + ch3205*x**0*z** 5 + ch3206*x**0*z** 6 + ch3207*x**0*z** 7 +
ch3208*x**0*z** 8 + ch3209*x**0*z** 9 + ch320a*x**0*z**10 + ch320b*x**0*z**11 +
ch320c*x**0*z**12 + ch320d*x**0*z**13 + ch320e*x**0*z**14 + ch320f*x**0*z**15,
ch3200=L0x2001563c, ch3201=L0x200156a8, ch3202=L0x20015714, ch3203=L0x20015780,
ch3204=L0x200157ec, ch3205=L0x20015858, ch3206=L0x200158c4, ch3207=L0x20015930,
ch3208=L0x2001599c, ch3209=L0x20015a08, ch320a=L0x20015a74, ch320b=L0x20015ae0,
ch320c=L0x20015b4c, ch320d=L0x20015bb8, ch320e=L0x20015c24, ch320f=L0x20015c90,
CH321**2 =
ch3210*x**1*z** 0 + ch3211*x**1*z** 1 + ch3212*x**1*z** 2 + ch3213*x**1*z** 3 +
ch3214*x**1*z** 4 + ch3215*x**1*z** 5 + ch3216*x**1*z** 6 + ch3217*x**1*z** 7 +
ch3218*x**1*z** 8 + ch3219*x**1*z** 9 + ch321a*x**1*z**10 + ch321b*x**1*z**11 +
ch321c*x**1*z**12 + ch321d*x**1*z**13 + ch321e*x**1*z**14 + ch321f*x**1*z**15,
ch3210=L0x20015640, ch3211=L0x200156ac, ch3212=L0x20015718, ch3213=L0x20015784,
ch3214=L0x200157f0, ch3215=L0x2001585c, ch3216=L0x200158c8, ch3217=L0x20015934,
ch3218=L0x200159a0, ch3219=L0x20015a0c, ch321a=L0x20015a78, ch321b=L0x20015ae4,
ch321c=L0x20015b50, ch321d=L0x20015bbc, ch321e=L0x20015c28, ch321f=L0x20015c94,
CH322**2 =
ch3220*x**2*z** 0 + ch3221*x**2*z** 1 + ch3222*x**2*z** 2 + ch3223*x**2*z** 3 +
ch3224*x**2*z** 4 + ch3225*x**2*z** 5 + ch3226*x**2*z** 6 + ch3227*x**2*z** 7 +
ch3228*x**2*z** 8 + ch3229*x**2*z** 9 + ch322a*x**2*z**10 + ch322b*x**2*z**11 +
ch322c*x**2*z**12 + ch322d*x**2*z**13 + ch322e*x**2*z**14 + ch322f*x**2*z**15,
ch3220=L0x20015644, ch3221=L0x200156b0, ch3222=L0x2001571c, ch3223=L0x20015788,
ch3224=L0x200157f4, ch3225=L0x20015860, ch3226=L0x200158cc, ch3227=L0x20015938,
ch3228=L0x200159a4, ch3229=L0x20015a10, ch322a=L0x20015a7c, ch322b=L0x20015ae8,
ch322c=L0x20015b54, ch322d=L0x20015bc0, ch322e=L0x20015c2c, ch322f=L0x20015c98
] && true;


ghost CH420@sint32, CH421@sint32, CH422@sint32,
      ch4200@sint32, ch4201@sint32, ch4202@sint32, ch4203@sint32,
      ch4204@sint32, ch4205@sint32, ch4206@sint32, ch4207@sint32,
      ch4208@sint32, ch4209@sint32, ch420a@sint32, ch420b@sint32,
      ch420c@sint32, ch420d@sint32, ch420e@sint32, ch420f@sint32,
      ch4210@sint32, ch4211@sint32, ch4212@sint32, ch4213@sint32,
      ch4214@sint32, ch4215@sint32, ch4216@sint32, ch4217@sint32,
      ch4218@sint32, ch4219@sint32, ch421a@sint32, ch421b@sint32,
      ch421c@sint32, ch421d@sint32, ch421e@sint32, ch421f@sint32,
      ch4220@sint32, ch4221@sint32, ch4222@sint32, ch4223@sint32,
      ch4224@sint32, ch4225@sint32, ch4226@sint32, ch4227@sint32,
      ch4228@sint32, ch4229@sint32, ch422a@sint32, ch422b@sint32,
      ch422c@sint32, ch422d@sint32, ch422e@sint32, ch422f@sint32: and [
CH420**2 =
ch4200*x**0*z** 0 + ch4201*x**0*z** 1 + ch4202*x**0*z** 2 + ch4203*x**0*z** 3 +
ch4204*x**0*z** 4 + ch4205*x**0*z** 5 + ch4206*x**0*z** 6 + ch4207*x**0*z** 7 +
ch4208*x**0*z** 8 + ch4209*x**0*z** 9 + ch420a*x**0*z**10 + ch420b*x**0*z**11 +
ch420c*x**0*z**12 + ch420d*x**0*z**13 + ch420e*x**0*z**14 + ch420f*x**0*z**15,
ch4200=L0x20015648, ch4201=L0x200156b4, ch4202=L0x20015720, ch4203=L0x2001578c,
ch4204=L0x200157f8, ch4205=L0x20015864, ch4206=L0x200158d0, ch4207=L0x2001593c,
ch4208=L0x200159a8, ch4209=L0x20015a14, ch420a=L0x20015a80, ch420b=L0x20015aec,
ch420c=L0x20015b58, ch420d=L0x20015bc4, ch420e=L0x20015c30, ch420f=L0x20015c9c,
CH421**2 =
ch4210*x**1*z** 0 + ch4211*x**1*z** 1 + ch4212*x**1*z** 2 + ch4213*x**1*z** 3 +
ch4214*x**1*z** 4 + ch4215*x**1*z** 5 + ch4216*x**1*z** 6 + ch4217*x**1*z** 7 +
ch4218*x**1*z** 8 + ch4219*x**1*z** 9 + ch421a*x**1*z**10 + ch421b*x**1*z**11 +
ch421c*x**1*z**12 + ch421d*x**1*z**13 + ch421e*x**1*z**14 + ch421f*x**1*z**15,
ch4210=L0x2001564c, ch4211=L0x200156b8, ch4212=L0x20015724, ch4213=L0x20015790,
ch4214=L0x200157fc, ch4215=L0x20015868, ch4216=L0x200158d4, ch4217=L0x20015940,
ch4218=L0x200159ac, ch4219=L0x20015a18, ch421a=L0x20015a84, ch421b=L0x20015af0,
ch421c=L0x20015b5c, ch421d=L0x20015bc8, ch421e=L0x20015c34, ch421f=L0x20015ca0,
CH422**2 =
ch4220*x**2*z** 0 + ch4221*x**2*z** 1 + ch4222*x**2*z** 2 + ch4223*x**2*z** 3 +
ch4224*x**2*z** 4 + ch4225*x**2*z** 5 + ch4226*x**2*z** 6 + ch4227*x**2*z** 7 +
ch4228*x**2*z** 8 + ch4229*x**2*z** 9 + ch422a*x**2*z**10 + ch422b*x**2*z**11 +
ch422c*x**2*z**12 + ch422d*x**2*z**13 + ch422e*x**2*z**14 + ch422f*x**2*z**15,
ch4220=L0x20015650, ch4221=L0x200156bc, ch4222=L0x20015728, ch4223=L0x20015794,
ch4224=L0x20015800, ch4225=L0x2001586c, ch4226=L0x200158d8, ch4227=L0x20015944,
ch4228=L0x200159b0, ch4229=L0x20015a1c, ch422a=L0x20015a88, ch422b=L0x20015af4,
ch422c=L0x20015b60, ch422d=L0x20015bcc, ch422e=L0x20015c38, ch422f=L0x20015ca4
] && true;


ghost CH520@sint32, CH521@sint32, CH522@sint32,
      ch5200@sint32, ch5201@sint32, ch5202@sint32, ch5203@sint32,
      ch5204@sint32, ch5205@sint32, ch5206@sint32, ch5207@sint32,
      ch5208@sint32, ch5209@sint32, ch520a@sint32, ch520b@sint32,
      ch520c@sint32, ch520d@sint32, ch520e@sint32, ch520f@sint32,
      ch5210@sint32, ch5211@sint32, ch5212@sint32, ch5213@sint32,
      ch5214@sint32, ch5215@sint32, ch5216@sint32, ch5217@sint32,
      ch5218@sint32, ch5219@sint32, ch521a@sint32, ch521b@sint32,
      ch521c@sint32, ch521d@sint32, ch521e@sint32, ch521f@sint32,
      ch5220@sint32, ch5221@sint32, ch5222@sint32, ch5223@sint32,
      ch5224@sint32, ch5225@sint32, ch5226@sint32, ch5227@sint32,
      ch5228@sint32, ch5229@sint32, ch522a@sint32, ch522b@sint32,
      ch522c@sint32, ch522d@sint32, ch522e@sint32, ch522f@sint32: and [
CH520**2 =
ch5200*x**0*z** 0 + ch5201*x**0*z** 1 + ch5202*x**0*z** 2 + ch5203*x**0*z** 3 +
ch5204*x**0*z** 4 + ch5205*x**0*z** 5 + ch5206*x**0*z** 6 + ch5207*x**0*z** 7 +
ch5208*x**0*z** 8 + ch5209*x**0*z** 9 + ch520a*x**0*z**10 + ch520b*x**0*z**11 +
ch520c*x**0*z**12 + ch520d*x**0*z**13 + ch520e*x**0*z**14 + ch520f*x**0*z**15,
ch5200=L0x20015654, ch5201=L0x200156c0, ch5202=L0x2001572c, ch5203=L0x20015798,
ch5204=L0x20015804, ch5205=L0x20015870, ch5206=L0x200158dc, ch5207=L0x20015948,
ch5208=L0x200159b4, ch5209=L0x20015a20, ch520a=L0x20015a8c, ch520b=L0x20015af8,
ch520c=L0x20015b64, ch520d=L0x20015bd0, ch520e=L0x20015c3c, ch520f=L0x20015ca8,
CH521**2 =
ch5210*x**1*z** 0 + ch5211*x**1*z** 1 + ch5212*x**1*z** 2 + ch5213*x**1*z** 3 +
ch5214*x**1*z** 4 + ch5215*x**1*z** 5 + ch5216*x**1*z** 6 + ch5217*x**1*z** 7 +
ch5218*x**1*z** 8 + ch5219*x**1*z** 9 + ch521a*x**1*z**10 + ch521b*x**1*z**11 +
ch521c*x**1*z**12 + ch521d*x**1*z**13 + ch521e*x**1*z**14 + ch521f*x**1*z**15,
ch5210=L0x20015658, ch5211=L0x200156c4, ch5212=L0x20015730, ch5213=L0x2001579c,
ch5214=L0x20015808, ch5215=L0x20015874, ch5216=L0x200158e0, ch5217=L0x2001594c,
ch5218=L0x200159b8, ch5219=L0x20015a24, ch521a=L0x20015a90, ch521b=L0x20015afc,
ch521c=L0x20015b68, ch521d=L0x20015bd4, ch521e=L0x20015c40, ch521f=L0x20015cac,
CH522**2 =
ch5220*x**2*z** 0 + ch5221*x**2*z** 1 + ch5222*x**2*z** 2 + ch5223*x**2*z** 3 +
ch5224*x**2*z** 4 + ch5225*x**2*z** 5 + ch5226*x**2*z** 6 + ch5227*x**2*z** 7 +
ch5228*x**2*z** 8 + ch5229*x**2*z** 9 + ch522a*x**2*z**10 + ch522b*x**2*z**11 +
ch522c*x**2*z**12 + ch522d*x**2*z**13 + ch522e*x**2*z**14 + ch522f*x**2*z**15,
ch5220=L0x2001565c, ch5221=L0x200156c8, ch5222=L0x20015734, ch5223=L0x200157a0,
ch5224=L0x2001580c, ch5225=L0x20015878, ch5226=L0x200158e4, ch5227=L0x20015950,
ch5228=L0x200159bc, ch5229=L0x20015a28, ch522a=L0x20015a94, ch522b=L0x20015b00,
ch522c=L0x20015b6c, ch522d=L0x20015bd8, ch522e=L0x20015c44, ch522f=L0x20015cb0
] && true;


ghost CH620@sint32, CH621@sint32, CH622@sint32,
      ch6200@sint32, ch6201@sint32, ch6202@sint32, ch6203@sint32,
      ch6204@sint32, ch6205@sint32, ch6206@sint32, ch6207@sint32,
      ch6208@sint32, ch6209@sint32, ch620a@sint32, ch620b@sint32,
      ch620c@sint32, ch620d@sint32, ch620e@sint32, ch620f@sint32,
      ch6210@sint32, ch6211@sint32, ch6212@sint32, ch6213@sint32,
      ch6214@sint32, ch6215@sint32, ch6216@sint32, ch6217@sint32,
      ch6218@sint32, ch6219@sint32, ch621a@sint32, ch621b@sint32,
      ch621c@sint32, ch621d@sint32, ch621e@sint32, ch621f@sint32,
      ch6220@sint32, ch6221@sint32, ch6222@sint32, ch6223@sint32,
      ch6224@sint32, ch6225@sint32, ch6226@sint32, ch6227@sint32,
      ch6228@sint32, ch6229@sint32, ch622a@sint32, ch622b@sint32,
      ch622c@sint32, ch622d@sint32, ch622e@sint32, ch622f@sint32: and [
CH620**2 =
ch6200*x**0*z** 0 + ch6201*x**0*z** 1 + ch6202*x**0*z** 2 + ch6203*x**0*z** 3 +
ch6204*x**0*z** 4 + ch6205*x**0*z** 5 + ch6206*x**0*z** 6 + ch6207*x**0*z** 7 +
ch6208*x**0*z** 8 + ch6209*x**0*z** 9 + ch620a*x**0*z**10 + ch620b*x**0*z**11 +
ch620c*x**0*z**12 + ch620d*x**0*z**13 + ch620e*x**0*z**14 + ch620f*x**0*z**15,
ch6200=L0x20015660, ch6201=L0x200156cc, ch6202=L0x20015738, ch6203=L0x200157a4,
ch6204=L0x20015810, ch6205=L0x2001587c, ch6206=L0x200158e8, ch6207=L0x20015954,
ch6208=L0x200159c0, ch6209=L0x20015a2c, ch620a=L0x20015a98, ch620b=L0x20015b04,
ch620c=L0x20015b70, ch620d=L0x20015bdc, ch620e=L0x20015c48, ch620f=L0x20015cb4,
CH621**2 =
ch6210*x**1*z** 0 + ch6211*x**1*z** 1 + ch6212*x**1*z** 2 + ch6213*x**1*z** 3 +
ch6214*x**1*z** 4 + ch6215*x**1*z** 5 + ch6216*x**1*z** 6 + ch6217*x**1*z** 7 +
ch6218*x**1*z** 8 + ch6219*x**1*z** 9 + ch621a*x**1*z**10 + ch621b*x**1*z**11 +
ch621c*x**1*z**12 + ch621d*x**1*z**13 + ch621e*x**1*z**14 + ch621f*x**1*z**15,
ch6210=L0x20015664, ch6211=L0x200156d0, ch6212=L0x2001573c, ch6213=L0x200157a8,
ch6214=L0x20015814, ch6215=L0x20015880, ch6216=L0x200158ec, ch6217=L0x20015958,
ch6218=L0x200159c4, ch6219=L0x20015a30, ch621a=L0x20015a9c, ch621b=L0x20015b08,
ch621c=L0x20015b74, ch621d=L0x20015be0, ch621e=L0x20015c4c, ch621f=L0x20015cb8,
CH622**2 =
ch6220*x**2*z** 0 + ch6221*x**2*z** 1 + ch6222*x**2*z** 2 + ch6223*x**2*z** 3 +
ch6224*x**2*z** 4 + ch6225*x**2*z** 5 + ch6226*x**2*z** 6 + ch6227*x**2*z** 7 +
ch6228*x**2*z** 8 + ch6229*x**2*z** 9 + ch622a*x**2*z**10 + ch622b*x**2*z**11 +
ch622c*x**2*z**12 + ch622d*x**2*z**13 + ch622e*x**2*z**14 + ch622f*x**2*z**15,
ch6220=L0x20015668, ch6221=L0x200156d4, ch6222=L0x20015740, ch6223=L0x200157ac,
ch6224=L0x20015818, ch6225=L0x20015884, ch6226=L0x200158f0, ch6227=L0x2001595c,
ch6228=L0x200159c8, ch6229=L0x20015a34, ch622a=L0x20015aa0, ch622b=L0x20015b0c,
ch622c=L0x20015b78, ch622d=L0x20015be4, ch622e=L0x20015c50, ch622f=L0x20015cbc
] && true;


ghost CH720@sint32, CH721@sint32, CH722@sint32,
      ch7200@sint32, ch7201@sint32, ch7202@sint32, ch7203@sint32,
      ch7204@sint32, ch7205@sint32, ch7206@sint32, ch7207@sint32,
      ch7208@sint32, ch7209@sint32, ch720a@sint32, ch720b@sint32,
      ch720c@sint32, ch720d@sint32, ch720e@sint32, ch720f@sint32,
      ch7210@sint32, ch7211@sint32, ch7212@sint32, ch7213@sint32,
      ch7214@sint32, ch7215@sint32, ch7216@sint32, ch7217@sint32,
      ch7218@sint32, ch7219@sint32, ch721a@sint32, ch721b@sint32,
      ch721c@sint32, ch721d@sint32, ch721e@sint32, ch721f@sint32,
      ch7220@sint32, ch7221@sint32, ch7222@sint32, ch7223@sint32,
      ch7224@sint32, ch7225@sint32, ch7226@sint32, ch7227@sint32,
      ch7228@sint32, ch7229@sint32, ch722a@sint32, ch722b@sint32,
      ch722c@sint32, ch722d@sint32, ch722e@sint32, ch722f@sint32: and [
CH720**2 =
ch7200*x**0*z** 0 + ch7201*x**0*z** 1 + ch7202*x**0*z** 2 + ch7203*x**0*z** 3 +
ch7204*x**0*z** 4 + ch7205*x**0*z** 5 + ch7206*x**0*z** 6 + ch7207*x**0*z** 7 +
ch7208*x**0*z** 8 + ch7209*x**0*z** 9 + ch720a*x**0*z**10 + ch720b*x**0*z**11 +
ch720c*x**0*z**12 + ch720d*x**0*z**13 + ch720e*x**0*z**14 + ch720f*x**0*z**15,
ch7200=L0x2001566c, ch7201=L0x200156d8, ch7202=L0x20015744, ch7203=L0x200157b0,
ch7204=L0x2001581c, ch7205=L0x20015888, ch7206=L0x200158f4, ch7207=L0x20015960,
ch7208=L0x200159cc, ch7209=L0x20015a38, ch720a=L0x20015aa4, ch720b=L0x20015b10,
ch720c=L0x20015b7c, ch720d=L0x20015be8, ch720e=L0x20015c54, ch720f=L0x20015cc0,
CH721**2 =
ch7210*x**1*z** 0 + ch7211*x**1*z** 1 + ch7212*x**1*z** 2 + ch7213*x**1*z** 3 +
ch7214*x**1*z** 4 + ch7215*x**1*z** 5 + ch7216*x**1*z** 6 + ch7217*x**1*z** 7 +
ch7218*x**1*z** 8 + ch7219*x**1*z** 9 + ch721a*x**1*z**10 + ch721b*x**1*z**11 +
ch721c*x**1*z**12 + ch721d*x**1*z**13 + ch721e*x**1*z**14 + ch721f*x**1*z**15,
ch7210=L0x20015670, ch7211=L0x200156dc, ch7212=L0x20015748, ch7213=L0x200157b4,
ch7214=L0x20015820, ch7215=L0x2001588c, ch7216=L0x200158f8, ch7217=L0x20015964,
ch7218=L0x200159d0, ch7219=L0x20015a3c, ch721a=L0x20015aa8, ch721b=L0x20015b14,
ch721c=L0x20015b80, ch721d=L0x20015bec, ch721e=L0x20015c58, ch721f=L0x20015cc4,
CH722**2 =
ch7220*x**2*z** 0 + ch7221*x**2*z** 1 + ch7222*x**2*z** 2 + ch7223*x**2*z** 3 +
ch7224*x**2*z** 4 + ch7225*x**2*z** 5 + ch7226*x**2*z** 6 + ch7227*x**2*z** 7 +
ch7228*x**2*z** 8 + ch7229*x**2*z** 9 + ch722a*x**2*z**10 + ch722b*x**2*z**11 +
ch722c*x**2*z**12 + ch722d*x**2*z**13 + ch722e*x**2*z**14 + ch722f*x**2*z**15,
ch7220=L0x20015674, ch7221=L0x200156e0, ch7222=L0x2001574c, ch7223=L0x200157b8,
ch7224=L0x20015824, ch7225=L0x20015890, ch7226=L0x200158fc, ch7227=L0x20015968,
ch7228=L0x200159d4, ch7229=L0x20015a40, ch722a=L0x20015aac, ch722b=L0x20015b18,
ch722c=L0x20015b84, ch722d=L0x20015bf0, ch722e=L0x20015c5c, ch722f=L0x20015cc8
] && true;


ghost CH820@sint32, CH821@sint32, CH822@sint32,
      ch8200@sint32, ch8201@sint32, ch8202@sint32, ch8203@sint32,
      ch8204@sint32, ch8205@sint32, ch8206@sint32, ch8207@sint32,
      ch8208@sint32, ch8209@sint32, ch820a@sint32, ch820b@sint32,
      ch820c@sint32, ch820d@sint32, ch820e@sint32, ch820f@sint32,
      ch8210@sint32, ch8211@sint32, ch8212@sint32, ch8213@sint32,
      ch8214@sint32, ch8215@sint32, ch8216@sint32, ch8217@sint32,
      ch8218@sint32, ch8219@sint32, ch821a@sint32, ch821b@sint32,
      ch821c@sint32, ch821d@sint32, ch821e@sint32, ch821f@sint32,
      ch8220@sint32, ch8221@sint32, ch8222@sint32, ch8223@sint32,
      ch8224@sint32, ch8225@sint32, ch8226@sint32, ch8227@sint32,
      ch8228@sint32, ch8229@sint32, ch822a@sint32, ch822b@sint32,
      ch822c@sint32, ch822d@sint32, ch822e@sint32, ch822f@sint32: and [
CH820**2 =
ch8200*x**0*z** 0 + ch8201*x**0*z** 1 + ch8202*x**0*z** 2 + ch8203*x**0*z** 3 +
ch8204*x**0*z** 4 + ch8205*x**0*z** 5 + ch8206*x**0*z** 6 + ch8207*x**0*z** 7 +
ch8208*x**0*z** 8 + ch8209*x**0*z** 9 + ch820a*x**0*z**10 + ch820b*x**0*z**11 +
ch820c*x**0*z**12 + ch820d*x**0*z**13 + ch820e*x**0*z**14 + ch820f*x**0*z**15,
ch8200=L0x20015678, ch8201=L0x200156e4, ch8202=L0x20015750, ch8203=L0x200157bc,
ch8204=L0x20015828, ch8205=L0x20015894, ch8206=L0x20015900, ch8207=L0x2001596c,
ch8208=L0x200159d8, ch8209=L0x20015a44, ch820a=L0x20015ab0, ch820b=L0x20015b1c,
ch820c=L0x20015b88, ch820d=L0x20015bf4, ch820e=L0x20015c60, ch820f=L0x20015ccc,
CH821**2 =
ch8210*x**1*z** 0 + ch8211*x**1*z** 1 + ch8212*x**1*z** 2 + ch8213*x**1*z** 3 +
ch8214*x**1*z** 4 + ch8215*x**1*z** 5 + ch8216*x**1*z** 6 + ch8217*x**1*z** 7 +
ch8218*x**1*z** 8 + ch8219*x**1*z** 9 + ch821a*x**1*z**10 + ch821b*x**1*z**11 +
ch821c*x**1*z**12 + ch821d*x**1*z**13 + ch821e*x**1*z**14 + ch821f*x**1*z**15,
ch8210=L0x2001567c, ch8211=L0x200156e8, ch8212=L0x20015754, ch8213=L0x200157c0,
ch8214=L0x2001582c, ch8215=L0x20015898, ch8216=L0x20015904, ch8217=L0x20015970,
ch8218=L0x200159dc, ch8219=L0x20015a48, ch821a=L0x20015ab4, ch821b=L0x20015b20,
ch821c=L0x20015b8c, ch821d=L0x20015bf8, ch821e=L0x20015c64, ch821f=L0x20015cd0,
CH822**2 =
ch8220*x**2*z** 0 + ch8221*x**2*z** 1 + ch8222*x**2*z** 2 + ch8223*x**2*z** 3 +
ch8224*x**2*z** 4 + ch8225*x**2*z** 5 + ch8226*x**2*z** 6 + ch8227*x**2*z** 7 +
ch8228*x**2*z** 8 + ch8229*x**2*z** 9 + ch822a*x**2*z**10 + ch822b*x**2*z**11 +
ch822c*x**2*z**12 + ch822d*x**2*z**13 + ch822e*x**2*z**14 + ch822f*x**2*z**15,
ch8220=L0x20015680, ch8221=L0x200156ec, ch8222=L0x20015758, ch8223=L0x200157c4,
ch8224=L0x20015830, ch8225=L0x2001589c, ch8226=L0x20015908, ch8227=L0x20015974,
ch8228=L0x200159e0, ch8229=L0x20015a4c, ch822a=L0x20015ab8, ch822b=L0x20015b24,
ch822c=L0x20015b90, ch822d=L0x20015bfc, ch822e=L0x20015c68, ch822f=L0x20015cd4
] && true;


ghost CH030@sint32, CH031@sint32, CH032@sint32,
      ch0300@sint32, ch0301@sint32, ch0302@sint32, ch0303@sint32,
      ch0304@sint32, ch0305@sint32, ch0306@sint32, ch0307@sint32,
      ch0308@sint32, ch0309@sint32, ch030a@sint32, ch030b@sint32,
      ch030c@sint32, ch030d@sint32, ch030e@sint32, ch030f@sint32,
      ch0310@sint32, ch0311@sint32, ch0312@sint32, ch0313@sint32,
      ch0314@sint32, ch0315@sint32, ch0316@sint32, ch0317@sint32,
      ch0318@sint32, ch0319@sint32, ch031a@sint32, ch031b@sint32,
      ch031c@sint32, ch031d@sint32, ch031e@sint32, ch031f@sint32,
      ch0320@sint32, ch0321@sint32, ch0322@sint32, ch0323@sint32,
      ch0324@sint32, ch0325@sint32, ch0326@sint32, ch0327@sint32,
      ch0328@sint32, ch0329@sint32, ch032a@sint32, ch032b@sint32,
      ch032c@sint32, ch032d@sint32, ch032e@sint32, ch032f@sint32: and [
CH030**2 =
ch0300*x**0*z** 0 + ch0301*x**0*z** 1 + ch0302*x**0*z** 2 + ch0303*x**0*z** 3 +
ch0304*x**0*z** 4 + ch0305*x**0*z** 5 + ch0306*x**0*z** 6 + ch0307*x**0*z** 7 +
ch0308*x**0*z** 8 + ch0309*x**0*z** 9 + ch030a*x**0*z**10 + ch030b*x**0*z**11 +
ch030c*x**0*z**12 + ch030d*x**0*z**13 + ch030e*x**0*z**14 + ch030f*x**0*z**15,
ch0300=L0x20015cd8, ch0301=L0x20015d44, ch0302=L0x20015db0, ch0303=L0x20015e1c,
ch0304=L0x20015e88, ch0305=L0x20015ef4, ch0306=L0x20015f60, ch0307=L0x20015fcc,
ch0308=L0x20016038, ch0309=L0x200160a4, ch030a=L0x20016110, ch030b=L0x2001617c,
ch030c=L0x200161e8, ch030d=L0x20016254, ch030e=L0x200162c0, ch030f=L0x2001632c,
CH031**2 =
ch0310*x**1*z** 0 + ch0311*x**1*z** 1 + ch0312*x**1*z** 2 + ch0313*x**1*z** 3 +
ch0314*x**1*z** 4 + ch0315*x**1*z** 5 + ch0316*x**1*z** 6 + ch0317*x**1*z** 7 +
ch0318*x**1*z** 8 + ch0319*x**1*z** 9 + ch031a*x**1*z**10 + ch031b*x**1*z**11 +
ch031c*x**1*z**12 + ch031d*x**1*z**13 + ch031e*x**1*z**14 + ch031f*x**1*z**15,
ch0310=L0x20015cdc, ch0311=L0x20015d48, ch0312=L0x20015db4, ch0313=L0x20015e20,
ch0314=L0x20015e8c, ch0315=L0x20015ef8, ch0316=L0x20015f64, ch0317=L0x20015fd0,
ch0318=L0x2001603c, ch0319=L0x200160a8, ch031a=L0x20016114, ch031b=L0x20016180,
ch031c=L0x200161ec, ch031d=L0x20016258, ch031e=L0x200162c4, ch031f=L0x20016330,
CH032**2 =
ch0320*x**2*z** 0 + ch0321*x**2*z** 1 + ch0322*x**2*z** 2 + ch0323*x**2*z** 3 +
ch0324*x**2*z** 4 + ch0325*x**2*z** 5 + ch0326*x**2*z** 6 + ch0327*x**2*z** 7 +
ch0328*x**2*z** 8 + ch0329*x**2*z** 9 + ch032a*x**2*z**10 + ch032b*x**2*z**11 +
ch032c*x**2*z**12 + ch032d*x**2*z**13 + ch032e*x**2*z**14 + ch032f*x**2*z**15,
ch0320=L0x20015ce0, ch0321=L0x20015d4c, ch0322=L0x20015db8, ch0323=L0x20015e24,
ch0324=L0x20015e90, ch0325=L0x20015efc, ch0326=L0x20015f68, ch0327=L0x20015fd4,
ch0328=L0x20016040, ch0329=L0x200160ac, ch032a=L0x20016118, ch032b=L0x20016184,
ch032c=L0x200161f0, ch032d=L0x2001625c, ch032e=L0x200162c8, ch032f=L0x20016334
] && true;


ghost CH130@sint32, CH131@sint32, CH132@sint32,
      ch1300@sint32, ch1301@sint32, ch1302@sint32, ch1303@sint32,
      ch1304@sint32, ch1305@sint32, ch1306@sint32, ch1307@sint32,
      ch1308@sint32, ch1309@sint32, ch130a@sint32, ch130b@sint32,
      ch130c@sint32, ch130d@sint32, ch130e@sint32, ch130f@sint32,
      ch1310@sint32, ch1311@sint32, ch1312@sint32, ch1313@sint32,
      ch1314@sint32, ch1315@sint32, ch1316@sint32, ch1317@sint32,
      ch1318@sint32, ch1319@sint32, ch131a@sint32, ch131b@sint32,
      ch131c@sint32, ch131d@sint32, ch131e@sint32, ch131f@sint32,
      ch1320@sint32, ch1321@sint32, ch1322@sint32, ch1323@sint32,
      ch1324@sint32, ch1325@sint32, ch1326@sint32, ch1327@sint32,
      ch1328@sint32, ch1329@sint32, ch132a@sint32, ch132b@sint32,
      ch132c@sint32, ch132d@sint32, ch132e@sint32, ch132f@sint32: and [
CH130**2 =
ch1300*x**0*z** 0 + ch1301*x**0*z** 1 + ch1302*x**0*z** 2 + ch1303*x**0*z** 3 +
ch1304*x**0*z** 4 + ch1305*x**0*z** 5 + ch1306*x**0*z** 6 + ch1307*x**0*z** 7 +
ch1308*x**0*z** 8 + ch1309*x**0*z** 9 + ch130a*x**0*z**10 + ch130b*x**0*z**11 +
ch130c*x**0*z**12 + ch130d*x**0*z**13 + ch130e*x**0*z**14 + ch130f*x**0*z**15,
ch1300=L0x20015ce4, ch1301=L0x20015d50, ch1302=L0x20015dbc, ch1303=L0x20015e28,
ch1304=L0x20015e94, ch1305=L0x20015f00, ch1306=L0x20015f6c, ch1307=L0x20015fd8,
ch1308=L0x20016044, ch1309=L0x200160b0, ch130a=L0x2001611c, ch130b=L0x20016188,
ch130c=L0x200161f4, ch130d=L0x20016260, ch130e=L0x200162cc, ch130f=L0x20016338,
CH131**2 =
ch1310*x**1*z** 0 + ch1311*x**1*z** 1 + ch1312*x**1*z** 2 + ch1313*x**1*z** 3 +
ch1314*x**1*z** 4 + ch1315*x**1*z** 5 + ch1316*x**1*z** 6 + ch1317*x**1*z** 7 +
ch1318*x**1*z** 8 + ch1319*x**1*z** 9 + ch131a*x**1*z**10 + ch131b*x**1*z**11 +
ch131c*x**1*z**12 + ch131d*x**1*z**13 + ch131e*x**1*z**14 + ch131f*x**1*z**15,
ch1310=L0x20015ce8, ch1311=L0x20015d54, ch1312=L0x20015dc0, ch1313=L0x20015e2c,
ch1314=L0x20015e98, ch1315=L0x20015f04, ch1316=L0x20015f70, ch1317=L0x20015fdc,
ch1318=L0x20016048, ch1319=L0x200160b4, ch131a=L0x20016120, ch131b=L0x2001618c,
ch131c=L0x200161f8, ch131d=L0x20016264, ch131e=L0x200162d0, ch131f=L0x2001633c,
CH132**2 =
ch1320*x**2*z** 0 + ch1321*x**2*z** 1 + ch1322*x**2*z** 2 + ch1323*x**2*z** 3 +
ch1324*x**2*z** 4 + ch1325*x**2*z** 5 + ch1326*x**2*z** 6 + ch1327*x**2*z** 7 +
ch1328*x**2*z** 8 + ch1329*x**2*z** 9 + ch132a*x**2*z**10 + ch132b*x**2*z**11 +
ch132c*x**2*z**12 + ch132d*x**2*z**13 + ch132e*x**2*z**14 + ch132f*x**2*z**15,
ch1320=L0x20015cec, ch1321=L0x20015d58, ch1322=L0x20015dc4, ch1323=L0x20015e30,
ch1324=L0x20015e9c, ch1325=L0x20015f08, ch1326=L0x20015f74, ch1327=L0x20015fe0,
ch1328=L0x2001604c, ch1329=L0x200160b8, ch132a=L0x20016124, ch132b=L0x20016190,
ch132c=L0x200161fc, ch132d=L0x20016268, ch132e=L0x200162d4, ch132f=L0x20016340
] && true;


ghost CH230@sint32, CH231@sint32, CH232@sint32,
      ch2300@sint32, ch2301@sint32, ch2302@sint32, ch2303@sint32,
      ch2304@sint32, ch2305@sint32, ch2306@sint32, ch2307@sint32,
      ch2308@sint32, ch2309@sint32, ch230a@sint32, ch230b@sint32,
      ch230c@sint32, ch230d@sint32, ch230e@sint32, ch230f@sint32,
      ch2310@sint32, ch2311@sint32, ch2312@sint32, ch2313@sint32,
      ch2314@sint32, ch2315@sint32, ch2316@sint32, ch2317@sint32,
      ch2318@sint32, ch2319@sint32, ch231a@sint32, ch231b@sint32,
      ch231c@sint32, ch231d@sint32, ch231e@sint32, ch231f@sint32,
      ch2320@sint32, ch2321@sint32, ch2322@sint32, ch2323@sint32,
      ch2324@sint32, ch2325@sint32, ch2326@sint32, ch2327@sint32,
      ch2328@sint32, ch2329@sint32, ch232a@sint32, ch232b@sint32,
      ch232c@sint32, ch232d@sint32, ch232e@sint32, ch232f@sint32: and [
CH230**2 =
ch2300*x**0*z** 0 + ch2301*x**0*z** 1 + ch2302*x**0*z** 2 + ch2303*x**0*z** 3 +
ch2304*x**0*z** 4 + ch2305*x**0*z** 5 + ch2306*x**0*z** 6 + ch2307*x**0*z** 7 +
ch2308*x**0*z** 8 + ch2309*x**0*z** 9 + ch230a*x**0*z**10 + ch230b*x**0*z**11 +
ch230c*x**0*z**12 + ch230d*x**0*z**13 + ch230e*x**0*z**14 + ch230f*x**0*z**15,
ch2300=L0x20015cf0, ch2301=L0x20015d5c, ch2302=L0x20015dc8, ch2303=L0x20015e34,
ch2304=L0x20015ea0, ch2305=L0x20015f0c, ch2306=L0x20015f78, ch2307=L0x20015fe4,
ch2308=L0x20016050, ch2309=L0x200160bc, ch230a=L0x20016128, ch230b=L0x20016194,
ch230c=L0x20016200, ch230d=L0x2001626c, ch230e=L0x200162d8, ch230f=L0x20016344,
CH231**2 =
ch2310*x**1*z** 0 + ch2311*x**1*z** 1 + ch2312*x**1*z** 2 + ch2313*x**1*z** 3 +
ch2314*x**1*z** 4 + ch2315*x**1*z** 5 + ch2316*x**1*z** 6 + ch2317*x**1*z** 7 +
ch2318*x**1*z** 8 + ch2319*x**1*z** 9 + ch231a*x**1*z**10 + ch231b*x**1*z**11 +
ch231c*x**1*z**12 + ch231d*x**1*z**13 + ch231e*x**1*z**14 + ch231f*x**1*z**15,
ch2310=L0x20015cf4, ch2311=L0x20015d60, ch2312=L0x20015dcc, ch2313=L0x20015e38,
ch2314=L0x20015ea4, ch2315=L0x20015f10, ch2316=L0x20015f7c, ch2317=L0x20015fe8,
ch2318=L0x20016054, ch2319=L0x200160c0, ch231a=L0x2001612c, ch231b=L0x20016198,
ch231c=L0x20016204, ch231d=L0x20016270, ch231e=L0x200162dc, ch231f=L0x20016348,
CH232**2 =
ch2320*x**2*z** 0 + ch2321*x**2*z** 1 + ch2322*x**2*z** 2 + ch2323*x**2*z** 3 +
ch2324*x**2*z** 4 + ch2325*x**2*z** 5 + ch2326*x**2*z** 6 + ch2327*x**2*z** 7 +
ch2328*x**2*z** 8 + ch2329*x**2*z** 9 + ch232a*x**2*z**10 + ch232b*x**2*z**11 +
ch232c*x**2*z**12 + ch232d*x**2*z**13 + ch232e*x**2*z**14 + ch232f*x**2*z**15,
ch2320=L0x20015cf8, ch2321=L0x20015d64, ch2322=L0x20015dd0, ch2323=L0x20015e3c,
ch2324=L0x20015ea8, ch2325=L0x20015f14, ch2326=L0x20015f80, ch2327=L0x20015fec,
ch2328=L0x20016058, ch2329=L0x200160c4, ch232a=L0x20016130, ch232b=L0x2001619c,
ch232c=L0x20016208, ch232d=L0x20016274, ch232e=L0x200162e0, ch232f=L0x2001634c
] && true;


ghost CH330@sint32, CH331@sint32, CH332@sint32,
      ch3300@sint32, ch3301@sint32, ch3302@sint32, ch3303@sint32,
      ch3304@sint32, ch3305@sint32, ch3306@sint32, ch3307@sint32,
      ch3308@sint32, ch3309@sint32, ch330a@sint32, ch330b@sint32,
      ch330c@sint32, ch330d@sint32, ch330e@sint32, ch330f@sint32,
      ch3310@sint32, ch3311@sint32, ch3312@sint32, ch3313@sint32,
      ch3314@sint32, ch3315@sint32, ch3316@sint32, ch3317@sint32,
      ch3318@sint32, ch3319@sint32, ch331a@sint32, ch331b@sint32,
      ch331c@sint32, ch331d@sint32, ch331e@sint32, ch331f@sint32,
      ch3320@sint32, ch3321@sint32, ch3322@sint32, ch3323@sint32,
      ch3324@sint32, ch3325@sint32, ch3326@sint32, ch3327@sint32,
      ch3328@sint32, ch3329@sint32, ch332a@sint32, ch332b@sint32,
      ch332c@sint32, ch332d@sint32, ch332e@sint32, ch332f@sint32: and [
CH330**2 =
ch3300*x**0*z** 0 + ch3301*x**0*z** 1 + ch3302*x**0*z** 2 + ch3303*x**0*z** 3 +
ch3304*x**0*z** 4 + ch3305*x**0*z** 5 + ch3306*x**0*z** 6 + ch3307*x**0*z** 7 +
ch3308*x**0*z** 8 + ch3309*x**0*z** 9 + ch330a*x**0*z**10 + ch330b*x**0*z**11 +
ch330c*x**0*z**12 + ch330d*x**0*z**13 + ch330e*x**0*z**14 + ch330f*x**0*z**15,
ch3300=L0x20015cfc, ch3301=L0x20015d68, ch3302=L0x20015dd4, ch3303=L0x20015e40,
ch3304=L0x20015eac, ch3305=L0x20015f18, ch3306=L0x20015f84, ch3307=L0x20015ff0,
ch3308=L0x2001605c, ch3309=L0x200160c8, ch330a=L0x20016134, ch330b=L0x200161a0,
ch330c=L0x2001620c, ch330d=L0x20016278, ch330e=L0x200162e4, ch330f=L0x20016350,
CH331**2 =
ch3310*x**1*z** 0 + ch3311*x**1*z** 1 + ch3312*x**1*z** 2 + ch3313*x**1*z** 3 +
ch3314*x**1*z** 4 + ch3315*x**1*z** 5 + ch3316*x**1*z** 6 + ch3317*x**1*z** 7 +
ch3318*x**1*z** 8 + ch3319*x**1*z** 9 + ch331a*x**1*z**10 + ch331b*x**1*z**11 +
ch331c*x**1*z**12 + ch331d*x**1*z**13 + ch331e*x**1*z**14 + ch331f*x**1*z**15,
ch3310=L0x20015d00, ch3311=L0x20015d6c, ch3312=L0x20015dd8, ch3313=L0x20015e44,
ch3314=L0x20015eb0, ch3315=L0x20015f1c, ch3316=L0x20015f88, ch3317=L0x20015ff4,
ch3318=L0x20016060, ch3319=L0x200160cc, ch331a=L0x20016138, ch331b=L0x200161a4,
ch331c=L0x20016210, ch331d=L0x2001627c, ch331e=L0x200162e8, ch331f=L0x20016354,
CH332**2 =
ch3320*x**2*z** 0 + ch3321*x**2*z** 1 + ch3322*x**2*z** 2 + ch3323*x**2*z** 3 +
ch3324*x**2*z** 4 + ch3325*x**2*z** 5 + ch3326*x**2*z** 6 + ch3327*x**2*z** 7 +
ch3328*x**2*z** 8 + ch3329*x**2*z** 9 + ch332a*x**2*z**10 + ch332b*x**2*z**11 +
ch332c*x**2*z**12 + ch332d*x**2*z**13 + ch332e*x**2*z**14 + ch332f*x**2*z**15,
ch3320=L0x20015d04, ch3321=L0x20015d70, ch3322=L0x20015ddc, ch3323=L0x20015e48,
ch3324=L0x20015eb4, ch3325=L0x20015f20, ch3326=L0x20015f8c, ch3327=L0x20015ff8,
ch3328=L0x20016064, ch3329=L0x200160d0, ch332a=L0x2001613c, ch332b=L0x200161a8,
ch332c=L0x20016214, ch332d=L0x20016280, ch332e=L0x200162ec, ch332f=L0x20016358
] && true;


ghost CH430@sint32, CH431@sint32, CH432@sint32,
      ch4300@sint32, ch4301@sint32, ch4302@sint32, ch4303@sint32,
      ch4304@sint32, ch4305@sint32, ch4306@sint32, ch4307@sint32,
      ch4308@sint32, ch4309@sint32, ch430a@sint32, ch430b@sint32,
      ch430c@sint32, ch430d@sint32, ch430e@sint32, ch430f@sint32,
      ch4310@sint32, ch4311@sint32, ch4312@sint32, ch4313@sint32,
      ch4314@sint32, ch4315@sint32, ch4316@sint32, ch4317@sint32,
      ch4318@sint32, ch4319@sint32, ch431a@sint32, ch431b@sint32,
      ch431c@sint32, ch431d@sint32, ch431e@sint32, ch431f@sint32,
      ch4320@sint32, ch4321@sint32, ch4322@sint32, ch4323@sint32,
      ch4324@sint32, ch4325@sint32, ch4326@sint32, ch4327@sint32,
      ch4328@sint32, ch4329@sint32, ch432a@sint32, ch432b@sint32,
      ch432c@sint32, ch432d@sint32, ch432e@sint32, ch432f@sint32: and [
CH430**2 =
ch4300*x**0*z** 0 + ch4301*x**0*z** 1 + ch4302*x**0*z** 2 + ch4303*x**0*z** 3 +
ch4304*x**0*z** 4 + ch4305*x**0*z** 5 + ch4306*x**0*z** 6 + ch4307*x**0*z** 7 +
ch4308*x**0*z** 8 + ch4309*x**0*z** 9 + ch430a*x**0*z**10 + ch430b*x**0*z**11 +
ch430c*x**0*z**12 + ch430d*x**0*z**13 + ch430e*x**0*z**14 + ch430f*x**0*z**15,
ch4300=L0x20015d08, ch4301=L0x20015d74, ch4302=L0x20015de0, ch4303=L0x20015e4c,
ch4304=L0x20015eb8, ch4305=L0x20015f24, ch4306=L0x20015f90, ch4307=L0x20015ffc,
ch4308=L0x20016068, ch4309=L0x200160d4, ch430a=L0x20016140, ch430b=L0x200161ac,
ch430c=L0x20016218, ch430d=L0x20016284, ch430e=L0x200162f0, ch430f=L0x2001635c,
CH431**2 =
ch4310*x**1*z** 0 + ch4311*x**1*z** 1 + ch4312*x**1*z** 2 + ch4313*x**1*z** 3 +
ch4314*x**1*z** 4 + ch4315*x**1*z** 5 + ch4316*x**1*z** 6 + ch4317*x**1*z** 7 +
ch4318*x**1*z** 8 + ch4319*x**1*z** 9 + ch431a*x**1*z**10 + ch431b*x**1*z**11 +
ch431c*x**1*z**12 + ch431d*x**1*z**13 + ch431e*x**1*z**14 + ch431f*x**1*z**15,
ch4310=L0x20015d0c, ch4311=L0x20015d78, ch4312=L0x20015de4, ch4313=L0x20015e50,
ch4314=L0x20015ebc, ch4315=L0x20015f28, ch4316=L0x20015f94, ch4317=L0x20016000,
ch4318=L0x2001606c, ch4319=L0x200160d8, ch431a=L0x20016144, ch431b=L0x200161b0,
ch431c=L0x2001621c, ch431d=L0x20016288, ch431e=L0x200162f4, ch431f=L0x20016360,
CH432**2 =
ch4320*x**2*z** 0 + ch4321*x**2*z** 1 + ch4322*x**2*z** 2 + ch4323*x**2*z** 3 +
ch4324*x**2*z** 4 + ch4325*x**2*z** 5 + ch4326*x**2*z** 6 + ch4327*x**2*z** 7 +
ch4328*x**2*z** 8 + ch4329*x**2*z** 9 + ch432a*x**2*z**10 + ch432b*x**2*z**11 +
ch432c*x**2*z**12 + ch432d*x**2*z**13 + ch432e*x**2*z**14 + ch432f*x**2*z**15,
ch4320=L0x20015d10, ch4321=L0x20015d7c, ch4322=L0x20015de8, ch4323=L0x20015e54,
ch4324=L0x20015ec0, ch4325=L0x20015f2c, ch4326=L0x20015f98, ch4327=L0x20016004,
ch4328=L0x20016070, ch4329=L0x200160dc, ch432a=L0x20016148, ch432b=L0x200161b4,
ch432c=L0x20016220, ch432d=L0x2001628c, ch432e=L0x200162f8, ch432f=L0x20016364
] && true;


ghost CH530@sint32, CH531@sint32, CH532@sint32,
      ch5300@sint32, ch5301@sint32, ch5302@sint32, ch5303@sint32,
      ch5304@sint32, ch5305@sint32, ch5306@sint32, ch5307@sint32,
      ch5308@sint32, ch5309@sint32, ch530a@sint32, ch530b@sint32,
      ch530c@sint32, ch530d@sint32, ch530e@sint32, ch530f@sint32,
      ch5310@sint32, ch5311@sint32, ch5312@sint32, ch5313@sint32,
      ch5314@sint32, ch5315@sint32, ch5316@sint32, ch5317@sint32,
      ch5318@sint32, ch5319@sint32, ch531a@sint32, ch531b@sint32,
      ch531c@sint32, ch531d@sint32, ch531e@sint32, ch531f@sint32,
      ch5320@sint32, ch5321@sint32, ch5322@sint32, ch5323@sint32,
      ch5324@sint32, ch5325@sint32, ch5326@sint32, ch5327@sint32,
      ch5328@sint32, ch5329@sint32, ch532a@sint32, ch532b@sint32,
      ch532c@sint32, ch532d@sint32, ch532e@sint32, ch532f@sint32: and [
CH530**2 =
ch5300*x**0*z** 0 + ch5301*x**0*z** 1 + ch5302*x**0*z** 2 + ch5303*x**0*z** 3 +
ch5304*x**0*z** 4 + ch5305*x**0*z** 5 + ch5306*x**0*z** 6 + ch5307*x**0*z** 7 +
ch5308*x**0*z** 8 + ch5309*x**0*z** 9 + ch530a*x**0*z**10 + ch530b*x**0*z**11 +
ch530c*x**0*z**12 + ch530d*x**0*z**13 + ch530e*x**0*z**14 + ch530f*x**0*z**15,
ch5300=L0x20015d14, ch5301=L0x20015d80, ch5302=L0x20015dec, ch5303=L0x20015e58,
ch5304=L0x20015ec4, ch5305=L0x20015f30, ch5306=L0x20015f9c, ch5307=L0x20016008,
ch5308=L0x20016074, ch5309=L0x200160e0, ch530a=L0x2001614c, ch530b=L0x200161b8,
ch530c=L0x20016224, ch530d=L0x20016290, ch530e=L0x200162fc, ch530f=L0x20016368,
CH531**2 =
ch5310*x**1*z** 0 + ch5311*x**1*z** 1 + ch5312*x**1*z** 2 + ch5313*x**1*z** 3 +
ch5314*x**1*z** 4 + ch5315*x**1*z** 5 + ch5316*x**1*z** 6 + ch5317*x**1*z** 7 +
ch5318*x**1*z** 8 + ch5319*x**1*z** 9 + ch531a*x**1*z**10 + ch531b*x**1*z**11 +
ch531c*x**1*z**12 + ch531d*x**1*z**13 + ch531e*x**1*z**14 + ch531f*x**1*z**15,
ch5310=L0x20015d18, ch5311=L0x20015d84, ch5312=L0x20015df0, ch5313=L0x20015e5c,
ch5314=L0x20015ec8, ch5315=L0x20015f34, ch5316=L0x20015fa0, ch5317=L0x2001600c,
ch5318=L0x20016078, ch5319=L0x200160e4, ch531a=L0x20016150, ch531b=L0x200161bc,
ch531c=L0x20016228, ch531d=L0x20016294, ch531e=L0x20016300, ch531f=L0x2001636c,
CH532**2 =
ch5320*x**2*z** 0 + ch5321*x**2*z** 1 + ch5322*x**2*z** 2 + ch5323*x**2*z** 3 +
ch5324*x**2*z** 4 + ch5325*x**2*z** 5 + ch5326*x**2*z** 6 + ch5327*x**2*z** 7 +
ch5328*x**2*z** 8 + ch5329*x**2*z** 9 + ch532a*x**2*z**10 + ch532b*x**2*z**11 +
ch532c*x**2*z**12 + ch532d*x**2*z**13 + ch532e*x**2*z**14 + ch532f*x**2*z**15,
ch5320=L0x20015d1c, ch5321=L0x20015d88, ch5322=L0x20015df4, ch5323=L0x20015e60,
ch5324=L0x20015ecc, ch5325=L0x20015f38, ch5326=L0x20015fa4, ch5327=L0x20016010,
ch5328=L0x2001607c, ch5329=L0x200160e8, ch532a=L0x20016154, ch532b=L0x200161c0,
ch532c=L0x2001622c, ch532d=L0x20016298, ch532e=L0x20016304, ch532f=L0x20016370
] && true;


ghost CH630@sint32, CH631@sint32, CH632@sint32,
      ch6300@sint32, ch6301@sint32, ch6302@sint32, ch6303@sint32,
      ch6304@sint32, ch6305@sint32, ch6306@sint32, ch6307@sint32,
      ch6308@sint32, ch6309@sint32, ch630a@sint32, ch630b@sint32,
      ch630c@sint32, ch630d@sint32, ch630e@sint32, ch630f@sint32,
      ch6310@sint32, ch6311@sint32, ch6312@sint32, ch6313@sint32,
      ch6314@sint32, ch6315@sint32, ch6316@sint32, ch6317@sint32,
      ch6318@sint32, ch6319@sint32, ch631a@sint32, ch631b@sint32,
      ch631c@sint32, ch631d@sint32, ch631e@sint32, ch631f@sint32,
      ch6320@sint32, ch6321@sint32, ch6322@sint32, ch6323@sint32,
      ch6324@sint32, ch6325@sint32, ch6326@sint32, ch6327@sint32,
      ch6328@sint32, ch6329@sint32, ch632a@sint32, ch632b@sint32,
      ch632c@sint32, ch632d@sint32, ch632e@sint32, ch632f@sint32: and [
CH630**2 =
ch6300*x**0*z** 0 + ch6301*x**0*z** 1 + ch6302*x**0*z** 2 + ch6303*x**0*z** 3 +
ch6304*x**0*z** 4 + ch6305*x**0*z** 5 + ch6306*x**0*z** 6 + ch6307*x**0*z** 7 +
ch6308*x**0*z** 8 + ch6309*x**0*z** 9 + ch630a*x**0*z**10 + ch630b*x**0*z**11 +
ch630c*x**0*z**12 + ch630d*x**0*z**13 + ch630e*x**0*z**14 + ch630f*x**0*z**15,
ch6300=L0x20015d20, ch6301=L0x20015d8c, ch6302=L0x20015df8, ch6303=L0x20015e64,
ch6304=L0x20015ed0, ch6305=L0x20015f3c, ch6306=L0x20015fa8, ch6307=L0x20016014,
ch6308=L0x20016080, ch6309=L0x200160ec, ch630a=L0x20016158, ch630b=L0x200161c4,
ch630c=L0x20016230, ch630d=L0x2001629c, ch630e=L0x20016308, ch630f=L0x20016374,
CH631**2 =
ch6310*x**1*z** 0 + ch6311*x**1*z** 1 + ch6312*x**1*z** 2 + ch6313*x**1*z** 3 +
ch6314*x**1*z** 4 + ch6315*x**1*z** 5 + ch6316*x**1*z** 6 + ch6317*x**1*z** 7 +
ch6318*x**1*z** 8 + ch6319*x**1*z** 9 + ch631a*x**1*z**10 + ch631b*x**1*z**11 +
ch631c*x**1*z**12 + ch631d*x**1*z**13 + ch631e*x**1*z**14 + ch631f*x**1*z**15,
ch6310=L0x20015d24, ch6311=L0x20015d90, ch6312=L0x20015dfc, ch6313=L0x20015e68,
ch6314=L0x20015ed4, ch6315=L0x20015f40, ch6316=L0x20015fac, ch6317=L0x20016018,
ch6318=L0x20016084, ch6319=L0x200160f0, ch631a=L0x2001615c, ch631b=L0x200161c8,
ch631c=L0x20016234, ch631d=L0x200162a0, ch631e=L0x2001630c, ch631f=L0x20016378,
CH632**2 =
ch6320*x**2*z** 0 + ch6321*x**2*z** 1 + ch6322*x**2*z** 2 + ch6323*x**2*z** 3 +
ch6324*x**2*z** 4 + ch6325*x**2*z** 5 + ch6326*x**2*z** 6 + ch6327*x**2*z** 7 +
ch6328*x**2*z** 8 + ch6329*x**2*z** 9 + ch632a*x**2*z**10 + ch632b*x**2*z**11 +
ch632c*x**2*z**12 + ch632d*x**2*z**13 + ch632e*x**2*z**14 + ch632f*x**2*z**15,
ch6320=L0x20015d28, ch6321=L0x20015d94, ch6322=L0x20015e00, ch6323=L0x20015e6c,
ch6324=L0x20015ed8, ch6325=L0x20015f44, ch6326=L0x20015fb0, ch6327=L0x2001601c,
ch6328=L0x20016088, ch6329=L0x200160f4, ch632a=L0x20016160, ch632b=L0x200161cc,
ch632c=L0x20016238, ch632d=L0x200162a4, ch632e=L0x20016310, ch632f=L0x2001637c
] && true;


ghost CH730@sint32, CH731@sint32, CH732@sint32,
      ch7300@sint32, ch7301@sint32, ch7302@sint32, ch7303@sint32,
      ch7304@sint32, ch7305@sint32, ch7306@sint32, ch7307@sint32,
      ch7308@sint32, ch7309@sint32, ch730a@sint32, ch730b@sint32,
      ch730c@sint32, ch730d@sint32, ch730e@sint32, ch730f@sint32,
      ch7310@sint32, ch7311@sint32, ch7312@sint32, ch7313@sint32,
      ch7314@sint32, ch7315@sint32, ch7316@sint32, ch7317@sint32,
      ch7318@sint32, ch7319@sint32, ch731a@sint32, ch731b@sint32,
      ch731c@sint32, ch731d@sint32, ch731e@sint32, ch731f@sint32,
      ch7320@sint32, ch7321@sint32, ch7322@sint32, ch7323@sint32,
      ch7324@sint32, ch7325@sint32, ch7326@sint32, ch7327@sint32,
      ch7328@sint32, ch7329@sint32, ch732a@sint32, ch732b@sint32,
      ch732c@sint32, ch732d@sint32, ch732e@sint32, ch732f@sint32: and [
CH730**2 =
ch7300*x**0*z** 0 + ch7301*x**0*z** 1 + ch7302*x**0*z** 2 + ch7303*x**0*z** 3 +
ch7304*x**0*z** 4 + ch7305*x**0*z** 5 + ch7306*x**0*z** 6 + ch7307*x**0*z** 7 +
ch7308*x**0*z** 8 + ch7309*x**0*z** 9 + ch730a*x**0*z**10 + ch730b*x**0*z**11 +
ch730c*x**0*z**12 + ch730d*x**0*z**13 + ch730e*x**0*z**14 + ch730f*x**0*z**15,
ch7300=L0x20015d2c, ch7301=L0x20015d98, ch7302=L0x20015e04, ch7303=L0x20015e70,
ch7304=L0x20015edc, ch7305=L0x20015f48, ch7306=L0x20015fb4, ch7307=L0x20016020,
ch7308=L0x2001608c, ch7309=L0x200160f8, ch730a=L0x20016164, ch730b=L0x200161d0,
ch730c=L0x2001623c, ch730d=L0x200162a8, ch730e=L0x20016314, ch730f=L0x20016380,
CH731**2 =
ch7310*x**1*z** 0 + ch7311*x**1*z** 1 + ch7312*x**1*z** 2 + ch7313*x**1*z** 3 +
ch7314*x**1*z** 4 + ch7315*x**1*z** 5 + ch7316*x**1*z** 6 + ch7317*x**1*z** 7 +
ch7318*x**1*z** 8 + ch7319*x**1*z** 9 + ch731a*x**1*z**10 + ch731b*x**1*z**11 +
ch731c*x**1*z**12 + ch731d*x**1*z**13 + ch731e*x**1*z**14 + ch731f*x**1*z**15,
ch7310=L0x20015d30, ch7311=L0x20015d9c, ch7312=L0x20015e08, ch7313=L0x20015e74,
ch7314=L0x20015ee0, ch7315=L0x20015f4c, ch7316=L0x20015fb8, ch7317=L0x20016024,
ch7318=L0x20016090, ch7319=L0x200160fc, ch731a=L0x20016168, ch731b=L0x200161d4,
ch731c=L0x20016240, ch731d=L0x200162ac, ch731e=L0x20016318, ch731f=L0x20016384,
CH732**2 =
ch7320*x**2*z** 0 + ch7321*x**2*z** 1 + ch7322*x**2*z** 2 + ch7323*x**2*z** 3 +
ch7324*x**2*z** 4 + ch7325*x**2*z** 5 + ch7326*x**2*z** 6 + ch7327*x**2*z** 7 +
ch7328*x**2*z** 8 + ch7329*x**2*z** 9 + ch732a*x**2*z**10 + ch732b*x**2*z**11 +
ch732c*x**2*z**12 + ch732d*x**2*z**13 + ch732e*x**2*z**14 + ch732f*x**2*z**15,
ch7320=L0x20015d34, ch7321=L0x20015da0, ch7322=L0x20015e0c, ch7323=L0x20015e78,
ch7324=L0x20015ee4, ch7325=L0x20015f50, ch7326=L0x20015fbc, ch7327=L0x20016028,
ch7328=L0x20016094, ch7329=L0x20016100, ch732a=L0x2001616c, ch732b=L0x200161d8,
ch732c=L0x20016244, ch732d=L0x200162b0, ch732e=L0x2001631c, ch732f=L0x20016388
] && true;


ghost CH830@sint32, CH831@sint32, CH832@sint32,
      ch8300@sint32, ch8301@sint32, ch8302@sint32, ch8303@sint32,
      ch8304@sint32, ch8305@sint32, ch8306@sint32, ch8307@sint32,
      ch8308@sint32, ch8309@sint32, ch830a@sint32, ch830b@sint32,
      ch830c@sint32, ch830d@sint32, ch830e@sint32, ch830f@sint32,
      ch8310@sint32, ch8311@sint32, ch8312@sint32, ch8313@sint32,
      ch8314@sint32, ch8315@sint32, ch8316@sint32, ch8317@sint32,
      ch8318@sint32, ch8319@sint32, ch831a@sint32, ch831b@sint32,
      ch831c@sint32, ch831d@sint32, ch831e@sint32, ch831f@sint32,
      ch8320@sint32, ch8321@sint32, ch8322@sint32, ch8323@sint32,
      ch8324@sint32, ch8325@sint32, ch8326@sint32, ch8327@sint32,
      ch8328@sint32, ch8329@sint32, ch832a@sint32, ch832b@sint32,
      ch832c@sint32, ch832d@sint32, ch832e@sint32, ch832f@sint32: and [
CH830**2 =
ch8300*x**0*z** 0 + ch8301*x**0*z** 1 + ch8302*x**0*z** 2 + ch8303*x**0*z** 3 +
ch8304*x**0*z** 4 + ch8305*x**0*z** 5 + ch8306*x**0*z** 6 + ch8307*x**0*z** 7 +
ch8308*x**0*z** 8 + ch8309*x**0*z** 9 + ch830a*x**0*z**10 + ch830b*x**0*z**11 +
ch830c*x**0*z**12 + ch830d*x**0*z**13 + ch830e*x**0*z**14 + ch830f*x**0*z**15,
ch8300=L0x20015d38, ch8301=L0x20015da4, ch8302=L0x20015e10, ch8303=L0x20015e7c,
ch8304=L0x20015ee8, ch8305=L0x20015f54, ch8306=L0x20015fc0, ch8307=L0x2001602c,
ch8308=L0x20016098, ch8309=L0x20016104, ch830a=L0x20016170, ch830b=L0x200161dc,
ch830c=L0x20016248, ch830d=L0x200162b4, ch830e=L0x20016320, ch830f=L0x2001638c,
CH831**2 =
ch8310*x**1*z** 0 + ch8311*x**1*z** 1 + ch8312*x**1*z** 2 + ch8313*x**1*z** 3 +
ch8314*x**1*z** 4 + ch8315*x**1*z** 5 + ch8316*x**1*z** 6 + ch8317*x**1*z** 7 +
ch8318*x**1*z** 8 + ch8319*x**1*z** 9 + ch831a*x**1*z**10 + ch831b*x**1*z**11 +
ch831c*x**1*z**12 + ch831d*x**1*z**13 + ch831e*x**1*z**14 + ch831f*x**1*z**15,
ch8310=L0x20015d3c, ch8311=L0x20015da8, ch8312=L0x20015e14, ch8313=L0x20015e80,
ch8314=L0x20015eec, ch8315=L0x20015f58, ch8316=L0x20015fc4, ch8317=L0x20016030,
ch8318=L0x2001609c, ch8319=L0x20016108, ch831a=L0x20016174, ch831b=L0x200161e0,
ch831c=L0x2001624c, ch831d=L0x200162b8, ch831e=L0x20016324, ch831f=L0x20016390,
CH832**2 =
ch8320*x**2*z** 0 + ch8321*x**2*z** 1 + ch8322*x**2*z** 2 + ch8323*x**2*z** 3 +
ch8324*x**2*z** 4 + ch8325*x**2*z** 5 + ch8326*x**2*z** 6 + ch8327*x**2*z** 7 +
ch8328*x**2*z** 8 + ch8329*x**2*z** 9 + ch832a*x**2*z**10 + ch832b*x**2*z**11 +
ch832c*x**2*z**12 + ch832d*x**2*z**13 + ch832e*x**2*z**14 + ch832f*x**2*z**15,
ch8320=L0x20015d40, ch8321=L0x20015dac, ch8322=L0x20015e18, ch8323=L0x20015e84,
ch8324=L0x20015ef0, ch8325=L0x20015f5c, ch8326=L0x20015fc8, ch8327=L0x20016034,
ch8328=L0x200160a0, ch8329=L0x2001610c, ch832a=L0x20016178, ch832b=L0x200161e4,
ch832c=L0x20016250, ch832d=L0x200162bc, ch832e=L0x20016328, ch832f=L0x20016394
] && true;





(******************** summarize ********************)


(**************** CUT 397, - *****************)

ecut and [
eqmod CG00**2+CG01**2+CG02**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -       1, z**32 -       1 ],
CG00**2 =
cg00000*x**0*y**0*z** 0+cg00100*x**0*y**1*z** 0+cg00200*x**0*y**2*z** 0+
cg00001*x**0*y**0*z** 1+cg00101*x**0*y**1*z** 1+cg00201*x**0*y**2*z** 1+
cg00002*x**0*y**0*z** 2+cg00102*x**0*y**1*z** 2+cg00202*x**0*y**2*z** 2+
cg00003*x**0*y**0*z** 3+cg00103*x**0*y**1*z** 3+cg00203*x**0*y**2*z** 3+
cg00004*x**0*y**0*z** 4+cg00104*x**0*y**1*z** 4+cg00204*x**0*y**2*z** 4+
cg00005*x**0*y**0*z** 5+cg00105*x**0*y**1*z** 5+cg00205*x**0*y**2*z** 5+
cg00006*x**0*y**0*z** 6+cg00106*x**0*y**1*z** 6+cg00206*x**0*y**2*z** 6+
cg00007*x**0*y**0*z** 7+cg00107*x**0*y**1*z** 7+cg00207*x**0*y**2*z** 7+
cg00008*x**0*y**0*z** 8+cg00108*x**0*y**1*z** 8+cg00208*x**0*y**2*z** 8+
cg00009*x**0*y**0*z** 9+cg00109*x**0*y**1*z** 9+cg00209*x**0*y**2*z** 9+
cg00010*x**0*y**0*z**10+cg00110*x**0*y**1*z**10+cg00210*x**0*y**2*z**10+
cg00011*x**0*y**0*z**11+cg00111*x**0*y**1*z**11+cg00211*x**0*y**2*z**11+
cg00012*x**0*y**0*z**12+cg00112*x**0*y**1*z**12+cg00212*x**0*y**2*z**12+
cg00013*x**0*y**0*z**13+cg00113*x**0*y**1*z**13+cg00213*x**0*y**2*z**13+
cg00014*x**0*y**0*z**14+cg00114*x**0*y**1*z**14+cg00214*x**0*y**2*z**14+
cg00015*x**0*y**0*z**15+cg00115*x**0*y**1*z**15+cg00215*x**0*y**2*z**15+
cg00016*x**0*y**0*z**16+cg00116*x**0*y**1*z**16+cg00216*x**0*y**2*z**16+
cg00017*x**0*y**0*z**17+cg00117*x**0*y**1*z**17+cg00217*x**0*y**2*z**17+
cg00018*x**0*y**0*z**18+cg00118*x**0*y**1*z**18+cg00218*x**0*y**2*z**18+
cg00019*x**0*y**0*z**19+cg00119*x**0*y**1*z**19+cg00219*x**0*y**2*z**19+
cg00020*x**0*y**0*z**20+cg00120*x**0*y**1*z**20+cg00220*x**0*y**2*z**20+
cg00021*x**0*y**0*z**21+cg00121*x**0*y**1*z**21+cg00221*x**0*y**2*z**21+
cg00022*x**0*y**0*z**22+cg00122*x**0*y**1*z**22+cg00222*x**0*y**2*z**22+
cg00023*x**0*y**0*z**23+cg00123*x**0*y**1*z**23+cg00223*x**0*y**2*z**23+
cg00024*x**0*y**0*z**24+cg00124*x**0*y**1*z**24+cg00224*x**0*y**2*z**24+
cg00025*x**0*y**0*z**25+cg00125*x**0*y**1*z**25+cg00225*x**0*y**2*z**25+
cg00026*x**0*y**0*z**26+cg00126*x**0*y**1*z**26+cg00226*x**0*y**2*z**26+
cg00027*x**0*y**0*z**27+cg00127*x**0*y**1*z**27+cg00227*x**0*y**2*z**27+
cg00028*x**0*y**0*z**28+cg00128*x**0*y**1*z**28+cg00228*x**0*y**2*z**28+
cg00029*x**0*y**0*z**29+cg00129*x**0*y**1*z**29+cg00229*x**0*y**2*z**29+
cg00030*x**0*y**0*z**30+cg00130*x**0*y**1*z**30+cg00230*x**0*y**2*z**30+
cg00031*x**0*y**0*z**31+cg00131*x**0*y**1*z**31+cg00231*x**0*y**2*z**31,
CG01**2 =
cg01000*x**1*y**0*z** 0+cg01100*x**1*y**1*z** 0+cg01200*x**1*y**2*z** 0+
cg01001*x**1*y**0*z** 1+cg01101*x**1*y**1*z** 1+cg01201*x**1*y**2*z** 1+
cg01002*x**1*y**0*z** 2+cg01102*x**1*y**1*z** 2+cg01202*x**1*y**2*z** 2+
cg01003*x**1*y**0*z** 3+cg01103*x**1*y**1*z** 3+cg01203*x**1*y**2*z** 3+
cg01004*x**1*y**0*z** 4+cg01104*x**1*y**1*z** 4+cg01204*x**1*y**2*z** 4+
cg01005*x**1*y**0*z** 5+cg01105*x**1*y**1*z** 5+cg01205*x**1*y**2*z** 5+
cg01006*x**1*y**0*z** 6+cg01106*x**1*y**1*z** 6+cg01206*x**1*y**2*z** 6+
cg01007*x**1*y**0*z** 7+cg01107*x**1*y**1*z** 7+cg01207*x**1*y**2*z** 7+
cg01008*x**1*y**0*z** 8+cg01108*x**1*y**1*z** 8+cg01208*x**1*y**2*z** 8+
cg01009*x**1*y**0*z** 9+cg01109*x**1*y**1*z** 9+cg01209*x**1*y**2*z** 9+
cg01010*x**1*y**0*z**10+cg01110*x**1*y**1*z**10+cg01210*x**1*y**2*z**10+
cg01011*x**1*y**0*z**11+cg01111*x**1*y**1*z**11+cg01211*x**1*y**2*z**11+
cg01012*x**1*y**0*z**12+cg01112*x**1*y**1*z**12+cg01212*x**1*y**2*z**12+
cg01013*x**1*y**0*z**13+cg01113*x**1*y**1*z**13+cg01213*x**1*y**2*z**13+
cg01014*x**1*y**0*z**14+cg01114*x**1*y**1*z**14+cg01214*x**1*y**2*z**14+
cg01015*x**1*y**0*z**15+cg01115*x**1*y**1*z**15+cg01215*x**1*y**2*z**15+
cg01016*x**1*y**0*z**16+cg01116*x**1*y**1*z**16+cg01216*x**1*y**2*z**16+
cg01017*x**1*y**0*z**17+cg01117*x**1*y**1*z**17+cg01217*x**1*y**2*z**17+
cg01018*x**1*y**0*z**18+cg01118*x**1*y**1*z**18+cg01218*x**1*y**2*z**18+
cg01019*x**1*y**0*z**19+cg01119*x**1*y**1*z**19+cg01219*x**1*y**2*z**19+
cg01020*x**1*y**0*z**20+cg01120*x**1*y**1*z**20+cg01220*x**1*y**2*z**20+
cg01021*x**1*y**0*z**21+cg01121*x**1*y**1*z**21+cg01221*x**1*y**2*z**21+
cg01022*x**1*y**0*z**22+cg01122*x**1*y**1*z**22+cg01222*x**1*y**2*z**22+
cg01023*x**1*y**0*z**23+cg01123*x**1*y**1*z**23+cg01223*x**1*y**2*z**23+
cg01024*x**1*y**0*z**24+cg01124*x**1*y**1*z**24+cg01224*x**1*y**2*z**24+
cg01025*x**1*y**0*z**25+cg01125*x**1*y**1*z**25+cg01225*x**1*y**2*z**25+
cg01026*x**1*y**0*z**26+cg01126*x**1*y**1*z**26+cg01226*x**1*y**2*z**26+
cg01027*x**1*y**0*z**27+cg01127*x**1*y**1*z**27+cg01227*x**1*y**2*z**27+
cg01028*x**1*y**0*z**28+cg01128*x**1*y**1*z**28+cg01228*x**1*y**2*z**28+
cg01029*x**1*y**0*z**29+cg01129*x**1*y**1*z**29+cg01229*x**1*y**2*z**29+
cg01030*x**1*y**0*z**30+cg01130*x**1*y**1*z**30+cg01230*x**1*y**2*z**30+
cg01031*x**1*y**0*z**31+cg01131*x**1*y**1*z**31+cg01231*x**1*y**2*z**31,
CG02**2 =
cg02000*x**2*y**0*z** 0+cg02100*x**2*y**1*z** 0+cg02200*x**2*y**2*z** 0+
cg02001*x**2*y**0*z** 1+cg02101*x**2*y**1*z** 1+cg02201*x**2*y**2*z** 1+
cg02002*x**2*y**0*z** 2+cg02102*x**2*y**1*z** 2+cg02202*x**2*y**2*z** 2+
cg02003*x**2*y**0*z** 3+cg02103*x**2*y**1*z** 3+cg02203*x**2*y**2*z** 3+
cg02004*x**2*y**0*z** 4+cg02104*x**2*y**1*z** 4+cg02204*x**2*y**2*z** 4+
cg02005*x**2*y**0*z** 5+cg02105*x**2*y**1*z** 5+cg02205*x**2*y**2*z** 5+
cg02006*x**2*y**0*z** 6+cg02106*x**2*y**1*z** 6+cg02206*x**2*y**2*z** 6+
cg02007*x**2*y**0*z** 7+cg02107*x**2*y**1*z** 7+cg02207*x**2*y**2*z** 7+
cg02008*x**2*y**0*z** 8+cg02108*x**2*y**1*z** 8+cg02208*x**2*y**2*z** 8+
cg02009*x**2*y**0*z** 9+cg02109*x**2*y**1*z** 9+cg02209*x**2*y**2*z** 9+
cg02010*x**2*y**0*z**10+cg02110*x**2*y**1*z**10+cg02210*x**2*y**2*z**10+
cg02011*x**2*y**0*z**11+cg02111*x**2*y**1*z**11+cg02211*x**2*y**2*z**11+
cg02012*x**2*y**0*z**12+cg02112*x**2*y**1*z**12+cg02212*x**2*y**2*z**12+
cg02013*x**2*y**0*z**13+cg02113*x**2*y**1*z**13+cg02213*x**2*y**2*z**13+
cg02014*x**2*y**0*z**14+cg02114*x**2*y**1*z**14+cg02214*x**2*y**2*z**14+
cg02015*x**2*y**0*z**15+cg02115*x**2*y**1*z**15+cg02215*x**2*y**2*z**15+
cg02016*x**2*y**0*z**16+cg02116*x**2*y**1*z**16+cg02216*x**2*y**2*z**16+
cg02017*x**2*y**0*z**17+cg02117*x**2*y**1*z**17+cg02217*x**2*y**2*z**17+
cg02018*x**2*y**0*z**18+cg02118*x**2*y**1*z**18+cg02218*x**2*y**2*z**18+
cg02019*x**2*y**0*z**19+cg02119*x**2*y**1*z**19+cg02219*x**2*y**2*z**19+
cg02020*x**2*y**0*z**20+cg02120*x**2*y**1*z**20+cg02220*x**2*y**2*z**20+
cg02021*x**2*y**0*z**21+cg02121*x**2*y**1*z**21+cg02221*x**2*y**2*z**21+
cg02022*x**2*y**0*z**22+cg02122*x**2*y**1*z**22+cg02222*x**2*y**2*z**22+
cg02023*x**2*y**0*z**23+cg02123*x**2*y**1*z**23+cg02223*x**2*y**2*z**23+
cg02024*x**2*y**0*z**24+cg02124*x**2*y**1*z**24+cg02224*x**2*y**2*z**24+
cg02025*x**2*y**0*z**25+cg02125*x**2*y**1*z**25+cg02225*x**2*y**2*z**25+
cg02026*x**2*y**0*z**26+cg02126*x**2*y**1*z**26+cg02226*x**2*y**2*z**26+
cg02027*x**2*y**0*z**27+cg02127*x**2*y**1*z**27+cg02227*x**2*y**2*z**27+
cg02028*x**2*y**0*z**28+cg02128*x**2*y**1*z**28+cg02228*x**2*y**2*z**28+
cg02029*x**2*y**0*z**29+cg02129*x**2*y**1*z**29+cg02229*x**2*y**2*z**29+
cg02030*x**2*y**0*z**30+cg02130*x**2*y**1*z**30+cg02230*x**2*y**2*z**30+
cg02031*x**2*y**0*z**31+cg02131*x**2*y**1*z**31+cg02231*x**2*y**2*z**31
] prove with [ precondition ];


(**************** CUT 398, - *****************)

ecut eqmod CH000**2+CH001**2+CH002**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -       1, z**16 -       1 ]
     prove with [ all ghosts, cuts [ 289, 325, 361 ] ],
     eqmod CH100**2+CH101**2+CH102**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  452650, z**16 -       1 ]
     prove with [ all ghosts, cuts [ 290, 326, 362 ] ],
     eqmod CH200**2+CH201**2+CH202**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2912918, z**16 -       1 ]
     prove with [ all ghosts, cuts [ 291, 327, 363 ] ],
     eqmod CH010**2+CH011**2+CH012**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -       1, z**16 - 3365568 ]
     prove with [ all ghosts, cuts [ 298, 334, 370 ] ],
     eqmod CH110**2+CH111**2+CH112**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  452650, z**16 - 3365568 ]
     prove with [ all ghosts, cuts [ 299, 335, 371 ] ],
     eqmod CH210**2+CH211**2+CH212**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2912918, z**16 - 3365568 ]
     prove with [ all ghosts, cuts [ 300, 336, 372 ] ];



(**************** CUT 399, - *****************)

ecut and [
eqmod CG10**2+CG11**2+CG12**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -  452650, z**32 -       1 ],
CG10**2 =
cg10000*x**0*y**0*z** 0+cg10100*x**0*y**1*z** 0+cg10200*x**0*y**2*z** 0+
cg10001*x**0*y**0*z** 1+cg10101*x**0*y**1*z** 1+cg10201*x**0*y**2*z** 1+
cg10002*x**0*y**0*z** 2+cg10102*x**0*y**1*z** 2+cg10202*x**0*y**2*z** 2+
cg10003*x**0*y**0*z** 3+cg10103*x**0*y**1*z** 3+cg10203*x**0*y**2*z** 3+
cg10004*x**0*y**0*z** 4+cg10104*x**0*y**1*z** 4+cg10204*x**0*y**2*z** 4+
cg10005*x**0*y**0*z** 5+cg10105*x**0*y**1*z** 5+cg10205*x**0*y**2*z** 5+
cg10006*x**0*y**0*z** 6+cg10106*x**0*y**1*z** 6+cg10206*x**0*y**2*z** 6+
cg10007*x**0*y**0*z** 7+cg10107*x**0*y**1*z** 7+cg10207*x**0*y**2*z** 7+
cg10008*x**0*y**0*z** 8+cg10108*x**0*y**1*z** 8+cg10208*x**0*y**2*z** 8+
cg10009*x**0*y**0*z** 9+cg10109*x**0*y**1*z** 9+cg10209*x**0*y**2*z** 9+
cg10010*x**0*y**0*z**10+cg10110*x**0*y**1*z**10+cg10210*x**0*y**2*z**10+
cg10011*x**0*y**0*z**11+cg10111*x**0*y**1*z**11+cg10211*x**0*y**2*z**11+
cg10012*x**0*y**0*z**12+cg10112*x**0*y**1*z**12+cg10212*x**0*y**2*z**12+
cg10013*x**0*y**0*z**13+cg10113*x**0*y**1*z**13+cg10213*x**0*y**2*z**13+
cg10014*x**0*y**0*z**14+cg10114*x**0*y**1*z**14+cg10214*x**0*y**2*z**14+
cg10015*x**0*y**0*z**15+cg10115*x**0*y**1*z**15+cg10215*x**0*y**2*z**15+
cg10016*x**0*y**0*z**16+cg10116*x**0*y**1*z**16+cg10216*x**0*y**2*z**16+
cg10017*x**0*y**0*z**17+cg10117*x**0*y**1*z**17+cg10217*x**0*y**2*z**17+
cg10018*x**0*y**0*z**18+cg10118*x**0*y**1*z**18+cg10218*x**0*y**2*z**18+
cg10019*x**0*y**0*z**19+cg10119*x**0*y**1*z**19+cg10219*x**0*y**2*z**19+
cg10020*x**0*y**0*z**20+cg10120*x**0*y**1*z**20+cg10220*x**0*y**2*z**20+
cg10021*x**0*y**0*z**21+cg10121*x**0*y**1*z**21+cg10221*x**0*y**2*z**21+
cg10022*x**0*y**0*z**22+cg10122*x**0*y**1*z**22+cg10222*x**0*y**2*z**22+
cg10023*x**0*y**0*z**23+cg10123*x**0*y**1*z**23+cg10223*x**0*y**2*z**23+
cg10024*x**0*y**0*z**24+cg10124*x**0*y**1*z**24+cg10224*x**0*y**2*z**24+
cg10025*x**0*y**0*z**25+cg10125*x**0*y**1*z**25+cg10225*x**0*y**2*z**25+
cg10026*x**0*y**0*z**26+cg10126*x**0*y**1*z**26+cg10226*x**0*y**2*z**26+
cg10027*x**0*y**0*z**27+cg10127*x**0*y**1*z**27+cg10227*x**0*y**2*z**27+
cg10028*x**0*y**0*z**28+cg10128*x**0*y**1*z**28+cg10228*x**0*y**2*z**28+
cg10029*x**0*y**0*z**29+cg10129*x**0*y**1*z**29+cg10229*x**0*y**2*z**29+
cg10030*x**0*y**0*z**30+cg10130*x**0*y**1*z**30+cg10230*x**0*y**2*z**30+
cg10031*x**0*y**0*z**31+cg10131*x**0*y**1*z**31+cg10231*x**0*y**2*z**31,
CG11**2 =
cg11000*x**1*y**0*z** 0+cg11100*x**1*y**1*z** 0+cg11200*x**1*y**2*z** 0+
cg11001*x**1*y**0*z** 1+cg11101*x**1*y**1*z** 1+cg11201*x**1*y**2*z** 1+
cg11002*x**1*y**0*z** 2+cg11102*x**1*y**1*z** 2+cg11202*x**1*y**2*z** 2+
cg11003*x**1*y**0*z** 3+cg11103*x**1*y**1*z** 3+cg11203*x**1*y**2*z** 3+
cg11004*x**1*y**0*z** 4+cg11104*x**1*y**1*z** 4+cg11204*x**1*y**2*z** 4+
cg11005*x**1*y**0*z** 5+cg11105*x**1*y**1*z** 5+cg11205*x**1*y**2*z** 5+
cg11006*x**1*y**0*z** 6+cg11106*x**1*y**1*z** 6+cg11206*x**1*y**2*z** 6+
cg11007*x**1*y**0*z** 7+cg11107*x**1*y**1*z** 7+cg11207*x**1*y**2*z** 7+
cg11008*x**1*y**0*z** 8+cg11108*x**1*y**1*z** 8+cg11208*x**1*y**2*z** 8+
cg11009*x**1*y**0*z** 9+cg11109*x**1*y**1*z** 9+cg11209*x**1*y**2*z** 9+
cg11010*x**1*y**0*z**10+cg11110*x**1*y**1*z**10+cg11210*x**1*y**2*z**10+
cg11011*x**1*y**0*z**11+cg11111*x**1*y**1*z**11+cg11211*x**1*y**2*z**11+
cg11012*x**1*y**0*z**12+cg11112*x**1*y**1*z**12+cg11212*x**1*y**2*z**12+
cg11013*x**1*y**0*z**13+cg11113*x**1*y**1*z**13+cg11213*x**1*y**2*z**13+
cg11014*x**1*y**0*z**14+cg11114*x**1*y**1*z**14+cg11214*x**1*y**2*z**14+
cg11015*x**1*y**0*z**15+cg11115*x**1*y**1*z**15+cg11215*x**1*y**2*z**15+
cg11016*x**1*y**0*z**16+cg11116*x**1*y**1*z**16+cg11216*x**1*y**2*z**16+
cg11017*x**1*y**0*z**17+cg11117*x**1*y**1*z**17+cg11217*x**1*y**2*z**17+
cg11018*x**1*y**0*z**18+cg11118*x**1*y**1*z**18+cg11218*x**1*y**2*z**18+
cg11019*x**1*y**0*z**19+cg11119*x**1*y**1*z**19+cg11219*x**1*y**2*z**19+
cg11020*x**1*y**0*z**20+cg11120*x**1*y**1*z**20+cg11220*x**1*y**2*z**20+
cg11021*x**1*y**0*z**21+cg11121*x**1*y**1*z**21+cg11221*x**1*y**2*z**21+
cg11022*x**1*y**0*z**22+cg11122*x**1*y**1*z**22+cg11222*x**1*y**2*z**22+
cg11023*x**1*y**0*z**23+cg11123*x**1*y**1*z**23+cg11223*x**1*y**2*z**23+
cg11024*x**1*y**0*z**24+cg11124*x**1*y**1*z**24+cg11224*x**1*y**2*z**24+
cg11025*x**1*y**0*z**25+cg11125*x**1*y**1*z**25+cg11225*x**1*y**2*z**25+
cg11026*x**1*y**0*z**26+cg11126*x**1*y**1*z**26+cg11226*x**1*y**2*z**26+
cg11027*x**1*y**0*z**27+cg11127*x**1*y**1*z**27+cg11227*x**1*y**2*z**27+
cg11028*x**1*y**0*z**28+cg11128*x**1*y**1*z**28+cg11228*x**1*y**2*z**28+
cg11029*x**1*y**0*z**29+cg11129*x**1*y**1*z**29+cg11229*x**1*y**2*z**29+
cg11030*x**1*y**0*z**30+cg11130*x**1*y**1*z**30+cg11230*x**1*y**2*z**30+
cg11031*x**1*y**0*z**31+cg11131*x**1*y**1*z**31+cg11231*x**1*y**2*z**31,
CG12**2 =
cg12000*x**2*y**0*z** 0+cg12100*x**2*y**1*z** 0+cg12200*x**2*y**2*z** 0+
cg12001*x**2*y**0*z** 1+cg12101*x**2*y**1*z** 1+cg12201*x**2*y**2*z** 1+
cg12002*x**2*y**0*z** 2+cg12102*x**2*y**1*z** 2+cg12202*x**2*y**2*z** 2+
cg12003*x**2*y**0*z** 3+cg12103*x**2*y**1*z** 3+cg12203*x**2*y**2*z** 3+
cg12004*x**2*y**0*z** 4+cg12104*x**2*y**1*z** 4+cg12204*x**2*y**2*z** 4+
cg12005*x**2*y**0*z** 5+cg12105*x**2*y**1*z** 5+cg12205*x**2*y**2*z** 5+
cg12006*x**2*y**0*z** 6+cg12106*x**2*y**1*z** 6+cg12206*x**2*y**2*z** 6+
cg12007*x**2*y**0*z** 7+cg12107*x**2*y**1*z** 7+cg12207*x**2*y**2*z** 7+
cg12008*x**2*y**0*z** 8+cg12108*x**2*y**1*z** 8+cg12208*x**2*y**2*z** 8+
cg12009*x**2*y**0*z** 9+cg12109*x**2*y**1*z** 9+cg12209*x**2*y**2*z** 9+
cg12010*x**2*y**0*z**10+cg12110*x**2*y**1*z**10+cg12210*x**2*y**2*z**10+
cg12011*x**2*y**0*z**11+cg12111*x**2*y**1*z**11+cg12211*x**2*y**2*z**11+
cg12012*x**2*y**0*z**12+cg12112*x**2*y**1*z**12+cg12212*x**2*y**2*z**12+
cg12013*x**2*y**0*z**13+cg12113*x**2*y**1*z**13+cg12213*x**2*y**2*z**13+
cg12014*x**2*y**0*z**14+cg12114*x**2*y**1*z**14+cg12214*x**2*y**2*z**14+
cg12015*x**2*y**0*z**15+cg12115*x**2*y**1*z**15+cg12215*x**2*y**2*z**15+
cg12016*x**2*y**0*z**16+cg12116*x**2*y**1*z**16+cg12216*x**2*y**2*z**16+
cg12017*x**2*y**0*z**17+cg12117*x**2*y**1*z**17+cg12217*x**2*y**2*z**17+
cg12018*x**2*y**0*z**18+cg12118*x**2*y**1*z**18+cg12218*x**2*y**2*z**18+
cg12019*x**2*y**0*z**19+cg12119*x**2*y**1*z**19+cg12219*x**2*y**2*z**19+
cg12020*x**2*y**0*z**20+cg12120*x**2*y**1*z**20+cg12220*x**2*y**2*z**20+
cg12021*x**2*y**0*z**21+cg12121*x**2*y**1*z**21+cg12221*x**2*y**2*z**21+
cg12022*x**2*y**0*z**22+cg12122*x**2*y**1*z**22+cg12222*x**2*y**2*z**22+
cg12023*x**2*y**0*z**23+cg12123*x**2*y**1*z**23+cg12223*x**2*y**2*z**23+
cg12024*x**2*y**0*z**24+cg12124*x**2*y**1*z**24+cg12224*x**2*y**2*z**24+
cg12025*x**2*y**0*z**25+cg12125*x**2*y**1*z**25+cg12225*x**2*y**2*z**25+
cg12026*x**2*y**0*z**26+cg12126*x**2*y**1*z**26+cg12226*x**2*y**2*z**26+
cg12027*x**2*y**0*z**27+cg12127*x**2*y**1*z**27+cg12227*x**2*y**2*z**27+
cg12028*x**2*y**0*z**28+cg12128*x**2*y**1*z**28+cg12228*x**2*y**2*z**28+
cg12029*x**2*y**0*z**29+cg12129*x**2*y**1*z**29+cg12229*x**2*y**2*z**29+
cg12030*x**2*y**0*z**30+cg12130*x**2*y**1*z**30+cg12230*x**2*y**2*z**30+
cg12031*x**2*y**0*z**31+cg12131*x**2*y**1*z**31+cg12231*x**2*y**2*z**31
] prove with [ precondition ];


(**************** CUT 400, - *****************)

ecut eqmod CH300**2+CH301**2+CH302**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2113025, z**16 -       1 ]
     prove with [ all ghosts, cuts [ 292, 328, 364 ] ],
     eqmod CH400**2+CH401**2+CH402**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3077709, z**16 -       1 ]
     prove with [ all ghosts, cuts [ 293, 329, 365 ] ],
     eqmod CH500**2+CH501**2+CH502**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 1540404, z**16 -       1 ]
     prove with [ all ghosts, cuts [ 294, 330, 366 ] ],
     eqmod CH310**2+CH311**2+CH312**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2113025, z**16 - 3365568 ]
     prove with [ all ghosts, cuts [ 301, 337, 373 ] ],
     eqmod CH410**2+CH411**2+CH412**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3077709, z**16 - 3365568 ]
     prove with [ all ghosts, cuts [ 302, 338, 374 ] ],
     eqmod CH510**2+CH511**2+CH512**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 1540404, z**16 - 3365568 ]
     prove with [ all ghosts, cuts [ 303, 339, 375 ] ];



(**************** CUT 401, - *****************)

ecut and [
eqmod CG20**2+CG21**2+CG22**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 - 2912918, z**32 -       1 ],
CG20**2 =
cg20000*x**0*y**0*z** 0+cg20100*x**0*y**1*z** 0+cg20200*x**0*y**2*z** 0+
cg20001*x**0*y**0*z** 1+cg20101*x**0*y**1*z** 1+cg20201*x**0*y**2*z** 1+
cg20002*x**0*y**0*z** 2+cg20102*x**0*y**1*z** 2+cg20202*x**0*y**2*z** 2+
cg20003*x**0*y**0*z** 3+cg20103*x**0*y**1*z** 3+cg20203*x**0*y**2*z** 3+
cg20004*x**0*y**0*z** 4+cg20104*x**0*y**1*z** 4+cg20204*x**0*y**2*z** 4+
cg20005*x**0*y**0*z** 5+cg20105*x**0*y**1*z** 5+cg20205*x**0*y**2*z** 5+
cg20006*x**0*y**0*z** 6+cg20106*x**0*y**1*z** 6+cg20206*x**0*y**2*z** 6+
cg20007*x**0*y**0*z** 7+cg20107*x**0*y**1*z** 7+cg20207*x**0*y**2*z** 7+
cg20008*x**0*y**0*z** 8+cg20108*x**0*y**1*z** 8+cg20208*x**0*y**2*z** 8+
cg20009*x**0*y**0*z** 9+cg20109*x**0*y**1*z** 9+cg20209*x**0*y**2*z** 9+
cg20010*x**0*y**0*z**10+cg20110*x**0*y**1*z**10+cg20210*x**0*y**2*z**10+
cg20011*x**0*y**0*z**11+cg20111*x**0*y**1*z**11+cg20211*x**0*y**2*z**11+
cg20012*x**0*y**0*z**12+cg20112*x**0*y**1*z**12+cg20212*x**0*y**2*z**12+
cg20013*x**0*y**0*z**13+cg20113*x**0*y**1*z**13+cg20213*x**0*y**2*z**13+
cg20014*x**0*y**0*z**14+cg20114*x**0*y**1*z**14+cg20214*x**0*y**2*z**14+
cg20015*x**0*y**0*z**15+cg20115*x**0*y**1*z**15+cg20215*x**0*y**2*z**15+
cg20016*x**0*y**0*z**16+cg20116*x**0*y**1*z**16+cg20216*x**0*y**2*z**16+
cg20017*x**0*y**0*z**17+cg20117*x**0*y**1*z**17+cg20217*x**0*y**2*z**17+
cg20018*x**0*y**0*z**18+cg20118*x**0*y**1*z**18+cg20218*x**0*y**2*z**18+
cg20019*x**0*y**0*z**19+cg20119*x**0*y**1*z**19+cg20219*x**0*y**2*z**19+
cg20020*x**0*y**0*z**20+cg20120*x**0*y**1*z**20+cg20220*x**0*y**2*z**20+
cg20021*x**0*y**0*z**21+cg20121*x**0*y**1*z**21+cg20221*x**0*y**2*z**21+
cg20022*x**0*y**0*z**22+cg20122*x**0*y**1*z**22+cg20222*x**0*y**2*z**22+
cg20023*x**0*y**0*z**23+cg20123*x**0*y**1*z**23+cg20223*x**0*y**2*z**23+
cg20024*x**0*y**0*z**24+cg20124*x**0*y**1*z**24+cg20224*x**0*y**2*z**24+
cg20025*x**0*y**0*z**25+cg20125*x**0*y**1*z**25+cg20225*x**0*y**2*z**25+
cg20026*x**0*y**0*z**26+cg20126*x**0*y**1*z**26+cg20226*x**0*y**2*z**26+
cg20027*x**0*y**0*z**27+cg20127*x**0*y**1*z**27+cg20227*x**0*y**2*z**27+
cg20028*x**0*y**0*z**28+cg20128*x**0*y**1*z**28+cg20228*x**0*y**2*z**28+
cg20029*x**0*y**0*z**29+cg20129*x**0*y**1*z**29+cg20229*x**0*y**2*z**29+
cg20030*x**0*y**0*z**30+cg20130*x**0*y**1*z**30+cg20230*x**0*y**2*z**30+
cg20031*x**0*y**0*z**31+cg20131*x**0*y**1*z**31+cg20231*x**0*y**2*z**31,
CG21**2 =
cg21000*x**1*y**0*z** 0+cg21100*x**1*y**1*z** 0+cg21200*x**1*y**2*z** 0+
cg21001*x**1*y**0*z** 1+cg21101*x**1*y**1*z** 1+cg21201*x**1*y**2*z** 1+
cg21002*x**1*y**0*z** 2+cg21102*x**1*y**1*z** 2+cg21202*x**1*y**2*z** 2+
cg21003*x**1*y**0*z** 3+cg21103*x**1*y**1*z** 3+cg21203*x**1*y**2*z** 3+
cg21004*x**1*y**0*z** 4+cg21104*x**1*y**1*z** 4+cg21204*x**1*y**2*z** 4+
cg21005*x**1*y**0*z** 5+cg21105*x**1*y**1*z** 5+cg21205*x**1*y**2*z** 5+
cg21006*x**1*y**0*z** 6+cg21106*x**1*y**1*z** 6+cg21206*x**1*y**2*z** 6+
cg21007*x**1*y**0*z** 7+cg21107*x**1*y**1*z** 7+cg21207*x**1*y**2*z** 7+
cg21008*x**1*y**0*z** 8+cg21108*x**1*y**1*z** 8+cg21208*x**1*y**2*z** 8+
cg21009*x**1*y**0*z** 9+cg21109*x**1*y**1*z** 9+cg21209*x**1*y**2*z** 9+
cg21010*x**1*y**0*z**10+cg21110*x**1*y**1*z**10+cg21210*x**1*y**2*z**10+
cg21011*x**1*y**0*z**11+cg21111*x**1*y**1*z**11+cg21211*x**1*y**2*z**11+
cg21012*x**1*y**0*z**12+cg21112*x**1*y**1*z**12+cg21212*x**1*y**2*z**12+
cg21013*x**1*y**0*z**13+cg21113*x**1*y**1*z**13+cg21213*x**1*y**2*z**13+
cg21014*x**1*y**0*z**14+cg21114*x**1*y**1*z**14+cg21214*x**1*y**2*z**14+
cg21015*x**1*y**0*z**15+cg21115*x**1*y**1*z**15+cg21215*x**1*y**2*z**15+
cg21016*x**1*y**0*z**16+cg21116*x**1*y**1*z**16+cg21216*x**1*y**2*z**16+
cg21017*x**1*y**0*z**17+cg21117*x**1*y**1*z**17+cg21217*x**1*y**2*z**17+
cg21018*x**1*y**0*z**18+cg21118*x**1*y**1*z**18+cg21218*x**1*y**2*z**18+
cg21019*x**1*y**0*z**19+cg21119*x**1*y**1*z**19+cg21219*x**1*y**2*z**19+
cg21020*x**1*y**0*z**20+cg21120*x**1*y**1*z**20+cg21220*x**1*y**2*z**20+
cg21021*x**1*y**0*z**21+cg21121*x**1*y**1*z**21+cg21221*x**1*y**2*z**21+
cg21022*x**1*y**0*z**22+cg21122*x**1*y**1*z**22+cg21222*x**1*y**2*z**22+
cg21023*x**1*y**0*z**23+cg21123*x**1*y**1*z**23+cg21223*x**1*y**2*z**23+
cg21024*x**1*y**0*z**24+cg21124*x**1*y**1*z**24+cg21224*x**1*y**2*z**24+
cg21025*x**1*y**0*z**25+cg21125*x**1*y**1*z**25+cg21225*x**1*y**2*z**25+
cg21026*x**1*y**0*z**26+cg21126*x**1*y**1*z**26+cg21226*x**1*y**2*z**26+
cg21027*x**1*y**0*z**27+cg21127*x**1*y**1*z**27+cg21227*x**1*y**2*z**27+
cg21028*x**1*y**0*z**28+cg21128*x**1*y**1*z**28+cg21228*x**1*y**2*z**28+
cg21029*x**1*y**0*z**29+cg21129*x**1*y**1*z**29+cg21229*x**1*y**2*z**29+
cg21030*x**1*y**0*z**30+cg21130*x**1*y**1*z**30+cg21230*x**1*y**2*z**30+
cg21031*x**1*y**0*z**31+cg21131*x**1*y**1*z**31+cg21231*x**1*y**2*z**31,
CG22**2 =
cg22000*x**2*y**0*z** 0+cg22100*x**2*y**1*z** 0+cg22200*x**2*y**2*z** 0+
cg22001*x**2*y**0*z** 1+cg22101*x**2*y**1*z** 1+cg22201*x**2*y**2*z** 1+
cg22002*x**2*y**0*z** 2+cg22102*x**2*y**1*z** 2+cg22202*x**2*y**2*z** 2+
cg22003*x**2*y**0*z** 3+cg22103*x**2*y**1*z** 3+cg22203*x**2*y**2*z** 3+
cg22004*x**2*y**0*z** 4+cg22104*x**2*y**1*z** 4+cg22204*x**2*y**2*z** 4+
cg22005*x**2*y**0*z** 5+cg22105*x**2*y**1*z** 5+cg22205*x**2*y**2*z** 5+
cg22006*x**2*y**0*z** 6+cg22106*x**2*y**1*z** 6+cg22206*x**2*y**2*z** 6+
cg22007*x**2*y**0*z** 7+cg22107*x**2*y**1*z** 7+cg22207*x**2*y**2*z** 7+
cg22008*x**2*y**0*z** 8+cg22108*x**2*y**1*z** 8+cg22208*x**2*y**2*z** 8+
cg22009*x**2*y**0*z** 9+cg22109*x**2*y**1*z** 9+cg22209*x**2*y**2*z** 9+
cg22010*x**2*y**0*z**10+cg22110*x**2*y**1*z**10+cg22210*x**2*y**2*z**10+
cg22011*x**2*y**0*z**11+cg22111*x**2*y**1*z**11+cg22211*x**2*y**2*z**11+
cg22012*x**2*y**0*z**12+cg22112*x**2*y**1*z**12+cg22212*x**2*y**2*z**12+
cg22013*x**2*y**0*z**13+cg22113*x**2*y**1*z**13+cg22213*x**2*y**2*z**13+
cg22014*x**2*y**0*z**14+cg22114*x**2*y**1*z**14+cg22214*x**2*y**2*z**14+
cg22015*x**2*y**0*z**15+cg22115*x**2*y**1*z**15+cg22215*x**2*y**2*z**15+
cg22016*x**2*y**0*z**16+cg22116*x**2*y**1*z**16+cg22216*x**2*y**2*z**16+
cg22017*x**2*y**0*z**17+cg22117*x**2*y**1*z**17+cg22217*x**2*y**2*z**17+
cg22018*x**2*y**0*z**18+cg22118*x**2*y**1*z**18+cg22218*x**2*y**2*z**18+
cg22019*x**2*y**0*z**19+cg22119*x**2*y**1*z**19+cg22219*x**2*y**2*z**19+
cg22020*x**2*y**0*z**20+cg22120*x**2*y**1*z**20+cg22220*x**2*y**2*z**20+
cg22021*x**2*y**0*z**21+cg22121*x**2*y**1*z**21+cg22221*x**2*y**2*z**21+
cg22022*x**2*y**0*z**22+cg22122*x**2*y**1*z**22+cg22222*x**2*y**2*z**22+
cg22023*x**2*y**0*z**23+cg22123*x**2*y**1*z**23+cg22223*x**2*y**2*z**23+
cg22024*x**2*y**0*z**24+cg22124*x**2*y**1*z**24+cg22224*x**2*y**2*z**24+
cg22025*x**2*y**0*z**25+cg22125*x**2*y**1*z**25+cg22225*x**2*y**2*z**25+
cg22026*x**2*y**0*z**26+cg22126*x**2*y**1*z**26+cg22226*x**2*y**2*z**26+
cg22027*x**2*y**0*z**27+cg22127*x**2*y**1*z**27+cg22227*x**2*y**2*z**27+
cg22028*x**2*y**0*z**28+cg22128*x**2*y**1*z**28+cg22228*x**2*y**2*z**28+
cg22029*x**2*y**0*z**29+cg22129*x**2*y**1*z**29+cg22229*x**2*y**2*z**29+
cg22030*x**2*y**0*z**30+cg22130*x**2*y**1*z**30+cg22230*x**2*y**2*z**30+
cg22031*x**2*y**0*z**31+cg22131*x**2*y**1*z**31+cg22231*x**2*y**2*z**31
] prove with [ precondition ];


(**************** CUT 402, - *****************)

ecut eqmod CH600**2+CH601**2+CH602**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3117017, z**16 -       1 ]
     prove with [ all ghosts, cuts [ 295, 331, 367 ] ],
     eqmod CH700**2+CH701**2+CH702**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  543301, z**16 -       1 ]
     prove with [ all ghosts, cuts [ 296, 332, 368 ] ],
     eqmod CH800**2+CH801**2+CH802**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3070820, z**16 -       1 ]
     prove with [ all ghosts, cuts [ 297, 333, 369 ] ],
     eqmod CH610**2+CH611**2+CH612**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3117017, z**16 - 3365568 ]
     prove with [ all ghosts, cuts [ 304, 340, 376 ] ],
     eqmod CH710**2+CH711**2+CH712**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  543301, z**16 - 3365568 ]
     prove with [ all ghosts, cuts [ 305, 341, 377 ] ],
     eqmod CH810**2+CH811**2+CH812**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3070820, z**16 - 3365568 ]
     prove with [ all ghosts, cuts [ 306, 342, 378 ] ];



(**************** CUT 403, - *****************)

ecut and [
eqmod CG30**2+CG31**2+CG32**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -       1, z**32 - 3365568 ],
CG30**2 =
cg30000*x**0*y**0*z** 0+cg30100*x**0*y**1*z** 0+cg30200*x**0*y**2*z** 0+
cg30001*x**0*y**0*z** 1+cg30101*x**0*y**1*z** 1+cg30201*x**0*y**2*z** 1+
cg30002*x**0*y**0*z** 2+cg30102*x**0*y**1*z** 2+cg30202*x**0*y**2*z** 2+
cg30003*x**0*y**0*z** 3+cg30103*x**0*y**1*z** 3+cg30203*x**0*y**2*z** 3+
cg30004*x**0*y**0*z** 4+cg30104*x**0*y**1*z** 4+cg30204*x**0*y**2*z** 4+
cg30005*x**0*y**0*z** 5+cg30105*x**0*y**1*z** 5+cg30205*x**0*y**2*z** 5+
cg30006*x**0*y**0*z** 6+cg30106*x**0*y**1*z** 6+cg30206*x**0*y**2*z** 6+
cg30007*x**0*y**0*z** 7+cg30107*x**0*y**1*z** 7+cg30207*x**0*y**2*z** 7+
cg30008*x**0*y**0*z** 8+cg30108*x**0*y**1*z** 8+cg30208*x**0*y**2*z** 8+
cg30009*x**0*y**0*z** 9+cg30109*x**0*y**1*z** 9+cg30209*x**0*y**2*z** 9+
cg30010*x**0*y**0*z**10+cg30110*x**0*y**1*z**10+cg30210*x**0*y**2*z**10+
cg30011*x**0*y**0*z**11+cg30111*x**0*y**1*z**11+cg30211*x**0*y**2*z**11+
cg30012*x**0*y**0*z**12+cg30112*x**0*y**1*z**12+cg30212*x**0*y**2*z**12+
cg30013*x**0*y**0*z**13+cg30113*x**0*y**1*z**13+cg30213*x**0*y**2*z**13+
cg30014*x**0*y**0*z**14+cg30114*x**0*y**1*z**14+cg30214*x**0*y**2*z**14+
cg30015*x**0*y**0*z**15+cg30115*x**0*y**1*z**15+cg30215*x**0*y**2*z**15+
cg30016*x**0*y**0*z**16+cg30116*x**0*y**1*z**16+cg30216*x**0*y**2*z**16+
cg30017*x**0*y**0*z**17+cg30117*x**0*y**1*z**17+cg30217*x**0*y**2*z**17+
cg30018*x**0*y**0*z**18+cg30118*x**0*y**1*z**18+cg30218*x**0*y**2*z**18+
cg30019*x**0*y**0*z**19+cg30119*x**0*y**1*z**19+cg30219*x**0*y**2*z**19+
cg30020*x**0*y**0*z**20+cg30120*x**0*y**1*z**20+cg30220*x**0*y**2*z**20+
cg30021*x**0*y**0*z**21+cg30121*x**0*y**1*z**21+cg30221*x**0*y**2*z**21+
cg30022*x**0*y**0*z**22+cg30122*x**0*y**1*z**22+cg30222*x**0*y**2*z**22+
cg30023*x**0*y**0*z**23+cg30123*x**0*y**1*z**23+cg30223*x**0*y**2*z**23+
cg30024*x**0*y**0*z**24+cg30124*x**0*y**1*z**24+cg30224*x**0*y**2*z**24+
cg30025*x**0*y**0*z**25+cg30125*x**0*y**1*z**25+cg30225*x**0*y**2*z**25+
cg30026*x**0*y**0*z**26+cg30126*x**0*y**1*z**26+cg30226*x**0*y**2*z**26+
cg30027*x**0*y**0*z**27+cg30127*x**0*y**1*z**27+cg30227*x**0*y**2*z**27+
cg30028*x**0*y**0*z**28+cg30128*x**0*y**1*z**28+cg30228*x**0*y**2*z**28+
cg30029*x**0*y**0*z**29+cg30129*x**0*y**1*z**29+cg30229*x**0*y**2*z**29+
cg30030*x**0*y**0*z**30+cg30130*x**0*y**1*z**30+cg30230*x**0*y**2*z**30+
cg30031*x**0*y**0*z**31+cg30131*x**0*y**1*z**31+cg30231*x**0*y**2*z**31,
CG31**2 =
cg31000*x**1*y**0*z** 0+cg31100*x**1*y**1*z** 0+cg31200*x**1*y**2*z** 0+
cg31001*x**1*y**0*z** 1+cg31101*x**1*y**1*z** 1+cg31201*x**1*y**2*z** 1+
cg31002*x**1*y**0*z** 2+cg31102*x**1*y**1*z** 2+cg31202*x**1*y**2*z** 2+
cg31003*x**1*y**0*z** 3+cg31103*x**1*y**1*z** 3+cg31203*x**1*y**2*z** 3+
cg31004*x**1*y**0*z** 4+cg31104*x**1*y**1*z** 4+cg31204*x**1*y**2*z** 4+
cg31005*x**1*y**0*z** 5+cg31105*x**1*y**1*z** 5+cg31205*x**1*y**2*z** 5+
cg31006*x**1*y**0*z** 6+cg31106*x**1*y**1*z** 6+cg31206*x**1*y**2*z** 6+
cg31007*x**1*y**0*z** 7+cg31107*x**1*y**1*z** 7+cg31207*x**1*y**2*z** 7+
cg31008*x**1*y**0*z** 8+cg31108*x**1*y**1*z** 8+cg31208*x**1*y**2*z** 8+
cg31009*x**1*y**0*z** 9+cg31109*x**1*y**1*z** 9+cg31209*x**1*y**2*z** 9+
cg31010*x**1*y**0*z**10+cg31110*x**1*y**1*z**10+cg31210*x**1*y**2*z**10+
cg31011*x**1*y**0*z**11+cg31111*x**1*y**1*z**11+cg31211*x**1*y**2*z**11+
cg31012*x**1*y**0*z**12+cg31112*x**1*y**1*z**12+cg31212*x**1*y**2*z**12+
cg31013*x**1*y**0*z**13+cg31113*x**1*y**1*z**13+cg31213*x**1*y**2*z**13+
cg31014*x**1*y**0*z**14+cg31114*x**1*y**1*z**14+cg31214*x**1*y**2*z**14+
cg31015*x**1*y**0*z**15+cg31115*x**1*y**1*z**15+cg31215*x**1*y**2*z**15+
cg31016*x**1*y**0*z**16+cg31116*x**1*y**1*z**16+cg31216*x**1*y**2*z**16+
cg31017*x**1*y**0*z**17+cg31117*x**1*y**1*z**17+cg31217*x**1*y**2*z**17+
cg31018*x**1*y**0*z**18+cg31118*x**1*y**1*z**18+cg31218*x**1*y**2*z**18+
cg31019*x**1*y**0*z**19+cg31119*x**1*y**1*z**19+cg31219*x**1*y**2*z**19+
cg31020*x**1*y**0*z**20+cg31120*x**1*y**1*z**20+cg31220*x**1*y**2*z**20+
cg31021*x**1*y**0*z**21+cg31121*x**1*y**1*z**21+cg31221*x**1*y**2*z**21+
cg31022*x**1*y**0*z**22+cg31122*x**1*y**1*z**22+cg31222*x**1*y**2*z**22+
cg31023*x**1*y**0*z**23+cg31123*x**1*y**1*z**23+cg31223*x**1*y**2*z**23+
cg31024*x**1*y**0*z**24+cg31124*x**1*y**1*z**24+cg31224*x**1*y**2*z**24+
cg31025*x**1*y**0*z**25+cg31125*x**1*y**1*z**25+cg31225*x**1*y**2*z**25+
cg31026*x**1*y**0*z**26+cg31126*x**1*y**1*z**26+cg31226*x**1*y**2*z**26+
cg31027*x**1*y**0*z**27+cg31127*x**1*y**1*z**27+cg31227*x**1*y**2*z**27+
cg31028*x**1*y**0*z**28+cg31128*x**1*y**1*z**28+cg31228*x**1*y**2*z**28+
cg31029*x**1*y**0*z**29+cg31129*x**1*y**1*z**29+cg31229*x**1*y**2*z**29+
cg31030*x**1*y**0*z**30+cg31130*x**1*y**1*z**30+cg31230*x**1*y**2*z**30+
cg31031*x**1*y**0*z**31+cg31131*x**1*y**1*z**31+cg31231*x**1*y**2*z**31,
CG32**2 =
cg32000*x**2*y**0*z** 0+cg32100*x**2*y**1*z** 0+cg32200*x**2*y**2*z** 0+
cg32001*x**2*y**0*z** 1+cg32101*x**2*y**1*z** 1+cg32201*x**2*y**2*z** 1+
cg32002*x**2*y**0*z** 2+cg32102*x**2*y**1*z** 2+cg32202*x**2*y**2*z** 2+
cg32003*x**2*y**0*z** 3+cg32103*x**2*y**1*z** 3+cg32203*x**2*y**2*z** 3+
cg32004*x**2*y**0*z** 4+cg32104*x**2*y**1*z** 4+cg32204*x**2*y**2*z** 4+
cg32005*x**2*y**0*z** 5+cg32105*x**2*y**1*z** 5+cg32205*x**2*y**2*z** 5+
cg32006*x**2*y**0*z** 6+cg32106*x**2*y**1*z** 6+cg32206*x**2*y**2*z** 6+
cg32007*x**2*y**0*z** 7+cg32107*x**2*y**1*z** 7+cg32207*x**2*y**2*z** 7+
cg32008*x**2*y**0*z** 8+cg32108*x**2*y**1*z** 8+cg32208*x**2*y**2*z** 8+
cg32009*x**2*y**0*z** 9+cg32109*x**2*y**1*z** 9+cg32209*x**2*y**2*z** 9+
cg32010*x**2*y**0*z**10+cg32110*x**2*y**1*z**10+cg32210*x**2*y**2*z**10+
cg32011*x**2*y**0*z**11+cg32111*x**2*y**1*z**11+cg32211*x**2*y**2*z**11+
cg32012*x**2*y**0*z**12+cg32112*x**2*y**1*z**12+cg32212*x**2*y**2*z**12+
cg32013*x**2*y**0*z**13+cg32113*x**2*y**1*z**13+cg32213*x**2*y**2*z**13+
cg32014*x**2*y**0*z**14+cg32114*x**2*y**1*z**14+cg32214*x**2*y**2*z**14+
cg32015*x**2*y**0*z**15+cg32115*x**2*y**1*z**15+cg32215*x**2*y**2*z**15+
cg32016*x**2*y**0*z**16+cg32116*x**2*y**1*z**16+cg32216*x**2*y**2*z**16+
cg32017*x**2*y**0*z**17+cg32117*x**2*y**1*z**17+cg32217*x**2*y**2*z**17+
cg32018*x**2*y**0*z**18+cg32118*x**2*y**1*z**18+cg32218*x**2*y**2*z**18+
cg32019*x**2*y**0*z**19+cg32119*x**2*y**1*z**19+cg32219*x**2*y**2*z**19+
cg32020*x**2*y**0*z**20+cg32120*x**2*y**1*z**20+cg32220*x**2*y**2*z**20+
cg32021*x**2*y**0*z**21+cg32121*x**2*y**1*z**21+cg32221*x**2*y**2*z**21+
cg32022*x**2*y**0*z**22+cg32122*x**2*y**1*z**22+cg32222*x**2*y**2*z**22+
cg32023*x**2*y**0*z**23+cg32123*x**2*y**1*z**23+cg32223*x**2*y**2*z**23+
cg32024*x**2*y**0*z**24+cg32124*x**2*y**1*z**24+cg32224*x**2*y**2*z**24+
cg32025*x**2*y**0*z**25+cg32125*x**2*y**1*z**25+cg32225*x**2*y**2*z**25+
cg32026*x**2*y**0*z**26+cg32126*x**2*y**1*z**26+cg32226*x**2*y**2*z**26+
cg32027*x**2*y**0*z**27+cg32127*x**2*y**1*z**27+cg32227*x**2*y**2*z**27+
cg32028*x**2*y**0*z**28+cg32128*x**2*y**1*z**28+cg32228*x**2*y**2*z**28+
cg32029*x**2*y**0*z**29+cg32129*x**2*y**1*z**29+cg32229*x**2*y**2*z**29+
cg32030*x**2*y**0*z**30+cg32130*x**2*y**1*z**30+cg32230*x**2*y**2*z**30+
cg32031*x**2*y**0*z**31+cg32131*x**2*y**1*z**31+cg32231*x**2*y**2*z**31
] prove with [ precondition ];


(**************** CUT 404, - *****************)

ecut eqmod CH020**2+CH021**2+CH022**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -       1, z**16 - 2630352 ]
     prove with [ all ghosts, cuts [ 307, 343, 379 ] ],
     eqmod CH120**2+CH121**2+CH122**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  452650, z**16 - 2630352 ]
     prove with [ all ghosts, cuts [ 308, 344, 380 ] ],
     eqmod CH220**2+CH221**2+CH222**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2912918, z**16 - 2630352 ]
     prove with [ all ghosts, cuts [ 309, 345, 381 ] ],
     eqmod CH030**2+CH031**2+CH032**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -       1, z**16 -  735217 ]
     prove with [ all ghosts, cuts [ 316, 352, 388 ] ],
     eqmod CH130**2+CH131**2+CH132**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  452650, z**16 -  735217 ]
     prove with [ all ghosts, cuts [ 317, 353, 389 ] ],
     eqmod CH230**2+CH231**2+CH232**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2912918, z**16 -  735217 ]
     prove with [ all ghosts, cuts [ 318, 354, 390 ] ];



(**************** CUT 405, - *****************)

ecut and [
eqmod CG40**2+CG41**2+CG42**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 -  452650, z**32 - 3365568 ],
CG40**2 =
cg40000*x**0*y**0*z** 0+cg40100*x**0*y**1*z** 0+cg40200*x**0*y**2*z** 0+
cg40001*x**0*y**0*z** 1+cg40101*x**0*y**1*z** 1+cg40201*x**0*y**2*z** 1+
cg40002*x**0*y**0*z** 2+cg40102*x**0*y**1*z** 2+cg40202*x**0*y**2*z** 2+
cg40003*x**0*y**0*z** 3+cg40103*x**0*y**1*z** 3+cg40203*x**0*y**2*z** 3+
cg40004*x**0*y**0*z** 4+cg40104*x**0*y**1*z** 4+cg40204*x**0*y**2*z** 4+
cg40005*x**0*y**0*z** 5+cg40105*x**0*y**1*z** 5+cg40205*x**0*y**2*z** 5+
cg40006*x**0*y**0*z** 6+cg40106*x**0*y**1*z** 6+cg40206*x**0*y**2*z** 6+
cg40007*x**0*y**0*z** 7+cg40107*x**0*y**1*z** 7+cg40207*x**0*y**2*z** 7+
cg40008*x**0*y**0*z** 8+cg40108*x**0*y**1*z** 8+cg40208*x**0*y**2*z** 8+
cg40009*x**0*y**0*z** 9+cg40109*x**0*y**1*z** 9+cg40209*x**0*y**2*z** 9+
cg40010*x**0*y**0*z**10+cg40110*x**0*y**1*z**10+cg40210*x**0*y**2*z**10+
cg40011*x**0*y**0*z**11+cg40111*x**0*y**1*z**11+cg40211*x**0*y**2*z**11+
cg40012*x**0*y**0*z**12+cg40112*x**0*y**1*z**12+cg40212*x**0*y**2*z**12+
cg40013*x**0*y**0*z**13+cg40113*x**0*y**1*z**13+cg40213*x**0*y**2*z**13+
cg40014*x**0*y**0*z**14+cg40114*x**0*y**1*z**14+cg40214*x**0*y**2*z**14+
cg40015*x**0*y**0*z**15+cg40115*x**0*y**1*z**15+cg40215*x**0*y**2*z**15+
cg40016*x**0*y**0*z**16+cg40116*x**0*y**1*z**16+cg40216*x**0*y**2*z**16+
cg40017*x**0*y**0*z**17+cg40117*x**0*y**1*z**17+cg40217*x**0*y**2*z**17+
cg40018*x**0*y**0*z**18+cg40118*x**0*y**1*z**18+cg40218*x**0*y**2*z**18+
cg40019*x**0*y**0*z**19+cg40119*x**0*y**1*z**19+cg40219*x**0*y**2*z**19+
cg40020*x**0*y**0*z**20+cg40120*x**0*y**1*z**20+cg40220*x**0*y**2*z**20+
cg40021*x**0*y**0*z**21+cg40121*x**0*y**1*z**21+cg40221*x**0*y**2*z**21+
cg40022*x**0*y**0*z**22+cg40122*x**0*y**1*z**22+cg40222*x**0*y**2*z**22+
cg40023*x**0*y**0*z**23+cg40123*x**0*y**1*z**23+cg40223*x**0*y**2*z**23+
cg40024*x**0*y**0*z**24+cg40124*x**0*y**1*z**24+cg40224*x**0*y**2*z**24+
cg40025*x**0*y**0*z**25+cg40125*x**0*y**1*z**25+cg40225*x**0*y**2*z**25+
cg40026*x**0*y**0*z**26+cg40126*x**0*y**1*z**26+cg40226*x**0*y**2*z**26+
cg40027*x**0*y**0*z**27+cg40127*x**0*y**1*z**27+cg40227*x**0*y**2*z**27+
cg40028*x**0*y**0*z**28+cg40128*x**0*y**1*z**28+cg40228*x**0*y**2*z**28+
cg40029*x**0*y**0*z**29+cg40129*x**0*y**1*z**29+cg40229*x**0*y**2*z**29+
cg40030*x**0*y**0*z**30+cg40130*x**0*y**1*z**30+cg40230*x**0*y**2*z**30+
cg40031*x**0*y**0*z**31+cg40131*x**0*y**1*z**31+cg40231*x**0*y**2*z**31,
CG41**2 =
cg41000*x**1*y**0*z** 0+cg41100*x**1*y**1*z** 0+cg41200*x**1*y**2*z** 0+
cg41001*x**1*y**0*z** 1+cg41101*x**1*y**1*z** 1+cg41201*x**1*y**2*z** 1+
cg41002*x**1*y**0*z** 2+cg41102*x**1*y**1*z** 2+cg41202*x**1*y**2*z** 2+
cg41003*x**1*y**0*z** 3+cg41103*x**1*y**1*z** 3+cg41203*x**1*y**2*z** 3+
cg41004*x**1*y**0*z** 4+cg41104*x**1*y**1*z** 4+cg41204*x**1*y**2*z** 4+
cg41005*x**1*y**0*z** 5+cg41105*x**1*y**1*z** 5+cg41205*x**1*y**2*z** 5+
cg41006*x**1*y**0*z** 6+cg41106*x**1*y**1*z** 6+cg41206*x**1*y**2*z** 6+
cg41007*x**1*y**0*z** 7+cg41107*x**1*y**1*z** 7+cg41207*x**1*y**2*z** 7+
cg41008*x**1*y**0*z** 8+cg41108*x**1*y**1*z** 8+cg41208*x**1*y**2*z** 8+
cg41009*x**1*y**0*z** 9+cg41109*x**1*y**1*z** 9+cg41209*x**1*y**2*z** 9+
cg41010*x**1*y**0*z**10+cg41110*x**1*y**1*z**10+cg41210*x**1*y**2*z**10+
cg41011*x**1*y**0*z**11+cg41111*x**1*y**1*z**11+cg41211*x**1*y**2*z**11+
cg41012*x**1*y**0*z**12+cg41112*x**1*y**1*z**12+cg41212*x**1*y**2*z**12+
cg41013*x**1*y**0*z**13+cg41113*x**1*y**1*z**13+cg41213*x**1*y**2*z**13+
cg41014*x**1*y**0*z**14+cg41114*x**1*y**1*z**14+cg41214*x**1*y**2*z**14+
cg41015*x**1*y**0*z**15+cg41115*x**1*y**1*z**15+cg41215*x**1*y**2*z**15+
cg41016*x**1*y**0*z**16+cg41116*x**1*y**1*z**16+cg41216*x**1*y**2*z**16+
cg41017*x**1*y**0*z**17+cg41117*x**1*y**1*z**17+cg41217*x**1*y**2*z**17+
cg41018*x**1*y**0*z**18+cg41118*x**1*y**1*z**18+cg41218*x**1*y**2*z**18+
cg41019*x**1*y**0*z**19+cg41119*x**1*y**1*z**19+cg41219*x**1*y**2*z**19+
cg41020*x**1*y**0*z**20+cg41120*x**1*y**1*z**20+cg41220*x**1*y**2*z**20+
cg41021*x**1*y**0*z**21+cg41121*x**1*y**1*z**21+cg41221*x**1*y**2*z**21+
cg41022*x**1*y**0*z**22+cg41122*x**1*y**1*z**22+cg41222*x**1*y**2*z**22+
cg41023*x**1*y**0*z**23+cg41123*x**1*y**1*z**23+cg41223*x**1*y**2*z**23+
cg41024*x**1*y**0*z**24+cg41124*x**1*y**1*z**24+cg41224*x**1*y**2*z**24+
cg41025*x**1*y**0*z**25+cg41125*x**1*y**1*z**25+cg41225*x**1*y**2*z**25+
cg41026*x**1*y**0*z**26+cg41126*x**1*y**1*z**26+cg41226*x**1*y**2*z**26+
cg41027*x**1*y**0*z**27+cg41127*x**1*y**1*z**27+cg41227*x**1*y**2*z**27+
cg41028*x**1*y**0*z**28+cg41128*x**1*y**1*z**28+cg41228*x**1*y**2*z**28+
cg41029*x**1*y**0*z**29+cg41129*x**1*y**1*z**29+cg41229*x**1*y**2*z**29+
cg41030*x**1*y**0*z**30+cg41130*x**1*y**1*z**30+cg41230*x**1*y**2*z**30+
cg41031*x**1*y**0*z**31+cg41131*x**1*y**1*z**31+cg41231*x**1*y**2*z**31,
CG42**2 =
cg42000*x**2*y**0*z** 0+cg42100*x**2*y**1*z** 0+cg42200*x**2*y**2*z** 0+
cg42001*x**2*y**0*z** 1+cg42101*x**2*y**1*z** 1+cg42201*x**2*y**2*z** 1+
cg42002*x**2*y**0*z** 2+cg42102*x**2*y**1*z** 2+cg42202*x**2*y**2*z** 2+
cg42003*x**2*y**0*z** 3+cg42103*x**2*y**1*z** 3+cg42203*x**2*y**2*z** 3+
cg42004*x**2*y**0*z** 4+cg42104*x**2*y**1*z** 4+cg42204*x**2*y**2*z** 4+
cg42005*x**2*y**0*z** 5+cg42105*x**2*y**1*z** 5+cg42205*x**2*y**2*z** 5+
cg42006*x**2*y**0*z** 6+cg42106*x**2*y**1*z** 6+cg42206*x**2*y**2*z** 6+
cg42007*x**2*y**0*z** 7+cg42107*x**2*y**1*z** 7+cg42207*x**2*y**2*z** 7+
cg42008*x**2*y**0*z** 8+cg42108*x**2*y**1*z** 8+cg42208*x**2*y**2*z** 8+
cg42009*x**2*y**0*z** 9+cg42109*x**2*y**1*z** 9+cg42209*x**2*y**2*z** 9+
cg42010*x**2*y**0*z**10+cg42110*x**2*y**1*z**10+cg42210*x**2*y**2*z**10+
cg42011*x**2*y**0*z**11+cg42111*x**2*y**1*z**11+cg42211*x**2*y**2*z**11+
cg42012*x**2*y**0*z**12+cg42112*x**2*y**1*z**12+cg42212*x**2*y**2*z**12+
cg42013*x**2*y**0*z**13+cg42113*x**2*y**1*z**13+cg42213*x**2*y**2*z**13+
cg42014*x**2*y**0*z**14+cg42114*x**2*y**1*z**14+cg42214*x**2*y**2*z**14+
cg42015*x**2*y**0*z**15+cg42115*x**2*y**1*z**15+cg42215*x**2*y**2*z**15+
cg42016*x**2*y**0*z**16+cg42116*x**2*y**1*z**16+cg42216*x**2*y**2*z**16+
cg42017*x**2*y**0*z**17+cg42117*x**2*y**1*z**17+cg42217*x**2*y**2*z**17+
cg42018*x**2*y**0*z**18+cg42118*x**2*y**1*z**18+cg42218*x**2*y**2*z**18+
cg42019*x**2*y**0*z**19+cg42119*x**2*y**1*z**19+cg42219*x**2*y**2*z**19+
cg42020*x**2*y**0*z**20+cg42120*x**2*y**1*z**20+cg42220*x**2*y**2*z**20+
cg42021*x**2*y**0*z**21+cg42121*x**2*y**1*z**21+cg42221*x**2*y**2*z**21+
cg42022*x**2*y**0*z**22+cg42122*x**2*y**1*z**22+cg42222*x**2*y**2*z**22+
cg42023*x**2*y**0*z**23+cg42123*x**2*y**1*z**23+cg42223*x**2*y**2*z**23+
cg42024*x**2*y**0*z**24+cg42124*x**2*y**1*z**24+cg42224*x**2*y**2*z**24+
cg42025*x**2*y**0*z**25+cg42125*x**2*y**1*z**25+cg42225*x**2*y**2*z**25+
cg42026*x**2*y**0*z**26+cg42126*x**2*y**1*z**26+cg42226*x**2*y**2*z**26+
cg42027*x**2*y**0*z**27+cg42127*x**2*y**1*z**27+cg42227*x**2*y**2*z**27+
cg42028*x**2*y**0*z**28+cg42128*x**2*y**1*z**28+cg42228*x**2*y**2*z**28+
cg42029*x**2*y**0*z**29+cg42129*x**2*y**1*z**29+cg42229*x**2*y**2*z**29+
cg42030*x**2*y**0*z**30+cg42130*x**2*y**1*z**30+cg42230*x**2*y**2*z**30+
cg42031*x**2*y**0*z**31+cg42131*x**2*y**1*z**31+cg42231*x**2*y**2*z**31
] prove with [ precondition ];


(**************** CUT 406, - *****************)

ecut eqmod CH320**2+CH321**2+CH322**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2113025, z**16 - 2630352 ]
     prove with [ all ghosts, cuts [ 310, 346, 382 ] ],
     eqmod CH420**2+CH421**2+CH422**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3077709, z**16 - 2630352 ]
     prove with [ all ghosts, cuts [ 311, 347, 383 ] ],
     eqmod CH520**2+CH521**2+CH522**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 1540404, z**16 - 2630352 ]
     prove with [ all ghosts, cuts [ 312, 348, 384 ] ],
     eqmod CH330**2+CH331**2+CH332**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2113025, z**16 -  735217 ]
     prove with [ all ghosts, cuts [ 319, 355, 391 ] ],
     eqmod CH430**2+CH431**2+CH432**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3077709, z**16 -  735217 ]
     prove with [ all ghosts, cuts [ 320, 356, 392 ] ],
     eqmod CH530**2+CH531**2+CH532**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 1540404, z**16 -  735217 ]
     prove with [ all ghosts, cuts [ 321, 357, 393 ] ];



(**************** CUT 407, - *****************)

ecut and [
eqmod CG50**2+CG51**2+CG52**2 CF0**2+CF1**2+CF2**2
      [ 3365569, y**3 - 2912918, z**32 - 3365568 ],
CG50**2 =
cg50000*x**0*y**0*z** 0+cg50100*x**0*y**1*z** 0+cg50200*x**0*y**2*z** 0+
cg50001*x**0*y**0*z** 1+cg50101*x**0*y**1*z** 1+cg50201*x**0*y**2*z** 1+
cg50002*x**0*y**0*z** 2+cg50102*x**0*y**1*z** 2+cg50202*x**0*y**2*z** 2+
cg50003*x**0*y**0*z** 3+cg50103*x**0*y**1*z** 3+cg50203*x**0*y**2*z** 3+
cg50004*x**0*y**0*z** 4+cg50104*x**0*y**1*z** 4+cg50204*x**0*y**2*z** 4+
cg50005*x**0*y**0*z** 5+cg50105*x**0*y**1*z** 5+cg50205*x**0*y**2*z** 5+
cg50006*x**0*y**0*z** 6+cg50106*x**0*y**1*z** 6+cg50206*x**0*y**2*z** 6+
cg50007*x**0*y**0*z** 7+cg50107*x**0*y**1*z** 7+cg50207*x**0*y**2*z** 7+
cg50008*x**0*y**0*z** 8+cg50108*x**0*y**1*z** 8+cg50208*x**0*y**2*z** 8+
cg50009*x**0*y**0*z** 9+cg50109*x**0*y**1*z** 9+cg50209*x**0*y**2*z** 9+
cg50010*x**0*y**0*z**10+cg50110*x**0*y**1*z**10+cg50210*x**0*y**2*z**10+
cg50011*x**0*y**0*z**11+cg50111*x**0*y**1*z**11+cg50211*x**0*y**2*z**11+
cg50012*x**0*y**0*z**12+cg50112*x**0*y**1*z**12+cg50212*x**0*y**2*z**12+
cg50013*x**0*y**0*z**13+cg50113*x**0*y**1*z**13+cg50213*x**0*y**2*z**13+
cg50014*x**0*y**0*z**14+cg50114*x**0*y**1*z**14+cg50214*x**0*y**2*z**14+
cg50015*x**0*y**0*z**15+cg50115*x**0*y**1*z**15+cg50215*x**0*y**2*z**15+
cg50016*x**0*y**0*z**16+cg50116*x**0*y**1*z**16+cg50216*x**0*y**2*z**16+
cg50017*x**0*y**0*z**17+cg50117*x**0*y**1*z**17+cg50217*x**0*y**2*z**17+
cg50018*x**0*y**0*z**18+cg50118*x**0*y**1*z**18+cg50218*x**0*y**2*z**18+
cg50019*x**0*y**0*z**19+cg50119*x**0*y**1*z**19+cg50219*x**0*y**2*z**19+
cg50020*x**0*y**0*z**20+cg50120*x**0*y**1*z**20+cg50220*x**0*y**2*z**20+
cg50021*x**0*y**0*z**21+cg50121*x**0*y**1*z**21+cg50221*x**0*y**2*z**21+
cg50022*x**0*y**0*z**22+cg50122*x**0*y**1*z**22+cg50222*x**0*y**2*z**22+
cg50023*x**0*y**0*z**23+cg50123*x**0*y**1*z**23+cg50223*x**0*y**2*z**23+
cg50024*x**0*y**0*z**24+cg50124*x**0*y**1*z**24+cg50224*x**0*y**2*z**24+
cg50025*x**0*y**0*z**25+cg50125*x**0*y**1*z**25+cg50225*x**0*y**2*z**25+
cg50026*x**0*y**0*z**26+cg50126*x**0*y**1*z**26+cg50226*x**0*y**2*z**26+
cg50027*x**0*y**0*z**27+cg50127*x**0*y**1*z**27+cg50227*x**0*y**2*z**27+
cg50028*x**0*y**0*z**28+cg50128*x**0*y**1*z**28+cg50228*x**0*y**2*z**28+
cg50029*x**0*y**0*z**29+cg50129*x**0*y**1*z**29+cg50229*x**0*y**2*z**29+
cg50030*x**0*y**0*z**30+cg50130*x**0*y**1*z**30+cg50230*x**0*y**2*z**30+
cg50031*x**0*y**0*z**31+cg50131*x**0*y**1*z**31+cg50231*x**0*y**2*z**31,
CG51**2 =
cg51000*x**1*y**0*z** 0+cg51100*x**1*y**1*z** 0+cg51200*x**1*y**2*z** 0+
cg51001*x**1*y**0*z** 1+cg51101*x**1*y**1*z** 1+cg51201*x**1*y**2*z** 1+
cg51002*x**1*y**0*z** 2+cg51102*x**1*y**1*z** 2+cg51202*x**1*y**2*z** 2+
cg51003*x**1*y**0*z** 3+cg51103*x**1*y**1*z** 3+cg51203*x**1*y**2*z** 3+
cg51004*x**1*y**0*z** 4+cg51104*x**1*y**1*z** 4+cg51204*x**1*y**2*z** 4+
cg51005*x**1*y**0*z** 5+cg51105*x**1*y**1*z** 5+cg51205*x**1*y**2*z** 5+
cg51006*x**1*y**0*z** 6+cg51106*x**1*y**1*z** 6+cg51206*x**1*y**2*z** 6+
cg51007*x**1*y**0*z** 7+cg51107*x**1*y**1*z** 7+cg51207*x**1*y**2*z** 7+
cg51008*x**1*y**0*z** 8+cg51108*x**1*y**1*z** 8+cg51208*x**1*y**2*z** 8+
cg51009*x**1*y**0*z** 9+cg51109*x**1*y**1*z** 9+cg51209*x**1*y**2*z** 9+
cg51010*x**1*y**0*z**10+cg51110*x**1*y**1*z**10+cg51210*x**1*y**2*z**10+
cg51011*x**1*y**0*z**11+cg51111*x**1*y**1*z**11+cg51211*x**1*y**2*z**11+
cg51012*x**1*y**0*z**12+cg51112*x**1*y**1*z**12+cg51212*x**1*y**2*z**12+
cg51013*x**1*y**0*z**13+cg51113*x**1*y**1*z**13+cg51213*x**1*y**2*z**13+
cg51014*x**1*y**0*z**14+cg51114*x**1*y**1*z**14+cg51214*x**1*y**2*z**14+
cg51015*x**1*y**0*z**15+cg51115*x**1*y**1*z**15+cg51215*x**1*y**2*z**15+
cg51016*x**1*y**0*z**16+cg51116*x**1*y**1*z**16+cg51216*x**1*y**2*z**16+
cg51017*x**1*y**0*z**17+cg51117*x**1*y**1*z**17+cg51217*x**1*y**2*z**17+
cg51018*x**1*y**0*z**18+cg51118*x**1*y**1*z**18+cg51218*x**1*y**2*z**18+
cg51019*x**1*y**0*z**19+cg51119*x**1*y**1*z**19+cg51219*x**1*y**2*z**19+
cg51020*x**1*y**0*z**20+cg51120*x**1*y**1*z**20+cg51220*x**1*y**2*z**20+
cg51021*x**1*y**0*z**21+cg51121*x**1*y**1*z**21+cg51221*x**1*y**2*z**21+
cg51022*x**1*y**0*z**22+cg51122*x**1*y**1*z**22+cg51222*x**1*y**2*z**22+
cg51023*x**1*y**0*z**23+cg51123*x**1*y**1*z**23+cg51223*x**1*y**2*z**23+
cg51024*x**1*y**0*z**24+cg51124*x**1*y**1*z**24+cg51224*x**1*y**2*z**24+
cg51025*x**1*y**0*z**25+cg51125*x**1*y**1*z**25+cg51225*x**1*y**2*z**25+
cg51026*x**1*y**0*z**26+cg51126*x**1*y**1*z**26+cg51226*x**1*y**2*z**26+
cg51027*x**1*y**0*z**27+cg51127*x**1*y**1*z**27+cg51227*x**1*y**2*z**27+
cg51028*x**1*y**0*z**28+cg51128*x**1*y**1*z**28+cg51228*x**1*y**2*z**28+
cg51029*x**1*y**0*z**29+cg51129*x**1*y**1*z**29+cg51229*x**1*y**2*z**29+
cg51030*x**1*y**0*z**30+cg51130*x**1*y**1*z**30+cg51230*x**1*y**2*z**30+
cg51031*x**1*y**0*z**31+cg51131*x**1*y**1*z**31+cg51231*x**1*y**2*z**31,
CG52**2 =
cg52000*x**2*y**0*z** 0+cg52100*x**2*y**1*z** 0+cg52200*x**2*y**2*z** 0+
cg52001*x**2*y**0*z** 1+cg52101*x**2*y**1*z** 1+cg52201*x**2*y**2*z** 1+
cg52002*x**2*y**0*z** 2+cg52102*x**2*y**1*z** 2+cg52202*x**2*y**2*z** 2+
cg52003*x**2*y**0*z** 3+cg52103*x**2*y**1*z** 3+cg52203*x**2*y**2*z** 3+
cg52004*x**2*y**0*z** 4+cg52104*x**2*y**1*z** 4+cg52204*x**2*y**2*z** 4+
cg52005*x**2*y**0*z** 5+cg52105*x**2*y**1*z** 5+cg52205*x**2*y**2*z** 5+
cg52006*x**2*y**0*z** 6+cg52106*x**2*y**1*z** 6+cg52206*x**2*y**2*z** 6+
cg52007*x**2*y**0*z** 7+cg52107*x**2*y**1*z** 7+cg52207*x**2*y**2*z** 7+
cg52008*x**2*y**0*z** 8+cg52108*x**2*y**1*z** 8+cg52208*x**2*y**2*z** 8+
cg52009*x**2*y**0*z** 9+cg52109*x**2*y**1*z** 9+cg52209*x**2*y**2*z** 9+
cg52010*x**2*y**0*z**10+cg52110*x**2*y**1*z**10+cg52210*x**2*y**2*z**10+
cg52011*x**2*y**0*z**11+cg52111*x**2*y**1*z**11+cg52211*x**2*y**2*z**11+
cg52012*x**2*y**0*z**12+cg52112*x**2*y**1*z**12+cg52212*x**2*y**2*z**12+
cg52013*x**2*y**0*z**13+cg52113*x**2*y**1*z**13+cg52213*x**2*y**2*z**13+
cg52014*x**2*y**0*z**14+cg52114*x**2*y**1*z**14+cg52214*x**2*y**2*z**14+
cg52015*x**2*y**0*z**15+cg52115*x**2*y**1*z**15+cg52215*x**2*y**2*z**15+
cg52016*x**2*y**0*z**16+cg52116*x**2*y**1*z**16+cg52216*x**2*y**2*z**16+
cg52017*x**2*y**0*z**17+cg52117*x**2*y**1*z**17+cg52217*x**2*y**2*z**17+
cg52018*x**2*y**0*z**18+cg52118*x**2*y**1*z**18+cg52218*x**2*y**2*z**18+
cg52019*x**2*y**0*z**19+cg52119*x**2*y**1*z**19+cg52219*x**2*y**2*z**19+
cg52020*x**2*y**0*z**20+cg52120*x**2*y**1*z**20+cg52220*x**2*y**2*z**20+
cg52021*x**2*y**0*z**21+cg52121*x**2*y**1*z**21+cg52221*x**2*y**2*z**21+
cg52022*x**2*y**0*z**22+cg52122*x**2*y**1*z**22+cg52222*x**2*y**2*z**22+
cg52023*x**2*y**0*z**23+cg52123*x**2*y**1*z**23+cg52223*x**2*y**2*z**23+
cg52024*x**2*y**0*z**24+cg52124*x**2*y**1*z**24+cg52224*x**2*y**2*z**24+
cg52025*x**2*y**0*z**25+cg52125*x**2*y**1*z**25+cg52225*x**2*y**2*z**25+
cg52026*x**2*y**0*z**26+cg52126*x**2*y**1*z**26+cg52226*x**2*y**2*z**26+
cg52027*x**2*y**0*z**27+cg52127*x**2*y**1*z**27+cg52227*x**2*y**2*z**27+
cg52028*x**2*y**0*z**28+cg52128*x**2*y**1*z**28+cg52228*x**2*y**2*z**28+
cg52029*x**2*y**0*z**29+cg52129*x**2*y**1*z**29+cg52229*x**2*y**2*z**29+
cg52030*x**2*y**0*z**30+cg52130*x**2*y**1*z**30+cg52230*x**2*y**2*z**30+
cg52031*x**2*y**0*z**31+cg52131*x**2*y**1*z**31+cg52231*x**2*y**2*z**31
] prove with [ precondition ];


(**************** CUT 408, - *****************)

ecut eqmod CH620**2+CH621**2+CH622**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3117017, z**16 - 2630352 ]
     prove with [ all ghosts, cuts [ 313, 349, 385 ] ],
     eqmod CH720**2+CH721**2+CH722**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  543301, z**16 - 2630352 ]
     prove with [ all ghosts, cuts [ 314, 350, 386 ] ],
     eqmod CH820**2+CH821**2+CH822**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3070820, z**16 - 2630352 ]
     prove with [ all ghosts, cuts [ 315, 351, 387 ] ],
     eqmod CH630**2+CH631**2+CH632**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3117017, z**16 -  735217 ]
     prove with [ all ghosts, cuts [ 322, 358, 394 ] ],
     eqmod CH730**2+CH731**2+CH732**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  543301, z**16 -  735217 ]
     prove with [ all ghosts, cuts [ 323, 359, 395 ] ],
     eqmod CH830**2+CH831**2+CH832**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3070820, z**16 -  735217 ]
     prove with [ all ghosts, cuts [ 324, 360, 396 ] ];


{
(******************** postcondition ********************)

and [
     eqmod CH000**2+CH001**2+CH002**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -       1, z**16 -       1 ],
     eqmod CH100**2+CH101**2+CH102**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  452650, z**16 -       1 ],
     eqmod CH200**2+CH201**2+CH202**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2912918, z**16 -       1 ],
     eqmod CH300**2+CH301**2+CH302**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2113025, z**16 -       1 ],
     eqmod CH400**2+CH401**2+CH402**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3077709, z**16 -       1 ],
     eqmod CH500**2+CH501**2+CH502**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 1540404, z**16 -       1 ],
     eqmod CH600**2+CH601**2+CH602**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3117017, z**16 -       1 ],
     eqmod CH700**2+CH701**2+CH702**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  543301, z**16 -       1 ],
     eqmod CH800**2+CH801**2+CH802**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3070820, z**16 -       1 ],
     eqmod CH010**2+CH011**2+CH012**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -       1, z**16 - 3365568 ],
     eqmod CH110**2+CH111**2+CH112**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  452650, z**16 - 3365568 ],
     eqmod CH210**2+CH211**2+CH212**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2912918, z**16 - 3365568 ],
     eqmod CH310**2+CH311**2+CH312**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2113025, z**16 - 3365568 ],
     eqmod CH410**2+CH411**2+CH412**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3077709, z**16 - 3365568 ],
     eqmod CH510**2+CH511**2+CH512**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 1540404, z**16 - 3365568 ],
     eqmod CH610**2+CH611**2+CH612**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3117017, z**16 - 3365568 ],
     eqmod CH710**2+CH711**2+CH712**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  543301, z**16 - 3365568 ],
     eqmod CH810**2+CH811**2+CH812**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3070820, z**16 - 3365568 ],
     eqmod CH020**2+CH021**2+CH022**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -       1, z**16 - 2630352 ],
     eqmod CH120**2+CH121**2+CH122**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  452650, z**16 - 2630352 ],
     eqmod CH220**2+CH221**2+CH222**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2912918, z**16 - 2630352 ],
     eqmod CH320**2+CH321**2+CH322**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2113025, z**16 - 2630352 ],
     eqmod CH420**2+CH421**2+CH422**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3077709, z**16 - 2630352 ],
     eqmod CH520**2+CH521**2+CH522**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 1540404, z**16 - 2630352 ],
     eqmod CH620**2+CH621**2+CH622**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3117017, z**16 - 2630352 ],
     eqmod CH720**2+CH721**2+CH722**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  543301, z**16 - 2630352 ],
     eqmod CH820**2+CH821**2+CH822**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3070820, z**16 - 2630352 ],
     eqmod CH030**2+CH031**2+CH032**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -       1, z**16 -  735217 ],
     eqmod CH130**2+CH131**2+CH132**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  452650, z**16 -  735217 ],
     eqmod CH230**2+CH231**2+CH232**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2912918, z**16 -  735217 ],
     eqmod CH330**2+CH331**2+CH332**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 2113025, z**16 -  735217 ],
     eqmod CH430**2+CH431**2+CH432**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3077709, z**16 -  735217 ],
     eqmod CH530**2+CH531**2+CH532**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 1540404, z**16 -  735217 ],
     eqmod CH630**2+CH631**2+CH632**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3117017, z**16 -  735217 ],
     eqmod CH730**2+CH731**2+CH732**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y -  543301, z**16 -  735217 ],
     eqmod CH830**2+CH831**2+CH832**2 CF0**2+CF1**2+CF2**2
     [ 3365569, y - 3070820, z**16 -  735217 ]
] prove with [ cuts [ 398, 400, 402, 404, 406, 408 ] ] && and [
(-20205702)@32 <=s L0x20014898, L0x20014898 <=s 20205702@32,
(-16836037)@32 <=s L0x200148a4, L0x200148a4 <=s 16836037@32,
(-23571271)@32 <=s L0x200148b0, L0x200148b0 <=s 23571271@32,
(-20205702)@32 <=s L0x20014f58, L0x20014f58 <=s 20205702@32,
(-16836037)@32 <=s L0x20014f64, L0x20014f64 <=s 16836037@32,
(-23571271)@32 <=s L0x20014f70, L0x20014f70 <=s 23571271@32
,
(-20207750)@32 <=s L0x2001489c, L0x2001489c <=s 20207750@32,
(-16831941)@32 <=s L0x200148a8, L0x200148a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200148b4, L0x200148b4 <=s 23565127@32,
(-20207750)@32 <=s L0x20014f5c, L0x20014f5c <=s 20207750@32,
(-16831941)@32 <=s L0x20014f68, L0x20014f68 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f74, L0x20014f74 <=s 23565127@32
,
(-20207750)@32 <=s L0x200148a0, L0x200148a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200148ac, L0x200148ac <=s 16831941@32,
(-23565127)@32 <=s L0x200148b8, L0x200148b8 <=s 23565127@32,
(-20207750)@32 <=s L0x20014f60, L0x20014f60 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f6c, L0x20014f6c <=s 16831941@32,
(-23565127)@32 <=s L0x20014f78, L0x20014f78 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014904, L0x20014904 <=s 20205702@32,
(-16836037)@32 <=s L0x20014910, L0x20014910 <=s 16836037@32,
(-23571271)@32 <=s L0x2001491c, L0x2001491c <=s 23571271@32,
(-20205702)@32 <=s L0x20014fc4, L0x20014fc4 <=s 20205702@32,
(-16836037)@32 <=s L0x20014fd0, L0x20014fd0 <=s 16836037@32,
(-23571271)@32 <=s L0x20014fdc, L0x20014fdc <=s 23571271@32
,
(-20207750)@32 <=s L0x20014908, L0x20014908 <=s 20207750@32,
(-16831941)@32 <=s L0x20014914, L0x20014914 <=s 16831941@32,
(-23565127)@32 <=s L0x20014920, L0x20014920 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fc8, L0x20014fc8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014fd4, L0x20014fd4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014fe0, L0x20014fe0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001490c, L0x2001490c <=s 20207750@32,
(-16831941)@32 <=s L0x20014918, L0x20014918 <=s 16831941@32,
(-23565127)@32 <=s L0x20014924, L0x20014924 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fcc, L0x20014fcc <=s 20207750@32,
(-16831941)@32 <=s L0x20014fd8, L0x20014fd8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014fe4, L0x20014fe4 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014970, L0x20014970 <=s 20205702@32,
(-16836037)@32 <=s L0x2001497c, L0x2001497c <=s 16836037@32,
(-23571271)@32 <=s L0x20014988, L0x20014988 <=s 23571271@32,
(-20205702)@32 <=s L0x20015030, L0x20015030 <=s 20205702@32,
(-16836037)@32 <=s L0x2001503c, L0x2001503c <=s 16836037@32,
(-23571271)@32 <=s L0x20015048, L0x20015048 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014974, L0x20014974 <=s 20207750@32,
(-16831941)@32 <=s L0x20014980, L0x20014980 <=s 16831941@32,
(-23565127)@32 <=s L0x2001498c, L0x2001498c <=s 23565127@32,
(-20207750)@32 <=s L0x20015034, L0x20015034 <=s 20207750@32,
(-16831941)@32 <=s L0x20015040, L0x20015040 <=s 16831941@32,
(-23565127)@32 <=s L0x2001504c, L0x2001504c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014978, L0x20014978 <=s 20207750@32,
(-16831941)@32 <=s L0x20014984, L0x20014984 <=s 16831941@32,
(-23565127)@32 <=s L0x20014990, L0x20014990 <=s 23565127@32,
(-20207750)@32 <=s L0x20015038, L0x20015038 <=s 20207750@32,
(-16831941)@32 <=s L0x20015044, L0x20015044 <=s 16831941@32,
(-23565127)@32 <=s L0x20015050, L0x20015050 <=s 23565127@32
,
(-20205702)@32 <=s L0x200149dc, L0x200149dc <=s 20205702@32,
(-16836037)@32 <=s L0x200149e8, L0x200149e8 <=s 16836037@32,
(-23571271)@32 <=s L0x200149f4, L0x200149f4 <=s 23571271@32,
(-20205702)@32 <=s L0x2001509c, L0x2001509c <=s 20205702@32,
(-16836037)@32 <=s L0x200150a8, L0x200150a8 <=s 16836037@32,
(-23571271)@32 <=s L0x200150b4, L0x200150b4 <=s 23571271@32
,
(-20207750)@32 <=s L0x200149e0, L0x200149e0 <=s 20207750@32,
(-16831941)@32 <=s L0x200149ec, L0x200149ec <=s 16831941@32,
(-23565127)@32 <=s L0x200149f8, L0x200149f8 <=s 23565127@32,
(-20207750)@32 <=s L0x200150a0, L0x200150a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200150ac, L0x200150ac <=s 16831941@32,
(-23565127)@32 <=s L0x200150b8, L0x200150b8 <=s 23565127@32
,
(-20207750)@32 <=s L0x200149e4, L0x200149e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200149f0, L0x200149f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200149fc, L0x200149fc <=s 23565127@32,
(-20207750)@32 <=s L0x200150a4, L0x200150a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200150b0, L0x200150b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200150bc, L0x200150bc <=s 23565127@32
,
(-20205702)@32 <=s L0x20014a48, L0x20014a48 <=s 20205702@32,
(-16836037)@32 <=s L0x20014a54, L0x20014a54 <=s 16836037@32,
(-23571271)@32 <=s L0x20014a60, L0x20014a60 <=s 23571271@32,
(-20205702)@32 <=s L0x20015108, L0x20015108 <=s 20205702@32,
(-16836037)@32 <=s L0x20015114, L0x20015114 <=s 16836037@32,
(-23571271)@32 <=s L0x20015120, L0x20015120 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014a4c, L0x20014a4c <=s 20207750@32,
(-16831941)@32 <=s L0x20014a58, L0x20014a58 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a64, L0x20014a64 <=s 23565127@32,
(-20207750)@32 <=s L0x2001510c, L0x2001510c <=s 20207750@32,
(-16831941)@32 <=s L0x20015118, L0x20015118 <=s 16831941@32,
(-23565127)@32 <=s L0x20015124, L0x20015124 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a50, L0x20014a50 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a5c, L0x20014a5c <=s 16831941@32,
(-23565127)@32 <=s L0x20014a68, L0x20014a68 <=s 23565127@32,
(-20207750)@32 <=s L0x20015110, L0x20015110 <=s 20207750@32,
(-16831941)@32 <=s L0x2001511c, L0x2001511c <=s 16831941@32,
(-23565127)@32 <=s L0x20015128, L0x20015128 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014ab4, L0x20014ab4 <=s 20205702@32,
(-16836037)@32 <=s L0x20014ac0, L0x20014ac0 <=s 16836037@32,
(-23571271)@32 <=s L0x20014acc, L0x20014acc <=s 23571271@32,
(-20205702)@32 <=s L0x20015174, L0x20015174 <=s 20205702@32,
(-16836037)@32 <=s L0x20015180, L0x20015180 <=s 16836037@32,
(-23571271)@32 <=s L0x2001518c, L0x2001518c <=s 23571271@32
,
(-20207750)@32 <=s L0x20014ab8, L0x20014ab8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ac4, L0x20014ac4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ad0, L0x20014ad0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015178, L0x20015178 <=s 20207750@32,
(-16831941)@32 <=s L0x20015184, L0x20015184 <=s 16831941@32,
(-23565127)@32 <=s L0x20015190, L0x20015190 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014abc, L0x20014abc <=s 20207750@32,
(-16831941)@32 <=s L0x20014ac8, L0x20014ac8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ad4, L0x20014ad4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001517c, L0x2001517c <=s 20207750@32,
(-16831941)@32 <=s L0x20015188, L0x20015188 <=s 16831941@32,
(-23565127)@32 <=s L0x20015194, L0x20015194 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014b20, L0x20014b20 <=s 20205702@32,
(-16836037)@32 <=s L0x20014b2c, L0x20014b2c <=s 16836037@32,
(-23571271)@32 <=s L0x20014b38, L0x20014b38 <=s 23571271@32,
(-20205702)@32 <=s L0x200151e0, L0x200151e0 <=s 20205702@32,
(-16836037)@32 <=s L0x200151ec, L0x200151ec <=s 16836037@32,
(-23571271)@32 <=s L0x200151f8, L0x200151f8 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014b24, L0x20014b24 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b30, L0x20014b30 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b3c, L0x20014b3c <=s 23565127@32,
(-20207750)@32 <=s L0x200151e4, L0x200151e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200151f0, L0x200151f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200151fc, L0x200151fc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b28, L0x20014b28 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b34, L0x20014b34 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b40, L0x20014b40 <=s 23565127@32,
(-20207750)@32 <=s L0x200151e8, L0x200151e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200151f4, L0x200151f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015200, L0x20015200 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014b8c, L0x20014b8c <=s 20205702@32,
(-16836037)@32 <=s L0x20014b98, L0x20014b98 <=s 16836037@32,
(-23571271)@32 <=s L0x20014ba4, L0x20014ba4 <=s 23571271@32,
(-20205702)@32 <=s L0x2001524c, L0x2001524c <=s 20205702@32,
(-16836037)@32 <=s L0x20015258, L0x20015258 <=s 16836037@32,
(-23571271)@32 <=s L0x20015264, L0x20015264 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014b90, L0x20014b90 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b9c, L0x20014b9c <=s 16831941@32,
(-23565127)@32 <=s L0x20014ba8, L0x20014ba8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015250, L0x20015250 <=s 20207750@32,
(-16831941)@32 <=s L0x2001525c, L0x2001525c <=s 16831941@32,
(-23565127)@32 <=s L0x20015268, L0x20015268 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b94, L0x20014b94 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ba0, L0x20014ba0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bac, L0x20014bac <=s 23565127@32,
(-20207750)@32 <=s L0x20015254, L0x20015254 <=s 20207750@32,
(-16831941)@32 <=s L0x20015260, L0x20015260 <=s 16831941@32,
(-23565127)@32 <=s L0x2001526c, L0x2001526c <=s 23565127@32
,
(-20205702)@32 <=s L0x20014bf8, L0x20014bf8 <=s 20205702@32,
(-16836037)@32 <=s L0x20014c04, L0x20014c04 <=s 16836037@32,
(-23571271)@32 <=s L0x20014c10, L0x20014c10 <=s 23571271@32,
(-20205702)@32 <=s L0x200152b8, L0x200152b8 <=s 20205702@32,
(-16836037)@32 <=s L0x200152c4, L0x200152c4 <=s 16836037@32,
(-23571271)@32 <=s L0x200152d0, L0x200152d0 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014bfc, L0x20014bfc <=s 20207750@32,
(-16831941)@32 <=s L0x20014c08, L0x20014c08 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c14, L0x20014c14 <=s 23565127@32,
(-20207750)@32 <=s L0x200152bc, L0x200152bc <=s 20207750@32,
(-16831941)@32 <=s L0x200152c8, L0x200152c8 <=s 16831941@32,
(-23565127)@32 <=s L0x200152d4, L0x200152d4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c00, L0x20014c00 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c0c, L0x20014c0c <=s 16831941@32,
(-23565127)@32 <=s L0x20014c18, L0x20014c18 <=s 23565127@32,
(-20207750)@32 <=s L0x200152c0, L0x200152c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200152cc, L0x200152cc <=s 16831941@32,
(-23565127)@32 <=s L0x200152d8, L0x200152d8 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014c64, L0x20014c64 <=s 20205702@32,
(-16836037)@32 <=s L0x20014c70, L0x20014c70 <=s 16836037@32,
(-23571271)@32 <=s L0x20014c7c, L0x20014c7c <=s 23571271@32,
(-20205702)@32 <=s L0x20015324, L0x20015324 <=s 20205702@32,
(-16836037)@32 <=s L0x20015330, L0x20015330 <=s 16836037@32,
(-23571271)@32 <=s L0x2001533c, L0x2001533c <=s 23571271@32
,
(-20207750)@32 <=s L0x20014c68, L0x20014c68 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c74, L0x20014c74 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c80, L0x20014c80 <=s 23565127@32,
(-20207750)@32 <=s L0x20015328, L0x20015328 <=s 20207750@32,
(-16831941)@32 <=s L0x20015334, L0x20015334 <=s 16831941@32,
(-23565127)@32 <=s L0x20015340, L0x20015340 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c6c, L0x20014c6c <=s 20207750@32,
(-16831941)@32 <=s L0x20014c78, L0x20014c78 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c84, L0x20014c84 <=s 23565127@32,
(-20207750)@32 <=s L0x2001532c, L0x2001532c <=s 20207750@32,
(-16831941)@32 <=s L0x20015338, L0x20015338 <=s 16831941@32,
(-23565127)@32 <=s L0x20015344, L0x20015344 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014cd0, L0x20014cd0 <=s 20205702@32,
(-16836037)@32 <=s L0x20014cdc, L0x20014cdc <=s 16836037@32,
(-23571271)@32 <=s L0x20014ce8, L0x20014ce8 <=s 23571271@32,
(-20205702)@32 <=s L0x20015390, L0x20015390 <=s 20205702@32,
(-16836037)@32 <=s L0x2001539c, L0x2001539c <=s 16836037@32,
(-23571271)@32 <=s L0x200153a8, L0x200153a8 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014cd4, L0x20014cd4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ce0, L0x20014ce0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014cec, L0x20014cec <=s 23565127@32,
(-20207750)@32 <=s L0x20015394, L0x20015394 <=s 20207750@32,
(-16831941)@32 <=s L0x200153a0, L0x200153a0 <=s 16831941@32,
(-23565127)@32 <=s L0x200153ac, L0x200153ac <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cd8, L0x20014cd8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ce4, L0x20014ce4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014cf0, L0x20014cf0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015398, L0x20015398 <=s 20207750@32,
(-16831941)@32 <=s L0x200153a4, L0x200153a4 <=s 16831941@32,
(-23565127)@32 <=s L0x200153b0, L0x200153b0 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014d3c, L0x20014d3c <=s 20205702@32,
(-16836037)@32 <=s L0x20014d48, L0x20014d48 <=s 16836037@32,
(-23571271)@32 <=s L0x20014d54, L0x20014d54 <=s 23571271@32,
(-20205702)@32 <=s L0x200153fc, L0x200153fc <=s 20205702@32,
(-16836037)@32 <=s L0x20015408, L0x20015408 <=s 16836037@32,
(-23571271)@32 <=s L0x20015414, L0x20015414 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014d40, L0x20014d40 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d4c, L0x20014d4c <=s 16831941@32,
(-23565127)@32 <=s L0x20014d58, L0x20014d58 <=s 23565127@32,
(-20207750)@32 <=s L0x20015400, L0x20015400 <=s 20207750@32,
(-16831941)@32 <=s L0x2001540c, L0x2001540c <=s 16831941@32,
(-23565127)@32 <=s L0x20015418, L0x20015418 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d44, L0x20014d44 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d50, L0x20014d50 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d5c, L0x20014d5c <=s 23565127@32,
(-20207750)@32 <=s L0x20015404, L0x20015404 <=s 20207750@32,
(-16831941)@32 <=s L0x20015410, L0x20015410 <=s 16831941@32,
(-23565127)@32 <=s L0x2001541c, L0x2001541c <=s 23565127@32
,
(-20205702)@32 <=s L0x20014da8, L0x20014da8 <=s 20205702@32,
(-16836037)@32 <=s L0x20014db4, L0x20014db4 <=s 16836037@32,
(-23571271)@32 <=s L0x20014dc0, L0x20014dc0 <=s 23571271@32,
(-20205702)@32 <=s L0x20015468, L0x20015468 <=s 20205702@32,
(-16836037)@32 <=s L0x20015474, L0x20015474 <=s 16836037@32,
(-23571271)@32 <=s L0x20015480, L0x20015480 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014dac, L0x20014dac <=s 20207750@32,
(-16831941)@32 <=s L0x20014db8, L0x20014db8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014dc4, L0x20014dc4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001546c, L0x2001546c <=s 20207750@32,
(-16831941)@32 <=s L0x20015478, L0x20015478 <=s 16831941@32,
(-23565127)@32 <=s L0x20015484, L0x20015484 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014db0, L0x20014db0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014dbc, L0x20014dbc <=s 16831941@32,
(-23565127)@32 <=s L0x20014dc8, L0x20014dc8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015470, L0x20015470 <=s 20207750@32,
(-16831941)@32 <=s L0x2001547c, L0x2001547c <=s 16831941@32,
(-23565127)@32 <=s L0x20015488, L0x20015488 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014e14, L0x20014e14 <=s 20205702@32,
(-16836037)@32 <=s L0x20014e20, L0x20014e20 <=s 16836037@32,
(-23571271)@32 <=s L0x20014e2c, L0x20014e2c <=s 23571271@32,
(-20205702)@32 <=s L0x200154d4, L0x200154d4 <=s 20205702@32,
(-16836037)@32 <=s L0x200154e0, L0x200154e0 <=s 16836037@32,
(-23571271)@32 <=s L0x200154ec, L0x200154ec <=s 23571271@32
,
(-20207750)@32 <=s L0x20014e18, L0x20014e18 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e24, L0x20014e24 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e30, L0x20014e30 <=s 23565127@32,
(-20207750)@32 <=s L0x200154d8, L0x200154d8 <=s 20207750@32,
(-16831941)@32 <=s L0x200154e4, L0x200154e4 <=s 16831941@32,
(-23565127)@32 <=s L0x200154f0, L0x200154f0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e1c, L0x20014e1c <=s 20207750@32,
(-16831941)@32 <=s L0x20014e28, L0x20014e28 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e34, L0x20014e34 <=s 23565127@32,
(-20207750)@32 <=s L0x200154dc, L0x200154dc <=s 20207750@32,
(-16831941)@32 <=s L0x200154e8, L0x200154e8 <=s 16831941@32,
(-23565127)@32 <=s L0x200154f4, L0x200154f4 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014e80, L0x20014e80 <=s 20205702@32,
(-16836037)@32 <=s L0x20014e8c, L0x20014e8c <=s 16836037@32,
(-23571271)@32 <=s L0x20014e98, L0x20014e98 <=s 23571271@32,
(-20205702)@32 <=s L0x20015540, L0x20015540 <=s 20205702@32,
(-16836037)@32 <=s L0x2001554c, L0x2001554c <=s 16836037@32,
(-23571271)@32 <=s L0x20015558, L0x20015558 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014e84, L0x20014e84 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e90, L0x20014e90 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e9c, L0x20014e9c <=s 23565127@32,
(-20207750)@32 <=s L0x20015544, L0x20015544 <=s 20207750@32,
(-16831941)@32 <=s L0x20015550, L0x20015550 <=s 16831941@32,
(-23565127)@32 <=s L0x2001555c, L0x2001555c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e88, L0x20014e88 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e94, L0x20014e94 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ea0, L0x20014ea0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015548, L0x20015548 <=s 20207750@32,
(-16831941)@32 <=s L0x20015554, L0x20015554 <=s 16831941@32,
(-23565127)@32 <=s L0x20015560, L0x20015560 <=s 23565127@32
,
(-20205702)@32 <=s L0x20014eec, L0x20014eec <=s 20205702@32,
(-16836037)@32 <=s L0x20014ef8, L0x20014ef8 <=s 16836037@32,
(-23571271)@32 <=s L0x20014f04, L0x20014f04 <=s 23571271@32,
(-20205702)@32 <=s L0x200155ac, L0x200155ac <=s 20205702@32,
(-16836037)@32 <=s L0x200155b8, L0x200155b8 <=s 16836037@32,
(-23571271)@32 <=s L0x200155c4, L0x200155c4 <=s 23571271@32
,
(-20207750)@32 <=s L0x20014ef0, L0x20014ef0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014efc, L0x20014efc <=s 16831941@32,
(-23565127)@32 <=s L0x20014f08, L0x20014f08 <=s 23565127@32,
(-20207750)@32 <=s L0x200155b0, L0x200155b0 <=s 20207750@32,
(-16831941)@32 <=s L0x200155bc, L0x200155bc <=s 16831941@32,
(-23565127)@32 <=s L0x200155c8, L0x200155c8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ef4, L0x20014ef4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f00, L0x20014f00 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f0c, L0x20014f0c <=s 23565127@32,
(-20207750)@32 <=s L0x200155b4, L0x200155b4 <=s 20207750@32,
(-16831941)@32 <=s L0x200155c0, L0x200155c0 <=s 16831941@32,
(-23565127)@32 <=s L0x200155cc, L0x200155cc <=s 23565127@32
,
(-20207750)@32 <=s L0x200148bc, L0x200148bc <=s 20207750@32,
(-16831941)@32 <=s L0x200148c8, L0x200148c8 <=s 16831941@32,
(-23565127)@32 <=s L0x200148d4, L0x200148d4 <=s 23565127@32,
(-20207750)@32 <=s L0x20014f7c, L0x20014f7c <=s 20207750@32,
(-16831941)@32 <=s L0x20014f88, L0x20014f88 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f94, L0x20014f94 <=s 23565127@32
,
(-20207750)@32 <=s L0x200148c0, L0x200148c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200148cc, L0x200148cc <=s 16831941@32,
(-23565127)@32 <=s L0x200148d8, L0x200148d8 <=s 23565127@32,
(-20207750)@32 <=s L0x20014f80, L0x20014f80 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f8c, L0x20014f8c <=s 16831941@32,
(-23565127)@32 <=s L0x20014f98, L0x20014f98 <=s 23565127@32
,
(-20207750)@32 <=s L0x200148c4, L0x200148c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200148d0, L0x200148d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200148dc, L0x200148dc <=s 23565127@32,
(-20207750)@32 <=s L0x20014f84, L0x20014f84 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f90, L0x20014f90 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f9c, L0x20014f9c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014928, L0x20014928 <=s 20207750@32,
(-16831941)@32 <=s L0x20014934, L0x20014934 <=s 16831941@32,
(-23565127)@32 <=s L0x20014940, L0x20014940 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fe8, L0x20014fe8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ff4, L0x20014ff4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015000, L0x20015000 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001492c, L0x2001492c <=s 20207750@32,
(-16831941)@32 <=s L0x20014938, L0x20014938 <=s 16831941@32,
(-23565127)@32 <=s L0x20014944, L0x20014944 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fec, L0x20014fec <=s 20207750@32,
(-16831941)@32 <=s L0x20014ff8, L0x20014ff8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015004, L0x20015004 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014930, L0x20014930 <=s 20207750@32,
(-16831941)@32 <=s L0x2001493c, L0x2001493c <=s 16831941@32,
(-23565127)@32 <=s L0x20014948, L0x20014948 <=s 23565127@32,
(-20207750)@32 <=s L0x20014ff0, L0x20014ff0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ffc, L0x20014ffc <=s 16831941@32,
(-23565127)@32 <=s L0x20015008, L0x20015008 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014994, L0x20014994 <=s 20207750@32,
(-16831941)@32 <=s L0x200149a0, L0x200149a0 <=s 16831941@32,
(-23565127)@32 <=s L0x200149ac, L0x200149ac <=s 23565127@32,
(-20207750)@32 <=s L0x20015054, L0x20015054 <=s 20207750@32,
(-16831941)@32 <=s L0x20015060, L0x20015060 <=s 16831941@32,
(-23565127)@32 <=s L0x2001506c, L0x2001506c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014998, L0x20014998 <=s 20207750@32,
(-16831941)@32 <=s L0x200149a4, L0x200149a4 <=s 16831941@32,
(-23565127)@32 <=s L0x200149b0, L0x200149b0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015058, L0x20015058 <=s 20207750@32,
(-16831941)@32 <=s L0x20015064, L0x20015064 <=s 16831941@32,
(-23565127)@32 <=s L0x20015070, L0x20015070 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001499c, L0x2001499c <=s 20207750@32,
(-16831941)@32 <=s L0x200149a8, L0x200149a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200149b4, L0x200149b4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001505c, L0x2001505c <=s 20207750@32,
(-16831941)@32 <=s L0x20015068, L0x20015068 <=s 16831941@32,
(-23565127)@32 <=s L0x20015074, L0x20015074 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a00, L0x20014a00 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a0c, L0x20014a0c <=s 16831941@32,
(-23565127)@32 <=s L0x20014a18, L0x20014a18 <=s 23565127@32,
(-20207750)@32 <=s L0x200150c0, L0x200150c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200150cc, L0x200150cc <=s 16831941@32,
(-23565127)@32 <=s L0x200150d8, L0x200150d8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a04, L0x20014a04 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a10, L0x20014a10 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a1c, L0x20014a1c <=s 23565127@32,
(-20207750)@32 <=s L0x200150c4, L0x200150c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200150d0, L0x200150d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200150dc, L0x200150dc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a08, L0x20014a08 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a14, L0x20014a14 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a20, L0x20014a20 <=s 23565127@32,
(-20207750)@32 <=s L0x200150c8, L0x200150c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200150d4, L0x200150d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200150e0, L0x200150e0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a6c, L0x20014a6c <=s 20207750@32,
(-16831941)@32 <=s L0x20014a78, L0x20014a78 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a84, L0x20014a84 <=s 23565127@32,
(-20207750)@32 <=s L0x2001512c, L0x2001512c <=s 20207750@32,
(-16831941)@32 <=s L0x20015138, L0x20015138 <=s 16831941@32,
(-23565127)@32 <=s L0x20015144, L0x20015144 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a70, L0x20014a70 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a7c, L0x20014a7c <=s 16831941@32,
(-23565127)@32 <=s L0x20014a88, L0x20014a88 <=s 23565127@32,
(-20207750)@32 <=s L0x20015130, L0x20015130 <=s 20207750@32,
(-16831941)@32 <=s L0x2001513c, L0x2001513c <=s 16831941@32,
(-23565127)@32 <=s L0x20015148, L0x20015148 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a74, L0x20014a74 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a80, L0x20014a80 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a8c, L0x20014a8c <=s 23565127@32,
(-20207750)@32 <=s L0x20015134, L0x20015134 <=s 20207750@32,
(-16831941)@32 <=s L0x20015140, L0x20015140 <=s 16831941@32,
(-23565127)@32 <=s L0x2001514c, L0x2001514c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ad8, L0x20014ad8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ae4, L0x20014ae4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014af0, L0x20014af0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015198, L0x20015198 <=s 20207750@32,
(-16831941)@32 <=s L0x200151a4, L0x200151a4 <=s 16831941@32,
(-23565127)@32 <=s L0x200151b0, L0x200151b0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014adc, L0x20014adc <=s 20207750@32,
(-16831941)@32 <=s L0x20014ae8, L0x20014ae8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014af4, L0x20014af4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001519c, L0x2001519c <=s 20207750@32,
(-16831941)@32 <=s L0x200151a8, L0x200151a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200151b4, L0x200151b4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ae0, L0x20014ae0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014aec, L0x20014aec <=s 16831941@32,
(-23565127)@32 <=s L0x20014af8, L0x20014af8 <=s 23565127@32,
(-20207750)@32 <=s L0x200151a0, L0x200151a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200151ac, L0x200151ac <=s 16831941@32,
(-23565127)@32 <=s L0x200151b8, L0x200151b8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b44, L0x20014b44 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b50, L0x20014b50 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b5c, L0x20014b5c <=s 23565127@32,
(-20207750)@32 <=s L0x20015204, L0x20015204 <=s 20207750@32,
(-16831941)@32 <=s L0x20015210, L0x20015210 <=s 16831941@32,
(-23565127)@32 <=s L0x2001521c, L0x2001521c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b48, L0x20014b48 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b54, L0x20014b54 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b60, L0x20014b60 <=s 23565127@32,
(-20207750)@32 <=s L0x20015208, L0x20015208 <=s 20207750@32,
(-16831941)@32 <=s L0x20015214, L0x20015214 <=s 16831941@32,
(-23565127)@32 <=s L0x20015220, L0x20015220 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b4c, L0x20014b4c <=s 20207750@32,
(-16831941)@32 <=s L0x20014b58, L0x20014b58 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b64, L0x20014b64 <=s 23565127@32,
(-20207750)@32 <=s L0x2001520c, L0x2001520c <=s 20207750@32,
(-16831941)@32 <=s L0x20015218, L0x20015218 <=s 16831941@32,
(-23565127)@32 <=s L0x20015224, L0x20015224 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bb0, L0x20014bb0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014bbc, L0x20014bbc <=s 16831941@32,
(-23565127)@32 <=s L0x20014bc8, L0x20014bc8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015270, L0x20015270 <=s 20207750@32,
(-16831941)@32 <=s L0x2001527c, L0x2001527c <=s 16831941@32,
(-23565127)@32 <=s L0x20015288, L0x20015288 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bb4, L0x20014bb4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014bc0, L0x20014bc0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bcc, L0x20014bcc <=s 23565127@32,
(-20207750)@32 <=s L0x20015274, L0x20015274 <=s 20207750@32,
(-16831941)@32 <=s L0x20015280, L0x20015280 <=s 16831941@32,
(-23565127)@32 <=s L0x2001528c, L0x2001528c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bb8, L0x20014bb8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014bc4, L0x20014bc4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bd0, L0x20014bd0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015278, L0x20015278 <=s 20207750@32,
(-16831941)@32 <=s L0x20015284, L0x20015284 <=s 16831941@32,
(-23565127)@32 <=s L0x20015290, L0x20015290 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c1c, L0x20014c1c <=s 20207750@32,
(-16831941)@32 <=s L0x20014c28, L0x20014c28 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c34, L0x20014c34 <=s 23565127@32,
(-20207750)@32 <=s L0x200152dc, L0x200152dc <=s 20207750@32,
(-16831941)@32 <=s L0x200152e8, L0x200152e8 <=s 16831941@32,
(-23565127)@32 <=s L0x200152f4, L0x200152f4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c20, L0x20014c20 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c2c, L0x20014c2c <=s 16831941@32,
(-23565127)@32 <=s L0x20014c38, L0x20014c38 <=s 23565127@32,
(-20207750)@32 <=s L0x200152e0, L0x200152e0 <=s 20207750@32,
(-16831941)@32 <=s L0x200152ec, L0x200152ec <=s 16831941@32,
(-23565127)@32 <=s L0x200152f8, L0x200152f8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c24, L0x20014c24 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c30, L0x20014c30 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c3c, L0x20014c3c <=s 23565127@32,
(-20207750)@32 <=s L0x200152e4, L0x200152e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200152f0, L0x200152f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200152fc, L0x200152fc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c88, L0x20014c88 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c94, L0x20014c94 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ca0, L0x20014ca0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015348, L0x20015348 <=s 20207750@32,
(-16831941)@32 <=s L0x20015354, L0x20015354 <=s 16831941@32,
(-23565127)@32 <=s L0x20015360, L0x20015360 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c8c, L0x20014c8c <=s 20207750@32,
(-16831941)@32 <=s L0x20014c98, L0x20014c98 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ca4, L0x20014ca4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001534c, L0x2001534c <=s 20207750@32,
(-16831941)@32 <=s L0x20015358, L0x20015358 <=s 16831941@32,
(-23565127)@32 <=s L0x20015364, L0x20015364 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c90, L0x20014c90 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c9c, L0x20014c9c <=s 16831941@32,
(-23565127)@32 <=s L0x20014ca8, L0x20014ca8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015350, L0x20015350 <=s 20207750@32,
(-16831941)@32 <=s L0x2001535c, L0x2001535c <=s 16831941@32,
(-23565127)@32 <=s L0x20015368, L0x20015368 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cf4, L0x20014cf4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d00, L0x20014d00 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d0c, L0x20014d0c <=s 23565127@32,
(-20207750)@32 <=s L0x200153b4, L0x200153b4 <=s 20207750@32,
(-16831941)@32 <=s L0x200153c0, L0x200153c0 <=s 16831941@32,
(-23565127)@32 <=s L0x200153cc, L0x200153cc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cf8, L0x20014cf8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d04, L0x20014d04 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d10, L0x20014d10 <=s 23565127@32,
(-20207750)@32 <=s L0x200153b8, L0x200153b8 <=s 20207750@32,
(-16831941)@32 <=s L0x200153c4, L0x200153c4 <=s 16831941@32,
(-23565127)@32 <=s L0x200153d0, L0x200153d0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cfc, L0x20014cfc <=s 20207750@32,
(-16831941)@32 <=s L0x20014d08, L0x20014d08 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d14, L0x20014d14 <=s 23565127@32,
(-20207750)@32 <=s L0x200153bc, L0x200153bc <=s 20207750@32,
(-16831941)@32 <=s L0x200153c8, L0x200153c8 <=s 16831941@32,
(-23565127)@32 <=s L0x200153d4, L0x200153d4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d60, L0x20014d60 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d6c, L0x20014d6c <=s 16831941@32,
(-23565127)@32 <=s L0x20014d78, L0x20014d78 <=s 23565127@32,
(-20207750)@32 <=s L0x20015420, L0x20015420 <=s 20207750@32,
(-16831941)@32 <=s L0x2001542c, L0x2001542c <=s 16831941@32,
(-23565127)@32 <=s L0x20015438, L0x20015438 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d64, L0x20014d64 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d70, L0x20014d70 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d7c, L0x20014d7c <=s 23565127@32,
(-20207750)@32 <=s L0x20015424, L0x20015424 <=s 20207750@32,
(-16831941)@32 <=s L0x20015430, L0x20015430 <=s 16831941@32,
(-23565127)@32 <=s L0x2001543c, L0x2001543c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d68, L0x20014d68 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d74, L0x20014d74 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d80, L0x20014d80 <=s 23565127@32,
(-20207750)@32 <=s L0x20015428, L0x20015428 <=s 20207750@32,
(-16831941)@32 <=s L0x20015434, L0x20015434 <=s 16831941@32,
(-23565127)@32 <=s L0x20015440, L0x20015440 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014dcc, L0x20014dcc <=s 20207750@32,
(-16831941)@32 <=s L0x20014dd8, L0x20014dd8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014de4, L0x20014de4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001548c, L0x2001548c <=s 20207750@32,
(-16831941)@32 <=s L0x20015498, L0x20015498 <=s 16831941@32,
(-23565127)@32 <=s L0x200154a4, L0x200154a4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014dd0, L0x20014dd0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ddc, L0x20014ddc <=s 16831941@32,
(-23565127)@32 <=s L0x20014de8, L0x20014de8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015490, L0x20015490 <=s 20207750@32,
(-16831941)@32 <=s L0x2001549c, L0x2001549c <=s 16831941@32,
(-23565127)@32 <=s L0x200154a8, L0x200154a8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014dd4, L0x20014dd4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014de0, L0x20014de0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014dec, L0x20014dec <=s 23565127@32,
(-20207750)@32 <=s L0x20015494, L0x20015494 <=s 20207750@32,
(-16831941)@32 <=s L0x200154a0, L0x200154a0 <=s 16831941@32,
(-23565127)@32 <=s L0x200154ac, L0x200154ac <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e38, L0x20014e38 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e44, L0x20014e44 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e50, L0x20014e50 <=s 23565127@32,
(-20207750)@32 <=s L0x200154f8, L0x200154f8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015504, L0x20015504 <=s 16831941@32,
(-23565127)@32 <=s L0x20015510, L0x20015510 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e3c, L0x20014e3c <=s 20207750@32,
(-16831941)@32 <=s L0x20014e48, L0x20014e48 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e54, L0x20014e54 <=s 23565127@32,
(-20207750)@32 <=s L0x200154fc, L0x200154fc <=s 20207750@32,
(-16831941)@32 <=s L0x20015508, L0x20015508 <=s 16831941@32,
(-23565127)@32 <=s L0x20015514, L0x20015514 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e40, L0x20014e40 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e4c, L0x20014e4c <=s 16831941@32,
(-23565127)@32 <=s L0x20014e58, L0x20014e58 <=s 23565127@32,
(-20207750)@32 <=s L0x20015500, L0x20015500 <=s 20207750@32,
(-16831941)@32 <=s L0x2001550c, L0x2001550c <=s 16831941@32,
(-23565127)@32 <=s L0x20015518, L0x20015518 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ea4, L0x20014ea4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014eb0, L0x20014eb0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ebc, L0x20014ebc <=s 23565127@32,
(-20207750)@32 <=s L0x20015564, L0x20015564 <=s 20207750@32,
(-16831941)@32 <=s L0x20015570, L0x20015570 <=s 16831941@32,
(-23565127)@32 <=s L0x2001557c, L0x2001557c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ea8, L0x20014ea8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014eb4, L0x20014eb4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ec0, L0x20014ec0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015568, L0x20015568 <=s 20207750@32,
(-16831941)@32 <=s L0x20015574, L0x20015574 <=s 16831941@32,
(-23565127)@32 <=s L0x20015580, L0x20015580 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014eac, L0x20014eac <=s 20207750@32,
(-16831941)@32 <=s L0x20014eb8, L0x20014eb8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ec4, L0x20014ec4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001556c, L0x2001556c <=s 20207750@32,
(-16831941)@32 <=s L0x20015578, L0x20015578 <=s 16831941@32,
(-23565127)@32 <=s L0x20015584, L0x20015584 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014f10, L0x20014f10 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f1c, L0x20014f1c <=s 16831941@32,
(-23565127)@32 <=s L0x20014f28, L0x20014f28 <=s 23565127@32,
(-20207750)@32 <=s L0x200155d0, L0x200155d0 <=s 20207750@32,
(-16831941)@32 <=s L0x200155dc, L0x200155dc <=s 16831941@32,
(-23565127)@32 <=s L0x200155e8, L0x200155e8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014f14, L0x20014f14 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f20, L0x20014f20 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f2c, L0x20014f2c <=s 23565127@32,
(-20207750)@32 <=s L0x200155d4, L0x200155d4 <=s 20207750@32,
(-16831941)@32 <=s L0x200155e0, L0x200155e0 <=s 16831941@32,
(-23565127)@32 <=s L0x200155ec, L0x200155ec <=s 23565127@32
,
(-20207750)@32 <=s L0x20014f18, L0x20014f18 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f24, L0x20014f24 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f30, L0x20014f30 <=s 23565127@32,
(-20207750)@32 <=s L0x200155d8, L0x200155d8 <=s 20207750@32,
(-16831941)@32 <=s L0x200155e4, L0x200155e4 <=s 16831941@32,
(-23565127)@32 <=s L0x200155f0, L0x200155f0 <=s 23565127@32
,
(-20207750)@32 <=s L0x200148e0, L0x200148e0 <=s 20207750@32,
(-16831941)@32 <=s L0x200148ec, L0x200148ec <=s 16831941@32,
(-23565127)@32 <=s L0x200148f8, L0x200148f8 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fa0, L0x20014fa0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014fac, L0x20014fac <=s 16831941@32,
(-23565127)@32 <=s L0x20014fb8, L0x20014fb8 <=s 23565127@32
,
(-20207750)@32 <=s L0x200148e4, L0x200148e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200148f0, L0x200148f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200148fc, L0x200148fc <=s 23565127@32,
(-20207750)@32 <=s L0x20014fa4, L0x20014fa4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014fb0, L0x20014fb0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014fbc, L0x20014fbc <=s 23565127@32
,
(-20207750)@32 <=s L0x200148e8, L0x200148e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200148f4, L0x200148f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014900, L0x20014900 <=s 23565127@32,
(-20207750)@32 <=s L0x20014fa8, L0x20014fa8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014fb4, L0x20014fb4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014fc0, L0x20014fc0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001494c, L0x2001494c <=s 20207750@32,
(-16831941)@32 <=s L0x20014958, L0x20014958 <=s 16831941@32,
(-23565127)@32 <=s L0x20014964, L0x20014964 <=s 23565127@32,
(-20207750)@32 <=s L0x2001500c, L0x2001500c <=s 20207750@32,
(-16831941)@32 <=s L0x20015018, L0x20015018 <=s 16831941@32,
(-23565127)@32 <=s L0x20015024, L0x20015024 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014950, L0x20014950 <=s 20207750@32,
(-16831941)@32 <=s L0x2001495c, L0x2001495c <=s 16831941@32,
(-23565127)@32 <=s L0x20014968, L0x20014968 <=s 23565127@32,
(-20207750)@32 <=s L0x20015010, L0x20015010 <=s 20207750@32,
(-16831941)@32 <=s L0x2001501c, L0x2001501c <=s 16831941@32,
(-23565127)@32 <=s L0x20015028, L0x20015028 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014954, L0x20014954 <=s 20207750@32,
(-16831941)@32 <=s L0x20014960, L0x20014960 <=s 16831941@32,
(-23565127)@32 <=s L0x2001496c, L0x2001496c <=s 23565127@32,
(-20207750)@32 <=s L0x20015014, L0x20015014 <=s 20207750@32,
(-16831941)@32 <=s L0x20015020, L0x20015020 <=s 16831941@32,
(-23565127)@32 <=s L0x2001502c, L0x2001502c <=s 23565127@32
,
(-20207750)@32 <=s L0x200149b8, L0x200149b8 <=s 20207750@32,
(-16831941)@32 <=s L0x200149c4, L0x200149c4 <=s 16831941@32,
(-23565127)@32 <=s L0x200149d0, L0x200149d0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015078, L0x20015078 <=s 20207750@32,
(-16831941)@32 <=s L0x20015084, L0x20015084 <=s 16831941@32,
(-23565127)@32 <=s L0x20015090, L0x20015090 <=s 23565127@32
,
(-20207750)@32 <=s L0x200149bc, L0x200149bc <=s 20207750@32,
(-16831941)@32 <=s L0x200149c8, L0x200149c8 <=s 16831941@32,
(-23565127)@32 <=s L0x200149d4, L0x200149d4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001507c, L0x2001507c <=s 20207750@32,
(-16831941)@32 <=s L0x20015088, L0x20015088 <=s 16831941@32,
(-23565127)@32 <=s L0x20015094, L0x20015094 <=s 23565127@32
,
(-20207750)@32 <=s L0x200149c0, L0x200149c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200149cc, L0x200149cc <=s 16831941@32,
(-23565127)@32 <=s L0x200149d8, L0x200149d8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015080, L0x20015080 <=s 20207750@32,
(-16831941)@32 <=s L0x2001508c, L0x2001508c <=s 16831941@32,
(-23565127)@32 <=s L0x20015098, L0x20015098 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a24, L0x20014a24 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a30, L0x20014a30 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a3c, L0x20014a3c <=s 23565127@32,
(-20207750)@32 <=s L0x200150e4, L0x200150e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200150f0, L0x200150f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200150fc, L0x200150fc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a28, L0x20014a28 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a34, L0x20014a34 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a40, L0x20014a40 <=s 23565127@32,
(-20207750)@32 <=s L0x200150e8, L0x200150e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200150f4, L0x200150f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015100, L0x20015100 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a2c, L0x20014a2c <=s 20207750@32,
(-16831941)@32 <=s L0x20014a38, L0x20014a38 <=s 16831941@32,
(-23565127)@32 <=s L0x20014a44, L0x20014a44 <=s 23565127@32,
(-20207750)@32 <=s L0x200150ec, L0x200150ec <=s 20207750@32,
(-16831941)@32 <=s L0x200150f8, L0x200150f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015104, L0x20015104 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a90, L0x20014a90 <=s 20207750@32,
(-16831941)@32 <=s L0x20014a9c, L0x20014a9c <=s 16831941@32,
(-23565127)@32 <=s L0x20014aa8, L0x20014aa8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015150, L0x20015150 <=s 20207750@32,
(-16831941)@32 <=s L0x2001515c, L0x2001515c <=s 16831941@32,
(-23565127)@32 <=s L0x20015168, L0x20015168 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a94, L0x20014a94 <=s 20207750@32,
(-16831941)@32 <=s L0x20014aa0, L0x20014aa0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014aac, L0x20014aac <=s 23565127@32,
(-20207750)@32 <=s L0x20015154, L0x20015154 <=s 20207750@32,
(-16831941)@32 <=s L0x20015160, L0x20015160 <=s 16831941@32,
(-23565127)@32 <=s L0x2001516c, L0x2001516c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014a98, L0x20014a98 <=s 20207750@32,
(-16831941)@32 <=s L0x20014aa4, L0x20014aa4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ab0, L0x20014ab0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015158, L0x20015158 <=s 20207750@32,
(-16831941)@32 <=s L0x20015164, L0x20015164 <=s 16831941@32,
(-23565127)@32 <=s L0x20015170, L0x20015170 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014afc, L0x20014afc <=s 20207750@32,
(-16831941)@32 <=s L0x20014b08, L0x20014b08 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b14, L0x20014b14 <=s 23565127@32,
(-20207750)@32 <=s L0x200151bc, L0x200151bc <=s 20207750@32,
(-16831941)@32 <=s L0x200151c8, L0x200151c8 <=s 16831941@32,
(-23565127)@32 <=s L0x200151d4, L0x200151d4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b00, L0x20014b00 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b0c, L0x20014b0c <=s 16831941@32,
(-23565127)@32 <=s L0x20014b18, L0x20014b18 <=s 23565127@32,
(-20207750)@32 <=s L0x200151c0, L0x200151c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200151cc, L0x200151cc <=s 16831941@32,
(-23565127)@32 <=s L0x200151d8, L0x200151d8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b04, L0x20014b04 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b10, L0x20014b10 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b1c, L0x20014b1c <=s 23565127@32,
(-20207750)@32 <=s L0x200151c4, L0x200151c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200151d0, L0x200151d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200151dc, L0x200151dc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b68, L0x20014b68 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b74, L0x20014b74 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b80, L0x20014b80 <=s 23565127@32,
(-20207750)@32 <=s L0x20015228, L0x20015228 <=s 20207750@32,
(-16831941)@32 <=s L0x20015234, L0x20015234 <=s 16831941@32,
(-23565127)@32 <=s L0x20015240, L0x20015240 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b6c, L0x20014b6c <=s 20207750@32,
(-16831941)@32 <=s L0x20014b78, L0x20014b78 <=s 16831941@32,
(-23565127)@32 <=s L0x20014b84, L0x20014b84 <=s 23565127@32,
(-20207750)@32 <=s L0x2001522c, L0x2001522c <=s 20207750@32,
(-16831941)@32 <=s L0x20015238, L0x20015238 <=s 16831941@32,
(-23565127)@32 <=s L0x20015244, L0x20015244 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014b70, L0x20014b70 <=s 20207750@32,
(-16831941)@32 <=s L0x20014b7c, L0x20014b7c <=s 16831941@32,
(-23565127)@32 <=s L0x20014b88, L0x20014b88 <=s 23565127@32,
(-20207750)@32 <=s L0x20015230, L0x20015230 <=s 20207750@32,
(-16831941)@32 <=s L0x2001523c, L0x2001523c <=s 16831941@32,
(-23565127)@32 <=s L0x20015248, L0x20015248 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bd4, L0x20014bd4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014be0, L0x20014be0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bec, L0x20014bec <=s 23565127@32,
(-20207750)@32 <=s L0x20015294, L0x20015294 <=s 20207750@32,
(-16831941)@32 <=s L0x200152a0, L0x200152a0 <=s 16831941@32,
(-23565127)@32 <=s L0x200152ac, L0x200152ac <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bd8, L0x20014bd8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014be4, L0x20014be4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bf0, L0x20014bf0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015298, L0x20015298 <=s 20207750@32,
(-16831941)@32 <=s L0x200152a4, L0x200152a4 <=s 16831941@32,
(-23565127)@32 <=s L0x200152b0, L0x200152b0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014bdc, L0x20014bdc <=s 20207750@32,
(-16831941)@32 <=s L0x20014be8, L0x20014be8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014bf4, L0x20014bf4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001529c, L0x2001529c <=s 20207750@32,
(-16831941)@32 <=s L0x200152a8, L0x200152a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200152b4, L0x200152b4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c40, L0x20014c40 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c4c, L0x20014c4c <=s 16831941@32,
(-23565127)@32 <=s L0x20014c58, L0x20014c58 <=s 23565127@32,
(-20207750)@32 <=s L0x20015300, L0x20015300 <=s 20207750@32,
(-16831941)@32 <=s L0x2001530c, L0x2001530c <=s 16831941@32,
(-23565127)@32 <=s L0x20015318, L0x20015318 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c44, L0x20014c44 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c50, L0x20014c50 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c5c, L0x20014c5c <=s 23565127@32,
(-20207750)@32 <=s L0x20015304, L0x20015304 <=s 20207750@32,
(-16831941)@32 <=s L0x20015310, L0x20015310 <=s 16831941@32,
(-23565127)@32 <=s L0x2001531c, L0x2001531c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014c48, L0x20014c48 <=s 20207750@32,
(-16831941)@32 <=s L0x20014c54, L0x20014c54 <=s 16831941@32,
(-23565127)@32 <=s L0x20014c60, L0x20014c60 <=s 23565127@32,
(-20207750)@32 <=s L0x20015308, L0x20015308 <=s 20207750@32,
(-16831941)@32 <=s L0x20015314, L0x20015314 <=s 16831941@32,
(-23565127)@32 <=s L0x20015320, L0x20015320 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cac, L0x20014cac <=s 20207750@32,
(-16831941)@32 <=s L0x20014cb8, L0x20014cb8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014cc4, L0x20014cc4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001536c, L0x2001536c <=s 20207750@32,
(-16831941)@32 <=s L0x20015378, L0x20015378 <=s 16831941@32,
(-23565127)@32 <=s L0x20015384, L0x20015384 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cb0, L0x20014cb0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014cbc, L0x20014cbc <=s 16831941@32,
(-23565127)@32 <=s L0x20014cc8, L0x20014cc8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015370, L0x20015370 <=s 20207750@32,
(-16831941)@32 <=s L0x2001537c, L0x2001537c <=s 16831941@32,
(-23565127)@32 <=s L0x20015388, L0x20015388 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014cb4, L0x20014cb4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014cc0, L0x20014cc0 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ccc, L0x20014ccc <=s 23565127@32,
(-20207750)@32 <=s L0x20015374, L0x20015374 <=s 20207750@32,
(-16831941)@32 <=s L0x20015380, L0x20015380 <=s 16831941@32,
(-23565127)@32 <=s L0x2001538c, L0x2001538c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d18, L0x20014d18 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d24, L0x20014d24 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d30, L0x20014d30 <=s 23565127@32,
(-20207750)@32 <=s L0x200153d8, L0x200153d8 <=s 20207750@32,
(-16831941)@32 <=s L0x200153e4, L0x200153e4 <=s 16831941@32,
(-23565127)@32 <=s L0x200153f0, L0x200153f0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d1c, L0x20014d1c <=s 20207750@32,
(-16831941)@32 <=s L0x20014d28, L0x20014d28 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d34, L0x20014d34 <=s 23565127@32,
(-20207750)@32 <=s L0x200153dc, L0x200153dc <=s 20207750@32,
(-16831941)@32 <=s L0x200153e8, L0x200153e8 <=s 16831941@32,
(-23565127)@32 <=s L0x200153f4, L0x200153f4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d20, L0x20014d20 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d2c, L0x20014d2c <=s 16831941@32,
(-23565127)@32 <=s L0x20014d38, L0x20014d38 <=s 23565127@32,
(-20207750)@32 <=s L0x200153e0, L0x200153e0 <=s 20207750@32,
(-16831941)@32 <=s L0x200153ec, L0x200153ec <=s 16831941@32,
(-23565127)@32 <=s L0x200153f8, L0x200153f8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d84, L0x20014d84 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d90, L0x20014d90 <=s 16831941@32,
(-23565127)@32 <=s L0x20014d9c, L0x20014d9c <=s 23565127@32,
(-20207750)@32 <=s L0x20015444, L0x20015444 <=s 20207750@32,
(-16831941)@32 <=s L0x20015450, L0x20015450 <=s 16831941@32,
(-23565127)@32 <=s L0x2001545c, L0x2001545c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d88, L0x20014d88 <=s 20207750@32,
(-16831941)@32 <=s L0x20014d94, L0x20014d94 <=s 16831941@32,
(-23565127)@32 <=s L0x20014da0, L0x20014da0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015448, L0x20015448 <=s 20207750@32,
(-16831941)@32 <=s L0x20015454, L0x20015454 <=s 16831941@32,
(-23565127)@32 <=s L0x20015460, L0x20015460 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014d8c, L0x20014d8c <=s 20207750@32,
(-16831941)@32 <=s L0x20014d98, L0x20014d98 <=s 16831941@32,
(-23565127)@32 <=s L0x20014da4, L0x20014da4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001544c, L0x2001544c <=s 20207750@32,
(-16831941)@32 <=s L0x20015458, L0x20015458 <=s 16831941@32,
(-23565127)@32 <=s L0x20015464, L0x20015464 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014df0, L0x20014df0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014dfc, L0x20014dfc <=s 16831941@32,
(-23565127)@32 <=s L0x20014e08, L0x20014e08 <=s 23565127@32,
(-20207750)@32 <=s L0x200154b0, L0x200154b0 <=s 20207750@32,
(-16831941)@32 <=s L0x200154bc, L0x200154bc <=s 16831941@32,
(-23565127)@32 <=s L0x200154c8, L0x200154c8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014df4, L0x20014df4 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e00, L0x20014e00 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e0c, L0x20014e0c <=s 23565127@32,
(-20207750)@32 <=s L0x200154b4, L0x200154b4 <=s 20207750@32,
(-16831941)@32 <=s L0x200154c0, L0x200154c0 <=s 16831941@32,
(-23565127)@32 <=s L0x200154cc, L0x200154cc <=s 23565127@32
,
(-20207750)@32 <=s L0x20014df8, L0x20014df8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e04, L0x20014e04 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e10, L0x20014e10 <=s 23565127@32,
(-20207750)@32 <=s L0x200154b8, L0x200154b8 <=s 20207750@32,
(-16831941)@32 <=s L0x200154c4, L0x200154c4 <=s 16831941@32,
(-23565127)@32 <=s L0x200154d0, L0x200154d0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e5c, L0x20014e5c <=s 20207750@32,
(-16831941)@32 <=s L0x20014e68, L0x20014e68 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e74, L0x20014e74 <=s 23565127@32,
(-20207750)@32 <=s L0x2001551c, L0x2001551c <=s 20207750@32,
(-16831941)@32 <=s L0x20015528, L0x20015528 <=s 16831941@32,
(-23565127)@32 <=s L0x20015534, L0x20015534 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e60, L0x20014e60 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e6c, L0x20014e6c <=s 16831941@32,
(-23565127)@32 <=s L0x20014e78, L0x20014e78 <=s 23565127@32,
(-20207750)@32 <=s L0x20015520, L0x20015520 <=s 20207750@32,
(-16831941)@32 <=s L0x2001552c, L0x2001552c <=s 16831941@32,
(-23565127)@32 <=s L0x20015538, L0x20015538 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014e64, L0x20014e64 <=s 20207750@32,
(-16831941)@32 <=s L0x20014e70, L0x20014e70 <=s 16831941@32,
(-23565127)@32 <=s L0x20014e7c, L0x20014e7c <=s 23565127@32,
(-20207750)@32 <=s L0x20015524, L0x20015524 <=s 20207750@32,
(-16831941)@32 <=s L0x20015530, L0x20015530 <=s 16831941@32,
(-23565127)@32 <=s L0x2001553c, L0x2001553c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ec8, L0x20014ec8 <=s 20207750@32,
(-16831941)@32 <=s L0x20014ed4, L0x20014ed4 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ee0, L0x20014ee0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015588, L0x20015588 <=s 20207750@32,
(-16831941)@32 <=s L0x20015594, L0x20015594 <=s 16831941@32,
(-23565127)@32 <=s L0x200155a0, L0x200155a0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ecc, L0x20014ecc <=s 20207750@32,
(-16831941)@32 <=s L0x20014ed8, L0x20014ed8 <=s 16831941@32,
(-23565127)@32 <=s L0x20014ee4, L0x20014ee4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001558c, L0x2001558c <=s 20207750@32,
(-16831941)@32 <=s L0x20015598, L0x20015598 <=s 16831941@32,
(-23565127)@32 <=s L0x200155a4, L0x200155a4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014ed0, L0x20014ed0 <=s 20207750@32,
(-16831941)@32 <=s L0x20014edc, L0x20014edc <=s 16831941@32,
(-23565127)@32 <=s L0x20014ee8, L0x20014ee8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015590, L0x20015590 <=s 20207750@32,
(-16831941)@32 <=s L0x2001559c, L0x2001559c <=s 16831941@32,
(-23565127)@32 <=s L0x200155a8, L0x200155a8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014f34, L0x20014f34 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f40, L0x20014f40 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f4c, L0x20014f4c <=s 23565127@32,
(-20207750)@32 <=s L0x200155f4, L0x200155f4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015600, L0x20015600 <=s 16831941@32,
(-23565127)@32 <=s L0x2001560c, L0x2001560c <=s 23565127@32
,
(-20207750)@32 <=s L0x20014f38, L0x20014f38 <=s 20207750@32,
(-16831941)@32 <=s L0x20014f44, L0x20014f44 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f50, L0x20014f50 <=s 23565127@32,
(-20207750)@32 <=s L0x200155f8, L0x200155f8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015604, L0x20015604 <=s 16831941@32,
(-23565127)@32 <=s L0x20015610, L0x20015610 <=s 23565127@32
,
(-20207750)@32 <=s L0x20014f3c, L0x20014f3c <=s 20207750@32,
(-16831941)@32 <=s L0x20014f48, L0x20014f48 <=s 16831941@32,
(-23565127)@32 <=s L0x20014f54, L0x20014f54 <=s 23565127@32,
(-20207750)@32 <=s L0x200155fc, L0x200155fc <=s 20207750@32,
(-16831941)@32 <=s L0x20015608, L0x20015608 <=s 16831941@32,
(-23565127)@32 <=s L0x20015614, L0x20015614 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015618, L0x20015618 <=s 20207750@32,
(-16831941)@32 <=s L0x20015624, L0x20015624 <=s 16831941@32,
(-23565127)@32 <=s L0x20015630, L0x20015630 <=s 23565127@32,
(-20207750)@32 <=s L0x20015cd8, L0x20015cd8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ce4, L0x20015ce4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cf0, L0x20015cf0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001561c, L0x2001561c <=s 20207750@32,
(-16831941)@32 <=s L0x20015628, L0x20015628 <=s 16831941@32,
(-23565127)@32 <=s L0x20015634, L0x20015634 <=s 23565127@32,
(-20207750)@32 <=s L0x20015cdc, L0x20015cdc <=s 20207750@32,
(-16831941)@32 <=s L0x20015ce8, L0x20015ce8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cf4, L0x20015cf4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015620, L0x20015620 <=s 20207750@32,
(-16831941)@32 <=s L0x2001562c, L0x2001562c <=s 16831941@32,
(-23565127)@32 <=s L0x20015638, L0x20015638 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ce0, L0x20015ce0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015cec, L0x20015cec <=s 16831941@32,
(-23565127)@32 <=s L0x20015cf8, L0x20015cf8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015684, L0x20015684 <=s 20207750@32,
(-16831941)@32 <=s L0x20015690, L0x20015690 <=s 16831941@32,
(-23565127)@32 <=s L0x2001569c, L0x2001569c <=s 23565127@32,
(-20207750)@32 <=s L0x20015d44, L0x20015d44 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d50, L0x20015d50 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d5c, L0x20015d5c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015688, L0x20015688 <=s 20207750@32,
(-16831941)@32 <=s L0x20015694, L0x20015694 <=s 16831941@32,
(-23565127)@32 <=s L0x200156a0, L0x200156a0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d48, L0x20015d48 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d54, L0x20015d54 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d60, L0x20015d60 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001568c, L0x2001568c <=s 20207750@32,
(-16831941)@32 <=s L0x20015698, L0x20015698 <=s 16831941@32,
(-23565127)@32 <=s L0x200156a4, L0x200156a4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d4c, L0x20015d4c <=s 20207750@32,
(-16831941)@32 <=s L0x20015d58, L0x20015d58 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d64, L0x20015d64 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156f0, L0x200156f0 <=s 20207750@32,
(-16831941)@32 <=s L0x200156fc, L0x200156fc <=s 16831941@32,
(-23565127)@32 <=s L0x20015708, L0x20015708 <=s 23565127@32,
(-20207750)@32 <=s L0x20015db0, L0x20015db0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015dbc, L0x20015dbc <=s 16831941@32,
(-23565127)@32 <=s L0x20015dc8, L0x20015dc8 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156f4, L0x200156f4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015700, L0x20015700 <=s 16831941@32,
(-23565127)@32 <=s L0x2001570c, L0x2001570c <=s 23565127@32,
(-20207750)@32 <=s L0x20015db4, L0x20015db4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015dc0, L0x20015dc0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015dcc, L0x20015dcc <=s 23565127@32
,
(-20207750)@32 <=s L0x200156f8, L0x200156f8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015704, L0x20015704 <=s 16831941@32,
(-23565127)@32 <=s L0x20015710, L0x20015710 <=s 23565127@32,
(-20207750)@32 <=s L0x20015db8, L0x20015db8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015dc4, L0x20015dc4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015dd0, L0x20015dd0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001575c, L0x2001575c <=s 20207750@32,
(-16831941)@32 <=s L0x20015768, L0x20015768 <=s 16831941@32,
(-23565127)@32 <=s L0x20015774, L0x20015774 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e1c, L0x20015e1c <=s 20207750@32,
(-16831941)@32 <=s L0x20015e28, L0x20015e28 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e34, L0x20015e34 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015760, L0x20015760 <=s 20207750@32,
(-16831941)@32 <=s L0x2001576c, L0x2001576c <=s 16831941@32,
(-23565127)@32 <=s L0x20015778, L0x20015778 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e20, L0x20015e20 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e2c, L0x20015e2c <=s 16831941@32,
(-23565127)@32 <=s L0x20015e38, L0x20015e38 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015764, L0x20015764 <=s 20207750@32,
(-16831941)@32 <=s L0x20015770, L0x20015770 <=s 16831941@32,
(-23565127)@32 <=s L0x2001577c, L0x2001577c <=s 23565127@32,
(-20207750)@32 <=s L0x20015e24, L0x20015e24 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e30, L0x20015e30 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e3c, L0x20015e3c <=s 23565127@32
,
(-20207750)@32 <=s L0x200157c8, L0x200157c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200157d4, L0x200157d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200157e0, L0x200157e0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e88, L0x20015e88 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e94, L0x20015e94 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ea0, L0x20015ea0 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157cc, L0x200157cc <=s 20207750@32,
(-16831941)@32 <=s L0x200157d8, L0x200157d8 <=s 16831941@32,
(-23565127)@32 <=s L0x200157e4, L0x200157e4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e8c, L0x20015e8c <=s 20207750@32,
(-16831941)@32 <=s L0x20015e98, L0x20015e98 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ea4, L0x20015ea4 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157d0, L0x200157d0 <=s 20207750@32,
(-16831941)@32 <=s L0x200157dc, L0x200157dc <=s 16831941@32,
(-23565127)@32 <=s L0x200157e8, L0x200157e8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e90, L0x20015e90 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e9c, L0x20015e9c <=s 16831941@32,
(-23565127)@32 <=s L0x20015ea8, L0x20015ea8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015834, L0x20015834 <=s 20207750@32,
(-16831941)@32 <=s L0x20015840, L0x20015840 <=s 16831941@32,
(-23565127)@32 <=s L0x2001584c, L0x2001584c <=s 23565127@32,
(-20207750)@32 <=s L0x20015ef4, L0x20015ef4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f00, L0x20015f00 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f0c, L0x20015f0c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015838, L0x20015838 <=s 20207750@32,
(-16831941)@32 <=s L0x20015844, L0x20015844 <=s 16831941@32,
(-23565127)@32 <=s L0x20015850, L0x20015850 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ef8, L0x20015ef8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f04, L0x20015f04 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f10, L0x20015f10 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001583c, L0x2001583c <=s 20207750@32,
(-16831941)@32 <=s L0x20015848, L0x20015848 <=s 16831941@32,
(-23565127)@32 <=s L0x20015854, L0x20015854 <=s 23565127@32,
(-20207750)@32 <=s L0x20015efc, L0x20015efc <=s 20207750@32,
(-16831941)@32 <=s L0x20015f08, L0x20015f08 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f14, L0x20015f14 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158a0, L0x200158a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200158ac, L0x200158ac <=s 16831941@32,
(-23565127)@32 <=s L0x200158b8, L0x200158b8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f60, L0x20015f60 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f6c, L0x20015f6c <=s 16831941@32,
(-23565127)@32 <=s L0x20015f78, L0x20015f78 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158a4, L0x200158a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200158b0, L0x200158b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200158bc, L0x200158bc <=s 23565127@32,
(-20207750)@32 <=s L0x20015f64, L0x20015f64 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f70, L0x20015f70 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f7c, L0x20015f7c <=s 23565127@32
,
(-20207750)@32 <=s L0x200158a8, L0x200158a8 <=s 20207750@32,
(-16831941)@32 <=s L0x200158b4, L0x200158b4 <=s 16831941@32,
(-23565127)@32 <=s L0x200158c0, L0x200158c0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f68, L0x20015f68 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f74, L0x20015f74 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f80, L0x20015f80 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001590c, L0x2001590c <=s 20207750@32,
(-16831941)@32 <=s L0x20015918, L0x20015918 <=s 16831941@32,
(-23565127)@32 <=s L0x20015924, L0x20015924 <=s 23565127@32,
(-20207750)@32 <=s L0x20015fcc, L0x20015fcc <=s 20207750@32,
(-16831941)@32 <=s L0x20015fd8, L0x20015fd8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fe4, L0x20015fe4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015910, L0x20015910 <=s 20207750@32,
(-16831941)@32 <=s L0x2001591c, L0x2001591c <=s 16831941@32,
(-23565127)@32 <=s L0x20015928, L0x20015928 <=s 23565127@32,
(-20207750)@32 <=s L0x20015fd0, L0x20015fd0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015fdc, L0x20015fdc <=s 16831941@32,
(-23565127)@32 <=s L0x20015fe8, L0x20015fe8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015914, L0x20015914 <=s 20207750@32,
(-16831941)@32 <=s L0x20015920, L0x20015920 <=s 16831941@32,
(-23565127)@32 <=s L0x2001592c, L0x2001592c <=s 23565127@32,
(-20207750)@32 <=s L0x20015fd4, L0x20015fd4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015fe0, L0x20015fe0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fec, L0x20015fec <=s 23565127@32
,
(-20207750)@32 <=s L0x20015978, L0x20015978 <=s 20207750@32,
(-16831941)@32 <=s L0x20015984, L0x20015984 <=s 16831941@32,
(-23565127)@32 <=s L0x20015990, L0x20015990 <=s 23565127@32,
(-20207750)@32 <=s L0x20016038, L0x20016038 <=s 20207750@32,
(-16831941)@32 <=s L0x20016044, L0x20016044 <=s 16831941@32,
(-23565127)@32 <=s L0x20016050, L0x20016050 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001597c, L0x2001597c <=s 20207750@32,
(-16831941)@32 <=s L0x20015988, L0x20015988 <=s 16831941@32,
(-23565127)@32 <=s L0x20015994, L0x20015994 <=s 23565127@32,
(-20207750)@32 <=s L0x2001603c, L0x2001603c <=s 20207750@32,
(-16831941)@32 <=s L0x20016048, L0x20016048 <=s 16831941@32,
(-23565127)@32 <=s L0x20016054, L0x20016054 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015980, L0x20015980 <=s 20207750@32,
(-16831941)@32 <=s L0x2001598c, L0x2001598c <=s 16831941@32,
(-23565127)@32 <=s L0x20015998, L0x20015998 <=s 23565127@32,
(-20207750)@32 <=s L0x20016040, L0x20016040 <=s 20207750@32,
(-16831941)@32 <=s L0x2001604c, L0x2001604c <=s 16831941@32,
(-23565127)@32 <=s L0x20016058, L0x20016058 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159e4, L0x200159e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200159f0, L0x200159f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200159fc, L0x200159fc <=s 23565127@32,
(-20207750)@32 <=s L0x200160a4, L0x200160a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200160b0, L0x200160b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200160bc, L0x200160bc <=s 23565127@32
,
(-20207750)@32 <=s L0x200159e8, L0x200159e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200159f4, L0x200159f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a00, L0x20015a00 <=s 23565127@32,
(-20207750)@32 <=s L0x200160a8, L0x200160a8 <=s 20207750@32,
(-16831941)@32 <=s L0x200160b4, L0x200160b4 <=s 16831941@32,
(-23565127)@32 <=s L0x200160c0, L0x200160c0 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159ec, L0x200159ec <=s 20207750@32,
(-16831941)@32 <=s L0x200159f8, L0x200159f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a04, L0x20015a04 <=s 23565127@32,
(-20207750)@32 <=s L0x200160ac, L0x200160ac <=s 20207750@32,
(-16831941)@32 <=s L0x200160b8, L0x200160b8 <=s 16831941@32,
(-23565127)@32 <=s L0x200160c4, L0x200160c4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a50, L0x20015a50 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a5c, L0x20015a5c <=s 16831941@32,
(-23565127)@32 <=s L0x20015a68, L0x20015a68 <=s 23565127@32,
(-20207750)@32 <=s L0x20016110, L0x20016110 <=s 20207750@32,
(-16831941)@32 <=s L0x2001611c, L0x2001611c <=s 16831941@32,
(-23565127)@32 <=s L0x20016128, L0x20016128 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a54, L0x20015a54 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a60, L0x20015a60 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a6c, L0x20015a6c <=s 23565127@32,
(-20207750)@32 <=s L0x20016114, L0x20016114 <=s 20207750@32,
(-16831941)@32 <=s L0x20016120, L0x20016120 <=s 16831941@32,
(-23565127)@32 <=s L0x2001612c, L0x2001612c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a58, L0x20015a58 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a64, L0x20015a64 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a70, L0x20015a70 <=s 23565127@32,
(-20207750)@32 <=s L0x20016118, L0x20016118 <=s 20207750@32,
(-16831941)@32 <=s L0x20016124, L0x20016124 <=s 16831941@32,
(-23565127)@32 <=s L0x20016130, L0x20016130 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015abc, L0x20015abc <=s 20207750@32,
(-16831941)@32 <=s L0x20015ac8, L0x20015ac8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ad4, L0x20015ad4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001617c, L0x2001617c <=s 20207750@32,
(-16831941)@32 <=s L0x20016188, L0x20016188 <=s 16831941@32,
(-23565127)@32 <=s L0x20016194, L0x20016194 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015ac0, L0x20015ac0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015acc, L0x20015acc <=s 16831941@32,
(-23565127)@32 <=s L0x20015ad8, L0x20015ad8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016180, L0x20016180 <=s 20207750@32,
(-16831941)@32 <=s L0x2001618c, L0x2001618c <=s 16831941@32,
(-23565127)@32 <=s L0x20016198, L0x20016198 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015ac4, L0x20015ac4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ad0, L0x20015ad0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015adc, L0x20015adc <=s 23565127@32,
(-20207750)@32 <=s L0x20016184, L0x20016184 <=s 20207750@32,
(-16831941)@32 <=s L0x20016190, L0x20016190 <=s 16831941@32,
(-23565127)@32 <=s L0x2001619c, L0x2001619c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b28, L0x20015b28 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b34, L0x20015b34 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b40, L0x20015b40 <=s 23565127@32,
(-20207750)@32 <=s L0x200161e8, L0x200161e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200161f4, L0x200161f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20016200, L0x20016200 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b2c, L0x20015b2c <=s 20207750@32,
(-16831941)@32 <=s L0x20015b38, L0x20015b38 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b44, L0x20015b44 <=s 23565127@32,
(-20207750)@32 <=s L0x200161ec, L0x200161ec <=s 20207750@32,
(-16831941)@32 <=s L0x200161f8, L0x200161f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20016204, L0x20016204 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b30, L0x20015b30 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b3c, L0x20015b3c <=s 16831941@32,
(-23565127)@32 <=s L0x20015b48, L0x20015b48 <=s 23565127@32,
(-20207750)@32 <=s L0x200161f0, L0x200161f0 <=s 20207750@32,
(-16831941)@32 <=s L0x200161fc, L0x200161fc <=s 16831941@32,
(-23565127)@32 <=s L0x20016208, L0x20016208 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b94, L0x20015b94 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ba0, L0x20015ba0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bac, L0x20015bac <=s 23565127@32,
(-20207750)@32 <=s L0x20016254, L0x20016254 <=s 20207750@32,
(-16831941)@32 <=s L0x20016260, L0x20016260 <=s 16831941@32,
(-23565127)@32 <=s L0x2001626c, L0x2001626c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b98, L0x20015b98 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ba4, L0x20015ba4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bb0, L0x20015bb0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016258, L0x20016258 <=s 20207750@32,
(-16831941)@32 <=s L0x20016264, L0x20016264 <=s 16831941@32,
(-23565127)@32 <=s L0x20016270, L0x20016270 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b9c, L0x20015b9c <=s 20207750@32,
(-16831941)@32 <=s L0x20015ba8, L0x20015ba8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bb4, L0x20015bb4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001625c, L0x2001625c <=s 20207750@32,
(-16831941)@32 <=s L0x20016268, L0x20016268 <=s 16831941@32,
(-23565127)@32 <=s L0x20016274, L0x20016274 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c00, L0x20015c00 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c0c, L0x20015c0c <=s 16831941@32,
(-23565127)@32 <=s L0x20015c18, L0x20015c18 <=s 23565127@32,
(-20207750)@32 <=s L0x200162c0, L0x200162c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200162cc, L0x200162cc <=s 16831941@32,
(-23565127)@32 <=s L0x200162d8, L0x200162d8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c04, L0x20015c04 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c10, L0x20015c10 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c1c, L0x20015c1c <=s 23565127@32,
(-20207750)@32 <=s L0x200162c4, L0x200162c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200162d0, L0x200162d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200162dc, L0x200162dc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c08, L0x20015c08 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c14, L0x20015c14 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c20, L0x20015c20 <=s 23565127@32,
(-20207750)@32 <=s L0x200162c8, L0x200162c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200162d4, L0x200162d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200162e0, L0x200162e0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c6c, L0x20015c6c <=s 20207750@32,
(-16831941)@32 <=s L0x20015c78, L0x20015c78 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c84, L0x20015c84 <=s 23565127@32,
(-20207750)@32 <=s L0x2001632c, L0x2001632c <=s 20207750@32,
(-16831941)@32 <=s L0x20016338, L0x20016338 <=s 16831941@32,
(-23565127)@32 <=s L0x20016344, L0x20016344 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c70, L0x20015c70 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c7c, L0x20015c7c <=s 16831941@32,
(-23565127)@32 <=s L0x20015c88, L0x20015c88 <=s 23565127@32,
(-20207750)@32 <=s L0x20016330, L0x20016330 <=s 20207750@32,
(-16831941)@32 <=s L0x2001633c, L0x2001633c <=s 16831941@32,
(-23565127)@32 <=s L0x20016348, L0x20016348 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c74, L0x20015c74 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c80, L0x20015c80 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c8c, L0x20015c8c <=s 23565127@32,
(-20207750)@32 <=s L0x20016334, L0x20016334 <=s 20207750@32,
(-16831941)@32 <=s L0x20016340, L0x20016340 <=s 16831941@32,
(-23565127)@32 <=s L0x2001634c, L0x2001634c <=s 23565127@32
,
(-20207750)@32 <=s L0x2001563c, L0x2001563c <=s 20207750@32,
(-16831941)@32 <=s L0x20015648, L0x20015648 <=s 16831941@32,
(-23565127)@32 <=s L0x20015654, L0x20015654 <=s 23565127@32,
(-20207750)@32 <=s L0x20015cfc, L0x20015cfc <=s 20207750@32,
(-16831941)@32 <=s L0x20015d08, L0x20015d08 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d14, L0x20015d14 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015640, L0x20015640 <=s 20207750@32,
(-16831941)@32 <=s L0x2001564c, L0x2001564c <=s 16831941@32,
(-23565127)@32 <=s L0x20015658, L0x20015658 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d00, L0x20015d00 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d0c, L0x20015d0c <=s 16831941@32,
(-23565127)@32 <=s L0x20015d18, L0x20015d18 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015644, L0x20015644 <=s 20207750@32,
(-16831941)@32 <=s L0x20015650, L0x20015650 <=s 16831941@32,
(-23565127)@32 <=s L0x2001565c, L0x2001565c <=s 23565127@32,
(-20207750)@32 <=s L0x20015d04, L0x20015d04 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d10, L0x20015d10 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d1c, L0x20015d1c <=s 23565127@32
,
(-20207750)@32 <=s L0x200156a8, L0x200156a8 <=s 20207750@32,
(-16831941)@32 <=s L0x200156b4, L0x200156b4 <=s 16831941@32,
(-23565127)@32 <=s L0x200156c0, L0x200156c0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d68, L0x20015d68 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d74, L0x20015d74 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d80, L0x20015d80 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156ac, L0x200156ac <=s 20207750@32,
(-16831941)@32 <=s L0x200156b8, L0x200156b8 <=s 16831941@32,
(-23565127)@32 <=s L0x200156c4, L0x200156c4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d6c, L0x20015d6c <=s 20207750@32,
(-16831941)@32 <=s L0x20015d78, L0x20015d78 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d84, L0x20015d84 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156b0, L0x200156b0 <=s 20207750@32,
(-16831941)@32 <=s L0x200156bc, L0x200156bc <=s 16831941@32,
(-23565127)@32 <=s L0x200156c8, L0x200156c8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d70, L0x20015d70 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d7c, L0x20015d7c <=s 16831941@32,
(-23565127)@32 <=s L0x20015d88, L0x20015d88 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015714, L0x20015714 <=s 20207750@32,
(-16831941)@32 <=s L0x20015720, L0x20015720 <=s 16831941@32,
(-23565127)@32 <=s L0x2001572c, L0x2001572c <=s 23565127@32,
(-20207750)@32 <=s L0x20015dd4, L0x20015dd4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015de0, L0x20015de0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015dec, L0x20015dec <=s 23565127@32
,
(-20207750)@32 <=s L0x20015718, L0x20015718 <=s 20207750@32,
(-16831941)@32 <=s L0x20015724, L0x20015724 <=s 16831941@32,
(-23565127)@32 <=s L0x20015730, L0x20015730 <=s 23565127@32,
(-20207750)@32 <=s L0x20015dd8, L0x20015dd8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015de4, L0x20015de4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015df0, L0x20015df0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001571c, L0x2001571c <=s 20207750@32,
(-16831941)@32 <=s L0x20015728, L0x20015728 <=s 16831941@32,
(-23565127)@32 <=s L0x20015734, L0x20015734 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ddc, L0x20015ddc <=s 20207750@32,
(-16831941)@32 <=s L0x20015de8, L0x20015de8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015df4, L0x20015df4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015780, L0x20015780 <=s 20207750@32,
(-16831941)@32 <=s L0x2001578c, L0x2001578c <=s 16831941@32,
(-23565127)@32 <=s L0x20015798, L0x20015798 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e40, L0x20015e40 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e4c, L0x20015e4c <=s 16831941@32,
(-23565127)@32 <=s L0x20015e58, L0x20015e58 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015784, L0x20015784 <=s 20207750@32,
(-16831941)@32 <=s L0x20015790, L0x20015790 <=s 16831941@32,
(-23565127)@32 <=s L0x2001579c, L0x2001579c <=s 23565127@32,
(-20207750)@32 <=s L0x20015e44, L0x20015e44 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e50, L0x20015e50 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e5c, L0x20015e5c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015788, L0x20015788 <=s 20207750@32,
(-16831941)@32 <=s L0x20015794, L0x20015794 <=s 16831941@32,
(-23565127)@32 <=s L0x200157a0, L0x200157a0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e48, L0x20015e48 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e54, L0x20015e54 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e60, L0x20015e60 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157ec, L0x200157ec <=s 20207750@32,
(-16831941)@32 <=s L0x200157f8, L0x200157f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015804, L0x20015804 <=s 23565127@32,
(-20207750)@32 <=s L0x20015eac, L0x20015eac <=s 20207750@32,
(-16831941)@32 <=s L0x20015eb8, L0x20015eb8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ec4, L0x20015ec4 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157f0, L0x200157f0 <=s 20207750@32,
(-16831941)@32 <=s L0x200157fc, L0x200157fc <=s 16831941@32,
(-23565127)@32 <=s L0x20015808, L0x20015808 <=s 23565127@32,
(-20207750)@32 <=s L0x20015eb0, L0x20015eb0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ebc, L0x20015ebc <=s 16831941@32,
(-23565127)@32 <=s L0x20015ec8, L0x20015ec8 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157f4, L0x200157f4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015800, L0x20015800 <=s 16831941@32,
(-23565127)@32 <=s L0x2001580c, L0x2001580c <=s 23565127@32,
(-20207750)@32 <=s L0x20015eb4, L0x20015eb4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ec0, L0x20015ec0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ecc, L0x20015ecc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015858, L0x20015858 <=s 20207750@32,
(-16831941)@32 <=s L0x20015864, L0x20015864 <=s 16831941@32,
(-23565127)@32 <=s L0x20015870, L0x20015870 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f18, L0x20015f18 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f24, L0x20015f24 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f30, L0x20015f30 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001585c, L0x2001585c <=s 20207750@32,
(-16831941)@32 <=s L0x20015868, L0x20015868 <=s 16831941@32,
(-23565127)@32 <=s L0x20015874, L0x20015874 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f1c, L0x20015f1c <=s 20207750@32,
(-16831941)@32 <=s L0x20015f28, L0x20015f28 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f34, L0x20015f34 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015860, L0x20015860 <=s 20207750@32,
(-16831941)@32 <=s L0x2001586c, L0x2001586c <=s 16831941@32,
(-23565127)@32 <=s L0x20015878, L0x20015878 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f20, L0x20015f20 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f2c, L0x20015f2c <=s 16831941@32,
(-23565127)@32 <=s L0x20015f38, L0x20015f38 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158c4, L0x200158c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200158d0, L0x200158d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200158dc, L0x200158dc <=s 23565127@32,
(-20207750)@32 <=s L0x20015f84, L0x20015f84 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f90, L0x20015f90 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f9c, L0x20015f9c <=s 23565127@32
,
(-20207750)@32 <=s L0x200158c8, L0x200158c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200158d4, L0x200158d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200158e0, L0x200158e0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f88, L0x20015f88 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f94, L0x20015f94 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fa0, L0x20015fa0 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158cc, L0x200158cc <=s 20207750@32,
(-16831941)@32 <=s L0x200158d8, L0x200158d8 <=s 16831941@32,
(-23565127)@32 <=s L0x200158e4, L0x200158e4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f8c, L0x20015f8c <=s 20207750@32,
(-16831941)@32 <=s L0x20015f98, L0x20015f98 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fa4, L0x20015fa4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015930, L0x20015930 <=s 20207750@32,
(-16831941)@32 <=s L0x2001593c, L0x2001593c <=s 16831941@32,
(-23565127)@32 <=s L0x20015948, L0x20015948 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ff0, L0x20015ff0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ffc, L0x20015ffc <=s 16831941@32,
(-23565127)@32 <=s L0x20016008, L0x20016008 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015934, L0x20015934 <=s 20207750@32,
(-16831941)@32 <=s L0x20015940, L0x20015940 <=s 16831941@32,
(-23565127)@32 <=s L0x2001594c, L0x2001594c <=s 23565127@32,
(-20207750)@32 <=s L0x20015ff4, L0x20015ff4 <=s 20207750@32,
(-16831941)@32 <=s L0x20016000, L0x20016000 <=s 16831941@32,
(-23565127)@32 <=s L0x2001600c, L0x2001600c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015938, L0x20015938 <=s 20207750@32,
(-16831941)@32 <=s L0x20015944, L0x20015944 <=s 16831941@32,
(-23565127)@32 <=s L0x20015950, L0x20015950 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ff8, L0x20015ff8 <=s 20207750@32,
(-16831941)@32 <=s L0x20016004, L0x20016004 <=s 16831941@32,
(-23565127)@32 <=s L0x20016010, L0x20016010 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001599c, L0x2001599c <=s 20207750@32,
(-16831941)@32 <=s L0x200159a8, L0x200159a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200159b4, L0x200159b4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001605c, L0x2001605c <=s 20207750@32,
(-16831941)@32 <=s L0x20016068, L0x20016068 <=s 16831941@32,
(-23565127)@32 <=s L0x20016074, L0x20016074 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159a0, L0x200159a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200159ac, L0x200159ac <=s 16831941@32,
(-23565127)@32 <=s L0x200159b8, L0x200159b8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016060, L0x20016060 <=s 20207750@32,
(-16831941)@32 <=s L0x2001606c, L0x2001606c <=s 16831941@32,
(-23565127)@32 <=s L0x20016078, L0x20016078 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159a4, L0x200159a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200159b0, L0x200159b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200159bc, L0x200159bc <=s 23565127@32,
(-20207750)@32 <=s L0x20016064, L0x20016064 <=s 20207750@32,
(-16831941)@32 <=s L0x20016070, L0x20016070 <=s 16831941@32,
(-23565127)@32 <=s L0x2001607c, L0x2001607c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a08, L0x20015a08 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a14, L0x20015a14 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a20, L0x20015a20 <=s 23565127@32,
(-20207750)@32 <=s L0x200160c8, L0x200160c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200160d4, L0x200160d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200160e0, L0x200160e0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a0c, L0x20015a0c <=s 20207750@32,
(-16831941)@32 <=s L0x20015a18, L0x20015a18 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a24, L0x20015a24 <=s 23565127@32,
(-20207750)@32 <=s L0x200160cc, L0x200160cc <=s 20207750@32,
(-16831941)@32 <=s L0x200160d8, L0x200160d8 <=s 16831941@32,
(-23565127)@32 <=s L0x200160e4, L0x200160e4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a10, L0x20015a10 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a1c, L0x20015a1c <=s 16831941@32,
(-23565127)@32 <=s L0x20015a28, L0x20015a28 <=s 23565127@32,
(-20207750)@32 <=s L0x200160d0, L0x200160d0 <=s 20207750@32,
(-16831941)@32 <=s L0x200160dc, L0x200160dc <=s 16831941@32,
(-23565127)@32 <=s L0x200160e8, L0x200160e8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a74, L0x20015a74 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a80, L0x20015a80 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a8c, L0x20015a8c <=s 23565127@32,
(-20207750)@32 <=s L0x20016134, L0x20016134 <=s 20207750@32,
(-16831941)@32 <=s L0x20016140, L0x20016140 <=s 16831941@32,
(-23565127)@32 <=s L0x2001614c, L0x2001614c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a78, L0x20015a78 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a84, L0x20015a84 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a90, L0x20015a90 <=s 23565127@32,
(-20207750)@32 <=s L0x20016138, L0x20016138 <=s 20207750@32,
(-16831941)@32 <=s L0x20016144, L0x20016144 <=s 16831941@32,
(-23565127)@32 <=s L0x20016150, L0x20016150 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a7c, L0x20015a7c <=s 20207750@32,
(-16831941)@32 <=s L0x20015a88, L0x20015a88 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a94, L0x20015a94 <=s 23565127@32,
(-20207750)@32 <=s L0x2001613c, L0x2001613c <=s 20207750@32,
(-16831941)@32 <=s L0x20016148, L0x20016148 <=s 16831941@32,
(-23565127)@32 <=s L0x20016154, L0x20016154 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015ae0, L0x20015ae0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015aec, L0x20015aec <=s 16831941@32,
(-23565127)@32 <=s L0x20015af8, L0x20015af8 <=s 23565127@32,
(-20207750)@32 <=s L0x200161a0, L0x200161a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200161ac, L0x200161ac <=s 16831941@32,
(-23565127)@32 <=s L0x200161b8, L0x200161b8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015ae4, L0x20015ae4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015af0, L0x20015af0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015afc, L0x20015afc <=s 23565127@32,
(-20207750)@32 <=s L0x200161a4, L0x200161a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200161b0, L0x200161b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200161bc, L0x200161bc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015ae8, L0x20015ae8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015af4, L0x20015af4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b00, L0x20015b00 <=s 23565127@32,
(-20207750)@32 <=s L0x200161a8, L0x200161a8 <=s 20207750@32,
(-16831941)@32 <=s L0x200161b4, L0x200161b4 <=s 16831941@32,
(-23565127)@32 <=s L0x200161c0, L0x200161c0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b4c, L0x20015b4c <=s 20207750@32,
(-16831941)@32 <=s L0x20015b58, L0x20015b58 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b64, L0x20015b64 <=s 23565127@32,
(-20207750)@32 <=s L0x2001620c, L0x2001620c <=s 20207750@32,
(-16831941)@32 <=s L0x20016218, L0x20016218 <=s 16831941@32,
(-23565127)@32 <=s L0x20016224, L0x20016224 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b50, L0x20015b50 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b5c, L0x20015b5c <=s 16831941@32,
(-23565127)@32 <=s L0x20015b68, L0x20015b68 <=s 23565127@32,
(-20207750)@32 <=s L0x20016210, L0x20016210 <=s 20207750@32,
(-16831941)@32 <=s L0x2001621c, L0x2001621c <=s 16831941@32,
(-23565127)@32 <=s L0x20016228, L0x20016228 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b54, L0x20015b54 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b60, L0x20015b60 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b6c, L0x20015b6c <=s 23565127@32,
(-20207750)@32 <=s L0x20016214, L0x20016214 <=s 20207750@32,
(-16831941)@32 <=s L0x20016220, L0x20016220 <=s 16831941@32,
(-23565127)@32 <=s L0x2001622c, L0x2001622c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015bb8, L0x20015bb8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015bc4, L0x20015bc4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bd0, L0x20015bd0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016278, L0x20016278 <=s 20207750@32,
(-16831941)@32 <=s L0x20016284, L0x20016284 <=s 16831941@32,
(-23565127)@32 <=s L0x20016290, L0x20016290 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015bbc, L0x20015bbc <=s 20207750@32,
(-16831941)@32 <=s L0x20015bc8, L0x20015bc8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bd4, L0x20015bd4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001627c, L0x2001627c <=s 20207750@32,
(-16831941)@32 <=s L0x20016288, L0x20016288 <=s 16831941@32,
(-23565127)@32 <=s L0x20016294, L0x20016294 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015bc0, L0x20015bc0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015bcc, L0x20015bcc <=s 16831941@32,
(-23565127)@32 <=s L0x20015bd8, L0x20015bd8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016280, L0x20016280 <=s 20207750@32,
(-16831941)@32 <=s L0x2001628c, L0x2001628c <=s 16831941@32,
(-23565127)@32 <=s L0x20016298, L0x20016298 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c24, L0x20015c24 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c30, L0x20015c30 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c3c, L0x20015c3c <=s 23565127@32,
(-20207750)@32 <=s L0x200162e4, L0x200162e4 <=s 20207750@32,
(-16831941)@32 <=s L0x200162f0, L0x200162f0 <=s 16831941@32,
(-23565127)@32 <=s L0x200162fc, L0x200162fc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c28, L0x20015c28 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c34, L0x20015c34 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c40, L0x20015c40 <=s 23565127@32,
(-20207750)@32 <=s L0x200162e8, L0x200162e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200162f4, L0x200162f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20016300, L0x20016300 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c2c, L0x20015c2c <=s 20207750@32,
(-16831941)@32 <=s L0x20015c38, L0x20015c38 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c44, L0x20015c44 <=s 23565127@32,
(-20207750)@32 <=s L0x200162ec, L0x200162ec <=s 20207750@32,
(-16831941)@32 <=s L0x200162f8, L0x200162f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20016304, L0x20016304 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c90, L0x20015c90 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c9c, L0x20015c9c <=s 16831941@32,
(-23565127)@32 <=s L0x20015ca8, L0x20015ca8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016350, L0x20016350 <=s 20207750@32,
(-16831941)@32 <=s L0x2001635c, L0x2001635c <=s 16831941@32,
(-23565127)@32 <=s L0x20016368, L0x20016368 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c94, L0x20015c94 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ca0, L0x20015ca0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cac, L0x20015cac <=s 23565127@32,
(-20207750)@32 <=s L0x20016354, L0x20016354 <=s 20207750@32,
(-16831941)@32 <=s L0x20016360, L0x20016360 <=s 16831941@32,
(-23565127)@32 <=s L0x2001636c, L0x2001636c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c98, L0x20015c98 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ca4, L0x20015ca4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cb0, L0x20015cb0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016358, L0x20016358 <=s 20207750@32,
(-16831941)@32 <=s L0x20016364, L0x20016364 <=s 16831941@32,
(-23565127)@32 <=s L0x20016370, L0x20016370 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015660, L0x20015660 <=s 20207750@32,
(-16831941)@32 <=s L0x2001566c, L0x2001566c <=s 16831941@32,
(-23565127)@32 <=s L0x20015678, L0x20015678 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d20, L0x20015d20 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d2c, L0x20015d2c <=s 16831941@32,
(-23565127)@32 <=s L0x20015d38, L0x20015d38 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015664, L0x20015664 <=s 20207750@32,
(-16831941)@32 <=s L0x20015670, L0x20015670 <=s 16831941@32,
(-23565127)@32 <=s L0x2001567c, L0x2001567c <=s 23565127@32,
(-20207750)@32 <=s L0x20015d24, L0x20015d24 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d30, L0x20015d30 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d3c, L0x20015d3c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015668, L0x20015668 <=s 20207750@32,
(-16831941)@32 <=s L0x20015674, L0x20015674 <=s 16831941@32,
(-23565127)@32 <=s L0x20015680, L0x20015680 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d28, L0x20015d28 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d34, L0x20015d34 <=s 16831941@32,
(-23565127)@32 <=s L0x20015d40, L0x20015d40 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156cc, L0x200156cc <=s 20207750@32,
(-16831941)@32 <=s L0x200156d8, L0x200156d8 <=s 16831941@32,
(-23565127)@32 <=s L0x200156e4, L0x200156e4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d8c, L0x20015d8c <=s 20207750@32,
(-16831941)@32 <=s L0x20015d98, L0x20015d98 <=s 16831941@32,
(-23565127)@32 <=s L0x20015da4, L0x20015da4 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156d0, L0x200156d0 <=s 20207750@32,
(-16831941)@32 <=s L0x200156dc, L0x200156dc <=s 16831941@32,
(-23565127)@32 <=s L0x200156e8, L0x200156e8 <=s 23565127@32,
(-20207750)@32 <=s L0x20015d90, L0x20015d90 <=s 20207750@32,
(-16831941)@32 <=s L0x20015d9c, L0x20015d9c <=s 16831941@32,
(-23565127)@32 <=s L0x20015da8, L0x20015da8 <=s 23565127@32
,
(-20207750)@32 <=s L0x200156d4, L0x200156d4 <=s 20207750@32,
(-16831941)@32 <=s L0x200156e0, L0x200156e0 <=s 16831941@32,
(-23565127)@32 <=s L0x200156ec, L0x200156ec <=s 23565127@32,
(-20207750)@32 <=s L0x20015d94, L0x20015d94 <=s 20207750@32,
(-16831941)@32 <=s L0x20015da0, L0x20015da0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015dac, L0x20015dac <=s 23565127@32
,
(-20207750)@32 <=s L0x20015738, L0x20015738 <=s 20207750@32,
(-16831941)@32 <=s L0x20015744, L0x20015744 <=s 16831941@32,
(-23565127)@32 <=s L0x20015750, L0x20015750 <=s 23565127@32,
(-20207750)@32 <=s L0x20015df8, L0x20015df8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e04, L0x20015e04 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e10, L0x20015e10 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001573c, L0x2001573c <=s 20207750@32,
(-16831941)@32 <=s L0x20015748, L0x20015748 <=s 16831941@32,
(-23565127)@32 <=s L0x20015754, L0x20015754 <=s 23565127@32,
(-20207750)@32 <=s L0x20015dfc, L0x20015dfc <=s 20207750@32,
(-16831941)@32 <=s L0x20015e08, L0x20015e08 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e14, L0x20015e14 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015740, L0x20015740 <=s 20207750@32,
(-16831941)@32 <=s L0x2001574c, L0x2001574c <=s 16831941@32,
(-23565127)@32 <=s L0x20015758, L0x20015758 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e00, L0x20015e00 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e0c, L0x20015e0c <=s 16831941@32,
(-23565127)@32 <=s L0x20015e18, L0x20015e18 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157a4, L0x200157a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200157b0, L0x200157b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200157bc, L0x200157bc <=s 23565127@32,
(-20207750)@32 <=s L0x20015e64, L0x20015e64 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e70, L0x20015e70 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e7c, L0x20015e7c <=s 23565127@32
,
(-20207750)@32 <=s L0x200157a8, L0x200157a8 <=s 20207750@32,
(-16831941)@32 <=s L0x200157b4, L0x200157b4 <=s 16831941@32,
(-23565127)@32 <=s L0x200157c0, L0x200157c0 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e68, L0x20015e68 <=s 20207750@32,
(-16831941)@32 <=s L0x20015e74, L0x20015e74 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e80, L0x20015e80 <=s 23565127@32
,
(-20207750)@32 <=s L0x200157ac, L0x200157ac <=s 20207750@32,
(-16831941)@32 <=s L0x200157b8, L0x200157b8 <=s 16831941@32,
(-23565127)@32 <=s L0x200157c4, L0x200157c4 <=s 23565127@32,
(-20207750)@32 <=s L0x20015e6c, L0x20015e6c <=s 20207750@32,
(-16831941)@32 <=s L0x20015e78, L0x20015e78 <=s 16831941@32,
(-23565127)@32 <=s L0x20015e84, L0x20015e84 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015810, L0x20015810 <=s 20207750@32,
(-16831941)@32 <=s L0x2001581c, L0x2001581c <=s 16831941@32,
(-23565127)@32 <=s L0x20015828, L0x20015828 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ed0, L0x20015ed0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015edc, L0x20015edc <=s 16831941@32,
(-23565127)@32 <=s L0x20015ee8, L0x20015ee8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015814, L0x20015814 <=s 20207750@32,
(-16831941)@32 <=s L0x20015820, L0x20015820 <=s 16831941@32,
(-23565127)@32 <=s L0x2001582c, L0x2001582c <=s 23565127@32,
(-20207750)@32 <=s L0x20015ed4, L0x20015ed4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ee0, L0x20015ee0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015eec, L0x20015eec <=s 23565127@32
,
(-20207750)@32 <=s L0x20015818, L0x20015818 <=s 20207750@32,
(-16831941)@32 <=s L0x20015824, L0x20015824 <=s 16831941@32,
(-23565127)@32 <=s L0x20015830, L0x20015830 <=s 23565127@32,
(-20207750)@32 <=s L0x20015ed8, L0x20015ed8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015ee4, L0x20015ee4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ef0, L0x20015ef0 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001587c, L0x2001587c <=s 20207750@32,
(-16831941)@32 <=s L0x20015888, L0x20015888 <=s 16831941@32,
(-23565127)@32 <=s L0x20015894, L0x20015894 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f3c, L0x20015f3c <=s 20207750@32,
(-16831941)@32 <=s L0x20015f48, L0x20015f48 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f54, L0x20015f54 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015880, L0x20015880 <=s 20207750@32,
(-16831941)@32 <=s L0x2001588c, L0x2001588c <=s 16831941@32,
(-23565127)@32 <=s L0x20015898, L0x20015898 <=s 23565127@32,
(-20207750)@32 <=s L0x20015f40, L0x20015f40 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f4c, L0x20015f4c <=s 16831941@32,
(-23565127)@32 <=s L0x20015f58, L0x20015f58 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015884, L0x20015884 <=s 20207750@32,
(-16831941)@32 <=s L0x20015890, L0x20015890 <=s 16831941@32,
(-23565127)@32 <=s L0x2001589c, L0x2001589c <=s 23565127@32,
(-20207750)@32 <=s L0x20015f44, L0x20015f44 <=s 20207750@32,
(-16831941)@32 <=s L0x20015f50, L0x20015f50 <=s 16831941@32,
(-23565127)@32 <=s L0x20015f5c, L0x20015f5c <=s 23565127@32
,
(-20207750)@32 <=s L0x200158e8, L0x200158e8 <=s 20207750@32,
(-16831941)@32 <=s L0x200158f4, L0x200158f4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015900, L0x20015900 <=s 23565127@32,
(-20207750)@32 <=s L0x20015fa8, L0x20015fa8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015fb4, L0x20015fb4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fc0, L0x20015fc0 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158ec, L0x200158ec <=s 20207750@32,
(-16831941)@32 <=s L0x200158f8, L0x200158f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015904, L0x20015904 <=s 23565127@32,
(-20207750)@32 <=s L0x20015fac, L0x20015fac <=s 20207750@32,
(-16831941)@32 <=s L0x20015fb8, L0x20015fb8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015fc4, L0x20015fc4 <=s 23565127@32
,
(-20207750)@32 <=s L0x200158f0, L0x200158f0 <=s 20207750@32,
(-16831941)@32 <=s L0x200158fc, L0x200158fc <=s 16831941@32,
(-23565127)@32 <=s L0x20015908, L0x20015908 <=s 23565127@32,
(-20207750)@32 <=s L0x20015fb0, L0x20015fb0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015fbc, L0x20015fbc <=s 16831941@32,
(-23565127)@32 <=s L0x20015fc8, L0x20015fc8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015954, L0x20015954 <=s 20207750@32,
(-16831941)@32 <=s L0x20015960, L0x20015960 <=s 16831941@32,
(-23565127)@32 <=s L0x2001596c, L0x2001596c <=s 23565127@32,
(-20207750)@32 <=s L0x20016014, L0x20016014 <=s 20207750@32,
(-16831941)@32 <=s L0x20016020, L0x20016020 <=s 16831941@32,
(-23565127)@32 <=s L0x2001602c, L0x2001602c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015958, L0x20015958 <=s 20207750@32,
(-16831941)@32 <=s L0x20015964, L0x20015964 <=s 16831941@32,
(-23565127)@32 <=s L0x20015970, L0x20015970 <=s 23565127@32,
(-20207750)@32 <=s L0x20016018, L0x20016018 <=s 20207750@32,
(-16831941)@32 <=s L0x20016024, L0x20016024 <=s 16831941@32,
(-23565127)@32 <=s L0x20016030, L0x20016030 <=s 23565127@32
,
(-20207750)@32 <=s L0x2001595c, L0x2001595c <=s 20207750@32,
(-16831941)@32 <=s L0x20015968, L0x20015968 <=s 16831941@32,
(-23565127)@32 <=s L0x20015974, L0x20015974 <=s 23565127@32,
(-20207750)@32 <=s L0x2001601c, L0x2001601c <=s 20207750@32,
(-16831941)@32 <=s L0x20016028, L0x20016028 <=s 16831941@32,
(-23565127)@32 <=s L0x20016034, L0x20016034 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159c0, L0x200159c0 <=s 20207750@32,
(-16831941)@32 <=s L0x200159cc, L0x200159cc <=s 16831941@32,
(-23565127)@32 <=s L0x200159d8, L0x200159d8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016080, L0x20016080 <=s 20207750@32,
(-16831941)@32 <=s L0x2001608c, L0x2001608c <=s 16831941@32,
(-23565127)@32 <=s L0x20016098, L0x20016098 <=s 23565127@32
,
(-20207750)@32 <=s L0x200159c4, L0x200159c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200159d0, L0x200159d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200159dc, L0x200159dc <=s 23565127@32,
(-20207750)@32 <=s L0x20016084, L0x20016084 <=s 20207750@32,
(-16831941)@32 <=s L0x20016090, L0x20016090 <=s 16831941@32,
(-23565127)@32 <=s L0x2001609c, L0x2001609c <=s 23565127@32
,
(-20207750)@32 <=s L0x200159c8, L0x200159c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200159d4, L0x200159d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200159e0, L0x200159e0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016088, L0x20016088 <=s 20207750@32,
(-16831941)@32 <=s L0x20016094, L0x20016094 <=s 16831941@32,
(-23565127)@32 <=s L0x200160a0, L0x200160a0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a2c, L0x20015a2c <=s 20207750@32,
(-16831941)@32 <=s L0x20015a38, L0x20015a38 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a44, L0x20015a44 <=s 23565127@32,
(-20207750)@32 <=s L0x200160ec, L0x200160ec <=s 20207750@32,
(-16831941)@32 <=s L0x200160f8, L0x200160f8 <=s 16831941@32,
(-23565127)@32 <=s L0x20016104, L0x20016104 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a30, L0x20015a30 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a3c, L0x20015a3c <=s 16831941@32,
(-23565127)@32 <=s L0x20015a48, L0x20015a48 <=s 23565127@32,
(-20207750)@32 <=s L0x200160f0, L0x200160f0 <=s 20207750@32,
(-16831941)@32 <=s L0x200160fc, L0x200160fc <=s 16831941@32,
(-23565127)@32 <=s L0x20016108, L0x20016108 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a34, L0x20015a34 <=s 20207750@32,
(-16831941)@32 <=s L0x20015a40, L0x20015a40 <=s 16831941@32,
(-23565127)@32 <=s L0x20015a4c, L0x20015a4c <=s 23565127@32,
(-20207750)@32 <=s L0x200160f4, L0x200160f4 <=s 20207750@32,
(-16831941)@32 <=s L0x20016100, L0x20016100 <=s 16831941@32,
(-23565127)@32 <=s L0x2001610c, L0x2001610c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a98, L0x20015a98 <=s 20207750@32,
(-16831941)@32 <=s L0x20015aa4, L0x20015aa4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ab0, L0x20015ab0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016158, L0x20016158 <=s 20207750@32,
(-16831941)@32 <=s L0x20016164, L0x20016164 <=s 16831941@32,
(-23565127)@32 <=s L0x20016170, L0x20016170 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015a9c, L0x20015a9c <=s 20207750@32,
(-16831941)@32 <=s L0x20015aa8, L0x20015aa8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ab4, L0x20015ab4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001615c, L0x2001615c <=s 20207750@32,
(-16831941)@32 <=s L0x20016168, L0x20016168 <=s 16831941@32,
(-23565127)@32 <=s L0x20016174, L0x20016174 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015aa0, L0x20015aa0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015aac, L0x20015aac <=s 16831941@32,
(-23565127)@32 <=s L0x20015ab8, L0x20015ab8 <=s 23565127@32,
(-20207750)@32 <=s L0x20016160, L0x20016160 <=s 20207750@32,
(-16831941)@32 <=s L0x2001616c, L0x2001616c <=s 16831941@32,
(-23565127)@32 <=s L0x20016178, L0x20016178 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b04, L0x20015b04 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b10, L0x20015b10 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b1c, L0x20015b1c <=s 23565127@32,
(-20207750)@32 <=s L0x200161c4, L0x200161c4 <=s 20207750@32,
(-16831941)@32 <=s L0x200161d0, L0x200161d0 <=s 16831941@32,
(-23565127)@32 <=s L0x200161dc, L0x200161dc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b08, L0x20015b08 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b14, L0x20015b14 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b20, L0x20015b20 <=s 23565127@32,
(-20207750)@32 <=s L0x200161c8, L0x200161c8 <=s 20207750@32,
(-16831941)@32 <=s L0x200161d4, L0x200161d4 <=s 16831941@32,
(-23565127)@32 <=s L0x200161e0, L0x200161e0 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b0c, L0x20015b0c <=s 20207750@32,
(-16831941)@32 <=s L0x20015b18, L0x20015b18 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b24, L0x20015b24 <=s 23565127@32,
(-20207750)@32 <=s L0x200161cc, L0x200161cc <=s 20207750@32,
(-16831941)@32 <=s L0x200161d8, L0x200161d8 <=s 16831941@32,
(-23565127)@32 <=s L0x200161e4, L0x200161e4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b70, L0x20015b70 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b7c, L0x20015b7c <=s 16831941@32,
(-23565127)@32 <=s L0x20015b88, L0x20015b88 <=s 23565127@32,
(-20207750)@32 <=s L0x20016230, L0x20016230 <=s 20207750@32,
(-16831941)@32 <=s L0x2001623c, L0x2001623c <=s 16831941@32,
(-23565127)@32 <=s L0x20016248, L0x20016248 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b74, L0x20015b74 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b80, L0x20015b80 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b8c, L0x20015b8c <=s 23565127@32,
(-20207750)@32 <=s L0x20016234, L0x20016234 <=s 20207750@32,
(-16831941)@32 <=s L0x20016240, L0x20016240 <=s 16831941@32,
(-23565127)@32 <=s L0x2001624c, L0x2001624c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015b78, L0x20015b78 <=s 20207750@32,
(-16831941)@32 <=s L0x20015b84, L0x20015b84 <=s 16831941@32,
(-23565127)@32 <=s L0x20015b90, L0x20015b90 <=s 23565127@32,
(-20207750)@32 <=s L0x20016238, L0x20016238 <=s 20207750@32,
(-16831941)@32 <=s L0x20016244, L0x20016244 <=s 16831941@32,
(-23565127)@32 <=s L0x20016250, L0x20016250 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015bdc, L0x20015bdc <=s 20207750@32,
(-16831941)@32 <=s L0x20015be8, L0x20015be8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bf4, L0x20015bf4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001629c, L0x2001629c <=s 20207750@32,
(-16831941)@32 <=s L0x200162a8, L0x200162a8 <=s 16831941@32,
(-23565127)@32 <=s L0x200162b4, L0x200162b4 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015be0, L0x20015be0 <=s 20207750@32,
(-16831941)@32 <=s L0x20015bec, L0x20015bec <=s 16831941@32,
(-23565127)@32 <=s L0x20015bf8, L0x20015bf8 <=s 23565127@32,
(-20207750)@32 <=s L0x200162a0, L0x200162a0 <=s 20207750@32,
(-16831941)@32 <=s L0x200162ac, L0x200162ac <=s 16831941@32,
(-23565127)@32 <=s L0x200162b8, L0x200162b8 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015be4, L0x20015be4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015bf0, L0x20015bf0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015bfc, L0x20015bfc <=s 23565127@32,
(-20207750)@32 <=s L0x200162a4, L0x200162a4 <=s 20207750@32,
(-16831941)@32 <=s L0x200162b0, L0x200162b0 <=s 16831941@32,
(-23565127)@32 <=s L0x200162bc, L0x200162bc <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c48, L0x20015c48 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c54, L0x20015c54 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c60, L0x20015c60 <=s 23565127@32,
(-20207750)@32 <=s L0x20016308, L0x20016308 <=s 20207750@32,
(-16831941)@32 <=s L0x20016314, L0x20016314 <=s 16831941@32,
(-23565127)@32 <=s L0x20016320, L0x20016320 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c4c, L0x20015c4c <=s 20207750@32,
(-16831941)@32 <=s L0x20015c58, L0x20015c58 <=s 16831941@32,
(-23565127)@32 <=s L0x20015c64, L0x20015c64 <=s 23565127@32,
(-20207750)@32 <=s L0x2001630c, L0x2001630c <=s 20207750@32,
(-16831941)@32 <=s L0x20016318, L0x20016318 <=s 16831941@32,
(-23565127)@32 <=s L0x20016324, L0x20016324 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015c50, L0x20015c50 <=s 20207750@32,
(-16831941)@32 <=s L0x20015c5c, L0x20015c5c <=s 16831941@32,
(-23565127)@32 <=s L0x20015c68, L0x20015c68 <=s 23565127@32,
(-20207750)@32 <=s L0x20016310, L0x20016310 <=s 20207750@32,
(-16831941)@32 <=s L0x2001631c, L0x2001631c <=s 16831941@32,
(-23565127)@32 <=s L0x20016328, L0x20016328 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015cb4, L0x20015cb4 <=s 20207750@32,
(-16831941)@32 <=s L0x20015cc0, L0x20015cc0 <=s 16831941@32,
(-23565127)@32 <=s L0x20015ccc, L0x20015ccc <=s 23565127@32,
(-20207750)@32 <=s L0x20016374, L0x20016374 <=s 20207750@32,
(-16831941)@32 <=s L0x20016380, L0x20016380 <=s 16831941@32,
(-23565127)@32 <=s L0x2001638c, L0x2001638c <=s 23565127@32
,
(-20207750)@32 <=s L0x20015cb8, L0x20015cb8 <=s 20207750@32,
(-16831941)@32 <=s L0x20015cc4, L0x20015cc4 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cd0, L0x20015cd0 <=s 23565127@32,
(-20207750)@32 <=s L0x20016378, L0x20016378 <=s 20207750@32,
(-16831941)@32 <=s L0x20016384, L0x20016384 <=s 16831941@32,
(-23565127)@32 <=s L0x20016390, L0x20016390 <=s 23565127@32
,
(-20207750)@32 <=s L0x20015cbc, L0x20015cbc <=s 20207750@32,
(-16831941)@32 <=s L0x20015cc8, L0x20015cc8 <=s 16831941@32,
(-23565127)@32 <=s L0x20015cd4, L0x20015cd4 <=s 23565127@32,
(-20207750)@32 <=s L0x2001637c, L0x2001637c <=s 20207750@32,
(-16831941)@32 <=s L0x20016388, L0x20016388 <=s 16831941@32,
(-23565127)@32 <=s L0x20016394, L0x20016394 <=s 23565127@32
] prove with [ all cuts ]
}

