(* server@szu: -v -jobs 26 -isafety -no_carry_constraint -slicing asm_base_mul_32.cl
Parsing Cryptoline file:                [OK]            0.078046 seconds
Checking well-formedness:               [OK]            0.047486 seconds
Transforming to SSA form:               [OK]            0.015032 seconds
Rewriting assignments:                  [OK]            0.016187 seconds
Verifying program safety:               [OK]            2084.428073 seconds
Verifying range assertions:             [OK]            0.081251 seconds
Verifying range specification:          [OK]            2498.532535 seconds
Rewriting value-preserved casting:      [OK]            0.000284 seconds
Verifying algebraic assertions:         [OK]            57.565682 seconds
Verifying algebraic specification:      [OK]            12.734968 seconds
Verification result:                    [OK]            4653.506856 seconds
*)

proc main (
  sint32 x,
  sint32 a000, sint32 a001, sint32 a002, sint32 a003,
  sint32 a004, sint32 a005, sint32 a006, sint32 a007,
  sint32 a008, sint32 a009, sint32 a010, sint32 a011,
  sint32 a012, sint32 a013, sint32 a014, sint32 a015,
  sint32 a016, sint32 a017, sint32 a018, sint32 a019,
  sint32 a020, sint32 a021, sint32 a022, sint32 a023,
  sint32 a024, sint32 a025, sint32 a026, sint32 a027,
  sint32 a028, sint32 a029, sint32 a030, sint32 a031,
  sint32 a032, sint32 a033, sint32 a034, sint32 a035,
  sint32 a036, sint32 a037, sint32 a038, sint32 a039,
  sint32 a040, sint32 a041, sint32 a042, sint32 a043,
  sint32 a044, sint32 a045, sint32 a046, sint32 a047,
  sint32 a048, sint32 a049, sint32 a050, sint32 a051,
  sint32 a052, sint32 a053, sint32 a054, sint32 a055,
  sint32 a056, sint32 a057, sint32 a058, sint32 a059,
  sint32 a060, sint32 a061, sint32 a062, sint32 a063,
  sint32 a064, sint32 a065, sint32 a066, sint32 a067,
  sint32 a068, sint32 a069, sint32 a070, sint32 a071,
  sint32 a072, sint32 a073, sint32 a074, sint32 a075,
  sint32 a076, sint32 a077, sint32 a078, sint32 a079,
  sint32 a080, sint32 a081, sint32 a082, sint32 a083,
  sint32 a084, sint32 a085, sint32 a086, sint32 a087,
  sint32 a088, sint32 a089, sint32 a090, sint32 a091,
  sint32 a092, sint32 a093, sint32 a094, sint32 a095,
  sint32 a096, sint32 a097, sint32 a098, sint32 a099,
  sint32 a100, sint32 a101, sint32 a102, sint32 a103,
  sint32 a104, sint32 a105, sint32 a106, sint32 a107,
  sint32 a108, sint32 a109, sint32 a110, sint32 a111,
  sint32 a112, sint32 a113, sint32 a114, sint32 a115,
  sint32 a116, sint32 a117, sint32 a118, sint32 a119,
  sint32 a120, sint32 a121, sint32 a122, sint32 a123,
  sint32 a124, sint32 a125, sint32 a126, sint32 a127,
  sint32 a128, sint32 a129, sint32 a130, sint32 a131,
  sint32 a132, sint32 a133, sint32 a134, sint32 a135,
  sint32 a136, sint32 a137, sint32 a138, sint32 a139,
  sint32 a140, sint32 a141, sint32 a142, sint32 a143,
  sint32 a144, sint32 a145, sint32 a146, sint32 a147,
  sint32 a148, sint32 a149, sint32 a150, sint32 a151,
  sint32 a152, sint32 a153, sint32 a154, sint32 a155,
  sint32 a156, sint32 a157, sint32 a158, sint32 a159,
  sint32 a160, sint32 a161, sint32 a162, sint32 a163,
  sint32 a164, sint32 a165, sint32 a166, sint32 a167,
  sint32 a168, sint32 a169, sint32 a170, sint32 a171,
  sint32 a172, sint32 a173, sint32 a174, sint32 a175,
  sint32 a176, sint32 a177, sint32 a178, sint32 a179,
  sint32 a180, sint32 a181, sint32 a182, sint32 a183,
  sint32 a184, sint32 a185, sint32 a186, sint32 a187,
  sint32 a188, sint32 a189, sint32 a190, sint32 a191,
  sint32 a192, sint32 a193, sint32 a194, sint32 a195,
  sint32 a196, sint32 a197, sint32 a198, sint32 a199,
  sint32 a200, sint32 a201, sint32 a202, sint32 a203,
  sint32 a204, sint32 a205, sint32 a206, sint32 a207,
  sint32 a208, sint32 a209, sint32 a210, sint32 a211,
  sint32 a212, sint32 a213, sint32 a214, sint32 a215,
  sint32 a216, sint32 a217, sint32 a218, sint32 a219,
  sint32 a220, sint32 a221, sint32 a222, sint32 a223,
  sint32 a224, sint32 a225, sint32 a226, sint32 a227,
  sint32 a228, sint32 a229, sint32 a230, sint32 a231,
  sint32 a232, sint32 a233, sint32 a234, sint32 a235,
  sint32 a236, sint32 a237, sint32 a238, sint32 a239,
  sint32 a240, sint32 a241, sint32 a242, sint32 a243,
  sint32 a244, sint32 a245, sint32 a246, sint32 a247,
  sint32 a248, sint32 a249, sint32 a250, sint32 a251,
  sint32 a252, sint32 a253, sint32 a254, sint32 a255,
  sint32 b000, sint32 b001, sint32 b002, sint32 b003,
  sint32 b004, sint32 b005, sint32 b006, sint32 b007,
  sint32 b008, sint32 b009, sint32 b010, sint32 b011,
  sint32 b012, sint32 b013, sint32 b014, sint32 b015,
  sint32 b016, sint32 b017, sint32 b018, sint32 b019,
  sint32 b020, sint32 b021, sint32 b022, sint32 b023,
  sint32 b024, sint32 b025, sint32 b026, sint32 b027,
  sint32 b028, sint32 b029, sint32 b030, sint32 b031,
  sint32 b032, sint32 b033, sint32 b034, sint32 b035,
  sint32 b036, sint32 b037, sint32 b038, sint32 b039,
  sint32 b040, sint32 b041, sint32 b042, sint32 b043,
  sint32 b044, sint32 b045, sint32 b046, sint32 b047,
  sint32 b048, sint32 b049, sint32 b050, sint32 b051,
  sint32 b052, sint32 b053, sint32 b054, sint32 b055,
  sint32 b056, sint32 b057, sint32 b058, sint32 b059,
  sint32 b060, sint32 b061, sint32 b062, sint32 b063,
  sint32 b064, sint32 b065, sint32 b066, sint32 b067,
  sint32 b068, sint32 b069, sint32 b070, sint32 b071,
  sint32 b072, sint32 b073, sint32 b074, sint32 b075,
  sint32 b076, sint32 b077, sint32 b078, sint32 b079,
  sint32 b080, sint32 b081, sint32 b082, sint32 b083,
  sint32 b084, sint32 b085, sint32 b086, sint32 b087,
  sint32 b088, sint32 b089, sint32 b090, sint32 b091,
  sint32 b092, sint32 b093, sint32 b094, sint32 b095,
  sint32 b096, sint32 b097, sint32 b098, sint32 b099,
  sint32 b100, sint32 b101, sint32 b102, sint32 b103,
  sint32 b104, sint32 b105, sint32 b106, sint32 b107,
  sint32 b108, sint32 b109, sint32 b110, sint32 b111,
  sint32 b112, sint32 b113, sint32 b114, sint32 b115,
  sint32 b116, sint32 b117, sint32 b118, sint32 b119,
  sint32 b120, sint32 b121, sint32 b122, sint32 b123,
  sint32 b124, sint32 b125, sint32 b126, sint32 b127,
  sint32 b128, sint32 b129, sint32 b130, sint32 b131,
  sint32 b132, sint32 b133, sint32 b134, sint32 b135,
  sint32 b136, sint32 b137, sint32 b138, sint32 b139,
  sint32 b140, sint32 b141, sint32 b142, sint32 b143,
  sint32 b144, sint32 b145, sint32 b146, sint32 b147,
  sint32 b148, sint32 b149, sint32 b150, sint32 b151,
  sint32 b152, sint32 b153, sint32 b154, sint32 b155,
  sint32 b156, sint32 b157, sint32 b158, sint32 b159,
  sint32 b160, sint32 b161, sint32 b162, sint32 b163,
  sint32 b164, sint32 b165, sint32 b166, sint32 b167,
  sint32 b168, sint32 b169, sint32 b170, sint32 b171,
  sint32 b172, sint32 b173, sint32 b174, sint32 b175,
  sint32 b176, sint32 b177, sint32 b178, sint32 b179,
  sint32 b180, sint32 b181, sint32 b182, sint32 b183,
  sint32 b184, sint32 b185, sint32 b186, sint32 b187,
  sint32 b188, sint32 b189, sint32 b190, sint32 b191,
  sint32 b192, sint32 b193, sint32 b194, sint32 b195,
  sint32 b196, sint32 b197, sint32 b198, sint32 b199,
  sint32 b200, sint32 b201, sint32 b202, sint32 b203,
  sint32 b204, sint32 b205, sint32 b206, sint32 b207,
  sint32 b208, sint32 b209, sint32 b210, sint32 b211,
  sint32 b212, sint32 b213, sint32 b214, sint32 b215,
  sint32 b216, sint32 b217, sint32 b218, sint32 b219,
  sint32 b220, sint32 b221, sint32 b222, sint32 b223,
  sint32 b224, sint32 b225, sint32 b226, sint32 b227,
  sint32 b228, sint32 b229, sint32 b230, sint32 b231,
  sint32 b232, sint32 b233, sint32 b234, sint32 b235,
  sint32 b236, sint32 b237, sint32 b238, sint32 b239,
  sint32 b240, sint32 b241, sint32 b242, sint32 b243,
  sint32 b244, sint32 b245, sint32 b246, sint32 b247,
  sint32 b248, sint32 b249, sint32 b250, sint32 b251,
  sint32 b252, sint32 b253, sint32 b254, sint32 b255
) =

{
  true
  &&
  (* range *)
  and [
    7@32 * (-25570049)@32 <=s a000, a000 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a001, a001 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a002, a002 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a003, a003 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a004, a004 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a005, a005 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a006, a006 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a007, a007 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a008, a008 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a009, a009 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a010, a010 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a011, a011 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a012, a012 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a013, a013 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a014, a014 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a015, a015 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a016, a016 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a017, a017 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a018, a018 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a019, a019 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a020, a020 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a021, a021 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a022, a022 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a023, a023 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a024, a024 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a025, a025 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a026, a026 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a027, a027 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a028, a028 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a029, a029 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a030, a030 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a031, a031 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a032, a032 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a033, a033 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a034, a034 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a035, a035 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a036, a036 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a037, a037 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a038, a038 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a039, a039 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a040, a040 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a041, a041 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a042, a042 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a043, a043 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a044, a044 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a045, a045 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a046, a046 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a047, a047 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a048, a048 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a049, a049 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a050, a050 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a051, a051 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a052, a052 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a053, a053 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a054, a054 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a055, a055 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a056, a056 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a057, a057 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a058, a058 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a059, a059 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a060, a060 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a061, a061 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a062, a062 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a063, a063 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a064, a064 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a065, a065 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a066, a066 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a067, a067 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a068, a068 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a069, a069 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a070, a070 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a071, a071 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a072, a072 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a073, a073 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a074, a074 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a075, a075 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a076, a076 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a077, a077 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a078, a078 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a079, a079 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a080, a080 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a081, a081 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a082, a082 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a083, a083 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a084, a084 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a085, a085 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a086, a086 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a087, a087 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a088, a088 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a089, a089 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a090, a090 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a091, a091 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a092, a092 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a093, a093 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a094, a094 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a095, a095 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a096, a096 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a097, a097 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a098, a098 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a099, a099 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a100, a100 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a101, a101 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a102, a102 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a103, a103 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a104, a104 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a105, a105 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a106, a106 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a107, a107 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a108, a108 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a109, a109 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a110, a110 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a111, a111 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a112, a112 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a113, a113 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a114, a114 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a115, a115 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a116, a116 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a117, a117 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a118, a118 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a119, a119 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a120, a120 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a121, a121 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a122, a122 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a123, a123 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a124, a124 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a125, a125 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a126, a126 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a127, a127 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a128, a128 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a129, a129 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a130, a130 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a131, a131 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a132, a132 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a133, a133 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a134, a134 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a135, a135 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a136, a136 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a137, a137 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a138, a138 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a139, a139 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a140, a140 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a141, a141 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a142, a142 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a143, a143 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a144, a144 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a145, a145 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a146, a146 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a147, a147 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a148, a148 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a149, a149 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a150, a150 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a151, a151 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a152, a152 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a153, a153 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a154, a154 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a155, a155 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a156, a156 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a157, a157 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a158, a158 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a159, a159 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a160, a160 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a161, a161 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a162, a162 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a163, a163 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a164, a164 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a165, a165 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a166, a166 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a167, a167 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a168, a168 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a169, a169 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a170, a170 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a171, a171 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a172, a172 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a173, a173 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a174, a174 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a175, a175 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a176, a176 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a177, a177 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a178, a178 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a179, a179 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a180, a180 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a181, a181 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a182, a182 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a183, a183 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a184, a184 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a185, a185 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a186, a186 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a187, a187 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a188, a188 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a189, a189 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a190, a190 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a191, a191 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a192, a192 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a193, a193 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a194, a194 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a195, a195 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a196, a196 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a197, a197 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a198, a198 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a199, a199 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a200, a200 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a201, a201 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a202, a202 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a203, a203 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a204, a204 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a205, a205 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a206, a206 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a207, a207 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a208, a208 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a209, a209 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a210, a210 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a211, a211 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a212, a212 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a213, a213 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a214, a214 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a215, a215 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a216, a216 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a217, a217 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a218, a218 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a219, a219 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a220, a220 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a221, a221 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a222, a222 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a223, a223 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a224, a224 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a225, a225 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a226, a226 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a227, a227 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a228, a228 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a229, a229 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a230, a230 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a231, a231 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a232, a232 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a233, a233 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a234, a234 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a235, a235 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a236, a236 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a237, a237 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a238, a238 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a239, a239 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a240, a240 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a241, a241 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a242, a242 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a243, a243 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a244, a244 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a245, a245 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a246, a246 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a247, a247 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a248, a248 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a249, a249 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a250, a250 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a251, a251 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a252, a252 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a253, a253 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a254, a254 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s a255, a255 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b000, b000 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b001, b001 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b002, b002 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b003, b003 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b004, b004 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b005, b005 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b006, b006 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b007, b007 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b008, b008 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b009, b009 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b010, b010 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b011, b011 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b012, b012 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b013, b013 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b014, b014 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b015, b015 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b016, b016 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b017, b017 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b018, b018 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b019, b019 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b020, b020 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b021, b021 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b022, b022 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b023, b023 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b024, b024 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b025, b025 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b026, b026 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b027, b027 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b028, b028 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b029, b029 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b030, b030 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b031, b031 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b032, b032 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b033, b033 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b034, b034 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b035, b035 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b036, b036 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b037, b037 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b038, b038 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b039, b039 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b040, b040 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b041, b041 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b042, b042 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b043, b043 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b044, b044 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b045, b045 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b046, b046 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b047, b047 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b048, b048 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b049, b049 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b050, b050 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b051, b051 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b052, b052 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b053, b053 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b054, b054 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b055, b055 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b056, b056 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b057, b057 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b058, b058 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b059, b059 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b060, b060 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b061, b061 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b062, b062 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b063, b063 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b064, b064 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b065, b065 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b066, b066 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b067, b067 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b068, b068 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b069, b069 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b070, b070 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b071, b071 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b072, b072 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b073, b073 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b074, b074 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b075, b075 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b076, b076 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b077, b077 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b078, b078 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b079, b079 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b080, b080 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b081, b081 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b082, b082 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b083, b083 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b084, b084 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b085, b085 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b086, b086 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b087, b087 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b088, b088 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b089, b089 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b090, b090 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b091, b091 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b092, b092 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b093, b093 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b094, b094 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b095, b095 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b096, b096 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b097, b097 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b098, b098 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b099, b099 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b100, b100 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b101, b101 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b102, b102 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b103, b103 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b104, b104 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b105, b105 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b106, b106 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b107, b107 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b108, b108 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b109, b109 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b110, b110 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b111, b111 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b112, b112 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b113, b113 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b114, b114 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b115, b115 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b116, b116 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b117, b117 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b118, b118 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b119, b119 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b120, b120 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b121, b121 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b122, b122 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b123, b123 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b124, b124 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b125, b125 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b126, b126 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b127, b127 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b128, b128 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b129, b129 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b130, b130 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b131, b131 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b132, b132 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b133, b133 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b134, b134 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b135, b135 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b136, b136 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b137, b137 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b138, b138 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b139, b139 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b140, b140 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b141, b141 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b142, b142 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b143, b143 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b144, b144 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b145, b145 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b146, b146 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b147, b147 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b148, b148 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b149, b149 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b150, b150 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b151, b151 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b152, b152 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b153, b153 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b154, b154 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b155, b155 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b156, b156 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b157, b157 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b158, b158 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b159, b159 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b160, b160 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b161, b161 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b162, b162 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b163, b163 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b164, b164 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b165, b165 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b166, b166 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b167, b167 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b168, b168 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b169, b169 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b170, b170 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b171, b171 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b172, b172 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b173, b173 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b174, b174 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b175, b175 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b176, b176 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b177, b177 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b178, b178 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b179, b179 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b180, b180 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b181, b181 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b182, b182 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b183, b183 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b184, b184 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b185, b185 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b186, b186 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b187, b187 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b188, b188 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b189, b189 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b190, b190 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b191, b191 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b192, b192 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b193, b193 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b194, b194 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b195, b195 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b196, b196 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b197, b197 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b198, b198 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b199, b199 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b200, b200 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b201, b201 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b202, b202 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b203, b203 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b204, b204 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b205, b205 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b206, b206 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b207, b207 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b208, b208 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b209, b209 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b210, b210 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b211, b211 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b212, b212 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b213, b213 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b214, b214 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b215, b215 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b216, b216 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b217, b217 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b218, b218 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b219, b219 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b220, b220 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b221, b221 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b222, b222 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b223, b223 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b224, b224 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b225, b225 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b226, b226 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b227, b227 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b228, b228 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b229, b229 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b230, b230 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b231, b231 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b232, b232 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b233, b233 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b234, b234 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b235, b235 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b236, b236 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b237, b237 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b238, b238 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b239, b239 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b240, b240 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b241, b241 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b242, b242 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b243, b243 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b244, b244 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b245, b245 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b246, b246 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b247, b247 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b248, b248 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b249, b249 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b250, b250 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b251, b251 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b252, b252 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b253, b253 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b254, b254 <s 7@32 * 25570049@32,
    7@32 * (-25570049)@32 <=s b255, b255 <s 7@32 * 25570049@32
  ]
}

(* === params === *)

mov Q 25570049@sint32;
mov Qprime (-1991431425)@sint32;

(* === inits === *)

(* src a *)
mov L0x20019a48 a000;  mov L0x20019a4c a001;
mov L0x20019a50 a002;  mov L0x20019a54 a003;
mov L0x20019a58 a004;  mov L0x20019a5c a005;
mov L0x20019a60 a006;  mov L0x20019a64 a007;
mov L0x20019a68 a008;  mov L0x20019a6c a009;
mov L0x20019a70 a010;  mov L0x20019a74 a011;
mov L0x20019a78 a012;  mov L0x20019a7c a013;
mov L0x20019a80 a014;  mov L0x20019a84 a015;
mov L0x20019a88 a016;  mov L0x20019a8c a017;
mov L0x20019a90 a018;  mov L0x20019a94 a019;
mov L0x20019a98 a020;  mov L0x20019a9c a021;
mov L0x20019aa0 a022;  mov L0x20019aa4 a023;
mov L0x20019aa8 a024;  mov L0x20019aac a025;
mov L0x20019ab0 a026;  mov L0x20019ab4 a027;
mov L0x20019ab8 a028;  mov L0x20019abc a029;
mov L0x20019ac0 a030;  mov L0x20019ac4 a031;
mov L0x20019ac8 a032;  mov L0x20019acc a033;
mov L0x20019ad0 a034;  mov L0x20019ad4 a035;
mov L0x20019ad8 a036;  mov L0x20019adc a037;
mov L0x20019ae0 a038;  mov L0x20019ae4 a039;
mov L0x20019ae8 a040;  mov L0x20019aec a041;
mov L0x20019af0 a042;  mov L0x20019af4 a043;
mov L0x20019af8 a044;  mov L0x20019afc a045;
mov L0x20019b00 a046;  mov L0x20019b04 a047;
mov L0x20019b08 a048;  mov L0x20019b0c a049;
mov L0x20019b10 a050;  mov L0x20019b14 a051;
mov L0x20019b18 a052;  mov L0x20019b1c a053;
mov L0x20019b20 a054;  mov L0x20019b24 a055;
mov L0x20019b28 a056;  mov L0x20019b2c a057;
mov L0x20019b30 a058;  mov L0x20019b34 a059;
mov L0x20019b38 a060;  mov L0x20019b3c a061;
mov L0x20019b40 a062;  mov L0x20019b44 a063;
mov L0x20019b48 a064;  mov L0x20019b4c a065;
mov L0x20019b50 a066;  mov L0x20019b54 a067;
mov L0x20019b58 a068;  mov L0x20019b5c a069;
mov L0x20019b60 a070;  mov L0x20019b64 a071;
mov L0x20019b68 a072;  mov L0x20019b6c a073;
mov L0x20019b70 a074;  mov L0x20019b74 a075;
mov L0x20019b78 a076;  mov L0x20019b7c a077;
mov L0x20019b80 a078;  mov L0x20019b84 a079;
mov L0x20019b88 a080;  mov L0x20019b8c a081;
mov L0x20019b90 a082;  mov L0x20019b94 a083;
mov L0x20019b98 a084;  mov L0x20019b9c a085;
mov L0x20019ba0 a086;  mov L0x20019ba4 a087;
mov L0x20019ba8 a088;  mov L0x20019bac a089;
mov L0x20019bb0 a090;  mov L0x20019bb4 a091;
mov L0x20019bb8 a092;  mov L0x20019bbc a093;
mov L0x20019bc0 a094;  mov L0x20019bc4 a095;
mov L0x20019bc8 a096;  mov L0x20019bcc a097;
mov L0x20019bd0 a098;  mov L0x20019bd4 a099;
mov L0x20019bd8 a100;  mov L0x20019bdc a101;
mov L0x20019be0 a102;  mov L0x20019be4 a103;
mov L0x20019be8 a104;  mov L0x20019bec a105;
mov L0x20019bf0 a106;  mov L0x20019bf4 a107;
mov L0x20019bf8 a108;  mov L0x20019bfc a109;
mov L0x20019c00 a110;  mov L0x20019c04 a111;
mov L0x20019c08 a112;  mov L0x20019c0c a113;
mov L0x20019c10 a114;  mov L0x20019c14 a115;
mov L0x20019c18 a116;  mov L0x20019c1c a117;
mov L0x20019c20 a118;  mov L0x20019c24 a119;
mov L0x20019c28 a120;  mov L0x20019c2c a121;
mov L0x20019c30 a122;  mov L0x20019c34 a123;
mov L0x20019c38 a124;  mov L0x20019c3c a125;
mov L0x20019c40 a126;  mov L0x20019c44 a127;
mov L0x20019c48 a128;  mov L0x20019c4c a129;
mov L0x20019c50 a130;  mov L0x20019c54 a131;
mov L0x20019c58 a132;  mov L0x20019c5c a133;
mov L0x20019c60 a134;  mov L0x20019c64 a135;
mov L0x20019c68 a136;  mov L0x20019c6c a137;
mov L0x20019c70 a138;  mov L0x20019c74 a139;
mov L0x20019c78 a140;  mov L0x20019c7c a141;
mov L0x20019c80 a142;  mov L0x20019c84 a143;
mov L0x20019c88 a144;  mov L0x20019c8c a145;
mov L0x20019c90 a146;  mov L0x20019c94 a147;
mov L0x20019c98 a148;  mov L0x20019c9c a149;
mov L0x20019ca0 a150;  mov L0x20019ca4 a151;
mov L0x20019ca8 a152;  mov L0x20019cac a153;
mov L0x20019cb0 a154;  mov L0x20019cb4 a155;
mov L0x20019cb8 a156;  mov L0x20019cbc a157;
mov L0x20019cc0 a158;  mov L0x20019cc4 a159;
mov L0x20019cc8 a160;  mov L0x20019ccc a161;
mov L0x20019cd0 a162;  mov L0x20019cd4 a163;
mov L0x20019cd8 a164;  mov L0x20019cdc a165;
mov L0x20019ce0 a166;  mov L0x20019ce4 a167;
mov L0x20019ce8 a168;  mov L0x20019cec a169;
mov L0x20019cf0 a170;  mov L0x20019cf4 a171;
mov L0x20019cf8 a172;  mov L0x20019cfc a173;
mov L0x20019d00 a174;  mov L0x20019d04 a175;
mov L0x20019d08 a176;  mov L0x20019d0c a177;
mov L0x20019d10 a178;  mov L0x20019d14 a179;
mov L0x20019d18 a180;  mov L0x20019d1c a181;
mov L0x20019d20 a182;  mov L0x20019d24 a183;
mov L0x20019d28 a184;  mov L0x20019d2c a185;
mov L0x20019d30 a186;  mov L0x20019d34 a187;
mov L0x20019d38 a188;  mov L0x20019d3c a189;
mov L0x20019d40 a190;  mov L0x20019d44 a191;
mov L0x20019d48 a192;  mov L0x20019d4c a193;
mov L0x20019d50 a194;  mov L0x20019d54 a195;
mov L0x20019d58 a196;  mov L0x20019d5c a197;
mov L0x20019d60 a198;  mov L0x20019d64 a199;
mov L0x20019d68 a200;  mov L0x20019d6c a201;
mov L0x20019d70 a202;  mov L0x20019d74 a203;
mov L0x20019d78 a204;  mov L0x20019d7c a205;
mov L0x20019d80 a206;  mov L0x20019d84 a207;
mov L0x20019d88 a208;  mov L0x20019d8c a209;
mov L0x20019d90 a210;  mov L0x20019d94 a211;
mov L0x20019d98 a212;  mov L0x20019d9c a213;
mov L0x20019da0 a214;  mov L0x20019da4 a215;
mov L0x20019da8 a216;  mov L0x20019dac a217;
mov L0x20019db0 a218;  mov L0x20019db4 a219;
mov L0x20019db8 a220;  mov L0x20019dbc a221;
mov L0x20019dc0 a222;  mov L0x20019dc4 a223;
mov L0x20019dc8 a224;  mov L0x20019dcc a225;
mov L0x20019dd0 a226;  mov L0x20019dd4 a227;
mov L0x20019dd8 a228;  mov L0x20019ddc a229;
mov L0x20019de0 a230;  mov L0x20019de4 a231;
mov L0x20019de8 a232;  mov L0x20019dec a233;
mov L0x20019df0 a234;  mov L0x20019df4 a235;
mov L0x20019df8 a236;  mov L0x20019dfc a237;
mov L0x20019e00 a238;  mov L0x20019e04 a239;
mov L0x20019e08 a240;  mov L0x20019e0c a241;
mov L0x20019e10 a242;  mov L0x20019e14 a243;
mov L0x20019e18 a244;  mov L0x20019e1c a245;
mov L0x20019e20 a246;  mov L0x20019e24 a247;
mov L0x20019e28 a248;  mov L0x20019e2c a249;
mov L0x20019e30 a250;  mov L0x20019e34 a251;
mov L0x20019e38 a252;  mov L0x20019e3c a253;
mov L0x20019e40 a254;  mov L0x20019e44 a255;

(* src b *)
mov L0x20019e48 b000;  mov L0x20019e4c b001;
mov L0x20019e50 b002;  mov L0x20019e54 b003;
mov L0x20019e58 b004;  mov L0x20019e5c b005;
mov L0x20019e60 b006;  mov L0x20019e64 b007;
mov L0x20019e68 b008;  mov L0x20019e6c b009;
mov L0x20019e70 b010;  mov L0x20019e74 b011;
mov L0x20019e78 b012;  mov L0x20019e7c b013;
mov L0x20019e80 b014;  mov L0x20019e84 b015;
mov L0x20019e88 b016;  mov L0x20019e8c b017;
mov L0x20019e90 b018;  mov L0x20019e94 b019;
mov L0x20019e98 b020;  mov L0x20019e9c b021;
mov L0x20019ea0 b022;  mov L0x20019ea4 b023;
mov L0x20019ea8 b024;  mov L0x20019eac b025;
mov L0x20019eb0 b026;  mov L0x20019eb4 b027;
mov L0x20019eb8 b028;  mov L0x20019ebc b029;
mov L0x20019ec0 b030;  mov L0x20019ec4 b031;
mov L0x20019ec8 b032;  mov L0x20019ecc b033;
mov L0x20019ed0 b034;  mov L0x20019ed4 b035;
mov L0x20019ed8 b036;  mov L0x20019edc b037;
mov L0x20019ee0 b038;  mov L0x20019ee4 b039;
mov L0x20019ee8 b040;  mov L0x20019eec b041;
mov L0x20019ef0 b042;  mov L0x20019ef4 b043;
mov L0x20019ef8 b044;  mov L0x20019efc b045;
mov L0x20019f00 b046;  mov L0x20019f04 b047;
mov L0x20019f08 b048;  mov L0x20019f0c b049;
mov L0x20019f10 b050;  mov L0x20019f14 b051;
mov L0x20019f18 b052;  mov L0x20019f1c b053;
mov L0x20019f20 b054;  mov L0x20019f24 b055;
mov L0x20019f28 b056;  mov L0x20019f2c b057;
mov L0x20019f30 b058;  mov L0x20019f34 b059;
mov L0x20019f38 b060;  mov L0x20019f3c b061;
mov L0x20019f40 b062;  mov L0x20019f44 b063;
mov L0x20019f48 b064;  mov L0x20019f4c b065;
mov L0x20019f50 b066;  mov L0x20019f54 b067;
mov L0x20019f58 b068;  mov L0x20019f5c b069;
mov L0x20019f60 b070;  mov L0x20019f64 b071;
mov L0x20019f68 b072;  mov L0x20019f6c b073;
mov L0x20019f70 b074;  mov L0x20019f74 b075;
mov L0x20019f78 b076;  mov L0x20019f7c b077;
mov L0x20019f80 b078;  mov L0x20019f84 b079;
mov L0x20019f88 b080;  mov L0x20019f8c b081;
mov L0x20019f90 b082;  mov L0x20019f94 b083;
mov L0x20019f98 b084;  mov L0x20019f9c b085;
mov L0x20019fa0 b086;  mov L0x20019fa4 b087;
mov L0x20019fa8 b088;  mov L0x20019fac b089;
mov L0x20019fb0 b090;  mov L0x20019fb4 b091;
mov L0x20019fb8 b092;  mov L0x20019fbc b093;
mov L0x20019fc0 b094;  mov L0x20019fc4 b095;
mov L0x20019fc8 b096;  mov L0x20019fcc b097;
mov L0x20019fd0 b098;  mov L0x20019fd4 b099;
mov L0x20019fd8 b100;  mov L0x20019fdc b101;
mov L0x20019fe0 b102;  mov L0x20019fe4 b103;
mov L0x20019fe8 b104;  mov L0x20019fec b105;
mov L0x20019ff0 b106;  mov L0x20019ff4 b107;
mov L0x20019ff8 b108;  mov L0x20019ffc b109;
mov L0x2001a000 b110;  mov L0x2001a004 b111;
mov L0x2001a008 b112;  mov L0x2001a00c b113;
mov L0x2001a010 b114;  mov L0x2001a014 b115;
mov L0x2001a018 b116;  mov L0x2001a01c b117;
mov L0x2001a020 b118;  mov L0x2001a024 b119;
mov L0x2001a028 b120;  mov L0x2001a02c b121;
mov L0x2001a030 b122;  mov L0x2001a034 b123;
mov L0x2001a038 b124;  mov L0x2001a03c b125;
mov L0x2001a040 b126;  mov L0x2001a044 b127;
mov L0x2001a048 b128;  mov L0x2001a04c b129;
mov L0x2001a050 b130;  mov L0x2001a054 b131;
mov L0x2001a058 b132;  mov L0x2001a05c b133;
mov L0x2001a060 b134;  mov L0x2001a064 b135;
mov L0x2001a068 b136;  mov L0x2001a06c b137;
mov L0x2001a070 b138;  mov L0x2001a074 b139;
mov L0x2001a078 b140;  mov L0x2001a07c b141;
mov L0x2001a080 b142;  mov L0x2001a084 b143;
mov L0x2001a088 b144;  mov L0x2001a08c b145;
mov L0x2001a090 b146;  mov L0x2001a094 b147;
mov L0x2001a098 b148;  mov L0x2001a09c b149;
mov L0x2001a0a0 b150;  mov L0x2001a0a4 b151;
mov L0x2001a0a8 b152;  mov L0x2001a0ac b153;
mov L0x2001a0b0 b154;  mov L0x2001a0b4 b155;
mov L0x2001a0b8 b156;  mov L0x2001a0bc b157;
mov L0x2001a0c0 b158;  mov L0x2001a0c4 b159;
mov L0x2001a0c8 b160;  mov L0x2001a0cc b161;
mov L0x2001a0d0 b162;  mov L0x2001a0d4 b163;
mov L0x2001a0d8 b164;  mov L0x2001a0dc b165;
mov L0x2001a0e0 b166;  mov L0x2001a0e4 b167;
mov L0x2001a0e8 b168;  mov L0x2001a0ec b169;
mov L0x2001a0f0 b170;  mov L0x2001a0f4 b171;
mov L0x2001a0f8 b172;  mov L0x2001a0fc b173;
mov L0x2001a100 b174;  mov L0x2001a104 b175;
mov L0x2001a108 b176;  mov L0x2001a10c b177;
mov L0x2001a110 b178;  mov L0x2001a114 b179;
mov L0x2001a118 b180;  mov L0x2001a11c b181;
mov L0x2001a120 b182;  mov L0x2001a124 b183;
mov L0x2001a128 b184;  mov L0x2001a12c b185;
mov L0x2001a130 b186;  mov L0x2001a134 b187;
mov L0x2001a138 b188;  mov L0x2001a13c b189;
mov L0x2001a140 b190;  mov L0x2001a144 b191;
mov L0x2001a148 b192;  mov L0x2001a14c b193;
mov L0x2001a150 b194;  mov L0x2001a154 b195;
mov L0x2001a158 b196;  mov L0x2001a15c b197;
mov L0x2001a160 b198;  mov L0x2001a164 b199;
mov L0x2001a168 b200;  mov L0x2001a16c b201;
mov L0x2001a170 b202;  mov L0x2001a174 b203;
mov L0x2001a178 b204;  mov L0x2001a17c b205;
mov L0x2001a180 b206;  mov L0x2001a184 b207;
mov L0x2001a188 b208;  mov L0x2001a18c b209;
mov L0x2001a190 b210;  mov L0x2001a194 b211;
mov L0x2001a198 b212;  mov L0x2001a19c b213;
mov L0x2001a1a0 b214;  mov L0x2001a1a4 b215;
mov L0x2001a1a8 b216;  mov L0x2001a1ac b217;
mov L0x2001a1b0 b218;  mov L0x2001a1b4 b219;
mov L0x2001a1b8 b220;  mov L0x2001a1bc b221;
mov L0x2001a1c0 b222;  mov L0x2001a1c4 b223;
mov L0x2001a1c8 b224;  mov L0x2001a1cc b225;
mov L0x2001a1d0 b226;  mov L0x2001a1d4 b227;
mov L0x2001a1d8 b228;  mov L0x2001a1dc b229;
mov L0x2001a1e0 b230;  mov L0x2001a1e4 b231;
mov L0x2001a1e8 b232;  mov L0x2001a1ec b233;
mov L0x2001a1f0 b234;  mov L0x2001a1f4 b235;
mov L0x2001a1f8 b236;  mov L0x2001a1fc b237;
mov L0x2001a200 b238;  mov L0x2001a204 b239;
mov L0x2001a208 b240;  mov L0x2001a20c b241;
mov L0x2001a210 b242;  mov L0x2001a214 b243;
mov L0x2001a218 b244;  mov L0x2001a21c b245;
mov L0x2001a220 b246;  mov L0x2001a224 b247;
mov L0x2001a228 b248;  mov L0x2001a22c b249;
mov L0x2001a230 b250;  mov L0x2001a234 b251;
mov L0x2001a238 b252;  mov L0x2001a23c b253;
mov L0x2001a240 b254;  mov L0x2001a244 b255;

(* mul_table *)
mov L0x800f720 (   170812)@sint32;  mov L0x800f724 (-10085157)@sint32;
mov L0x800f728 ( 11283370)@sint32;  mov L0x800f72c ( 10064939)@sint32;
mov L0x800f730 (-11057244)@sint32;  mov L0x800f734 (   831656)@sint32;
mov L0x800f738 (  9511410)@sint32;  mov L0x800f73c (  2088463)@sint32;
mov L0x800f740 ( -8183720)@sint32;  mov L0x800f744 ( -7836837)@sint32;
mov L0x800f748 (  5141500)@sint32;  mov L0x800f74c (-12428729)@sint32;
mov L0x800f750 (-12289212)@sint32;  mov L0x800f754 (-12672803)@sint32;
mov L0x800f758 (-11196795)@sint32;  mov L0x800f75c (  6573093)@sint32;
mov L0x800f760 ( -9922532)@sint32;  mov L0x800f764 (  2470383)@sint32;
mov L0x800f768 ( 10695464)@sint32;  mov L0x800f76c ( -3892856)@sint32;
mov L0x800f770 (  4619612)@sint32;  mov L0x800f774 (  6074925)@sint32;
mov L0x800f778 ( 12120915)@sint32;  mov L0x800f77c (  7751589)@sint32;
mov L0x800f780 ( -1196321)@sint32;  mov L0x800f784 (  8484143)@sint32;
mov L0x800f788 ( 11670672)@sint32;  mov L0x800f78c ( -3041171)@sint32;
mov L0x800f790 (  6717881)@sint32;  mov L0x800f794 ( -6139656)@sint32;
mov L0x800f798 ( -6988319)@sint32;  mov L0x800f79c ( -1579586)@sint32;

(* regs *)
nondet L0x200194f0@uint32;
nondet L0x200194f4@uint32;
nondet r0@uint32;
nondet r1@uint32;
mov r2 Qprime;
mov r3 Q;

(* inp_a_polys *)
ghost inp_a_poly_0@bit :
  inp_a_poly_0 * inp_a_poly_0 = 
    L0x20019a48 * (x**0) + L0x20019a4c * (x**1) + 
    L0x20019a50 * (x**2) + L0x20019a54 * (x**3)
  && true;

ghost inp_a_poly_1@bit :
  inp_a_poly_1 * inp_a_poly_1 = 
    L0x20019a58 * (x**0) + L0x20019a5c * (x**1) + 
    L0x20019a60 * (x**2) + L0x20019a64 * (x**3)
  && true;

ghost inp_a_poly_2@bit :
  inp_a_poly_2 * inp_a_poly_2 = 
    L0x20019a68 * (x**0) + L0x20019a6c * (x**1) + 
    L0x20019a70 * (x**2) + L0x20019a74 * (x**3)
  && true;

ghost inp_a_poly_3@bit :
  inp_a_poly_3 * inp_a_poly_3 = 
    L0x20019a78 * (x**0) + L0x20019a7c * (x**1) + 
    L0x20019a80 * (x**2) + L0x20019a84 * (x**3)
  && true;

ghost inp_a_poly_4@bit :
  inp_a_poly_4 * inp_a_poly_4 = 
    L0x20019a88 * (x**0) + L0x20019a8c * (x**1) + 
    L0x20019a90 * (x**2) + L0x20019a94 * (x**3)
  && true;

ghost inp_a_poly_5@bit :
  inp_a_poly_5 * inp_a_poly_5 = 
    L0x20019a98 * (x**0) + L0x20019a9c * (x**1) + 
    L0x20019aa0 * (x**2) + L0x20019aa4 * (x**3)
  && true;

ghost inp_a_poly_6@bit :
  inp_a_poly_6 * inp_a_poly_6 = 
    L0x20019aa8 * (x**0) + L0x20019aac * (x**1) + 
    L0x20019ab0 * (x**2) + L0x20019ab4 * (x**3)
  && true;

ghost inp_a_poly_7@bit :
  inp_a_poly_7 * inp_a_poly_7 = 
    L0x20019ab8 * (x**0) + L0x20019abc * (x**1) + 
    L0x20019ac0 * (x**2) + L0x20019ac4 * (x**3)
  && true;

ghost inp_a_poly_8@bit :
  inp_a_poly_8 * inp_a_poly_8 = 
    L0x20019ac8 * (x**0) + L0x20019acc * (x**1) + 
    L0x20019ad0 * (x**2) + L0x20019ad4 * (x**3)
  && true;

ghost inp_a_poly_9@bit :
  inp_a_poly_9 * inp_a_poly_9 = 
    L0x20019ad8 * (x**0) + L0x20019adc * (x**1) + 
    L0x20019ae0 * (x**2) + L0x20019ae4 * (x**3)
  && true;

ghost inp_a_poly_10@bit :
  inp_a_poly_10 * inp_a_poly_10 = 
    L0x20019ae8 * (x**0) + L0x20019aec * (x**1) + 
    L0x20019af0 * (x**2) + L0x20019af4 * (x**3)
  && true;

ghost inp_a_poly_11@bit :
  inp_a_poly_11 * inp_a_poly_11 = 
    L0x20019af8 * (x**0) + L0x20019afc * (x**1) + 
    L0x20019b00 * (x**2) + L0x20019b04 * (x**3)
  && true;

ghost inp_a_poly_12@bit :
  inp_a_poly_12 * inp_a_poly_12 = 
    L0x20019b08 * (x**0) + L0x20019b0c * (x**1) + 
    L0x20019b10 * (x**2) + L0x20019b14 * (x**3)
  && true;

ghost inp_a_poly_13@bit :
  inp_a_poly_13 * inp_a_poly_13 = 
    L0x20019b18 * (x**0) + L0x20019b1c * (x**1) + 
    L0x20019b20 * (x**2) + L0x20019b24 * (x**3)
  && true;

ghost inp_a_poly_14@bit :
  inp_a_poly_14 * inp_a_poly_14 = 
    L0x20019b28 * (x**0) + L0x20019b2c * (x**1) + 
    L0x20019b30 * (x**2) + L0x20019b34 * (x**3)
  && true;

ghost inp_a_poly_15@bit :
  inp_a_poly_15 * inp_a_poly_15 = 
    L0x20019b38 * (x**0) + L0x20019b3c * (x**1) + 
    L0x20019b40 * (x**2) + L0x20019b44 * (x**3)
  && true;

ghost inp_a_poly_16@bit :
  inp_a_poly_16 * inp_a_poly_16 = 
    L0x20019b48 * (x**0) + L0x20019b4c * (x**1) + 
    L0x20019b50 * (x**2) + L0x20019b54 * (x**3)
  && true;

ghost inp_a_poly_17@bit :
  inp_a_poly_17 * inp_a_poly_17 = 
    L0x20019b58 * (x**0) + L0x20019b5c * (x**1) + 
    L0x20019b60 * (x**2) + L0x20019b64 * (x**3)
  && true;

ghost inp_a_poly_18@bit :
  inp_a_poly_18 * inp_a_poly_18 = 
    L0x20019b68 * (x**0) + L0x20019b6c * (x**1) + 
    L0x20019b70 * (x**2) + L0x20019b74 * (x**3)
  && true;

ghost inp_a_poly_19@bit :
  inp_a_poly_19 * inp_a_poly_19 = 
    L0x20019b78 * (x**0) + L0x20019b7c * (x**1) + 
    L0x20019b80 * (x**2) + L0x20019b84 * (x**3)
  && true;

ghost inp_a_poly_20@bit :
  inp_a_poly_20 * inp_a_poly_20 = 
    L0x20019b88 * (x**0) + L0x20019b8c * (x**1) + 
    L0x20019b90 * (x**2) + L0x20019b94 * (x**3)
  && true;

ghost inp_a_poly_21@bit :
  inp_a_poly_21 * inp_a_poly_21 = 
    L0x20019b98 * (x**0) + L0x20019b9c * (x**1) + 
    L0x20019ba0 * (x**2) + L0x20019ba4 * (x**3)
  && true;

ghost inp_a_poly_22@bit :
  inp_a_poly_22 * inp_a_poly_22 = 
    L0x20019ba8 * (x**0) + L0x20019bac * (x**1) + 
    L0x20019bb0 * (x**2) + L0x20019bb4 * (x**3)
  && true;

ghost inp_a_poly_23@bit :
  inp_a_poly_23 * inp_a_poly_23 = 
    L0x20019bb8 * (x**0) + L0x20019bbc * (x**1) + 
    L0x20019bc0 * (x**2) + L0x20019bc4 * (x**3)
  && true;

ghost inp_a_poly_24@bit :
  inp_a_poly_24 * inp_a_poly_24 = 
    L0x20019bc8 * (x**0) + L0x20019bcc * (x**1) + 
    L0x20019bd0 * (x**2) + L0x20019bd4 * (x**3)
  && true;

ghost inp_a_poly_25@bit :
  inp_a_poly_25 * inp_a_poly_25 = 
    L0x20019bd8 * (x**0) + L0x20019bdc * (x**1) + 
    L0x20019be0 * (x**2) + L0x20019be4 * (x**3)
  && true;

ghost inp_a_poly_26@bit :
  inp_a_poly_26 * inp_a_poly_26 = 
    L0x20019be8 * (x**0) + L0x20019bec * (x**1) + 
    L0x20019bf0 * (x**2) + L0x20019bf4 * (x**3)
  && true;

ghost inp_a_poly_27@bit :
  inp_a_poly_27 * inp_a_poly_27 = 
    L0x20019bf8 * (x**0) + L0x20019bfc * (x**1) + 
    L0x20019c00 * (x**2) + L0x20019c04 * (x**3)
  && true;

ghost inp_a_poly_28@bit :
  inp_a_poly_28 * inp_a_poly_28 = 
    L0x20019c08 * (x**0) + L0x20019c0c * (x**1) + 
    L0x20019c10 * (x**2) + L0x20019c14 * (x**3)
  && true;

ghost inp_a_poly_29@bit :
  inp_a_poly_29 * inp_a_poly_29 = 
    L0x20019c18 * (x**0) + L0x20019c1c * (x**1) + 
    L0x20019c20 * (x**2) + L0x20019c24 * (x**3)
  && true;

ghost inp_a_poly_30@bit :
  inp_a_poly_30 * inp_a_poly_30 = 
    L0x20019c28 * (x**0) + L0x20019c2c * (x**1) + 
    L0x20019c30 * (x**2) + L0x20019c34 * (x**3)
  && true;

ghost inp_a_poly_31@bit :
  inp_a_poly_31 * inp_a_poly_31 = 
    L0x20019c38 * (x**0) + L0x20019c3c * (x**1) + 
    L0x20019c40 * (x**2) + L0x20019c44 * (x**3)
  && true;

ghost inp_a_poly_32@bit :
  inp_a_poly_32 * inp_a_poly_32 = 
    L0x20019c48 * (x**0) + L0x20019c4c * (x**1) + 
    L0x20019c50 * (x**2) + L0x20019c54 * (x**3)
  && true;

ghost inp_a_poly_33@bit :
  inp_a_poly_33 * inp_a_poly_33 = 
    L0x20019c58 * (x**0) + L0x20019c5c * (x**1) + 
    L0x20019c60 * (x**2) + L0x20019c64 * (x**3)
  && true;

ghost inp_a_poly_34@bit :
  inp_a_poly_34 * inp_a_poly_34 = 
    L0x20019c68 * (x**0) + L0x20019c6c * (x**1) + 
    L0x20019c70 * (x**2) + L0x20019c74 * (x**3)
  && true;

ghost inp_a_poly_35@bit :
  inp_a_poly_35 * inp_a_poly_35 = 
    L0x20019c78 * (x**0) + L0x20019c7c * (x**1) + 
    L0x20019c80 * (x**2) + L0x20019c84 * (x**3)
  && true;

ghost inp_a_poly_36@bit :
  inp_a_poly_36 * inp_a_poly_36 = 
    L0x20019c88 * (x**0) + L0x20019c8c * (x**1) + 
    L0x20019c90 * (x**2) + L0x20019c94 * (x**3)
  && true;

ghost inp_a_poly_37@bit :
  inp_a_poly_37 * inp_a_poly_37 = 
    L0x20019c98 * (x**0) + L0x20019c9c * (x**1) + 
    L0x20019ca0 * (x**2) + L0x20019ca4 * (x**3)
  && true;

ghost inp_a_poly_38@bit :
  inp_a_poly_38 * inp_a_poly_38 = 
    L0x20019ca8 * (x**0) + L0x20019cac * (x**1) + 
    L0x20019cb0 * (x**2) + L0x20019cb4 * (x**3)
  && true;

ghost inp_a_poly_39@bit :
  inp_a_poly_39 * inp_a_poly_39 = 
    L0x20019cb8 * (x**0) + L0x20019cbc * (x**1) + 
    L0x20019cc0 * (x**2) + L0x20019cc4 * (x**3)
  && true;

ghost inp_a_poly_40@bit :
  inp_a_poly_40 * inp_a_poly_40 = 
    L0x20019cc8 * (x**0) + L0x20019ccc * (x**1) + 
    L0x20019cd0 * (x**2) + L0x20019cd4 * (x**3)
  && true;

ghost inp_a_poly_41@bit :
  inp_a_poly_41 * inp_a_poly_41 = 
    L0x20019cd8 * (x**0) + L0x20019cdc * (x**1) + 
    L0x20019ce0 * (x**2) + L0x20019ce4 * (x**3)
  && true;

ghost inp_a_poly_42@bit :
  inp_a_poly_42 * inp_a_poly_42 = 
    L0x20019ce8 * (x**0) + L0x20019cec * (x**1) + 
    L0x20019cf0 * (x**2) + L0x20019cf4 * (x**3)
  && true;

ghost inp_a_poly_43@bit :
  inp_a_poly_43 * inp_a_poly_43 = 
    L0x20019cf8 * (x**0) + L0x20019cfc * (x**1) + 
    L0x20019d00 * (x**2) + L0x20019d04 * (x**3)
  && true;

ghost inp_a_poly_44@bit :
  inp_a_poly_44 * inp_a_poly_44 = 
    L0x20019d08 * (x**0) + L0x20019d0c * (x**1) + 
    L0x20019d10 * (x**2) + L0x20019d14 * (x**3)
  && true;

ghost inp_a_poly_45@bit :
  inp_a_poly_45 * inp_a_poly_45 = 
    L0x20019d18 * (x**0) + L0x20019d1c * (x**1) + 
    L0x20019d20 * (x**2) + L0x20019d24 * (x**3)
  && true;

ghost inp_a_poly_46@bit :
  inp_a_poly_46 * inp_a_poly_46 = 
    L0x20019d28 * (x**0) + L0x20019d2c * (x**1) + 
    L0x20019d30 * (x**2) + L0x20019d34 * (x**3)
  && true;

ghost inp_a_poly_47@bit :
  inp_a_poly_47 * inp_a_poly_47 = 
    L0x20019d38 * (x**0) + L0x20019d3c * (x**1) + 
    L0x20019d40 * (x**2) + L0x20019d44 * (x**3)
  && true;

ghost inp_a_poly_48@bit :
  inp_a_poly_48 * inp_a_poly_48 = 
    L0x20019d48 * (x**0) + L0x20019d4c * (x**1) + 
    L0x20019d50 * (x**2) + L0x20019d54 * (x**3)
  && true;

ghost inp_a_poly_49@bit :
  inp_a_poly_49 * inp_a_poly_49 = 
    L0x20019d58 * (x**0) + L0x20019d5c * (x**1) + 
    L0x20019d60 * (x**2) + L0x20019d64 * (x**3)
  && true;

ghost inp_a_poly_50@bit :
  inp_a_poly_50 * inp_a_poly_50 = 
    L0x20019d68 * (x**0) + L0x20019d6c * (x**1) + 
    L0x20019d70 * (x**2) + L0x20019d74 * (x**3)
  && true;

ghost inp_a_poly_51@bit :
  inp_a_poly_51 * inp_a_poly_51 = 
    L0x20019d78 * (x**0) + L0x20019d7c * (x**1) + 
    L0x20019d80 * (x**2) + L0x20019d84 * (x**3)
  && true;

ghost inp_a_poly_52@bit :
  inp_a_poly_52 * inp_a_poly_52 = 
    L0x20019d88 * (x**0) + L0x20019d8c * (x**1) + 
    L0x20019d90 * (x**2) + L0x20019d94 * (x**3)
  && true;

ghost inp_a_poly_53@bit :
  inp_a_poly_53 * inp_a_poly_53 = 
    L0x20019d98 * (x**0) + L0x20019d9c * (x**1) + 
    L0x20019da0 * (x**2) + L0x20019da4 * (x**3)
  && true;

ghost inp_a_poly_54@bit :
  inp_a_poly_54 * inp_a_poly_54 = 
    L0x20019da8 * (x**0) + L0x20019dac * (x**1) + 
    L0x20019db0 * (x**2) + L0x20019db4 * (x**3)
  && true;

ghost inp_a_poly_55@bit :
  inp_a_poly_55 * inp_a_poly_55 = 
    L0x20019db8 * (x**0) + L0x20019dbc * (x**1) + 
    L0x20019dc0 * (x**2) + L0x20019dc4 * (x**3)
  && true;

ghost inp_a_poly_56@bit :
  inp_a_poly_56 * inp_a_poly_56 = 
    L0x20019dc8 * (x**0) + L0x20019dcc * (x**1) + 
    L0x20019dd0 * (x**2) + L0x20019dd4 * (x**3)
  && true;

ghost inp_a_poly_57@bit :
  inp_a_poly_57 * inp_a_poly_57 = 
    L0x20019dd8 * (x**0) + L0x20019ddc * (x**1) + 
    L0x20019de0 * (x**2) + L0x20019de4 * (x**3)
  && true;

ghost inp_a_poly_58@bit :
  inp_a_poly_58 * inp_a_poly_58 = 
    L0x20019de8 * (x**0) + L0x20019dec * (x**1) + 
    L0x20019df0 * (x**2) + L0x20019df4 * (x**3)
  && true;

ghost inp_a_poly_59@bit :
  inp_a_poly_59 * inp_a_poly_59 = 
    L0x20019df8 * (x**0) + L0x20019dfc * (x**1) + 
    L0x20019e00 * (x**2) + L0x20019e04 * (x**3)
  && true;

ghost inp_a_poly_60@bit :
  inp_a_poly_60 * inp_a_poly_60 = 
    L0x20019e08 * (x**0) + L0x20019e0c * (x**1) + 
    L0x20019e10 * (x**2) + L0x20019e14 * (x**3)
  && true;

ghost inp_a_poly_61@bit :
  inp_a_poly_61 * inp_a_poly_61 = 
    L0x20019e18 * (x**0) + L0x20019e1c * (x**1) + 
    L0x20019e20 * (x**2) + L0x20019e24 * (x**3)
  && true;

ghost inp_a_poly_62@bit :
  inp_a_poly_62 * inp_a_poly_62 = 
    L0x20019e28 * (x**0) + L0x20019e2c * (x**1) + 
    L0x20019e30 * (x**2) + L0x20019e34 * (x**3)
  && true;

ghost inp_a_poly_63@bit :
  inp_a_poly_63 * inp_a_poly_63 = 
    L0x20019e38 * (x**0) + L0x20019e3c * (x**1) + 
    L0x20019e40 * (x**2) + L0x20019e44 * (x**3)
  && true;

(* inp_b_polys *)
ghost inp_b_poly_0@bit :
  inp_b_poly_0 * inp_b_poly_0 = 
    L0x20019e48 * (x**0) + L0x20019e4c * (x**1) + 
    L0x20019e50 * (x**2) + L0x20019e54 * (x**3)
  && true;

ghost inp_b_poly_1@bit :
  inp_b_poly_1 * inp_b_poly_1 = 
    L0x20019e58 * (x**0) + L0x20019e5c * (x**1) + 
    L0x20019e60 * (x**2) + L0x20019e64 * (x**3)
  && true;

ghost inp_b_poly_2@bit :
  inp_b_poly_2 * inp_b_poly_2 = 
    L0x20019e68 * (x**0) + L0x20019e6c * (x**1) + 
    L0x20019e70 * (x**2) + L0x20019e74 * (x**3)
  && true;

ghost inp_b_poly_3@bit :
  inp_b_poly_3 * inp_b_poly_3 = 
    L0x20019e78 * (x**0) + L0x20019e7c * (x**1) + 
    L0x20019e80 * (x**2) + L0x20019e84 * (x**3)
  && true;

ghost inp_b_poly_4@bit :
  inp_b_poly_4 * inp_b_poly_4 = 
    L0x20019e88 * (x**0) + L0x20019e8c * (x**1) + 
    L0x20019e90 * (x**2) + L0x20019e94 * (x**3)
  && true;

ghost inp_b_poly_5@bit :
  inp_b_poly_5 * inp_b_poly_5 = 
    L0x20019e98 * (x**0) + L0x20019e9c * (x**1) + 
    L0x20019ea0 * (x**2) + L0x20019ea4 * (x**3)
  && true;

ghost inp_b_poly_6@bit :
  inp_b_poly_6 * inp_b_poly_6 = 
    L0x20019ea8 * (x**0) + L0x20019eac * (x**1) + 
    L0x20019eb0 * (x**2) + L0x20019eb4 * (x**3)
  && true;

ghost inp_b_poly_7@bit :
  inp_b_poly_7 * inp_b_poly_7 = 
    L0x20019eb8 * (x**0) + L0x20019ebc * (x**1) + 
    L0x20019ec0 * (x**2) + L0x20019ec4 * (x**3)
  && true;

ghost inp_b_poly_8@bit :
  inp_b_poly_8 * inp_b_poly_8 = 
    L0x20019ec8 * (x**0) + L0x20019ecc * (x**1) + 
    L0x20019ed0 * (x**2) + L0x20019ed4 * (x**3)
  && true;

ghost inp_b_poly_9@bit :
  inp_b_poly_9 * inp_b_poly_9 = 
    L0x20019ed8 * (x**0) + L0x20019edc * (x**1) + 
    L0x20019ee0 * (x**2) + L0x20019ee4 * (x**3)
  && true;

ghost inp_b_poly_10@bit :
  inp_b_poly_10 * inp_b_poly_10 = 
    L0x20019ee8 * (x**0) + L0x20019eec * (x**1) + 
    L0x20019ef0 * (x**2) + L0x20019ef4 * (x**3)
  && true;

ghost inp_b_poly_11@bit :
  inp_b_poly_11 * inp_b_poly_11 = 
    L0x20019ef8 * (x**0) + L0x20019efc * (x**1) + 
    L0x20019f00 * (x**2) + L0x20019f04 * (x**3)
  && true;

ghost inp_b_poly_12@bit :
  inp_b_poly_12 * inp_b_poly_12 = 
    L0x20019f08 * (x**0) + L0x20019f0c * (x**1) + 
    L0x20019f10 * (x**2) + L0x20019f14 * (x**3)
  && true;

ghost inp_b_poly_13@bit :
  inp_b_poly_13 * inp_b_poly_13 = 
    L0x20019f18 * (x**0) + L0x20019f1c * (x**1) + 
    L0x20019f20 * (x**2) + L0x20019f24 * (x**3)
  && true;

ghost inp_b_poly_14@bit :
  inp_b_poly_14 * inp_b_poly_14 = 
    L0x20019f28 * (x**0) + L0x20019f2c * (x**1) + 
    L0x20019f30 * (x**2) + L0x20019f34 * (x**3)
  && true;

ghost inp_b_poly_15@bit :
  inp_b_poly_15 * inp_b_poly_15 = 
    L0x20019f38 * (x**0) + L0x20019f3c * (x**1) + 
    L0x20019f40 * (x**2) + L0x20019f44 * (x**3)
  && true;

ghost inp_b_poly_16@bit :
  inp_b_poly_16 * inp_b_poly_16 = 
    L0x20019f48 * (x**0) + L0x20019f4c * (x**1) + 
    L0x20019f50 * (x**2) + L0x20019f54 * (x**3)
  && true;

ghost inp_b_poly_17@bit :
  inp_b_poly_17 * inp_b_poly_17 = 
    L0x20019f58 * (x**0) + L0x20019f5c * (x**1) + 
    L0x20019f60 * (x**2) + L0x20019f64 * (x**3)
  && true;

ghost inp_b_poly_18@bit :
  inp_b_poly_18 * inp_b_poly_18 = 
    L0x20019f68 * (x**0) + L0x20019f6c * (x**1) + 
    L0x20019f70 * (x**2) + L0x20019f74 * (x**3)
  && true;

ghost inp_b_poly_19@bit :
  inp_b_poly_19 * inp_b_poly_19 = 
    L0x20019f78 * (x**0) + L0x20019f7c * (x**1) + 
    L0x20019f80 * (x**2) + L0x20019f84 * (x**3)
  && true;

ghost inp_b_poly_20@bit :
  inp_b_poly_20 * inp_b_poly_20 = 
    L0x20019f88 * (x**0) + L0x20019f8c * (x**1) + 
    L0x20019f90 * (x**2) + L0x20019f94 * (x**3)
  && true;

ghost inp_b_poly_21@bit :
  inp_b_poly_21 * inp_b_poly_21 = 
    L0x20019f98 * (x**0) + L0x20019f9c * (x**1) + 
    L0x20019fa0 * (x**2) + L0x20019fa4 * (x**3)
  && true;

ghost inp_b_poly_22@bit :
  inp_b_poly_22 * inp_b_poly_22 = 
    L0x20019fa8 * (x**0) + L0x20019fac * (x**1) + 
    L0x20019fb0 * (x**2) + L0x20019fb4 * (x**3)
  && true;

ghost inp_b_poly_23@bit :
  inp_b_poly_23 * inp_b_poly_23 = 
    L0x20019fb8 * (x**0) + L0x20019fbc * (x**1) + 
    L0x20019fc0 * (x**2) + L0x20019fc4 * (x**3)
  && true;

ghost inp_b_poly_24@bit :
  inp_b_poly_24 * inp_b_poly_24 = 
    L0x20019fc8 * (x**0) + L0x20019fcc * (x**1) + 
    L0x20019fd0 * (x**2) + L0x20019fd4 * (x**3)
  && true;

ghost inp_b_poly_25@bit :
  inp_b_poly_25 * inp_b_poly_25 = 
    L0x20019fd8 * (x**0) + L0x20019fdc * (x**1) + 
    L0x20019fe0 * (x**2) + L0x20019fe4 * (x**3)
  && true;

ghost inp_b_poly_26@bit :
  inp_b_poly_26 * inp_b_poly_26 = 
    L0x20019fe8 * (x**0) + L0x20019fec * (x**1) + 
    L0x20019ff0 * (x**2) + L0x20019ff4 * (x**3)
  && true;

ghost inp_b_poly_27@bit :
  inp_b_poly_27 * inp_b_poly_27 = 
    L0x20019ff8 * (x**0) + L0x20019ffc * (x**1) + 
    L0x2001a000 * (x**2) + L0x2001a004 * (x**3)
  && true;

ghost inp_b_poly_28@bit :
  inp_b_poly_28 * inp_b_poly_28 = 
    L0x2001a008 * (x**0) + L0x2001a00c * (x**1) + 
    L0x2001a010 * (x**2) + L0x2001a014 * (x**3)
  && true;

ghost inp_b_poly_29@bit :
  inp_b_poly_29 * inp_b_poly_29 = 
    L0x2001a018 * (x**0) + L0x2001a01c * (x**1) + 
    L0x2001a020 * (x**2) + L0x2001a024 * (x**3)
  && true;

ghost inp_b_poly_30@bit :
  inp_b_poly_30 * inp_b_poly_30 = 
    L0x2001a028 * (x**0) + L0x2001a02c * (x**1) + 
    L0x2001a030 * (x**2) + L0x2001a034 * (x**3)
  && true;

ghost inp_b_poly_31@bit :
  inp_b_poly_31 * inp_b_poly_31 = 
    L0x2001a038 * (x**0) + L0x2001a03c * (x**1) + 
    L0x2001a040 * (x**2) + L0x2001a044 * (x**3)
  && true;

ghost inp_b_poly_32@bit :
  inp_b_poly_32 * inp_b_poly_32 = 
    L0x2001a048 * (x**0) + L0x2001a04c * (x**1) + 
    L0x2001a050 * (x**2) + L0x2001a054 * (x**3)
  && true;

ghost inp_b_poly_33@bit :
  inp_b_poly_33 * inp_b_poly_33 = 
    L0x2001a058 * (x**0) + L0x2001a05c * (x**1) + 
    L0x2001a060 * (x**2) + L0x2001a064 * (x**3)
  && true;

ghost inp_b_poly_34@bit :
  inp_b_poly_34 * inp_b_poly_34 = 
    L0x2001a068 * (x**0) + L0x2001a06c * (x**1) + 
    L0x2001a070 * (x**2) + L0x2001a074 * (x**3)
  && true;

ghost inp_b_poly_35@bit :
  inp_b_poly_35 * inp_b_poly_35 = 
    L0x2001a078 * (x**0) + L0x2001a07c * (x**1) + 
    L0x2001a080 * (x**2) + L0x2001a084 * (x**3)
  && true;

ghost inp_b_poly_36@bit :
  inp_b_poly_36 * inp_b_poly_36 = 
    L0x2001a088 * (x**0) + L0x2001a08c * (x**1) + 
    L0x2001a090 * (x**2) + L0x2001a094 * (x**3)
  && true;

ghost inp_b_poly_37@bit :
  inp_b_poly_37 * inp_b_poly_37 = 
    L0x2001a098 * (x**0) + L0x2001a09c * (x**1) + 
    L0x2001a0a0 * (x**2) + L0x2001a0a4 * (x**3)
  && true;

ghost inp_b_poly_38@bit :
  inp_b_poly_38 * inp_b_poly_38 = 
    L0x2001a0a8 * (x**0) + L0x2001a0ac * (x**1) + 
    L0x2001a0b0 * (x**2) + L0x2001a0b4 * (x**3)
  && true;

ghost inp_b_poly_39@bit :
  inp_b_poly_39 * inp_b_poly_39 = 
    L0x2001a0b8 * (x**0) + L0x2001a0bc * (x**1) + 
    L0x2001a0c0 * (x**2) + L0x2001a0c4 * (x**3)
  && true;

ghost inp_b_poly_40@bit :
  inp_b_poly_40 * inp_b_poly_40 = 
    L0x2001a0c8 * (x**0) + L0x2001a0cc * (x**1) + 
    L0x2001a0d0 * (x**2) + L0x2001a0d4 * (x**3)
  && true;

ghost inp_b_poly_41@bit :
  inp_b_poly_41 * inp_b_poly_41 = 
    L0x2001a0d8 * (x**0) + L0x2001a0dc * (x**1) + 
    L0x2001a0e0 * (x**2) + L0x2001a0e4 * (x**3)
  && true;

ghost inp_b_poly_42@bit :
  inp_b_poly_42 * inp_b_poly_42 = 
    L0x2001a0e8 * (x**0) + L0x2001a0ec * (x**1) + 
    L0x2001a0f0 * (x**2) + L0x2001a0f4 * (x**3)
  && true;

ghost inp_b_poly_43@bit :
  inp_b_poly_43 * inp_b_poly_43 = 
    L0x2001a0f8 * (x**0) + L0x2001a0fc * (x**1) + 
    L0x2001a100 * (x**2) + L0x2001a104 * (x**3)
  && true;

ghost inp_b_poly_44@bit :
  inp_b_poly_44 * inp_b_poly_44 = 
    L0x2001a108 * (x**0) + L0x2001a10c * (x**1) + 
    L0x2001a110 * (x**2) + L0x2001a114 * (x**3)
  && true;

ghost inp_b_poly_45@bit :
  inp_b_poly_45 * inp_b_poly_45 = 
    L0x2001a118 * (x**0) + L0x2001a11c * (x**1) + 
    L0x2001a120 * (x**2) + L0x2001a124 * (x**3)
  && true;

ghost inp_b_poly_46@bit :
  inp_b_poly_46 * inp_b_poly_46 = 
    L0x2001a128 * (x**0) + L0x2001a12c * (x**1) + 
    L0x2001a130 * (x**2) + L0x2001a134 * (x**3)
  && true;

ghost inp_b_poly_47@bit :
  inp_b_poly_47 * inp_b_poly_47 = 
    L0x2001a138 * (x**0) + L0x2001a13c * (x**1) + 
    L0x2001a140 * (x**2) + L0x2001a144 * (x**3)
  && true;

ghost inp_b_poly_48@bit :
  inp_b_poly_48 * inp_b_poly_48 = 
    L0x2001a148 * (x**0) + L0x2001a14c * (x**1) + 
    L0x2001a150 * (x**2) + L0x2001a154 * (x**3)
  && true;

ghost inp_b_poly_49@bit :
  inp_b_poly_49 * inp_b_poly_49 = 
    L0x2001a158 * (x**0) + L0x2001a15c * (x**1) + 
    L0x2001a160 * (x**2) + L0x2001a164 * (x**3)
  && true;

ghost inp_b_poly_50@bit :
  inp_b_poly_50 * inp_b_poly_50 = 
    L0x2001a168 * (x**0) + L0x2001a16c * (x**1) + 
    L0x2001a170 * (x**2) + L0x2001a174 * (x**3)
  && true;

ghost inp_b_poly_51@bit :
  inp_b_poly_51 * inp_b_poly_51 = 
    L0x2001a178 * (x**0) + L0x2001a17c * (x**1) + 
    L0x2001a180 * (x**2) + L0x2001a184 * (x**3)
  && true;

ghost inp_b_poly_52@bit :
  inp_b_poly_52 * inp_b_poly_52 = 
    L0x2001a188 * (x**0) + L0x2001a18c * (x**1) + 
    L0x2001a190 * (x**2) + L0x2001a194 * (x**3)
  && true;

ghost inp_b_poly_53@bit :
  inp_b_poly_53 * inp_b_poly_53 = 
    L0x2001a198 * (x**0) + L0x2001a19c * (x**1) + 
    L0x2001a1a0 * (x**2) + L0x2001a1a4 * (x**3)
  && true;

ghost inp_b_poly_54@bit :
  inp_b_poly_54 * inp_b_poly_54 = 
    L0x2001a1a8 * (x**0) + L0x2001a1ac * (x**1) + 
    L0x2001a1b0 * (x**2) + L0x2001a1b4 * (x**3)
  && true;

ghost inp_b_poly_55@bit :
  inp_b_poly_55 * inp_b_poly_55 = 
    L0x2001a1b8 * (x**0) + L0x2001a1bc * (x**1) + 
    L0x2001a1c0 * (x**2) + L0x2001a1c4 * (x**3)
  && true;

ghost inp_b_poly_56@bit :
  inp_b_poly_56 * inp_b_poly_56 = 
    L0x2001a1c8 * (x**0) + L0x2001a1cc * (x**1) + 
    L0x2001a1d0 * (x**2) + L0x2001a1d4 * (x**3)
  && true;

ghost inp_b_poly_57@bit :
  inp_b_poly_57 * inp_b_poly_57 = 
    L0x2001a1d8 * (x**0) + L0x2001a1dc * (x**1) + 
    L0x2001a1e0 * (x**2) + L0x2001a1e4 * (x**3)
  && true;

ghost inp_b_poly_58@bit :
  inp_b_poly_58 * inp_b_poly_58 = 
    L0x2001a1e8 * (x**0) + L0x2001a1ec * (x**1) + 
    L0x2001a1f0 * (x**2) + L0x2001a1f4 * (x**3)
  && true;

ghost inp_b_poly_59@bit :
  inp_b_poly_59 * inp_b_poly_59 = 
    L0x2001a1f8 * (x**0) + L0x2001a1fc * (x**1) + 
    L0x2001a200 * (x**2) + L0x2001a204 * (x**3)
  && true;

ghost inp_b_poly_60@bit :
  inp_b_poly_60 * inp_b_poly_60 = 
    L0x2001a208 * (x**0) + L0x2001a20c * (x**1) + 
    L0x2001a210 * (x**2) + L0x2001a214 * (x**3)
  && true;

ghost inp_b_poly_61@bit :
  inp_b_poly_61 * inp_b_poly_61 = 
    L0x2001a218 * (x**0) + L0x2001a21c * (x**1) + 
    L0x2001a220 * (x**2) + L0x2001a224 * (x**3)
  && true;

ghost inp_b_poly_62@bit :
  inp_b_poly_62 * inp_b_poly_62 = 
    L0x2001a228 * (x**0) + L0x2001a22c * (x**1) + 
    L0x2001a230 * (x**2) + L0x2001a234 * (x**3)
  && true;

ghost inp_b_poly_63@bit :
  inp_b_poly_63 * inp_b_poly_63 = 
    L0x2001a238 * (x**0) + L0x2001a23c * (x**1) + 
    L0x2001a240 * (x**2) + L0x2001a244 * (x**3)
  && true;


(* #! -> SP = 0x200194f0 *)
#! 0x200194f0 = 0x200194f0;
(* vldr	s12, [sp, #40]	; 0x28                      #! EA = L0x200194f0; PC = 0x8006020 *)
mov s12 L0x200194f0;
(* vldr	s13, [sp, #44]	; 0x2c                      #! EA = L0x200194f4; PC = 0x8006024 *)
mov s13 L0x200194f4;
(* vmov	s0, s1, r0, r1                             #! PC = 0x8006028 *)
mov s0 r0;
mov s1 r1;
(* add.w	r12, r0, #1024	; 0x400                    #! PC = 0x800602c *)
adds discard r12 r0 1024@uint32;
(* vmov	s2, r12                                    #! PC = 0x8006030 *)
mov s2 r12;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019e4c; Value = 0xffebd04d; PC = 0x8006038 *)
mov r5 L0x20019e4c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019e50; Value = 0x00365dd1; PC = 0x800603c *)
mov r6 L0x20019e50;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019e54; Value = 0x008f3340; PC = 0x8006040 *)
mov r7 L0x20019e54;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019e48; Value = 0x00266ba1; PC = 0x8006044 *)
mov r4 L0x20019e48;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019a4c; Value = 0xffced9c1; PC = 0x8006048 *)
mov r9 L0x20019a4c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019a50; Value = 0x002f47fb; PC = 0x800604c *)
mov r10 L0x20019a50;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019a54; Value = 0x00b94274; PC = 0x8006050 *)
mov r11 L0x20019a54;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019a48; Value = 0x008bf131; PC = 0x8006054 *)
mov r8 L0x20019a48;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f720; Value = 0x00029b3c; PC = 0x8006060 *)
mov r1 L0x800f720;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a254; PC = 0x8006084 *)
mov L0x2001a254 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a250; PC = 0x80060b0 *)
mov L0x2001a250 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a24c; PC = 0x80060dc *)
mov L0x2001a24c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a248; PC = 0x8006108 *)
mov L0x2001a248 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019e5c; Value = 0x0015a839; PC = 0x8006114 *)
mov r5 L0x20019e5c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019e60; Value = 0xffc3b8fb; PC = 0x8006118 *)
mov r6 L0x20019e60;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019e64; Value = 0x010830b4; PC = 0x800611c *)
mov r7 L0x20019e64;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019e58; Value = 0xffd235f1; PC = 0x8006120 *)
mov r4 L0x20019e58;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019a5c; Value = 0xfe8a5813; PC = 0x8006124 *)
mov r9 L0x20019a5c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019a60; Value = 0x0170b0f9; PC = 0x8006128 *)
mov r10 L0x20019a60;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019a64; Value = 0xff7a7b40; PC = 0x800612c *)
mov r11 L0x20019a64;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019a58; Value = 0x00187ee9; PC = 0x8006130 *)
mov r8 L0x20019a58;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a264; PC = 0x8006158 *)
mov L0x2001a264 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a260; PC = 0x8006184 *)
mov L0x2001a260 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a25c; PC = 0x80061b0 *)
mov L0x2001a25c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a258; PC = 0x80061dc *)
mov L0x2001a258 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019e6c; Value = 0xfeed702d; PC = 0x8006038 *)
mov r5 L0x20019e6c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019e70; Value = 0x014ab858; PC = 0x800603c *)
mov r6 L0x20019e70;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019e74; Value = 0x0103bf3c; PC = 0x8006040 *)
mov r7 L0x20019e74;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019e68; Value = 0x006a0c3c; PC = 0x8006044 *)
mov r4 L0x20019e68;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019a6c; Value = 0xff44bb5b; PC = 0x8006048 *)
mov r9 L0x20019a6c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019a70; Value = 0x00079dd2; PC = 0x800604c *)
mov r10 L0x20019a70;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019a74; Value = 0xffa25079; PC = 0x8006050 *)
mov r11 L0x20019a74;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019a68; Value = 0xff678268; PC = 0x8006054 *)
mov r8 L0x20019a68;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f724; Value = 0xff661cdb; PC = 0x8006060 *)
mov r1 L0x800f724;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a274; PC = 0x8006084 *)
mov L0x2001a274 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a270; PC = 0x80060b0 *)
mov L0x2001a270 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a26c; PC = 0x80060dc *)
mov L0x2001a26c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a268; PC = 0x8006108 *)
mov L0x2001a268 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019e7c; Value = 0xfefed5e5; PC = 0x8006114 *)
mov r5 L0x20019e7c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019e80; Value = 0x0071d770; PC = 0x8006118 *)
mov r6 L0x20019e80;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019e84; Value = 0x0052d110; PC = 0x800611c *)
mov r7 L0x20019e84;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019e78; Value = 0x0120acba; PC = 0x8006120 *)
mov r4 L0x20019e78;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019a7c; Value = 0xffdbc5a9; PC = 0x8006124 *)
mov r9 L0x20019a7c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019a80; Value = 0xff3dc716; PC = 0x8006128 *)
mov r10 L0x20019a80;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019a84; Value = 0xfe23fdd3; PC = 0x800612c *)
mov r11 L0x20019a84;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019a78; Value = 0x006dda52; PC = 0x8006130 *)
mov r8 L0x20019a78;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a284; PC = 0x8006158 *)
mov L0x2001a284 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a280; PC = 0x8006184 *)
mov L0x2001a280 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a27c; PC = 0x80061b0 *)
mov L0x2001a27c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a278; PC = 0x80061dc *)
mov L0x2001a278 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019e8c; Value = 0x00ce7ec5; PC = 0x8006038 *)
mov r5 L0x20019e8c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019e90; Value = 0xff85050d; PC = 0x800603c *)
mov r6 L0x20019e90;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019e94; Value = 0x00aa046b; PC = 0x8006040 *)
mov r7 L0x20019e94;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019e88; Value = 0x00a785f4; PC = 0x8006044 *)
mov r4 L0x20019e88;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019a8c; Value = 0x0072cd67; PC = 0x8006048 *)
mov r9 L0x20019a8c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019a90; Value = 0x002efced; PC = 0x800604c *)
mov r10 L0x20019a90;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019a94; Value = 0xffdb1172; PC = 0x8006050 *)
mov r11 L0x20019a94;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019a88; Value = 0x00a141fe; PC = 0x8006054 *)
mov r8 L0x20019a88;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f728; Value = 0x00ac2baa; PC = 0x8006060 *)
mov r1 L0x800f728;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a294; PC = 0x8006084 *)
mov L0x2001a294 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a290; PC = 0x80060b0 *)
mov L0x2001a290 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a28c; PC = 0x80060dc *)
mov L0x2001a28c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a288; PC = 0x8006108 *)
mov L0x2001a288 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019e9c; Value = 0x0025bbdb; PC = 0x8006114 *)
mov r5 L0x20019e9c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019ea0; Value = 0xffddc057; PC = 0x8006118 *)
mov r6 L0x20019ea0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019ea4; Value = 0x00651f15; PC = 0x800611c *)
mov r7 L0x20019ea4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019e98; Value = 0x00c283fe; PC = 0x8006120 *)
mov r4 L0x20019e98;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019a9c; Value = 0xff71eb85; PC = 0x8006124 *)
mov r9 L0x20019a9c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019aa0; Value = 0x008f3c93; PC = 0x8006128 *)
mov r10 L0x20019aa0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019aa4; Value = 0xffdc2606; PC = 0x800612c *)
mov r11 L0x20019aa4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019a98; Value = 0xffcf5458; PC = 0x8006130 *)
mov r8 L0x20019a98;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a2a4; PC = 0x8006158 *)
mov L0x2001a2a4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a2a0; PC = 0x8006184 *)
mov L0x2001a2a0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a29c; PC = 0x80061b0 *)
mov L0x2001a29c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a298; PC = 0x80061dc *)
mov L0x2001a298 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019eac; Value = 0x006adf92; PC = 0x8006038 *)
mov r5 L0x20019eac;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019eb0; Value = 0xffc46601; PC = 0x800603c *)
mov r6 L0x20019eb0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019eb4; Value = 0x006cffab; PC = 0x8006040 *)
mov r7 L0x20019eb4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019ea8; Value = 0x0127442c; PC = 0x8006044 *)
mov r4 L0x20019ea8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019aac; Value = 0xfe97a4a5; PC = 0x8006048 *)
mov r9 L0x20019aac;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019ab0; Value = 0x01c720ab; PC = 0x800604c *)
mov r10 L0x20019ab0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019ab4; Value = 0xffdf9c7a; PC = 0x8006050 *)
mov r11 L0x20019ab4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019aa8; Value = 0xff8b5f03; PC = 0x8006054 *)
mov r8 L0x20019aa8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f72c; Value = 0x0099942b; PC = 0x8006060 *)
mov r1 L0x800f72c;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a2b4; PC = 0x8006084 *)
mov L0x2001a2b4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a2b0; PC = 0x80060b0 *)
mov L0x2001a2b0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a2ac; PC = 0x80060dc *)
mov L0x2001a2ac lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a2a8; PC = 0x8006108 *)
mov L0x2001a2a8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019ebc; Value = 0x017acc36; PC = 0x8006114 *)
mov r5 L0x20019ebc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019ec0; Value = 0xfe74598f; PC = 0x8006118 *)
mov r6 L0x20019ec0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019ec4; Value = 0x006ca8f5; PC = 0x800611c *)
mov r7 L0x20019ec4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019eb8; Value = 0x010815ca; PC = 0x8006120 *)
mov r4 L0x20019eb8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019abc; Value = 0xffc6f627; PC = 0x8006124 *)
mov r9 L0x20019abc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019ac0; Value = 0x00a0b2f9; PC = 0x8006128 *)
mov r10 L0x20019ac0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019ac4; Value = 0xfee9818e; PC = 0x800612c *)
mov r11 L0x20019ac4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019ab8; Value = 0x00b43873; PC = 0x8006130 *)
mov r8 L0x20019ab8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a2c4; PC = 0x8006158 *)
mov L0x2001a2c4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a2c0; PC = 0x8006184 *)
mov L0x2001a2c0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a2bc; PC = 0x80061b0 *)
mov L0x2001a2bc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a2b8; PC = 0x80061dc *)
mov L0x2001a2b8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019ecc; Value = 0xffd75ff8; PC = 0x8006038 *)
mov r5 L0x20019ecc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019ed0; Value = 0xfeeee240; PC = 0x800603c *)
mov r6 L0x20019ed0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019ed4; Value = 0x00821dbd; PC = 0x8006040 *)
mov r7 L0x20019ed4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019ec8; Value = 0x0115e32d; PC = 0x8006044 *)
mov r4 L0x20019ec8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019acc; Value = 0x00cfa228; PC = 0x8006048 *)
mov r9 L0x20019acc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019ad0; Value = 0xfe7570c8; PC = 0x800604c *)
mov r10 L0x20019ad0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019ad4; Value = 0xffa75a1e; PC = 0x8006050 *)
mov r11 L0x20019ad4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019ac8; Value = 0xffd1496b; PC = 0x8006054 *)
mov r8 L0x20019ac8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f730; Value = 0xff5747a4; PC = 0x8006060 *)
mov r1 L0x800f730;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a2d4; PC = 0x8006084 *)
mov L0x2001a2d4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a2d0; PC = 0x80060b0 *)
mov L0x2001a2d0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a2cc; PC = 0x80060dc *)
mov L0x2001a2cc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a2c8; PC = 0x8006108 *)
mov L0x2001a2c8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019edc; Value = 0xffd141ce; PC = 0x8006114 *)
mov r5 L0x20019edc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019ee0; Value = 0x002cc1ac; PC = 0x8006118 *)
mov r6 L0x20019ee0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019ee4; Value = 0xffd630d9; PC = 0x800611c *)
mov r7 L0x20019ee4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019ed8; Value = 0x011dfcc5; PC = 0x8006120 *)
mov r4 L0x20019ed8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019adc; Value = 0x00f9e2be; PC = 0x8006124 *)
mov r9 L0x20019adc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019ae0; Value = 0xffabe60e; PC = 0x8006128 *)
mov r10 L0x20019ae0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019ae4; Value = 0x0103c15a; PC = 0x800612c *)
mov r11 L0x20019ae4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019ad8; Value = 0xff9db2fd; PC = 0x8006130 *)
mov r8 L0x20019ad8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a2e4; PC = 0x8006158 *)
mov L0x2001a2e4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a2e0; PC = 0x8006184 *)
mov L0x2001a2e0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a2dc; PC = 0x80061b0 *)
mov L0x2001a2dc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a2d8; PC = 0x80061dc *)
mov L0x2001a2d8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019eec; Value = 0xff7b42eb; PC = 0x8006038 *)
mov r5 L0x20019eec;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019ef0; Value = 0xfdd2bcfb; PC = 0x800603c *)
mov r6 L0x20019ef0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019ef4; Value = 0xff0ccd08; PC = 0x8006040 *)
mov r7 L0x20019ef4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019ee8; Value = 0x0085468a; PC = 0x8006044 *)
mov r4 L0x20019ee8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019aec; Value = 0xff71b9a5; PC = 0x8006048 *)
mov r9 L0x20019aec;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019af0; Value = 0xfea575fb; PC = 0x800604c *)
mov r10 L0x20019af0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019af4; Value = 0x017bc7ae; PC = 0x8006050 *)
mov r11 L0x20019af4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019ae8; Value = 0x0063d021; PC = 0x8006054 *)
mov r8 L0x20019ae8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f734; Value = 0x000cb0a8; PC = 0x8006060 *)
mov r1 L0x800f734;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a2f4; PC = 0x8006084 *)
mov L0x2001a2f4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a2f0; PC = 0x80060b0 *)
mov L0x2001a2f0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a2ec; PC = 0x80060dc *)
mov L0x2001a2ec lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a2e8; PC = 0x8006108 *)
mov L0x2001a2e8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019efc; Value = 0x0071c907; PC = 0x8006114 *)
mov r5 L0x20019efc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019f00; Value = 0xfec70889; PC = 0x8006118 *)
mov r6 L0x20019f00;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019f04; Value = 0xff93268a; PC = 0x800611c *)
mov r7 L0x20019f04;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019ef8; Value = 0xff48d3a4; PC = 0x8006120 *)
mov r4 L0x20019ef8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019afc; Value = 0x009a4181; PC = 0x8006124 *)
mov r9 L0x20019afc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019b00; Value = 0xfd9e195b; PC = 0x8006128 *)
mov r10 L0x20019b00;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019b04; Value = 0x00c151be; PC = 0x800612c *)
mov r11 L0x20019b04;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019af8; Value = 0xfedf93c7; PC = 0x8006130 *)
mov r8 L0x20019af8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a304; PC = 0x8006158 *)
mov L0x2001a304 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a300; PC = 0x8006184 *)
mov L0x2001a300 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a2fc; PC = 0x80061b0 *)
mov L0x2001a2fc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a2f8; PC = 0x80061dc *)
mov L0x2001a2f8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019f0c; Value = 0xfef7849f; PC = 0x8006038 *)
mov r5 L0x20019f0c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019f10; Value = 0xfe49b2a5; PC = 0x800603c *)
mov r6 L0x20019f10;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019f14; Value = 0xfe98ad1e; PC = 0x8006040 *)
mov r7 L0x20019f14;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019f08; Value = 0x00bd32d0; PC = 0x8006044 *)
mov r4 L0x20019f08;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019b0c; Value = 0x01100841; PC = 0x8006048 *)
mov r9 L0x20019b0c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019b10; Value = 0xff5cd7d1; PC = 0x800604c *)
mov r10 L0x20019b10;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019b14; Value = 0x0175669a; PC = 0x8006050 *)
mov r11 L0x20019b14;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019b08; Value = 0x01452480; PC = 0x8006054 *)
mov r8 L0x20019b08;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f738; Value = 0x009121f2; PC = 0x8006060 *)
mov r1 L0x800f738;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a314; PC = 0x8006084 *)
mov L0x2001a314 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a310; PC = 0x80060b0 *)
mov L0x2001a310 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a30c; PC = 0x80060dc *)
mov L0x2001a30c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a308; PC = 0x8006108 *)
mov L0x2001a308 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019f1c; Value = 0x0044225f; PC = 0x8006114 *)
mov r5 L0x20019f1c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019f20; Value = 0xff92112f; PC = 0x8006118 *)
mov r6 L0x20019f20;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019f24; Value = 0xff6bed1c; PC = 0x800611c *)
mov r7 L0x20019f24;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019f18; Value = 0x01dba05e; PC = 0x8006120 *)
mov r4 L0x20019f18;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019b1c; Value = 0x00f68c1f; PC = 0x8006124 *)
mov r9 L0x20019b1c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019b20; Value = 0x00219be9; PC = 0x8006128 *)
mov r10 L0x20019b20;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019b24; Value = 0x021a3cca; PC = 0x800612c *)
mov r11 L0x20019b24;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019b18; Value = 0x0072328c; PC = 0x8006130 *)
mov r8 L0x20019b18;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a324; PC = 0x8006158 *)
mov L0x2001a324 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a320; PC = 0x8006184 *)
mov L0x2001a320 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a31c; PC = 0x80061b0 *)
mov L0x2001a31c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a318; PC = 0x80061dc *)
mov L0x2001a318 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019f2c; Value = 0x014b6759; PC = 0x8006038 *)
mov r5 L0x20019f2c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019f30; Value = 0xffd664e5; PC = 0x800603c *)
mov r6 L0x20019f30;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019f34; Value = 0x000def8f; PC = 0x8006040 *)
mov r7 L0x20019f34;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019f28; Value = 0x0264d72b; PC = 0x8006044 *)
mov r4 L0x20019f28;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019b2c; Value = 0xff8a56d9; PC = 0x8006048 *)
mov r9 L0x20019b2c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019b30; Value = 0x001de059; PC = 0x800604c *)
mov r10 L0x20019b30;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019b34; Value = 0x00def579; PC = 0x8006050 *)
mov r11 L0x20019b34;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019b28; Value = 0x0118b6bc; PC = 0x8006054 *)
mov r8 L0x20019b28;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f73c; Value = 0x001fde0f; PC = 0x8006060 *)
mov r1 L0x800f73c;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a334; PC = 0x8006084 *)
mov L0x2001a334 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a330; PC = 0x80060b0 *)
mov L0x2001a330 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a32c; PC = 0x80060dc *)
mov L0x2001a32c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a328; PC = 0x8006108 *)
mov L0x2001a328 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019f3c; Value = 0x00435181; PC = 0x8006114 *)
mov r5 L0x20019f3c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019f40; Value = 0x00f11dff; PC = 0x8006118 *)
mov r6 L0x20019f40;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019f44; Value = 0xffa5471f; PC = 0x800611c *)
mov r7 L0x20019f44;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019f38; Value = 0x012539e7; PC = 0x8006120 *)
mov r4 L0x20019f38;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019b3c; Value = 0x00dc8deb; PC = 0x8006124 *)
mov r9 L0x20019b3c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019b40; Value = 0x00a9f6c1; PC = 0x8006128 *)
mov r10 L0x20019b40;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019b44; Value = 0xffc4371f; PC = 0x800612c *)
mov r11 L0x20019b44;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019b38; Value = 0xffc77118; PC = 0x8006130 *)
mov r8 L0x20019b38;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a344; PC = 0x8006158 *)
mov L0x2001a344 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a340; PC = 0x8006184 *)
mov L0x2001a340 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a33c; PC = 0x80061b0 *)
mov L0x2001a33c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a338; PC = 0x80061dc *)
mov L0x2001a338 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019f4c; Value = 0x014760c8; PC = 0x8006038 *)
mov r5 L0x20019f4c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019f50; Value = 0xff652901; PC = 0x800603c *)
mov r6 L0x20019f50;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019f54; Value = 0xfd50e2ac; PC = 0x8006040 *)
mov r7 L0x20019f54;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019f48; Value = 0xfecdd901; PC = 0x8006044 *)
mov r4 L0x20019f48;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019b4c; Value = 0xfe967a4a; PC = 0x8006048 *)
mov r9 L0x20019b4c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019b50; Value = 0x01735eb5; PC = 0x800604c *)
mov r10 L0x20019b50;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019b54; Value = 0xffde0302; PC = 0x8006050 *)
mov r11 L0x20019b54;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019b48; Value = 0xfe3553a5; PC = 0x8006054 *)
mov r8 L0x20019b48;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f740; Value = 0xff832058; PC = 0x8006060 *)
mov r1 L0x800f740;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a354; PC = 0x8006084 *)
mov L0x2001a354 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a350; PC = 0x80060b0 *)
mov L0x2001a350 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a34c; PC = 0x80060dc *)
mov L0x2001a34c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a348; PC = 0x8006108 *)
mov L0x2001a348 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019f5c; Value = 0x01a48bd0; PC = 0x8006114 *)
mov r5 L0x20019f5c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019f60; Value = 0xff164a6b; PC = 0x8006118 *)
mov r6 L0x20019f60;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019f64; Value = 0xfdd22062; PC = 0x800611c *)
mov r7 L0x20019f64;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019f58; Value = 0xfdd86c9f; PC = 0x8006120 *)
mov r4 L0x20019f58;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019b5c; Value = 0xfff52f38; PC = 0x8006124 *)
mov r9 L0x20019b5c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019b60; Value = 0x01db29b9; PC = 0x8006128 *)
mov r10 L0x20019b60;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019b64; Value = 0x0153f976; PC = 0x800612c *)
mov r11 L0x20019b64;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019b58; Value = 0xfe0d9061; PC = 0x8006130 *)
mov r8 L0x20019b58;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a364; PC = 0x8006158 *)
mov L0x2001a364 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a360; PC = 0x8006184 *)
mov L0x2001a360 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a35c; PC = 0x80061b0 *)
mov L0x2001a35c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a358; PC = 0x80061dc *)
mov L0x2001a358 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019f6c; Value = 0x00b4e902; PC = 0x8006038 *)
mov r5 L0x20019f6c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019f70; Value = 0xff93c667; PC = 0x800603c *)
mov r6 L0x20019f70;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019f74; Value = 0xfce36203; PC = 0x8006040 *)
mov r7 L0x20019f74;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019f68; Value = 0xff18f9ae; PC = 0x8006044 *)
mov r4 L0x20019f68;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019b6c; Value = 0x00810edd; PC = 0x8006048 *)
mov r9 L0x20019b6c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019b70; Value = 0x011ec6d4; PC = 0x800604c *)
mov r10 L0x20019b70;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019b74; Value = 0xffb3f628; PC = 0x8006050 *)
mov r11 L0x20019b74;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019b68; Value = 0xffc0f0d7; PC = 0x8006054 *)
mov r8 L0x20019b68;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f744; Value = 0xff886b5b; PC = 0x8006060 *)
mov r1 L0x800f744;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a374; PC = 0x8006084 *)
mov L0x2001a374 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a370; PC = 0x80060b0 *)
mov L0x2001a370 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a36c; PC = 0x80060dc *)
mov L0x2001a36c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a368; PC = 0x8006108 *)
mov L0x2001a368 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019f7c; Value = 0xff5d4176; PC = 0x8006114 *)
mov r5 L0x20019f7c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019f80; Value = 0x00cc70cd; PC = 0x8006118 *)
mov r6 L0x20019f80;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019f84; Value = 0xfe41a317; PC = 0x800611c *)
mov r7 L0x20019f84;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019f78; Value = 0xfface9d2; PC = 0x8006120 *)
mov r4 L0x20019f78;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019b7c; Value = 0xfff90f3d; PC = 0x8006124 *)
mov r9 L0x20019b7c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019b80; Value = 0x0175108e; PC = 0x8006128 *)
mov r10 L0x20019b80;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019b84; Value = 0xff340db4; PC = 0x800612c *)
mov r11 L0x20019b84;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019b78; Value = 0xfef56213; PC = 0x8006130 *)
mov r8 L0x20019b78;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a384; PC = 0x8006158 *)
mov L0x2001a384 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a380; PC = 0x8006184 *)
mov L0x2001a380 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a37c; PC = 0x80061b0 *)
mov L0x2001a37c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a378; PC = 0x80061dc *)
mov L0x2001a378 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019f8c; Value = 0x00aab0be; PC = 0x8006038 *)
mov r5 L0x20019f8c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019f90; Value = 0xff3bcd46; PC = 0x800603c *)
mov r6 L0x20019f90;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019f94; Value = 0xff7284b4; PC = 0x8006040 *)
mov r7 L0x20019f94;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019f88; Value = 0xff14c961; PC = 0x8006044 *)
mov r4 L0x20019f88;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019b8c; Value = 0x008a1e38; PC = 0x8006048 *)
mov r9 L0x20019b8c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019b90; Value = 0x000d4796; PC = 0x800604c *)
mov r10 L0x20019b90;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019b94; Value = 0xffdc74d8; PC = 0x8006050 *)
mov r11 L0x20019b94;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019b88; Value = 0xff7c421b; PC = 0x8006054 *)
mov r8 L0x20019b88;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f748; Value = 0x004e73fc; PC = 0x8006060 *)
mov r1 L0x800f748;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a394; PC = 0x8006084 *)
mov L0x2001a394 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a390; PC = 0x80060b0 *)
mov L0x2001a390 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a38c; PC = 0x80060dc *)
mov L0x2001a38c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a388; PC = 0x8006108 *)
mov L0x2001a388 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019f9c; Value = 0x01b3d3ac; PC = 0x8006114 *)
mov r5 L0x20019f9c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019fa0; Value = 0xfdc9aeae; PC = 0x8006118 *)
mov r6 L0x20019fa0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019fa4; Value = 0xfead35a0; PC = 0x800611c *)
mov r7 L0x20019fa4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019f98; Value = 0x006eeb2d; PC = 0x8006120 *)
mov r4 L0x20019f98;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019b9c; Value = 0x01faaf8c; PC = 0x8006124 *)
mov r9 L0x20019b9c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019ba0; Value = 0x00f29a46; PC = 0x8006128 *)
mov r10 L0x20019ba0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019ba4; Value = 0x007e34d8; PC = 0x800612c *)
mov r11 L0x20019ba4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019b98; Value = 0xfe628a6f; PC = 0x8006130 *)
mov r8 L0x20019b98;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a3a4; PC = 0x8006158 *)
mov L0x2001a3a4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a3a0; PC = 0x8006184 *)
mov L0x2001a3a0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a39c; PC = 0x80061b0 *)
mov L0x2001a39c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a398; PC = 0x80061dc *)
mov L0x2001a398 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019fac; Value = 0x016625f1; PC = 0x8006038 *)
mov r5 L0x20019fac;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019fb0; Value = 0xfe716a3a; PC = 0x800603c *)
mov r6 L0x20019fb0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019fb4; Value = 0xfefadc61; PC = 0x8006040 *)
mov r7 L0x20019fb4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019fa8; Value = 0xfea99764; PC = 0x8006044 *)
mov r4 L0x20019fa8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019bac; Value = 0x00287a20; PC = 0x8006048 *)
mov r9 L0x20019bac;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019bb0; Value = 0x01140916; PC = 0x800604c *)
mov r10 L0x20019bb0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019bb4; Value = 0xff0f3962; PC = 0x8006050 *)
mov r11 L0x20019bb4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019ba8; Value = 0x0042798f; PC = 0x8006054 *)
mov r8 L0x20019ba8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f74c; Value = 0xff425a47; PC = 0x8006060 *)
mov r1 L0x800f74c;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a3b4; PC = 0x8006084 *)
mov L0x2001a3b4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a3b0; PC = 0x80060b0 *)
mov L0x2001a3b0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a3ac; PC = 0x80060dc *)
mov L0x2001a3ac lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a3a8; PC = 0x8006108 *)
mov L0x2001a3a8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019fbc; Value = 0x01e04dd5; PC = 0x8006114 *)
mov r5 L0x20019fbc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019fc0; Value = 0xfef81c12; PC = 0x8006118 *)
mov r6 L0x20019fc0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019fc4; Value = 0xff08d123; PC = 0x800611c *)
mov r7 L0x20019fc4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019fb8; Value = 0xfe05e4c6; PC = 0x8006120 *)
mov r4 L0x20019fb8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019bbc; Value = 0x005df528; PC = 0x8006124 *)
mov r9 L0x20019bbc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019bc0; Value = 0xffbdeaae; PC = 0x8006128 *)
mov r10 L0x20019bc0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019bc4; Value = 0xfe8d268a; PC = 0x800612c *)
mov r11 L0x20019bc4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019bb8; Value = 0x0006edf7; PC = 0x8006130 *)
mov r8 L0x20019bb8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a3c4; PC = 0x8006158 *)
mov L0x2001a3c4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a3c0; PC = 0x8006184 *)
mov L0x2001a3c0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a3bc; PC = 0x80061b0 *)
mov L0x2001a3bc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a3b8; PC = 0x80061dc *)
mov L0x2001a3b8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019fcc; Value = 0x01bf3074; PC = 0x8006038 *)
mov r5 L0x20019fcc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019fd0; Value = 0xfe8b58a1; PC = 0x800603c *)
mov r6 L0x20019fd0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019fd4; Value = 0xff01a6a8; PC = 0x8006040 *)
mov r7 L0x20019fd4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019fc8; Value = 0x00c3d1dc; PC = 0x8006044 *)
mov r4 L0x20019fc8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019bcc; Value = 0xff14764b; PC = 0x8006048 *)
mov r9 L0x20019bcc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019bd0; Value = 0x01de79ba; PC = 0x800604c *)
mov r10 L0x20019bd0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019bd4; Value = 0xff2123f9; PC = 0x8006050 *)
mov r11 L0x20019bd4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019bc8; Value = 0xffa68524; PC = 0x8006054 *)
mov r8 L0x20019bc8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f750; Value = 0xff447b44; PC = 0x8006060 *)
mov r1 L0x800f750;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a3d4; PC = 0x8006084 *)
mov L0x2001a3d4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a3d0; PC = 0x80060b0 *)
mov L0x2001a3d0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a3cc; PC = 0x80060dc *)
mov L0x2001a3cc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a3c8; PC = 0x8006108 *)
mov L0x2001a3c8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019fdc; Value = 0x0045db5a; PC = 0x8006114 *)
mov r5 L0x20019fdc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019fe0; Value = 0xff38f3a9; PC = 0x8006118 *)
mov r6 L0x20019fe0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019fe4; Value = 0xfe1787ee; PC = 0x800611c *)
mov r7 L0x20019fe4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019fd8; Value = 0xffcdd870; PC = 0x8006120 *)
mov r4 L0x20019fd8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019bdc; Value = 0xff947499; PC = 0x8006124 *)
mov r9 L0x20019bdc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019be0; Value = 0x01ef1764; PC = 0x8006128 *)
mov r10 L0x20019be0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019be4; Value = 0xfdd24793; PC = 0x800612c *)
mov r11 L0x20019be4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019bd8; Value = 0xfec7bd5e; PC = 0x8006130 *)
mov r8 L0x20019bd8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a3e4; PC = 0x8006158 *)
mov L0x2001a3e4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a3e0; PC = 0x8006184 *)
mov L0x2001a3e0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a3dc; PC = 0x80061b0 *)
mov L0x2001a3dc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a3d8; PC = 0x80061dc *)
mov L0x2001a3d8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019fec; Value = 0x025dfae0; PC = 0x8006038 *)
mov r5 L0x20019fec;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x20019ff0; Value = 0xfe29ab1d; PC = 0x800603c *)
mov r6 L0x20019ff0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x20019ff4; Value = 0xffd27262; PC = 0x8006040 *)
mov r7 L0x20019ff4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019fe8; Value = 0x0065211c; PC = 0x8006044 *)
mov r4 L0x20019fe8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019bec; Value = 0xff2b0fec; PC = 0x8006048 *)
mov r9 L0x20019bec;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019bf0; Value = 0x0129cd4b; PC = 0x800604c *)
mov r10 L0x20019bf0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019bf4; Value = 0xfea9a35f; PC = 0x8006050 *)
mov r11 L0x20019bf4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019be8; Value = 0xfd417c5f; PC = 0x8006054 *)
mov r8 L0x20019be8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f754; Value = 0xff3ea0dd; PC = 0x8006060 *)
mov r1 L0x800f754;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a3f4; PC = 0x8006084 *)
mov L0x2001a3f4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a3f0; PC = 0x80060b0 *)
mov L0x2001a3f0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a3ec; PC = 0x80060dc *)
mov L0x2001a3ec lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a3e8; PC = 0x8006108 *)
mov L0x2001a3e8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x20019ffc; Value = 0x0186983e; PC = 0x8006114 *)
mov r5 L0x20019ffc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a000; Value = 0xfde85c91; PC = 0x8006118 *)
mov r6 L0x2001a000;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a004; Value = 0xffbe0d88; PC = 0x800611c *)
mov r7 L0x2001a004;
(* ldr.w	r4, [r12], #16                            #! EA = L0x20019ff8; Value = 0xff9a4554; PC = 0x8006120 *)
mov r4 L0x20019ff8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019bfc; Value = 0x001a9e84; PC = 0x8006124 *)
mov r9 L0x20019bfc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019c00; Value = 0xfff9e2ab; PC = 0x8006128 *)
mov r10 L0x20019c00;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019c04; Value = 0xfe951b55; PC = 0x800612c *)
mov r11 L0x20019c04;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019bf8; Value = 0xfe42fab3; PC = 0x8006130 *)
mov r8 L0x20019bf8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a404; PC = 0x8006158 *)
mov L0x2001a404 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a400; PC = 0x8006184 *)
mov L0x2001a400 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a3fc; PC = 0x80061b0 *)
mov L0x2001a3fc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a3f8; PC = 0x80061dc *)
mov L0x2001a3f8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a00c; Value = 0x00ee7e07; PC = 0x8006038 *)
mov r5 L0x2001a00c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a010; Value = 0xfe9136b6; PC = 0x800603c *)
mov r6 L0x2001a010;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a014; Value = 0x005d9ace; PC = 0x8006040 *)
mov r7 L0x2001a014;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a008; Value = 0x0133e581; PC = 0x8006044 *)
mov r4 L0x2001a008;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019c0c; Value = 0xff6ccd62; PC = 0x8006048 *)
mov r9 L0x20019c0c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019c10; Value = 0xfed30ce7; PC = 0x800604c *)
mov r10 L0x20019c10;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019c14; Value = 0xfdd02f66; PC = 0x8006050 *)
mov r11 L0x20019c14;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019c08; Value = 0x00399c16; PC = 0x8006054 *)
mov r8 L0x20019c08;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f758; Value = 0xff552685; PC = 0x8006060 *)
mov r1 L0x800f758;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a414; PC = 0x8006084 *)
mov L0x2001a414 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a410; PC = 0x80060b0 *)
mov L0x2001a410 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a40c; PC = 0x80060dc *)
mov L0x2001a40c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a408; PC = 0x8006108 *)
mov L0x2001a408 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a01c; Value = 0x01b3c13d; PC = 0x8006114 *)
mov r5 L0x2001a01c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a020; Value = 0xfec8b0e6; PC = 0x8006118 *)
mov r6 L0x2001a020;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a024; Value = 0x01de6c84; PC = 0x800611c *)
mov r7 L0x2001a024;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a018; Value = 0x013973d7; PC = 0x8006120 *)
mov r4 L0x2001a018;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019c1c; Value = 0x008e8232; PC = 0x8006124 *)
mov r9 L0x20019c1c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019c20; Value = 0xffe74011; PC = 0x8006128 *)
mov r10 L0x20019c20;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019c24; Value = 0xfef28ae0; PC = 0x800612c *)
mov r11 L0x20019c24;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019c18; Value = 0xff9cd3b6; PC = 0x8006130 *)
mov r8 L0x20019c18;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a424; PC = 0x8006158 *)
mov L0x2001a424 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a420; PC = 0x8006184 *)
mov L0x2001a420 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a41c; PC = 0x80061b0 *)
mov L0x2001a41c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a418; PC = 0x80061dc *)
mov L0x2001a418 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a02c; Value = 0x02a15bed; PC = 0x8006038 *)
mov r5 L0x2001a02c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a030; Value = 0xff1a20e3; PC = 0x800603c *)
mov r6 L0x2001a030;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a034; Value = 0x004433cd; PC = 0x8006040 *)
mov r7 L0x2001a034;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a028; Value = 0x001357f9; PC = 0x8006044 *)
mov r4 L0x2001a028;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019c2c; Value = 0x00b0cea0; PC = 0x8006048 *)
mov r9 L0x20019c2c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019c30; Value = 0x007ca10a; PC = 0x800604c *)
mov r10 L0x20019c30;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019c34; Value = 0xfe6d98a2; PC = 0x8006050 *)
mov r11 L0x20019c34;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019c28; Value = 0x0052e57a; PC = 0x8006054 *)
mov r8 L0x20019c28;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f75c; Value = 0x00644c25; PC = 0x8006060 *)
mov r1 L0x800f75c;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a434; PC = 0x8006084 *)
mov L0x2001a434 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a430; PC = 0x80060b0 *)
mov L0x2001a430 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a42c; PC = 0x80060dc *)
mov L0x2001a42c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a428; PC = 0x8006108 *)
mov L0x2001a428 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a03c; Value = 0x02c178d3; PC = 0x8006114 *)
mov r5 L0x2001a03c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a040; Value = 0x005b0319; PC = 0x8006118 *)
mov r6 L0x2001a040;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a044; Value = 0xff3db471; PC = 0x800611c *)
mov r7 L0x2001a044;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a038; Value = 0xffa7140b; PC = 0x8006120 *)
mov r4 L0x2001a038;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019c3c; Value = 0xff544390; PC = 0x8006124 *)
mov r9 L0x20019c3c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019c40; Value = 0x00cd077a; PC = 0x8006128 *)
mov r10 L0x20019c40;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019c44; Value = 0xfd970aa8; PC = 0x800612c *)
mov r11 L0x20019c44;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019c38; Value = 0xfeeea476; PC = 0x8006130 *)
mov r8 L0x20019c38;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a444; PC = 0x8006158 *)
mov L0x2001a444 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a440; PC = 0x8006184 *)
mov L0x2001a440 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a43c; PC = 0x80061b0 *)
mov L0x2001a43c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a438; PC = 0x80061dc *)
mov L0x2001a438 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a04c; Value = 0xfe4b84d9; PC = 0x8006038 *)
mov r5 L0x2001a04c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a050; Value = 0xffcd3c0d; PC = 0x800603c *)
mov r6 L0x2001a050;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a054; Value = 0x009393fa; PC = 0x8006040 *)
mov r7 L0x2001a054;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a048; Value = 0x01390846; PC = 0x8006044 *)
mov r4 L0x2001a048;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019c4c; Value = 0xfff130eb; PC = 0x8006048 *)
mov r9 L0x20019c4c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019c50; Value = 0xfe754776; PC = 0x800604c *)
mov r10 L0x20019c50;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019c54; Value = 0xffcbe85e; PC = 0x8006050 *)
mov r11 L0x20019c54;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019c48; Value = 0xff854da5; PC = 0x8006054 *)
mov r8 L0x20019c48;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f760; Value = 0xff68981c; PC = 0x8006060 *)
mov r1 L0x800f760;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a454; PC = 0x8006084 *)
mov L0x2001a454 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a450; PC = 0x80060b0 *)
mov L0x2001a450 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a44c; PC = 0x80060dc *)
mov L0x2001a44c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a448; PC = 0x8006108 *)
mov L0x2001a448 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a05c; Value = 0xfd6bfc35; PC = 0x8006114 *)
mov r5 L0x2001a05c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a060; Value = 0x011cea89; PC = 0x8006118 *)
mov r6 L0x2001a060;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a064; Value = 0xff28483e; PC = 0x800611c *)
mov r7 L0x2001a064;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a058; Value = 0x02045cf4; PC = 0x8006120 *)
mov r4 L0x2001a058;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019c5c; Value = 0xfe9bb751; PC = 0x8006124 *)
mov r9 L0x20019c5c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019c60; Value = 0xfcef9bd8; PC = 0x8006128 *)
mov r10 L0x20019c60;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019c64; Value = 0xff206318; PC = 0x800612c *)
mov r11 L0x20019c64;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019c58; Value = 0xff187c49; PC = 0x8006130 *)
mov r8 L0x20019c58;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a464; PC = 0x8006158 *)
mov L0x2001a464 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a460; PC = 0x8006184 *)
mov L0x2001a460 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a45c; PC = 0x80061b0 *)
mov L0x2001a45c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a458; PC = 0x80061dc *)
mov L0x2001a458 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a06c; Value = 0xff3bbfff; PC = 0x8006038 *)
mov r5 L0x2001a06c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a070; Value = 0x01bfa22f; PC = 0x800603c *)
mov r6 L0x2001a070;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a074; Value = 0xff07838a; PC = 0x8006040 *)
mov r7 L0x2001a074;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a068; Value = 0x00f67ab8; PC = 0x8006044 *)
mov r4 L0x2001a068;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019c6c; Value = 0x00109d41; PC = 0x8006048 *)
mov r9 L0x20019c6c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019c70; Value = 0xff9affda; PC = 0x800604c *)
mov r10 L0x20019c70;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019c74; Value = 0xfe23bad7; PC = 0x8006050 *)
mov r11 L0x20019c74;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019c68; Value = 0x00832dab; PC = 0x8006054 *)
mov r8 L0x20019c68;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f764; Value = 0x0025b1ef; PC = 0x8006060 *)
mov r1 L0x800f764;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a474; PC = 0x8006084 *)
mov L0x2001a474 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a470; PC = 0x80060b0 *)
mov L0x2001a470 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a46c; PC = 0x80060dc *)
mov L0x2001a46c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a468; PC = 0x8006108 *)
mov L0x2001a468 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a07c; Value = 0xfe066f6f; PC = 0x8006114 *)
mov r5 L0x2001a07c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a080; Value = 0x01e096a7; PC = 0x8006118 *)
mov r6 L0x2001a080;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a084; Value = 0xff2f9376; PC = 0x800611c *)
mov r7 L0x2001a084;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a078; Value = 0x001987f6; PC = 0x8006120 *)
mov r4 L0x2001a078;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019c7c; Value = 0x00d47e9b; PC = 0x8006124 *)
mov r9 L0x20019c7c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019c80; Value = 0xfe88b970; PC = 0x8006128 *)
mov r10 L0x20019c80;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019c84; Value = 0xff1bcf03; PC = 0x800612c *)
mov r11 L0x20019c84;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019c78; Value = 0xffdae163; PC = 0x8006130 *)
mov r8 L0x20019c78;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a484; PC = 0x8006158 *)
mov L0x2001a484 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a480; PC = 0x8006184 *)
mov L0x2001a480 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a47c; PC = 0x80061b0 *)
mov L0x2001a47c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a478; PC = 0x80061dc *)
mov L0x2001a478 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a08c; Value = 0xfec6feeb; PC = 0x8006038 *)
mov r5 L0x2001a08c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a090; Value = 0x00770260; PC = 0x800603c *)
mov r6 L0x2001a090;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a094; Value = 0xff80b1eb; PC = 0x8006040 *)
mov r7 L0x2001a094;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a088; Value = 0xffc3a598; PC = 0x8006044 *)
mov r4 L0x2001a088;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019c8c; Value = 0xff407433; PC = 0x8006048 *)
mov r9 L0x20019c8c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019c90; Value = 0x0005d9ae; PC = 0x800604c *)
mov r10 L0x20019c90;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019c94; Value = 0xff5dcfe6; PC = 0x8006050 *)
mov r11 L0x20019c94;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019c88; Value = 0x0183d332; PC = 0x8006054 *)
mov r8 L0x20019c88;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f768; Value = 0x00a33328; PC = 0x8006060 *)
mov r1 L0x800f768;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a494; PC = 0x8006084 *)
mov L0x2001a494 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a490; PC = 0x80060b0 *)
mov L0x2001a490 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a48c; PC = 0x80060dc *)
mov L0x2001a48c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a488; PC = 0x8006108 *)
mov L0x2001a488 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a09c; Value = 0xfde25935; PC = 0x8006114 *)
mov r5 L0x2001a09c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a0a0; Value = 0x014dd454; PC = 0x8006118 *)
mov r6 L0x2001a0a0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a0a4; Value = 0xfe17b673; PC = 0x800611c *)
mov r7 L0x2001a0a4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a098; Value = 0x00637416; PC = 0x8006120 *)
mov r4 L0x2001a098;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019c9c; Value = 0xffdfc713; PC = 0x8006124 *)
mov r9 L0x20019c9c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019ca0; Value = 0xffe0341a; PC = 0x8006128 *)
mov r10 L0x20019ca0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019ca4; Value = 0x00abf572; PC = 0x800612c *)
mov r11 L0x20019ca4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019c98; Value = 0x002e5030; PC = 0x8006130 *)
mov r8 L0x20019c98;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a4a4; PC = 0x8006158 *)
mov L0x2001a4a4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a4a0; PC = 0x8006184 *)
mov L0x2001a4a0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a49c; PC = 0x80061b0 *)
mov L0x2001a49c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a498; PC = 0x80061dc *)
mov L0x2001a498 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a0ac; Value = 0xfde9f670; PC = 0x8006038 *)
mov r5 L0x2001a0ac;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a0b0; Value = 0x008d881a; PC = 0x800603c *)
mov r6 L0x2001a0b0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a0b4; Value = 0xfdd4b8a2; PC = 0x8006040 *)
mov r7 L0x2001a0b4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a0a8; Value = 0xff437e72; PC = 0x8006044 *)
mov r4 L0x2001a0a8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019cac; Value = 0xff6f2b9e; PC = 0x8006048 *)
mov r9 L0x20019cac;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019cb0; Value = 0xff74e7b9; PC = 0x800604c *)
mov r10 L0x20019cb0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019cb4; Value = 0xfed423b1; PC = 0x8006050 *)
mov r11 L0x20019cb4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019ca8; Value = 0x00168d3d; PC = 0x8006054 *)
mov r8 L0x20019ca8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f76c; Value = 0xffc49988; PC = 0x8006060 *)
mov r1 L0x800f76c;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a4b4; PC = 0x8006084 *)
mov L0x2001a4b4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a4b0; PC = 0x80060b0 *)
mov L0x2001a4b0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a4ac; PC = 0x80060dc *)
mov L0x2001a4ac lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a4a8; PC = 0x8006108 *)
mov L0x2001a4a8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a0bc; Value = 0xfcccabbc; PC = 0x8006114 *)
mov r5 L0x2001a0bc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a0c0; Value = 0x01ec96ce; PC = 0x8006118 *)
mov r6 L0x2001a0c0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a0c4; Value = 0xff4fc5c8; PC = 0x800611c *)
mov r7 L0x2001a0c4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a0b8; Value = 0xffa1ba28; PC = 0x8006120 *)
mov r4 L0x2001a0b8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019cbc; Value = 0xffdd9dd4; PC = 0x8006124 *)
mov r9 L0x20019cbc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019cc0; Value = 0xff53afcf; PC = 0x8006128 *)
mov r10 L0x20019cc0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019cc4; Value = 0xff246d2f; PC = 0x800612c *)
mov r11 L0x20019cc4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019cb8; Value = 0x01899955; PC = 0x8006130 *)
mov r8 L0x20019cb8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a4c4; PC = 0x8006158 *)
mov L0x2001a4c4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a4c0; PC = 0x8006184 *)
mov L0x2001a4c0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a4bc; PC = 0x80061b0 *)
mov L0x2001a4bc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a4b8; PC = 0x80061dc *)
mov L0x2001a4b8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a0cc; Value = 0xffd94468; PC = 0x8006038 *)
mov r5 L0x2001a0cc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a0d0; Value = 0xff82a0fd; PC = 0x800603c *)
mov r6 L0x2001a0d0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a0d4; Value = 0x00c001eb; PC = 0x8006040 *)
mov r7 L0x2001a0d4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a0c8; Value = 0xffb0d2e9; PC = 0x8006044 *)
mov r4 L0x2001a0c8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019ccc; Value = 0xffdebb79; PC = 0x8006048 *)
mov r9 L0x20019ccc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019cd0; Value = 0xfffac10e; PC = 0x800604c *)
mov r10 L0x20019cd0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019cd4; Value = 0x0015a526; PC = 0x8006050 *)
mov r11 L0x20019cd4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019cc8; Value = 0x000930c1; PC = 0x8006054 *)
mov r8 L0x20019cc8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f770; Value = 0x00467d5c; PC = 0x8006060 *)
mov r1 L0x800f770;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a4d4; PC = 0x8006084 *)
mov L0x2001a4d4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a4d0; PC = 0x80060b0 *)
mov L0x2001a4d0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a4cc; PC = 0x80060dc *)
mov L0x2001a4cc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a4c8; PC = 0x8006108 *)
mov L0x2001a4c8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a0dc; Value = 0xffe3f90e; PC = 0x8006114 *)
mov r5 L0x2001a0dc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a0e0; Value = 0xff801ae1; PC = 0x8006118 *)
mov r6 L0x2001a0e0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a0e4; Value = 0x01ce3259; PC = 0x800611c *)
mov r7 L0x2001a0e4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a0d8; Value = 0xffe5420f; PC = 0x8006120 *)
mov r4 L0x2001a0d8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019cdc; Value = 0x003a94b5; PC = 0x8006124 *)
mov r9 L0x20019cdc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019ce0; Value = 0xff628026; PC = 0x8006128 *)
mov r10 L0x20019ce0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019ce4; Value = 0xff322ba2; PC = 0x800612c *)
mov r11 L0x20019ce4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019cd8; Value = 0xfedbad97; PC = 0x8006130 *)
mov r8 L0x20019cd8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a4e4; PC = 0x8006158 *)
mov L0x2001a4e4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a4e0; PC = 0x8006184 *)
mov L0x2001a4e0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a4dc; PC = 0x80061b0 *)
mov L0x2001a4dc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a4d8; PC = 0x80061dc *)
mov L0x2001a4d8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a0ec; Value = 0xfe87a04b; PC = 0x8006038 *)
mov r5 L0x2001a0ec;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a0f0; Value = 0x00202713; PC = 0x800603c *)
mov r6 L0x2001a0f0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a0f4; Value = 0x00ac76e1; PC = 0x8006040 *)
mov r7 L0x2001a0f4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a0e8; Value = 0xfdc11724; PC = 0x8006044 *)
mov r4 L0x2001a0e8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019cec; Value = 0xff6b2ca8; PC = 0x8006048 *)
mov r9 L0x20019cec;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019cf0; Value = 0xffa27bc6; PC = 0x800604c *)
mov r10 L0x20019cf0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019cf4; Value = 0x00fb6f62; PC = 0x8006050 *)
mov r11 L0x20019cf4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019ce8; Value = 0x00918fe0; PC = 0x8006054 *)
mov r8 L0x20019ce8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f774; Value = 0x005cb22d; PC = 0x8006060 *)
mov r1 L0x800f774;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a4f4; PC = 0x8006084 *)
mov L0x2001a4f4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a4f0; PC = 0x80060b0 *)
mov L0x2001a4f0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a4ec; PC = 0x80060dc *)
mov L0x2001a4ec lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a4e8; PC = 0x8006108 *)
mov L0x2001a4e8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a0fc; Value = 0xff2be29b; PC = 0x8006114 *)
mov r5 L0x2001a0fc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a100; Value = 0x00d195eb; PC = 0x8006118 *)
mov r6 L0x2001a100;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a104; Value = 0x00005843; PC = 0x800611c *)
mov r7 L0x2001a104;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a0f8; Value = 0xff090f0c; PC = 0x8006120 *)
mov r4 L0x2001a0f8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019cfc; Value = 0xff0b734a; PC = 0x8006124 *)
mov r9 L0x20019cfc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019d00; Value = 0xfeb534ae; PC = 0x8006128 *)
mov r10 L0x20019d00;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019d04; Value = 0x00a4157a; PC = 0x800612c *)
mov r11 L0x20019d04;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019cf8; Value = 0x00ead25c; PC = 0x8006130 *)
mov r8 L0x20019cf8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a504; PC = 0x8006158 *)
mov L0x2001a504 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a500; PC = 0x8006184 *)
mov L0x2001a500 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a4fc; PC = 0x80061b0 *)
mov L0x2001a4fc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a4f8; PC = 0x80061dc *)
mov L0x2001a4f8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a10c; Value = 0xff2f3b7b; PC = 0x8006038 *)
mov r5 L0x2001a10c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a110; Value = 0x00f607f5; PC = 0x800603c *)
mov r6 L0x2001a110;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a114; Value = 0x00a0c4f0; PC = 0x8006040 *)
mov r7 L0x2001a114;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a108; Value = 0xff459f25; PC = 0x8006044 *)
mov r4 L0x2001a108;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019d0c; Value = 0x00351b22; PC = 0x8006048 *)
mov r9 L0x20019d0c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019d10; Value = 0xff113e0a; PC = 0x800604c *)
mov r10 L0x20019d10;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019d14; Value = 0x00bbb73c; PC = 0x8006050 *)
mov r11 L0x20019d14;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019d08; Value = 0x00c7874b; PC = 0x8006054 *)
mov r8 L0x20019d08;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f778; Value = 0x00b8f353; PC = 0x8006060 *)
mov r1 L0x800f778;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a514; PC = 0x8006084 *)
mov L0x2001a514 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a510; PC = 0x80060b0 *)
mov L0x2001a510 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a50c; PC = 0x80060dc *)
mov L0x2001a50c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a508; PC = 0x8006108 *)
mov L0x2001a508 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a11c; Value = 0x0039e865; PC = 0x8006114 *)
mov r5 L0x2001a11c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a120; Value = 0xff94b4c1; PC = 0x8006118 *)
mov r6 L0x2001a120;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a124; Value = 0xff36d290; PC = 0x800611c *)
mov r7 L0x2001a124;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a118; Value = 0xfe63ea85; PC = 0x8006120 *)
mov r4 L0x2001a118;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019d1c; Value = 0x0170fd0e; PC = 0x8006124 *)
mov r9 L0x20019d1c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019d20; Value = 0x003c93a6; PC = 0x8006128 *)
mov r10 L0x20019d20;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019d24; Value = 0x0122266e; PC = 0x800612c *)
mov r11 L0x20019d24;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019d18; Value = 0x0159db97; PC = 0x8006130 *)
mov r8 L0x20019d18;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a524; PC = 0x8006158 *)
mov L0x2001a524 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a520; PC = 0x8006184 *)
mov L0x2001a520 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a51c; PC = 0x80061b0 *)
mov L0x2001a51c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a518; PC = 0x80061dc *)
mov L0x2001a518 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a12c; Value = 0xfe67cacd; PC = 0x8006038 *)
mov r5 L0x2001a12c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a130; Value = 0xfed97c0e; PC = 0x800603c *)
mov r6 L0x2001a130;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a134; Value = 0x000c4929; PC = 0x8006040 *)
mov r7 L0x2001a134;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a128; Value = 0xfe4bae86; PC = 0x8006044 *)
mov r4 L0x2001a128;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019d2c; Value = 0x00e8fab1; PC = 0x8006048 *)
mov r9 L0x20019d2c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019d30; Value = 0xff04701d; PC = 0x800604c *)
mov r10 L0x20019d30;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019d34; Value = 0x010264ae; PC = 0x8006050 *)
mov r11 L0x20019d34;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019d28; Value = 0x0004793e; PC = 0x8006054 *)
mov r8 L0x20019d28;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f77c; Value = 0x007647a5; PC = 0x8006060 *)
mov r1 L0x800f77c;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a534; PC = 0x8006084 *)
mov L0x2001a534 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a530; PC = 0x80060b0 *)
mov L0x2001a530 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a52c; PC = 0x80060dc *)
mov L0x2001a52c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a528; PC = 0x8006108 *)
mov L0x2001a528 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a13c; Value = 0xff53e9ef; PC = 0x8006114 *)
mov r5 L0x2001a13c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a140; Value = 0xffd8b5c8; PC = 0x8006118 *)
mov r6 L0x2001a140;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a144; Value = 0x006bea5f; PC = 0x800611c *)
mov r7 L0x2001a144;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a138; Value = 0xff3c7468; PC = 0x8006120 *)
mov r4 L0x2001a138;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019d3c; Value = 0x0101f3ef; PC = 0x8006124 *)
mov r9 L0x20019d3c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019d40; Value = 0xff181ec3; PC = 0x8006128 *)
mov r10 L0x20019d40;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019d44; Value = 0x01eb4e2c; PC = 0x800612c *)
mov r11 L0x20019d44;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019d38; Value = 0xffb04e8c; PC = 0x8006130 *)
mov r8 L0x20019d38;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a544; PC = 0x8006158 *)
mov L0x2001a544 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a540; PC = 0x8006184 *)
mov L0x2001a540 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a53c; PC = 0x80061b0 *)
mov L0x2001a53c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a538; PC = 0x80061dc *)
mov L0x2001a538 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a14c; Value = 0xfea9db0d; PC = 0x8006038 *)
mov r5 L0x2001a14c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a150; Value = 0x02432a2e; PC = 0x800603c *)
mov r6 L0x2001a150;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a154; Value = 0x01fd7425; PC = 0x8006040 *)
mov r7 L0x2001a154;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a148; Value = 0xfee92d90; PC = 0x8006044 *)
mov r4 L0x2001a148;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019d4c; Value = 0x001e1ab5; PC = 0x8006048 *)
mov r9 L0x20019d4c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019d50; Value = 0xfed7ef1e; PC = 0x800604c *)
mov r10 L0x20019d50;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019d54; Value = 0x0372cd9e; PC = 0x8006050 *)
mov r11 L0x20019d54;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019d48; Value = 0x00b68562; PC = 0x8006054 *)
mov r8 L0x20019d48;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f780; Value = 0xffedbedf; PC = 0x8006060 *)
mov r1 L0x800f780;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a554; PC = 0x8006084 *)
mov L0x2001a554 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a550; PC = 0x80060b0 *)
mov L0x2001a550 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a54c; PC = 0x80060dc *)
mov L0x2001a54c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a548; PC = 0x8006108 *)
mov L0x2001a548 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a15c; Value = 0xffcbee81; PC = 0x8006114 *)
mov r5 L0x2001a15c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a160; Value = 0x00fc00a0; PC = 0x8006118 *)
mov r6 L0x2001a160;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a164; Value = 0x02591427; PC = 0x800611c *)
mov r7 L0x2001a164;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a158; Value = 0xfefe8fa2; PC = 0x8006120 *)
mov r4 L0x2001a158;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019d5c; Value = 0xff88d7ef; PC = 0x8006124 *)
mov r9 L0x20019d5c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019d60; Value = 0xfd6cd028; PC = 0x8006128 *)
mov r10 L0x20019d60;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019d64; Value = 0x01f4e0f4; PC = 0x800612c *)
mov r11 L0x20019d64;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019d58; Value = 0x00262f0c; PC = 0x8006130 *)
mov r8 L0x20019d58;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a564; PC = 0x8006158 *)
mov L0x2001a564 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a560; PC = 0x8006184 *)
mov L0x2001a560 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a55c; PC = 0x80061b0 *)
mov L0x2001a55c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a558; PC = 0x80061dc *)
mov L0x2001a558 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a16c; Value = 0xff04ae4a; PC = 0x8006038 *)
mov r5 L0x2001a16c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a170; Value = 0x024a7f37; PC = 0x800603c *)
mov r6 L0x2001a170;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a174; Value = 0x0168a603; PC = 0x8006040 *)
mov r7 L0x2001a174;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a168; Value = 0xfee0ad0e; PC = 0x8006044 *)
mov r4 L0x2001a168;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019d6c; Value = 0x003b4ece; PC = 0x8006048 *)
mov r9 L0x20019d6c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019d70; Value = 0x003f17ac; PC = 0x800604c *)
mov r10 L0x20019d70;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019d74; Value = 0x0144c349; PC = 0x8006050 *)
mov r11 L0x20019d74;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019d68; Value = 0xfe857b52; PC = 0x8006054 *)
mov r8 L0x20019d68;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f784; Value = 0x0081752f; PC = 0x8006060 *)
mov r1 L0x800f784;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a574; PC = 0x8006084 *)
mov L0x2001a574 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a570; PC = 0x80060b0 *)
mov L0x2001a570 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a56c; PC = 0x80060dc *)
mov L0x2001a56c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a568; PC = 0x8006108 *)
mov L0x2001a568 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a17c; Value = 0xfe0ae23c; PC = 0x8006114 *)
mov r5 L0x2001a17c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a180; Value = 0x0237f94f; PC = 0x8006118 *)
mov r6 L0x2001a180;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a184; Value = 0xfff2b165; PC = 0x800611c *)
mov r7 L0x2001a184;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a178; Value = 0xfdbce174; PC = 0x8006120 *)
mov r4 L0x2001a178;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019d7c; Value = 0xff6ff736; PC = 0x8006124 *)
mov r9 L0x20019d7c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019d80; Value = 0xfee6d136; PC = 0x8006128 *)
mov r10 L0x20019d80;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019d84; Value = 0x0137d2fd; PC = 0x800612c *)
mov r11 L0x20019d84;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019d78; Value = 0xff64df14; PC = 0x8006130 *)
mov r8 L0x20019d78;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a584; PC = 0x8006158 *)
mov L0x2001a584 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a580; PC = 0x8006184 *)
mov L0x2001a580 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a57c; PC = 0x80061b0 *)
mov L0x2001a57c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a578; PC = 0x80061dc *)
mov L0x2001a578 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a18c; Value = 0x00561330; PC = 0x8006038 *)
mov r5 L0x2001a18c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a190; Value = 0x00f283d7; PC = 0x800603c *)
mov r6 L0x2001a190;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a194; Value = 0x00880333; PC = 0x8006040 *)
mov r7 L0x2001a194;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a188; Value = 0xff82272b; PC = 0x8006044 *)
mov r4 L0x2001a188;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019d8c; Value = 0x020e8768; PC = 0x8006048 *)
mov r9 L0x20019d8c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019d90; Value = 0xff4a74c8; PC = 0x800604c *)
mov r10 L0x20019d90;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019d94; Value = 0x013f6b1a; PC = 0x8006050 *)
mov r11 L0x20019d94;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019d88; Value = 0x0012c4c3; PC = 0x8006054 *)
mov r8 L0x20019d88;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f788; Value = 0x00b21490; PC = 0x8006060 *)
mov r1 L0x800f788;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a594; PC = 0x8006084 *)
mov L0x2001a594 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a590; PC = 0x80060b0 *)
mov L0x2001a590 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a58c; PC = 0x80060dc *)
mov L0x2001a58c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a588; PC = 0x8006108 *)
mov L0x2001a588 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a19c; Value = 0x002dc770; PC = 0x8006114 *)
mov r5 L0x2001a19c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a1a0; Value = 0x006541c5; PC = 0x8006118 *)
mov r6 L0x2001a1a0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a1a4; Value = 0x010c8ca5; PC = 0x800611c *)
mov r7 L0x2001a1a4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a198; Value = 0x00c04e19; PC = 0x8006120 *)
mov r4 L0x2001a198;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019d9c; Value = 0x01e4eafe; PC = 0x8006124 *)
mov r9 L0x20019d9c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019da0; Value = 0x0000f258; PC = 0x8006128 *)
mov r10 L0x20019da0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019da4; Value = 0x012dd00e; PC = 0x800612c *)
mov r11 L0x20019da4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019d98; Value = 0x00d5b51f; PC = 0x8006130 *)
mov r8 L0x20019d98;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a5a4; PC = 0x8006158 *)
mov L0x2001a5a4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a5a0; PC = 0x8006184 *)
mov L0x2001a5a0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a59c; PC = 0x80061b0 *)
mov L0x2001a59c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a598; PC = 0x80061dc *)
mov L0x2001a598 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a1ac; Value = 0xffd53734; PC = 0x8006038 *)
mov r5 L0x2001a1ac;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a1b0; Value = 0x01e4c3a0; PC = 0x800603c *)
mov r6 L0x2001a1b0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a1b4; Value = 0x000cc946; PC = 0x8006040 *)
mov r7 L0x2001a1b4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a1a8; Value = 0x004fde8d; PC = 0x8006044 *)
mov r4 L0x2001a1a8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019dac; Value = 0x00a964c3; PC = 0x8006048 *)
mov r9 L0x20019dac;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019db0; Value = 0x00fdff0c; PC = 0x800604c *)
mov r10 L0x20019db0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019db4; Value = 0x01193e8a; PC = 0x8006050 *)
mov r11 L0x20019db4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019da8; Value = 0x02391d66; PC = 0x8006054 *)
mov r8 L0x20019da8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f78c; Value = 0xffd1986d; PC = 0x8006060 *)
mov r1 L0x800f78c;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a5b4; PC = 0x8006084 *)
mov L0x2001a5b4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a5b0; PC = 0x80060b0 *)
mov L0x2001a5b0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a5ac; PC = 0x80060dc *)
mov L0x2001a5ac lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a5a8; PC = 0x8006108 *)
mov L0x2001a5a8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a1bc; Value = 0xfe769208; PC = 0x8006114 *)
mov r5 L0x2001a1bc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a1c0; Value = 0x018cfa28; PC = 0x8006118 *)
mov r6 L0x2001a1c0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a1c4; Value = 0x0137a07e; PC = 0x800611c *)
mov r7 L0x2001a1c4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a1b8; Value = 0xffec5cfb; PC = 0x8006120 *)
mov r4 L0x2001a1b8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019dbc; Value = 0x00488d87; PC = 0x8006124 *)
mov r9 L0x20019dbc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019dc0; Value = 0x0019dcc4; PC = 0x8006128 *)
mov r10 L0x20019dc0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019dc4; Value = 0xffae3b1e; PC = 0x800612c *)
mov r11 L0x20019dc4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019db8; Value = 0x0174a474; PC = 0x8006130 *)
mov r8 L0x20019db8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a5c4; PC = 0x8006158 *)
mov L0x2001a5c4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a5c0; PC = 0x8006184 *)
mov L0x2001a5c0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a5bc; PC = 0x80061b0 *)
mov L0x2001a5bc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a5b8; PC = 0x80061dc *)
mov L0x2001a5b8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a1cc; Value = 0x00b63e19; PC = 0x8006038 *)
mov r5 L0x2001a1cc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a1d0; Value = 0x00edccee; PC = 0x800603c *)
mov r6 L0x2001a1d0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a1d4; Value = 0x019d8bbf; PC = 0x8006040 *)
mov r7 L0x2001a1d4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a1c8; Value = 0xff31cfe9; PC = 0x8006044 *)
mov r4 L0x2001a1c8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019dcc; Value = 0x00b064ac; PC = 0x8006048 *)
mov r9 L0x20019dcc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019dd0; Value = 0xffe11027; PC = 0x800604c *)
mov r10 L0x20019dd0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019dd4; Value = 0x010799b0; PC = 0x8006050 *)
mov r11 L0x20019dd4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019dc8; Value = 0x001efc6b; PC = 0x8006054 *)
mov r8 L0x20019dc8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f790; Value = 0x006681b9; PC = 0x8006060 *)
mov r1 L0x800f790;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a5d4; PC = 0x8006084 *)
mov L0x2001a5d4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a5d0; PC = 0x80060b0 *)
mov L0x2001a5d0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a5cc; PC = 0x80060dc *)
mov L0x2001a5cc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a5c8; PC = 0x8006108 *)
mov L0x2001a5c8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a1dc; Value = 0x01747597; PC = 0x8006114 *)
mov r5 L0x2001a1dc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a1e0; Value = 0xff8fa34c; PC = 0x8006118 *)
mov r6 L0x2001a1e0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a1e4; Value = 0x00c9d915; PC = 0x800611c *)
mov r7 L0x2001a1e4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a1d8; Value = 0xffa1b835; PC = 0x8006120 *)
mov r4 L0x2001a1d8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019ddc; Value = 0xff7c410c; PC = 0x8006124 *)
mov r9 L0x20019ddc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019de0; Value = 0xff33dd99; PC = 0x8006128 *)
mov r10 L0x20019de0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019de4; Value = 0xffcf6530; PC = 0x800612c *)
mov r11 L0x20019de4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019dd8; Value = 0xff8d41c1; PC = 0x8006130 *)
mov r8 L0x20019dd8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a5e4; PC = 0x8006158 *)
mov L0x2001a5e4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a5e0; PC = 0x8006184 *)
mov L0x2001a5e0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a5dc; PC = 0x80061b0 *)
mov L0x2001a5dc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a5d8; PC = 0x80061dc *)
mov L0x2001a5d8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a1ec; Value = 0xff3bd1f0; PC = 0x8006038 *)
mov r5 L0x2001a1ec;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a1f0; Value = 0xfeabe42e; PC = 0x800603c *)
mov r6 L0x2001a1f0;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a1f4; Value = 0x017ed6ec; PC = 0x8006040 *)
mov r7 L0x2001a1f4;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a1e8; Value = 0x00758100; PC = 0x8006044 *)
mov r4 L0x2001a1e8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019dec; Value = 0xfe4f80dc; PC = 0x8006048 *)
mov r9 L0x20019dec;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019df0; Value = 0x0138afb1; PC = 0x800604c *)
mov r10 L0x20019df0;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019df4; Value = 0xff634a61; PC = 0x8006050 *)
mov r11 L0x20019df4;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019de8; Value = 0xffdee178; PC = 0x8006054 *)
mov r8 L0x20019de8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f794; Value = 0xffa250f8; PC = 0x8006060 *)
mov r1 L0x800f794;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a5f4; PC = 0x8006084 *)
mov L0x2001a5f4 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a5f0; PC = 0x80060b0 *)
mov L0x2001a5f0 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a5ec; PC = 0x80060dc *)
mov L0x2001a5ec lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a5e8; PC = 0x8006108 *)
mov L0x2001a5e8 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a1fc; Value = 0xffef02a0; PC = 0x8006114 *)
mov r5 L0x2001a1fc;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a200; Value = 0xffcbb390; PC = 0x8006118 *)
mov r6 L0x2001a200;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a204; Value = 0x019f353c; PC = 0x800611c *)
mov r7 L0x2001a204;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a1f8; Value = 0xff80a7e6; PC = 0x8006120 *)
mov r4 L0x2001a1f8;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019dfc; Value = 0xfee66de4; PC = 0x8006124 *)
mov r9 L0x20019dfc;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019e00; Value = 0x005f0073; PC = 0x8006128 *)
mov r10 L0x20019e00;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019e04; Value = 0xff362b03; PC = 0x800612c *)
mov r11 L0x20019e04;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019df8; Value = 0x005fa2fc; PC = 0x8006130 *)
mov r8 L0x20019df8;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a604; PC = 0x8006158 *)
mov L0x2001a604 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a600; PC = 0x8006184 *)
mov L0x2001a600 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a5fc; PC = 0x80061b0 *)
mov L0x2001a5fc lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a5f8; PC = 0x80061dc *)
mov L0x2001a5f8 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a20c; Value = 0x00f6e552; PC = 0x8006038 *)
mov r5 L0x2001a20c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a210; Value = 0x00aaf6e0; PC = 0x800603c *)
mov r6 L0x2001a210;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a214; Value = 0x0201aaaa; PC = 0x8006040 *)
mov r7 L0x2001a214;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a208; Value = 0x01256576; PC = 0x8006044 *)
mov r4 L0x2001a208;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019e0c; Value = 0xffe8e5ef; PC = 0x8006048 *)
mov r9 L0x20019e0c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019e10; Value = 0x01705d1c; PC = 0x800604c *)
mov r10 L0x20019e10;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019e14; Value = 0x009f500c; PC = 0x8006050 *)
mov r11 L0x20019e14;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019e08; Value = 0x012f33b6; PC = 0x8006054 *)
mov r8 L0x20019e08;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f798; Value = 0xff955de1; PC = 0x8006060 *)
mov r1 L0x800f798;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a614; PC = 0x8006084 *)
mov L0x2001a614 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a610; PC = 0x80060b0 *)
mov L0x2001a610 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a60c; PC = 0x80060dc *)
mov L0x2001a60c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a608; PC = 0x8006108 *)
mov L0x2001a608 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a21c; Value = 0x008f723c; PC = 0x8006114 *)
mov r5 L0x2001a21c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a220; Value = 0x009f01e8; PC = 0x8006118 *)
mov r6 L0x2001a220;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a224; Value = 0x00f9cc7e; PC = 0x800611c *)
mov r7 L0x2001a224;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a218; Value = 0x017925e0; PC = 0x8006120 *)
mov r4 L0x2001a218;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019e1c; Value = 0xff483435; PC = 0x8006124 *)
mov r9 L0x20019e1c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019e20; Value = 0x01ced3e4; PC = 0x8006128 *)
mov r10 L0x20019e20;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019e24; Value = 0x012cca80; PC = 0x800612c *)
mov r11 L0x20019e24;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019e18; Value = 0x002393c6; PC = 0x8006130 *)
mov r8 L0x20019e18;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a624; PC = 0x8006158 *)
mov L0x2001a624 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a620; PC = 0x8006184 *)
mov L0x2001a620 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a61c; PC = 0x80061b0 *)
mov L0x2001a61c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a618; PC = 0x80061dc *)
mov L0x2001a618 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006034 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a22c; Value = 0xffdeff3f; PC = 0x8006038 *)
mov r5 L0x2001a22c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a230; Value = 0x012f62f8; PC = 0x800603c *)
mov r6 L0x2001a230;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a234; Value = 0x00a66c29; PC = 0x8006040 *)
mov r7 L0x2001a234;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a228; Value = 0x0126262a; PC = 0x8006044 *)
mov r4 L0x2001a228;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019e2c; Value = 0x000b9e76; PC = 0x8006048 *)
mov r9 L0x20019e2c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019e30; Value = 0x00e4897e; PC = 0x800604c *)
mov r10 L0x20019e30;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019e34; Value = 0x0039234d; PC = 0x8006050 *)
mov r11 L0x20019e34;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019e28; Value = 0x00dd60bc; PC = 0x8006054 *)
mov r8 L0x20019e28;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006058 *)
mov s12 r12;
mov s13 lr;
(* vmov	r12, s1                                    #! PC = 0x800605c *)
mov r12 s1;
(* ldr.w	r1, [r12], #4                             #! EA = L0x800f79c; Value = 0xffe7e5be; PC = 0x8006060 *)
mov r1 L0x800f79c;
(* vmov	s1, r12                                    #! PC = 0x8006064 *)
mov s1 r12;
(* smull	r0, lr, r4, r11                           #! PC = 0x8006068 *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x800606c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006070 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006074 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006078 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800607c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006080 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a634; PC = 0x8006084 *)
mov L0x2001a634 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x8006088 *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x800608c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006090 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006094 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x8006098 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x800609c *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x80060a0 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060ac *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a630; PC = 0x80060b0 *)
mov L0x2001a630 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x80060b4 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x80060b8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060bc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060c0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060c4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x80060c8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80060cc *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80060d8 *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a62c; PC = 0x80060dc *)
mov L0x2001a62c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80060e0 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80060e4 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80060e8 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060ec *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80060f0 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80060f4 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80060f8 *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80060fc *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006100 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006104 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a628; PC = 0x8006108 *)
mov L0x2001a628 lr;
(* vmov	s0, r0                                     #! PC = 0x800610c *)
mov s0 r0;
(* vmov	r12, lr, s12, s13                          #! PC = 0x8006110 *)
mov r12 s12;
mov lr s13;
(* ldr.w	r5, [r12, #4]                             #! EA = L0x2001a23c; Value = 0x0049ef03; PC = 0x8006114 *)
mov r5 L0x2001a23c;
(* ldr.w	r6, [r12, #8]                             #! EA = L0x2001a240; Value = 0x00710d80; PC = 0x8006118 *)
mov r6 L0x2001a240;
(* ldr.w	r7, [r12, #12]                            #! EA = L0x2001a244; Value = 0xffbcb573; PC = 0x800611c *)
mov r7 L0x2001a244;
(* ldr.w	r4, [r12], #16                            #! EA = L0x2001a238; Value = 0xffe559cc; PC = 0x8006120 *)
mov r4 L0x2001a238;
(* ldr.w	r9, [lr, #4]                              #! EA = L0x20019e3c; Value = 0xff291556; PC = 0x8006124 *)
mov r9 L0x20019e3c;
(* ldr.w	r10, [lr, #8]                             #! EA = L0x20019e40; Value = 0x00ab20a6; PC = 0x8006128 *)
mov r10 L0x20019e40;
(* ldr.w	r11, [lr, #12]                            #! EA = L0x20019e44; Value = 0xfef04a6b; PC = 0x800612c *)
mov r11 L0x20019e44;
(* ldr.w	r8, [lr], #16                             #! EA = L0x20019e38; Value = 0x01624448; PC = 0x8006130 *)
mov r8 L0x20019e38;
(* vmov	s12, s13, r12, lr                          #! PC = 0x8006134 *)
mov s12 r12;
mov s13 lr;
(* rsb	r1, r1, #0                                  #! PC = 0x8006138 *)
sub r1 0@sint32 r1;
(* smull	r0, lr, r4, r11                           #! PC = 0x800613c *)
smull lr r0 r4 r11;
(* smlal	r0, lr, r5, r10                           #! PC = 0x8006140 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r9                            #! PC = 0x8006144 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r8                            #! PC = 0x8006148 *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x800614c *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006150 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006154 *)
mov r0 s0;
(* str.w	lr, [r0, #12]                             #! EA = L0x2001a644; PC = 0x8006158 *)
mov L0x2001a644 lr;
(* smull	r0, lr, r7, r11                           #! PC = 0x800615c *)
smull lr r0 r7 r11;
(* mul.w	r12, r0, r2                               #! PC = 0x8006160 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006164 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006168 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r10                           #! PC = 0x800616c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r9                            #! PC = 0x8006170 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r6, r8                            #! PC = 0x8006174 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006178 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x800617c *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x8006180 *)
mov r0 s0;
(* str.w	lr, [r0, #8]                              #! EA = L0x2001a640; PC = 0x8006184 *)
mov L0x2001a640 lr;
(* smull	r0, lr, r6, r11                           #! PC = 0x8006188 *)
smull lr r0 r6 r11;
(* smlal	r0, lr, r7, r10                           #! PC = 0x800618c *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x8006190 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x8006194 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x8006198 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r9                            #! PC = 0x800619c *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r5, r8                            #! PC = 0x80061a0 *)
cast r5@sint32 r5;
smull tmp_h tmp_l r5 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061a4 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061a8 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061ac *)
mov r0 s0;
(* str.w	lr, [r0, #4]                              #! EA = L0x2001a63c; PC = 0x80061b0 *)
mov L0x2001a63c lr;
(* smull	r0, lr, r5, r11                           #! PC = 0x80061b4 *)
smull lr r0 r5 r11;
(* smlal	r0, lr, r6, r10                           #! PC = 0x80061b8 *)
cast r6@sint32 r6;
smull tmp_h tmp_l r6 r10;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* smlal	r0, lr, r7, r9                            #! PC = 0x80061bc *)
cast r7@sint32 r7;
smull tmp_h tmp_l r7 r9;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061c0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061c4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* smull	r0, lr, lr, r1                            #! PC = 0x80061c8 *)
smull lr r0 lr r1;
(* smlal	r0, lr, r4, r8                            #! PC = 0x80061cc *)
cast r4@sint32 r4;
smull tmp_h tmp_l r4 r8;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
(* mul.w	r12, r0, r2                               #! PC = 0x80061d0 *)
cast r0_s@sint32 r0;
mull discard r12 r0_s r2;
(* smlal	r0, lr, r12, r3                           #! PC = 0x80061d4 *)
cast r12@sint32 r12;
smull tmp_h tmp_l r12 r3;
uadds carry r0 tmp_l r0;
sadc lr tmp_h lr carry;
assert eqmod r0 0 (2**32) && true;
assume eq r0 0 && true;
(* vmov	r0, s0                                     #! PC = 0x80061d8 *)
mov r0 s0;
(* str.w	lr, [r0], #16                             #! EA = L0x2001a638; PC = 0x80061dc *)
mov L0x2001a638 lr;
(* vmov	s0, r0                                     #! PC = 0x80061e0 *)
mov s0 r0;
(* vmov	r12, s2                                    #! PC = 0x80061e4 *)
mov r12 s2;
(* #bne.w	0x8006034 <_mul_32>                      #! PC = 0x80061ec *)
#bne.w	0x8006034 <_mul_32>                      #! 0x80061ec = 0x80061ec;


(* === post condition === *)
{
  (* algebraic *)
  and [
    (* output_poly_0 *)
    eqmod (inp_a_poly_0 * inp_a_poly_0) * (inp_b_poly_0 * inp_b_poly_0)
          (2**32) * (
            L0x2001a248 * (x**0) + L0x2001a24c * (x**1) + 
            L0x2001a250 * (x**2) + L0x2001a254 * (x**3)
          )
          [25570049, x**4 - 6103972],
    (* output_poly_1 *)
    eqmod (inp_a_poly_1 * inp_a_poly_1) * (inp_b_poly_1 * inp_b_poly_1)
          (2**32) * (
            L0x2001a258 * (x**0) + L0x2001a25c * (x**1) + 
            L0x2001a260 * (x**2) + L0x2001a264 * (x**3)
          )
          [25570049, x**4 - 19466077],
    (* output_poly_2 *)
    eqmod (inp_a_poly_2 * inp_a_poly_2) * (inp_b_poly_2 * inp_b_poly_2)
          (2**32) * (
            L0x2001a268 * (x**0) + L0x2001a26c * (x**1) + 
            L0x2001a270 * (x**2) + L0x2001a274 * (x**3)
          )
          [25570049, x**4 - 830930],
    (* output_poly_3 *)
    eqmod (inp_a_poly_3 * inp_a_poly_3) * (inp_b_poly_3 * inp_b_poly_3)
          (2**32) * (
            L0x2001a278 * (x**0) + L0x2001a27c * (x**1) + 
            L0x2001a280 * (x**2) + L0x2001a284 * (x**3)
          )
          [25570049, x**4 - 24739119],
    (* output_poly_4 *)
    eqmod (inp_a_poly_4 * inp_a_poly_4) * (inp_b_poly_4 * inp_b_poly_4)
          (2**32) * (
            L0x2001a288 * (x**0) + L0x2001a28c * (x**1) + 
            L0x2001a290 * (x**2) + L0x2001a294 * (x**3)
          )
          [25570049, x**4 - 678652],
    (* output_poly_5 *)
    eqmod (inp_a_poly_5 * inp_a_poly_5) * (inp_b_poly_5 * inp_b_poly_5)
          (2**32) * (
            L0x2001a298 * (x**0) + L0x2001a29c * (x**1) + 
            L0x2001a2a0 * (x**2) + L0x2001a2a4 * (x**3)
          )
          [25570049, x**4 - 24891397],
    (* output_poly_6 *)
    eqmod (inp_a_poly_6 * inp_a_poly_6) * (inp_b_poly_6 * inp_b_poly_6)
          (2**32) * (
            L0x2001a2a8 * (x**0) + L0x2001a2ac * (x**1) + 
            L0x2001a2b0 * (x**2) + L0x2001a2b4 * (x**3)
          )
          [25570049, x**4 - 9510986],
    (* output_poly_7 *)
    eqmod (inp_a_poly_7 * inp_a_poly_7) * (inp_b_poly_7 * inp_b_poly_7)
          (2**32) * (
            L0x2001a2b8 * (x**0) + L0x2001a2bc * (x**1) + 
            L0x2001a2c0 * (x**2) + L0x2001a2c4 * (x**3)
          )
          [25570049, x**4 - 16059063],
    (* output_poly_8 *)
    eqmod (inp_a_poly_8 * inp_a_poly_8) * (inp_b_poly_8 * inp_b_poly_8)
          (2**32) * (
            L0x2001a2c8 * (x**0) + L0x2001a2cc * (x**1) + 
            L0x2001a2d0 * (x**2) + L0x2001a2d4 * (x**3)
          )
          [25570049, x**4 - 24972806],
    (* output_poly_9 *)
    eqmod (inp_a_poly_9 * inp_a_poly_9) * (inp_b_poly_9 * inp_b_poly_9)
          (2**32) * (
            L0x2001a2d8 * (x**0) + L0x2001a2dc * (x**1) + 
            L0x2001a2e0 * (x**2) + L0x2001a2e4 * (x**3)
          )
          [25570049, x**4 - 597243],
    (* output_poly_10 *)
    eqmod (inp_a_poly_10 * inp_a_poly_10) * (inp_b_poly_10 * inp_b_poly_10)
          (2**32) * (
            L0x2001a2e8 * (x**0) + L0x2001a2ec * (x**1) + 
            L0x2001a2f0 * (x**2) + L0x2001a2f4 * (x**3)
          )
          [25570049, x**4 - 4410275],
    (* output_poly_11 *)
    eqmod (inp_a_poly_11 * inp_a_poly_11) * (inp_b_poly_11 * inp_b_poly_11)
          (2**32) * (
            L0x2001a2f8 * (x**0) + L0x2001a2fc * (x**1) + 
            L0x2001a300 * (x**2) + L0x2001a304 * (x**3)
          )
          [25570049, x**4 - 21159774],
    (* output_poly_12 *)
    eqmod (inp_a_poly_12 * inp_a_poly_12) * (inp_b_poly_12 * inp_b_poly_12)
          (2**32) * (
            L0x2001a308 * (x**0) + L0x2001a30c * (x**1) + 
            L0x2001a310 * (x**2) + L0x2001a314 * (x**3)
          )
          [25570049, x**4 - 2389590],
    (* output_poly_13 *)
    eqmod (inp_a_poly_13 * inp_a_poly_13) * (inp_b_poly_13 * inp_b_poly_13)
          (2**32) * (
            L0x2001a318 * (x**0) + L0x2001a31c * (x**1) + 
            L0x2001a320 * (x**2) + L0x2001a324 * (x**3)
          )
          [25570049, x**4 - 23180459],
    (* output_poly_14 *)
    eqmod (inp_a_poly_14 * inp_a_poly_14) * (inp_b_poly_14 * inp_b_poly_14)
          (2**32) * (
            L0x2001a328 * (x**0) + L0x2001a32c * (x**1) + 
            L0x2001a330 * (x**2) + L0x2001a334 * (x**3)
          )
          [25570049, x**4 - 4363503],
    (* output_poly_15 *)
    eqmod (inp_a_poly_15 * inp_a_poly_15) * (inp_b_poly_15 * inp_b_poly_15)
          (2**32) * (
            L0x2001a338 * (x**0) + L0x2001a33c * (x**1) + 
            L0x2001a340 * (x**2) + L0x2001a344 * (x**3)
          )
          [25570049, x**4 - 21206546],
    (* output_poly_16 *)
    eqmod (inp_a_poly_16 * inp_a_poly_16) * (inp_b_poly_16 * inp_b_poly_16)
          (2**32) * (
            L0x2001a348 * (x**0) + L0x2001a34c * (x**1) + 
            L0x2001a350 * (x**2) + L0x2001a354 * (x**3)
          )
          [25570049, x**4 - 21713494],
    (* output_poly_17 *)
    eqmod (inp_a_poly_17 * inp_a_poly_17) * (inp_b_poly_17 * inp_b_poly_17)
          (2**32) * (
            L0x2001a358 * (x**0) + L0x2001a35c * (x**1) + 
            L0x2001a360 * (x**2) + L0x2001a364 * (x**3)
          )
          [25570049, x**4 - 3856555],
    (* output_poly_18 *)
    eqmod (inp_a_poly_18 * inp_a_poly_18) * (inp_b_poly_18 * inp_b_poly_18)
          (2**32) * (
            L0x2001a368 * (x**0) + L0x2001a36c * (x**1) + 
            L0x2001a370 * (x**2) + L0x2001a374 * (x**3)
          )
          [25570049, x**4 - 23159929],
    (* output_poly_19 *)
    eqmod (inp_a_poly_19 * inp_a_poly_19) * (inp_b_poly_19 * inp_b_poly_19)
          (2**32) * (
            L0x2001a378 * (x**0) + L0x2001a37c * (x**1) + 
            L0x2001a380 * (x**2) + L0x2001a384 * (x**3)
          )
          [25570049, x**4 - 2410120],
    (* output_poly_20 *)
    eqmod (inp_a_poly_20 * inp_a_poly_20) * (inp_b_poly_20 * inp_b_poly_20)
          (2**32) * (
            L0x2001a388 * (x**0) + L0x2001a38c * (x**1) + 
            L0x2001a390 * (x**2) + L0x2001a394 * (x**3)
          )
          [25570049, x**4 - 23569609],
    (* output_poly_21 *)
    eqmod (inp_a_poly_21 * inp_a_poly_21) * (inp_b_poly_21 * inp_b_poly_21)
          (2**32) * (
            L0x2001a398 * (x**0) + L0x2001a39c * (x**1) + 
            L0x2001a3a0 * (x**2) + L0x2001a3a4 * (x**3)
          )
          [25570049, x**4 - 2000440],
    (* output_poly_22 *)
    eqmod (inp_a_poly_22 * inp_a_poly_22) * (inp_b_poly_22 * inp_b_poly_22)
          (2**32) * (
            L0x2001a3a8 * (x**0) + L0x2001a3ac * (x**1) + 
            L0x2001a3b0 * (x**2) + L0x2001a3b4 * (x**3)
          )
          [25570049, x**4 - 7617083],
    (* output_poly_23 *)
    eqmod (inp_a_poly_23 * inp_a_poly_23) * (inp_b_poly_23 * inp_b_poly_23)
          (2**32) * (
            L0x2001a3b8 * (x**0) + L0x2001a3bc * (x**1) + 
            L0x2001a3c0 * (x**2) + L0x2001a3c4 * (x**3)
          )
          [25570049, x**4 - 17952966],
    (* output_poly_24 *)
    eqmod (inp_a_poly_24 * inp_a_poly_24) * (inp_b_poly_24 * inp_b_poly_24)
          (2**32) * (
            L0x2001a3c8 * (x**0) + L0x2001a3cc * (x**1) + 
            L0x2001a3d0 * (x**2) + L0x2001a3d4 * (x**3)
          )
          [25570049, x**4 - 24859920],
    (* output_poly_25 *)
    eqmod (inp_a_poly_25 * inp_a_poly_25) * (inp_b_poly_25 * inp_b_poly_25)
          (2**32) * (
            L0x2001a3d8 * (x**0) + L0x2001a3dc * (x**1) + 
            L0x2001a3e0 * (x**2) + L0x2001a3e4 * (x**3)
          )
          [25570049, x**4 - 710129],
    (* output_poly_26 *)
    eqmod (inp_a_poly_26 * inp_a_poly_26) * (inp_b_poly_26 * inp_b_poly_26)
          (2**32) * (
            L0x2001a3e8 * (x**0) + L0x2001a3ec * (x**1) + 
            L0x2001a3f0 * (x**2) + L0x2001a3f4 * (x**3)
          )
          [25570049, x**4 - 4519508],
    (* output_poly_27 *)
    eqmod (inp_a_poly_27 * inp_a_poly_27) * (inp_b_poly_27 * inp_b_poly_27)
          (2**32) * (
            L0x2001a3f8 * (x**0) + L0x2001a3fc * (x**1) + 
            L0x2001a400 * (x**2) + L0x2001a404 * (x**3)
          )
          [25570049, x**4 - 21050541],
    (* output_poly_28 *)
    eqmod (inp_a_poly_28 * inp_a_poly_28) * (inp_b_poly_28 * inp_b_poly_28)
          (2**32) * (
            L0x2001a408 * (x**0) + L0x2001a40c * (x**1) + 
            L0x2001a410 * (x**2) + L0x2001a414 * (x**3)
          )
          [25570049, x**4 - 1712131],
    (* output_poly_29 *)
    eqmod (inp_a_poly_29 * inp_a_poly_29) * (inp_b_poly_29 * inp_b_poly_29)
          (2**32) * (
            L0x2001a418 * (x**0) + L0x2001a41c * (x**1) + 
            L0x2001a420 * (x**2) + L0x2001a424 * (x**3)
          )
          [25570049, x**4 - 23857918],
    (* output_poly_30 *)
    eqmod (inp_a_poly_30 * inp_a_poly_30) * (inp_b_poly_30 * inp_b_poly_30)
          (2**32) * (
            L0x2001a428 * (x**0) + L0x2001a42c * (x**1) + 
            L0x2001a430 * (x**2) + L0x2001a434 * (x**3)
          )
          [25570049, x**4 - 14036261],
    (* output_poly_31 *)
    eqmod (inp_a_poly_31 * inp_a_poly_31) * (inp_b_poly_31 * inp_b_poly_31)
          (2**32) * (
            L0x2001a438 * (x**0) + L0x2001a43c * (x**1) + 
            L0x2001a440 * (x**2) + L0x2001a444 * (x**3)
          )
          [25570049, x**4 - 11533788],
    (* output_poly_32 *)
    eqmod (inp_a_poly_32 * inp_a_poly_32) * (inp_b_poly_32 * inp_b_poly_32)
          (2**32) * (
            L0x2001a448 * (x**0) + L0x2001a44c * (x**1) + 
            L0x2001a450 * (x**2) + L0x2001a454 * (x**3)
          )
          [25570049, x**4 - 7847101],
    (* output_poly_33 *)
    eqmod (inp_a_poly_33 * inp_a_poly_33) * (inp_b_poly_33 * inp_b_poly_33)
          (2**32) * (
            L0x2001a458 * (x**0) + L0x2001a45c * (x**1) + 
            L0x2001a460 * (x**2) + L0x2001a464 * (x**3)
          )
          [25570049, x**4 - 17722948],
    (* output_poly_34 *)
    eqmod (inp_a_poly_34 * inp_a_poly_34) * (inp_b_poly_34 * inp_b_poly_34)
          (2**32) * (
            L0x2001a468 * (x**0) + L0x2001a46c * (x**1) + 
            L0x2001a470 * (x**2) + L0x2001a474 * (x**3)
          )
          [25570049, x**4 - 19240139],
    (* output_poly_35 *)
    eqmod (inp_a_poly_35 * inp_a_poly_35) * (inp_b_poly_35 * inp_b_poly_35)
          (2**32) * (
            L0x2001a478 * (x**0) + L0x2001a47c * (x**1) + 
            L0x2001a480 * (x**2) + L0x2001a484 * (x**3)
          )
          [25570049, x**4 - 6329910],
    (* output_poly_36 *)
    eqmod (inp_a_poly_36 * inp_a_poly_36) * (inp_b_poly_36 * inp_b_poly_36)
          (2**32) * (
            L0x2001a488 * (x**0) + L0x2001a48c * (x**1) + 
            L0x2001a490 * (x**2) + L0x2001a494 * (x**3)
          )
          [25570049, x**4 - 5192558],
    (* output_poly_37 *)
    eqmod (inp_a_poly_37 * inp_a_poly_37) * (inp_b_poly_37 * inp_b_poly_37)
          (2**32) * (
            L0x2001a498 * (x**0) + L0x2001a49c * (x**1) + 
            L0x2001a4a0 * (x**2) + L0x2001a4a4 * (x**3)
          )
          [25570049, x**4 - 20377491],
    (* output_poly_38 *)
    eqmod (inp_a_poly_38 * inp_a_poly_38) * (inp_b_poly_38 * inp_b_poly_38)
          (2**32) * (
            L0x2001a4a8 * (x**0) + L0x2001a4ac * (x**1) + 
            L0x2001a4b0 * (x**2) + L0x2001a4b4 * (x**3)
          )
          [25570049, x**4 - 25163199],
    (* output_poly_39 *)
    eqmod (inp_a_poly_39 * inp_a_poly_39) * (inp_b_poly_39 * inp_b_poly_39)
          (2**32) * (
            L0x2001a4b8 * (x**0) + L0x2001a4bc * (x**1) + 
            L0x2001a4c0 * (x**2) + L0x2001a4c4 * (x**3)
          )
          [25570049, x**4 - 406850],
    (* output_poly_40 *)
    eqmod (inp_a_poly_40 * inp_a_poly_40) * (inp_b_poly_40 * inp_b_poly_40)
          (2**32) * (
            L0x2001a4c8 * (x**0) + L0x2001a4cc * (x**1) + 
            L0x2001a4d0 * (x**2) + L0x2001a4d4 * (x**3)
          )
          [25570049, x**4 - 19921963],
    (* output_poly_41 *)
    eqmod (inp_a_poly_41 * inp_a_poly_41) * (inp_b_poly_41 * inp_b_poly_41)
          (2**32) * (
            L0x2001a4d8 * (x**0) + L0x2001a4dc * (x**1) + 
            L0x2001a4e0 * (x**2) + L0x2001a4e4 * (x**3)
          )
          [25570049, x**4 - 5648086],
    (* output_poly_42 *)
    eqmod (inp_a_poly_42 * inp_a_poly_42) * (inp_b_poly_42 * inp_b_poly_42)
          (2**32) * (
            L0x2001a4e8 * (x**0) + L0x2001a4ec * (x**1) + 
            L0x2001a4f0 * (x**2) + L0x2001a4f4 * (x**3)
          )
          [25570049, x**4 - 6202837],
    (* output_poly_43 *)
    eqmod (inp_a_poly_43 * inp_a_poly_43) * (inp_b_poly_43 * inp_b_poly_43)
          (2**32) * (
            L0x2001a4f8 * (x**0) + L0x2001a4fc * (x**1) + 
            L0x2001a500 * (x**2) + L0x2001a504 * (x**3)
          )
          [25570049, x**4 - 19367212],
    (* output_poly_44 *)
    eqmod (inp_a_poly_44 * inp_a_poly_44) * (inp_b_poly_44 * inp_b_poly_44)
          (2**32) * (
            L0x2001a508 * (x**0) + L0x2001a50c * (x**1) + 
            L0x2001a510 * (x**2) + L0x2001a514 * (x**3)
          )
          [25570049, x**4 - 17091305],
    (* output_poly_45 *)
    eqmod (inp_a_poly_45 * inp_a_poly_45) * (inp_b_poly_45 * inp_b_poly_45)
          (2**32) * (
            L0x2001a518 * (x**0) + L0x2001a51c * (x**1) + 
            L0x2001a520 * (x**2) + L0x2001a524 * (x**3)
          )
          [25570049, x**4 - 8478744],
    (* output_poly_46 *)
    eqmod (inp_a_poly_46 * inp_a_poly_46) * (inp_b_poly_46 * inp_b_poly_46)
          (2**32) * (
            L0x2001a528 * (x**0) + L0x2001a52c * (x**1) + 
            L0x2001a530 * (x**2) + L0x2001a534 * (x**3)
          )
          [25570049, x**4 - 18298658],
    (* output_poly_47 *)
    eqmod (inp_a_poly_47 * inp_a_poly_47) * (inp_b_poly_47 * inp_b_poly_47)
          (2**32) * (
            L0x2001a538 * (x**0) + L0x2001a53c * (x**1) + 
            L0x2001a540 * (x**2) + L0x2001a544 * (x**3)
          )
          [25570049, x**4 - 7271391],
    (* output_poly_48 *)
    eqmod (inp_a_poly_48 * inp_a_poly_48) * (inp_b_poly_48 * inp_b_poly_48)
          (2**32) * (
            L0x2001a548 * (x**0) + L0x2001a54c * (x**1) + 
            L0x2001a550 * (x**2) + L0x2001a554 * (x**3)
          )
          [25570049, x**4 - 17500092],
    (* output_poly_49 *)
    eqmod (inp_a_poly_49 * inp_a_poly_49) * (inp_b_poly_49 * inp_b_poly_49)
          (2**32) * (
            L0x2001a558 * (x**0) + L0x2001a55c * (x**1) + 
            L0x2001a560 * (x**2) + L0x2001a564 * (x**3)
          )
          [25570049, x**4 - 8069957],
    (* output_poly_50 *)
    eqmod (inp_a_poly_50 * inp_a_poly_50) * (inp_b_poly_50 * inp_b_poly_50)
          (2**32) * (
            L0x2001a568 * (x**0) + L0x2001a56c * (x**1) + 
            L0x2001a570 * (x**2) + L0x2001a574 * (x**3)
          )
          [25570049, x**4 - 3627172],
    (* output_poly_51 *)
    eqmod (inp_a_poly_51 * inp_a_poly_51) * (inp_b_poly_51 * inp_b_poly_51)
          (2**32) * (
            L0x2001a578 * (x**0) + L0x2001a57c * (x**1) + 
            L0x2001a580 * (x**2) + L0x2001a584 * (x**3)
          )
          [25570049, x**4 - 21942877],
    (* output_poly_52 *)
    eqmod (inp_a_poly_52 * inp_a_poly_52) * (inp_b_poly_52 * inp_b_poly_52)
          (2**32) * (
            L0x2001a588 * (x**0) + L0x2001a58c * (x**1) + 
            L0x2001a590 * (x**2) + L0x2001a594 * (x**3)
          )
          [25570049, x**4 - 4002385],
    (* output_poly_53 *)
    eqmod (inp_a_poly_53 * inp_a_poly_53) * (inp_b_poly_53 * inp_b_poly_53)
          (2**32) * (
            L0x2001a598 * (x**0) + L0x2001a59c * (x**1) + 
            L0x2001a5a0 * (x**2) + L0x2001a5a4 * (x**3)
          )
          [25570049, x**4 - 21567664],
    (* output_poly_54 *)
    eqmod (inp_a_poly_54 * inp_a_poly_54) * (inp_b_poly_54 * inp_b_poly_54)
          (2**32) * (
            L0x2001a5a8 * (x**0) + L0x2001a5ac * (x**1) + 
            L0x2001a5b0 * (x**2) + L0x2001a5b4 * (x**3)
          )
          [25570049, x**4 - 9845387],
    (* output_poly_55 *)
    eqmod (inp_a_poly_55 * inp_a_poly_55) * (inp_b_poly_55 * inp_b_poly_55)
          (2**32) * (
            L0x2001a5b8 * (x**0) + L0x2001a5bc * (x**1) + 
            L0x2001a5c0 * (x**2) + L0x2001a5c4 * (x**3)
          )
          [25570049, x**4 - 15724662],
    (* output_poly_56 *)
    eqmod (inp_a_poly_56 * inp_a_poly_56) * (inp_b_poly_56 * inp_b_poly_56)
          (2**32) * (
            L0x2001a5c8 * (x**0) + L0x2001a5cc * (x**1) + 
            L0x2001a5d0 * (x**2) + L0x2001a5d4 * (x**3)
          )
          [25570049, x**4 - 6096206],
    (* output_poly_57 *)
    eqmod (inp_a_poly_57 * inp_a_poly_57) * (inp_b_poly_57 * inp_b_poly_57)
          (2**32) * (
            L0x2001a5d8 * (x**0) + L0x2001a5dc * (x**1) + 
            L0x2001a5e0 * (x**2) + L0x2001a5e4 * (x**3)
          )
          [25570049, x**4 - 19473843],
    (* output_poly_58 *)
    eqmod (inp_a_poly_58 * inp_a_poly_58) * (inp_b_poly_58 * inp_b_poly_58)
          (2**32) * (
            L0x2001a5e8 * (x**0) + L0x2001a5ec * (x**1) + 
            L0x2001a5f0 * (x**2) + L0x2001a5f4 * (x**3)
          )
          [25570049, x**4 - 8645865],
    (* output_poly_59 *)
    eqmod (inp_a_poly_59 * inp_a_poly_59) * (inp_b_poly_59 * inp_b_poly_59)
          (2**32) * (
            L0x2001a5f8 * (x**0) + L0x2001a5fc * (x**1) + 
            L0x2001a600 * (x**2) + L0x2001a604 * (x**3)
          )
          [25570049, x**4 - 16924184],
    (* output_poly_60 *)
    eqmod (inp_a_poly_60 * inp_a_poly_60) * (inp_b_poly_60 * inp_b_poly_60)
          (2**32) * (
            L0x2001a608 * (x**0) + L0x2001a60c * (x**1) + 
            L0x2001a610 * (x**2) + L0x2001a614 * (x**3)
          )
          [25570049, x**4 - 18864008],
    (* output_poly_61 *)
    eqmod (inp_a_poly_61 * inp_a_poly_61) * (inp_b_poly_61 * inp_b_poly_61)
          (2**32) * (
            L0x2001a618 * (x**0) + L0x2001a61c * (x**1) + 
            L0x2001a620 * (x**2) + L0x2001a624 * (x**3)
          )
          [25570049, x**4 - 6706041],
    (* output_poly_62 *)
    eqmod (inp_a_poly_62 * inp_a_poly_62) * (inp_b_poly_62 * inp_b_poly_62)
          (2**32) * (
            L0x2001a628 * (x**0) + L0x2001a62c * (x**1) + 
            L0x2001a630 * (x**2) + L0x2001a634 * (x**3)
          )
          [25570049, x**4 - 18761234],
    (* output_poly_63 *)
    eqmod (inp_a_poly_63 * inp_a_poly_63) * (inp_b_poly_63 * inp_b_poly_63)
          (2**32) * (
            L0x2001a638 * (x**0) + L0x2001a63c * (x**1) + 
            L0x2001a640 * (x**2) + L0x2001a644 * (x**3)
          )
          [25570049, x**4 - 6808815]
  ]
  &&
  (* range *)
  and [
    2@32 * (-25570049)@32 <=s L0x2001a248, L0x2001a248 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a24c, L0x2001a24c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a250, L0x2001a250 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a254, L0x2001a254 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a258, L0x2001a258 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a25c, L0x2001a25c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a260, L0x2001a260 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a264, L0x2001a264 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a268, L0x2001a268 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a26c, L0x2001a26c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a270, L0x2001a270 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a274, L0x2001a274 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a278, L0x2001a278 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a27c, L0x2001a27c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a280, L0x2001a280 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a284, L0x2001a284 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a288, L0x2001a288 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a28c, L0x2001a28c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a290, L0x2001a290 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a294, L0x2001a294 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a298, L0x2001a298 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a29c, L0x2001a29c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2a0, L0x2001a2a0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2a4, L0x2001a2a4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2a8, L0x2001a2a8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2ac, L0x2001a2ac <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2b0, L0x2001a2b0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2b4, L0x2001a2b4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2b8, L0x2001a2b8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2bc, L0x2001a2bc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2c0, L0x2001a2c0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2c4, L0x2001a2c4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2c8, L0x2001a2c8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2cc, L0x2001a2cc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2d0, L0x2001a2d0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2d4, L0x2001a2d4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2d8, L0x2001a2d8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2dc, L0x2001a2dc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2e0, L0x2001a2e0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2e4, L0x2001a2e4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2e8, L0x2001a2e8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2ec, L0x2001a2ec <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2f0, L0x2001a2f0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2f4, L0x2001a2f4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2f8, L0x2001a2f8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a2fc, L0x2001a2fc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a300, L0x2001a300 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a304, L0x2001a304 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a308, L0x2001a308 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a30c, L0x2001a30c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a310, L0x2001a310 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a314, L0x2001a314 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a318, L0x2001a318 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a31c, L0x2001a31c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a320, L0x2001a320 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a324, L0x2001a324 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a328, L0x2001a328 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a32c, L0x2001a32c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a330, L0x2001a330 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a334, L0x2001a334 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a338, L0x2001a338 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a33c, L0x2001a33c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a340, L0x2001a340 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a344, L0x2001a344 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a348, L0x2001a348 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a34c, L0x2001a34c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a350, L0x2001a350 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a354, L0x2001a354 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a358, L0x2001a358 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a35c, L0x2001a35c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a360, L0x2001a360 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a364, L0x2001a364 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a368, L0x2001a368 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a36c, L0x2001a36c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a370, L0x2001a370 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a374, L0x2001a374 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a378, L0x2001a378 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a37c, L0x2001a37c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a380, L0x2001a380 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a384, L0x2001a384 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a388, L0x2001a388 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a38c, L0x2001a38c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a390, L0x2001a390 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a394, L0x2001a394 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a398, L0x2001a398 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a39c, L0x2001a39c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3a0, L0x2001a3a0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3a4, L0x2001a3a4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3a8, L0x2001a3a8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3ac, L0x2001a3ac <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3b0, L0x2001a3b0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3b4, L0x2001a3b4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3b8, L0x2001a3b8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3bc, L0x2001a3bc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3c0, L0x2001a3c0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3c4, L0x2001a3c4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3c8, L0x2001a3c8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3cc, L0x2001a3cc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3d0, L0x2001a3d0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3d4, L0x2001a3d4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3d8, L0x2001a3d8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3dc, L0x2001a3dc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3e0, L0x2001a3e0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3e4, L0x2001a3e4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3e8, L0x2001a3e8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3ec, L0x2001a3ec <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3f0, L0x2001a3f0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3f4, L0x2001a3f4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3f8, L0x2001a3f8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a3fc, L0x2001a3fc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a400, L0x2001a400 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a404, L0x2001a404 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a408, L0x2001a408 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a40c, L0x2001a40c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a410, L0x2001a410 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a414, L0x2001a414 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a418, L0x2001a418 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a41c, L0x2001a41c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a420, L0x2001a420 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a424, L0x2001a424 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a428, L0x2001a428 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a42c, L0x2001a42c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a430, L0x2001a430 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a434, L0x2001a434 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a438, L0x2001a438 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a43c, L0x2001a43c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a440, L0x2001a440 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a444, L0x2001a444 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a448, L0x2001a448 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a44c, L0x2001a44c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a450, L0x2001a450 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a454, L0x2001a454 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a458, L0x2001a458 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a45c, L0x2001a45c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a460, L0x2001a460 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a464, L0x2001a464 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a468, L0x2001a468 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a46c, L0x2001a46c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a470, L0x2001a470 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a474, L0x2001a474 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a478, L0x2001a478 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a47c, L0x2001a47c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a480, L0x2001a480 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a484, L0x2001a484 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a488, L0x2001a488 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a48c, L0x2001a48c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a490, L0x2001a490 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a494, L0x2001a494 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a498, L0x2001a498 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a49c, L0x2001a49c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4a0, L0x2001a4a0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4a4, L0x2001a4a4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4a8, L0x2001a4a8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4ac, L0x2001a4ac <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4b0, L0x2001a4b0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4b4, L0x2001a4b4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4b8, L0x2001a4b8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4bc, L0x2001a4bc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4c0, L0x2001a4c0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4c4, L0x2001a4c4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4c8, L0x2001a4c8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4cc, L0x2001a4cc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4d0, L0x2001a4d0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4d4, L0x2001a4d4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4d8, L0x2001a4d8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4dc, L0x2001a4dc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4e0, L0x2001a4e0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4e4, L0x2001a4e4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4e8, L0x2001a4e8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4ec, L0x2001a4ec <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4f0, L0x2001a4f0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4f4, L0x2001a4f4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4f8, L0x2001a4f8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a4fc, L0x2001a4fc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a500, L0x2001a500 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a504, L0x2001a504 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a508, L0x2001a508 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a50c, L0x2001a50c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a510, L0x2001a510 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a514, L0x2001a514 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a518, L0x2001a518 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a51c, L0x2001a51c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a520, L0x2001a520 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a524, L0x2001a524 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a528, L0x2001a528 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a52c, L0x2001a52c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a530, L0x2001a530 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a534, L0x2001a534 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a538, L0x2001a538 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a53c, L0x2001a53c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a540, L0x2001a540 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a544, L0x2001a544 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a548, L0x2001a548 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a54c, L0x2001a54c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a550, L0x2001a550 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a554, L0x2001a554 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a558, L0x2001a558 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a55c, L0x2001a55c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a560, L0x2001a560 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a564, L0x2001a564 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a568, L0x2001a568 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a56c, L0x2001a56c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a570, L0x2001a570 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a574, L0x2001a574 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a578, L0x2001a578 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a57c, L0x2001a57c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a580, L0x2001a580 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a584, L0x2001a584 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a588, L0x2001a588 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a58c, L0x2001a58c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a590, L0x2001a590 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a594, L0x2001a594 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a598, L0x2001a598 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a59c, L0x2001a59c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5a0, L0x2001a5a0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5a4, L0x2001a5a4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5a8, L0x2001a5a8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5ac, L0x2001a5ac <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5b0, L0x2001a5b0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5b4, L0x2001a5b4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5b8, L0x2001a5b8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5bc, L0x2001a5bc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5c0, L0x2001a5c0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5c4, L0x2001a5c4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5c8, L0x2001a5c8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5cc, L0x2001a5cc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5d0, L0x2001a5d0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5d4, L0x2001a5d4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5d8, L0x2001a5d8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5dc, L0x2001a5dc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5e0, L0x2001a5e0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5e4, L0x2001a5e4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5e8, L0x2001a5e8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5ec, L0x2001a5ec <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5f0, L0x2001a5f0 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5f4, L0x2001a5f4 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5f8, L0x2001a5f8 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a5fc, L0x2001a5fc <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a600, L0x2001a600 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a604, L0x2001a604 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a608, L0x2001a608 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a60c, L0x2001a60c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a610, L0x2001a610 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a614, L0x2001a614 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a618, L0x2001a618 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a61c, L0x2001a61c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a620, L0x2001a620 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a624, L0x2001a624 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a628, L0x2001a628 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a62c, L0x2001a62c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a630, L0x2001a630 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a634, L0x2001a634 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a638, L0x2001a638 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a63c, L0x2001a63c <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a640, L0x2001a640 <s 2@32 * 25570049@32,
    2@32 * (-25570049)@32 <=s L0x2001a644, L0x2001a644 <s 2@32 * 25570049@32
  ]
}
